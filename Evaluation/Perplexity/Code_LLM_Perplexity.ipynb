{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3umLO6bGMYab"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKvjT-HHMf7w"
      },
      "outputs": [],
      "source": [
        "!chmod +x setup.sh\n",
        "!./setup.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PulBsuePMpYN"
      },
      "outputs": [],
      "source": [
        "project_name = 'LLM-for-code-intelligence-Project'\n",
        "import os\n",
        "\n",
        "path= f'/content/drive/MyDrive/{project_name}'\n",
        "\n",
        "if not os.path.exists(path):\n",
        "  os.mkdir(path)\n",
        "\n",
        "os.chdir(path)\n",
        "\n",
        "repo_name = 'LLM-for-code-intelligence'\n",
        "repo_path = f'{path}/{repo_name}'\n",
        "url = f'https://github.com/ammarnasr/{repo_name}.git'\n",
        "\n",
        "\n",
        "\n",
        "if not os.path.exists(repo_path):\n",
        "    #clone the repo\n",
        "    print('Cloning the repo...')\n",
        "    !git clone $url\n",
        "else:\n",
        "    #pull the repo\n",
        "    print('Pulling the repo...')\n",
        "    !git -C $repo_name pull\n",
        "\n",
        "os.chdir(repo_path)\n",
        "\n",
        "print(f'Current Dir: {os.getcwd()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eiAogmt9M56j"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import pickle\n",
        "from tqdm.auto import tqdm\n",
        "from datasets import load_dataset\n",
        "from peft import PeftConfig, PeftModel\n",
        "from transformers import AutoModelForCausalLM\n",
        "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
        "os.chdir('/content/drive/MyDrive/LLM-for-code-intelligence-Project/LLM-for-code-intelligence/Evaluation/Perplexity')\n",
        "\n",
        "\n",
        "global DEVICE\n",
        "global MODEL\n",
        "global TOKENIZER\n",
        "global MAX_LENGTH\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def get_perplexity(text, stride):\n",
        "    encodings = TOKENIZER(text, return_tensors=\"pt\").to(DEVICE)\n",
        "    seq_len = encodings.input_ids.size(1)\n",
        "    nlls = []\n",
        "    prev_end_loc = 0\n",
        "    for begin_loc in range(0, seq_len, stride):\n",
        "        end_loc = min(begin_loc + MAX_LENGTH, seq_len)\n",
        "        trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
        "        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(DEVICE)\n",
        "        target_ids = input_ids.clone()\n",
        "        target_ids[:, :-trg_len] = -100\n",
        "        with torch.no_grad():\n",
        "            outputs = MODEL(input_ids, labels=target_ids)\n",
        "            neg_log_likelihood = outputs.loss\n",
        "        nlls.append(neg_log_likelihood)\n",
        "        prev_end_loc = end_loc\n",
        "        if end_loc == seq_len:\n",
        "            break\n",
        "    ppl = torch.exp(torch.stack(nlls).mean())\n",
        "    return ppl\n",
        "\n",
        "\n",
        "def save_github_code_eval_subset(language, license, size=1000, shuffle=True):\n",
        "    \"\"\"\n",
        "    Saves a subset of the github code dataset to be used for evaluation.\n",
        "    :param language: Programming language of the code\n",
        "    :param license: License of the code\n",
        "    :param size: Size of the subset\n",
        "    :param shuffle: Whether to shuffle the dataset before taking the subset\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    print(f\"Getting codepaarot/github-code dataset for {language} with {license} license and {size} samples from HuggingFace...\")\n",
        "    ds = load_dataset(\"codeparrot/github-code\", languages=[language], licenses=[license], streaming=True, split=\"train\")\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size=size)\n",
        "    ds = ds.take(size)\n",
        "    evaluation_dataset = []\n",
        "\n",
        "    for item in tqdm(ds, total=size, desc=\"Saving evaluation data Locally\"):\n",
        "        evaluation_dataset.append(item['code'])\n",
        "\n",
        "    eval_data_name = f\"./data/evaluation_data_{language}_{license}_{size}.pkl\"\n",
        "\n",
        "    with open(eval_data_name, 'wb') as f:\n",
        "        pickle.dump(evaluation_dataset, f)\n",
        "\n",
        "\n",
        "def load_saved_github_code_eval_subset(language, license, size=1000):\n",
        "    \"\"\"\n",
        "    Loads a subset of the github code dataset to be used for evaluation.\n",
        "    :param language: Programming language of the code\n",
        "    :param license: License of the code\n",
        "    :param size: Size of the subset\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    eval_data_name = f\"./data/evaluation_data_{language}_{license}_{size}.pkl\"\n",
        "    if not os.path.exists(eval_data_name):\n",
        "        print(f\"Saved evaluation data NOT found. Saving now...\")\n",
        "        save_github_code_eval_subset(language, license, size)\n",
        "    else:\n",
        "        print(f\"FOUND saved evaluation data\")\n",
        "    with open(eval_data_name, 'rb') as f:\n",
        "        evaluation_dataset = pickle.load(f)\n",
        "    return evaluation_dataset\n",
        "\n",
        "\n",
        "def save_results_dict(results_dict, language, license, stride, res_dir):\n",
        "    \"\"\"\n",
        "    Saves a results dictionary to be used for evaluation to a json file.\n",
        "    :param results_dict: Dictionary containing the results\n",
        "    :param language: Programming language of the code\n",
        "    :param license: License of the code\n",
        "    :param size: Size of the subset\n",
        "    :param res_dir: Directory to save the results\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    #check if directory name results exists\n",
        "    if not os.path.exists('results'):\n",
        "        os.makedirs('results')\n",
        "    #check if res_dir exists in results\n",
        "    final_res_dir = f\"./results/{res_dir}\"\n",
        "    if not os.path.exists(final_res_dir):\n",
        "        os.makedirs(final_res_dir)\n",
        "    results_name = f\"{final_res_dir}/results_{language}_{license}_{stride}.json\"\n",
        "    results_name_short = f\"{final_res_dir}/results_{language}_{license}_{stride}_short.json\"\n",
        "\n",
        "    with open(results_name, 'w') as f:\n",
        "        json.dump(results_dict, f)\n",
        "    print(f\"Saved results to {results_name}\")\n",
        "\n",
        "    # Delete text to save space\n",
        "    del results_dict['text']\n",
        "    with open(results_name_short, 'w') as f:\n",
        "        json.dump(results_dict, f)\n",
        "    print(f\"Saved short results to {results_name_short}\")\n",
        "\n",
        "\n",
        "def main(languages, licenses, stride, size=10000, n_samples=None, res_dir='base'):\n",
        "    lang_tbar = tqdm(languages, total=len(languages), unit='Language', position=0, leave=True)\n",
        "    for language in lang_tbar:\n",
        "        lang_tbar.set_description(f'Current language: {language}')\n",
        "        license_tbar = tqdm(licenses, total=len(licenses), unit='License', position=1, leave=True)\n",
        "        for license in license_tbar:\n",
        "            license_tbar.set_description(f'Current license: {license}')\n",
        "\n",
        "            print(f\"Loading Evaluation Data for {language} with {license} license and {size} samples...\")\n",
        "            eval_data = load_saved_github_code_eval_subset(language, license, size)\n",
        "\n",
        "\n",
        "            n_samples = len(eval_data) if n_samples is None else n_samples\n",
        "            ppl = 0\n",
        "            current_text_len = 0\n",
        "            results = {'text': [], 'perplexity': []}\n",
        "\n",
        "            tbar = tqdm(range(n_samples),total=n_samples, unit='Sample', position=0, leave=True, desc=f'Current perplexity: {ppl:.2f}| Current text length: {current_text_len}')\n",
        "            for i in tbar:\n",
        "                text = eval_data[i]\n",
        "                current_text_len = len(text)\n",
        "                ppl = get_perplexity(text, stride)\n",
        "                results['text'].append(text[100:])\n",
        "                results['perplexity'].append(ppl.item())\n",
        "                tbar.set_description(f'Current perplexity: {ppl:.2f}| Current text length: {current_text_len}')\n",
        "                tbar.refresh()\n",
        "\n",
        "            # Save results\n",
        "            results['avg_perplexity'] = sum(results['perplexity'])/len(results['perplexity'])\n",
        "            save_results_dict(results, language, license, stride, res_dir)\n",
        "            print(f\"Average perplexity: {results['avg_perplexity']:.2f}\")\n",
        "            print('='*50)\n",
        "\n",
        "\n",
        "def load_full_model(model_path, tokenizer_path):\n",
        "    model = AutoModelWithLMHead.from_pretrained(model_path)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "def load_lora_model(model_path):\n",
        "    config = PeftConfig.from_pretrained(model_path)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "    model =  AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, trust_remote_code=True, use_cache=False)\n",
        "    model = PeftModel.from_pretrained(model, model_path)\n",
        "    return model, tokenizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GWZVM9_FA5k"
      },
      "source": [
        "#### ONE Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kRSYSY4NEOq"
      },
      "outputs": [],
      "source": [
        "model_name = \"ammarnasr/codegne-finetuned-LoRa-the-stack-java-v3-checkpoint-800\"\n",
        "MODEL, TOKENIZER = load_lora_model(model_name)\n",
        "MODEL.to(DEVICE)\n",
        "MODEL.eval()\n",
        "MAX_LENGTH = MODEL.config.n_positions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2-pjcbLNg9i"
      },
      "outputs": [],
      "source": [
        "languages = ['Python', 'Java']\n",
        "licenses = ['mit', 'apache-2.0']\n",
        "stride = 1024\n",
        "n_samples = 10\n",
        "main(languages, licenses, stride, n_samples=n_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ALL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import pickle\n",
        "from tqdm.auto import tqdm\n",
        "from datasets import load_dataset\n",
        "from peft import PeftConfig, PeftModel\n",
        "from transformers import AutoModelForCausalLM\n",
        "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
        "\n",
        "\n",
        "global DEVICE\n",
        "global MODEL\n",
        "global TOKENIZER\n",
        "global MAX_LENGTH\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def get_perplexity(text, stride):\n",
        "    encodings = TOKENIZER(text, return_tensors=\"pt\").to(DEVICE)\n",
        "    seq_len = encodings.input_ids.size(1)\n",
        "    nlls = []\n",
        "    prev_end_loc = 0\n",
        "    for begin_loc in range(0, seq_len, stride):\n",
        "        end_loc = min(begin_loc + MAX_LENGTH, seq_len)\n",
        "        trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
        "        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(DEVICE)\n",
        "        target_ids = input_ids.clone()\n",
        "        target_ids[:, :-trg_len] = -100\n",
        "        with torch.no_grad():\n",
        "            outputs = MODEL(input_ids, labels=target_ids)\n",
        "            neg_log_likelihood = outputs.loss\n",
        "        nlls.append(neg_log_likelihood)\n",
        "        prev_end_loc = end_loc\n",
        "        if end_loc == seq_len:\n",
        "            break\n",
        "    ppl = torch.exp(torch.stack(nlls).mean())\n",
        "    return ppl\n",
        "\n",
        "\n",
        "def save_github_code_eval_subset(language, size=1000):\n",
        "    \"\"\"\n",
        "    Saves a subset of the github code dataset to be used for evaluation.\n",
        "    :param language: Programming language of the code\n",
        "    :param size: Size of the subset\n",
        "    :param shuffle: Whether to shuffle the dataset before taking the subset\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    language_lower = language.lower()\n",
        "    dataset_id = f'ammarnasr/the-stack-{language_lower}-clean'\n",
        "    ds = load_dataset(dataset_id, split=\"test\")\n",
        "    evaluation_dataset = []\n",
        "    for item in tqdm(ds, total=size, desc=\"Saving evaluation data Locally\"):\n",
        "        evaluation_dataset.append(item['content'])\n",
        "    eval_data_name = f\"./data/evaluation_data_{language}_{size}.pkl\"\n",
        "    save_dir = os.path.dirname(eval_data_name)\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "    with open(eval_data_name, 'wb') as f:\n",
        "        pickle.dump(evaluation_dataset, f)\n",
        "\n",
        "\n",
        "def load_saved_github_code_eval_subset(language, size=1000):\n",
        "    \"\"\"\n",
        "    Loads a subset of the github code dataset to be used for evaluation.\n",
        "    :param language: Programming language of the code\n",
        "    :param size: Size of the subset\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    eval_data_name = f\"./data/evaluation_data_{language}_{size}.pkl\"\n",
        "    if not os.path.exists(eval_data_name):\n",
        "        print(f\"Saved evaluation data NOT found. Saving now...\")\n",
        "        save_github_code_eval_subset(language, size)\n",
        "    else:\n",
        "        print(f\"FOUND saved evaluation data\")\n",
        "    with open(eval_data_name, 'rb') as f:\n",
        "        evaluation_dataset = pickle.load(f)\n",
        "    return evaluation_dataset\n",
        "\n",
        "\n",
        "def save_results_dict(results_dict, language, stride, res_dir):\n",
        "    \"\"\"\n",
        "    Saves a results dictionary to be used for evaluation to a json file.\n",
        "    :param results_dict: Dictionary containing the results\n",
        "    :param language: Programming language of the code\n",
        "    :param size: Size of the subset\n",
        "    :param res_dir: Directory to save the results\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    #check if directory name results exists\n",
        "    if not os.path.exists('results'):\n",
        "        os.makedirs('results')\n",
        "    #check if res_dir exists in results\n",
        "    final_res_dir = f\"./results/{res_dir}\"\n",
        "    if not os.path.exists(final_res_dir):\n",
        "        os.makedirs(final_res_dir)\n",
        "    results_name = f\"{final_res_dir}/results_{language}_{stride}.json\"\n",
        "    results_name_short = f\"{final_res_dir}/results_{language}_{stride}_short.json\"\n",
        "\n",
        "    with open(results_name, 'w') as f:\n",
        "        json.dump(results_dict, f)\n",
        "    print(f\"Saved results to {results_name}\")\n",
        "\n",
        "    # Delete text to save space\n",
        "    del results_dict['text']\n",
        "    with open(results_name_short, 'w') as f:\n",
        "        json.dump(results_dict, f)\n",
        "    print(f\"Saved short results to {results_name_short}\")\n",
        "\n",
        "\n",
        "def load_full_model(model_path, tokenizer_path):\n",
        "    model = AutoModelWithLMHead.from_pretrained(model_path)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "def load_lora_model(model_path):\n",
        "    config = PeftConfig.from_pretrained(model_path)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "    model =  AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, trust_remote_code=True, use_cache=False)\n",
        "    model = PeftModel.from_pretrained(model, model_path)\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "def main(languages, stride, size=10000, res_dir='base'):\n",
        "    lang_tbar = tqdm(languages, total=len(languages), unit='Language', position=0, leave=True)\n",
        "    for language in lang_tbar:\n",
        "        lang_tbar.set_description(f'Current language: {language}')\n",
        "        eval_data = load_saved_github_code_eval_subset(language, size)\n",
        "        ppl = 0\n",
        "        current_text_len = 0\n",
        "        results = {'text': [], 'perplexity': []}\n",
        "        tbar = tqdm(range(size),total=size, unit='Sample', position=0, leave=True, desc=f'Current perplexity: {ppl:.2f}| Current text length: {current_text_len}')\n",
        "        for i in tbar:\n",
        "            text = eval_data[i]\n",
        "            current_text_len = len(text)\n",
        "            ppl = get_perplexity(text, stride)\n",
        "            results['text'].append(text[100:])\n",
        "            results['perplexity'].append(ppl.item())\n",
        "            tbar.set_description(f'Current perplexity: {ppl:.2f}| Current text length: {current_text_len}')\n",
        "            tbar.refresh()\n",
        "        results['avg_perplexity'] = sum(results['perplexity'])/len(results['perplexity'])\n",
        "        save_results_dict(results, language, stride, res_dir)\n",
        "        print(f\"Average perplexity: {results['avg_perplexity']:.2f}\")\n",
        "        print('='*50)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBC4eY6Fa37n"
      },
      "outputs": [],
      "source": [
        "models_dict = {\n",
        "    'LoRa_Java': 'ammarnasr/codegen-350M-mono-java',\n",
        "    'Baseline-Mono': 'Salesforce/codegen-350M-mono',\n",
        "}\n",
        "languages = ['Java']\n",
        "stride = 1024\n",
        "size = 100\n",
        "for model_short_name, model_long_name in models_dict.items():\n",
        "    print(f\"Current Model: {model_short_name}\")\n",
        "\n",
        "    res_dir = f'{model_short_name}_results_{size}'\n",
        "    if not os.path.exists('results'):\n",
        "        os.makedirs('results')\n",
        "    final_res_dir = f\"./results/{res_dir}\"\n",
        "    if not os.path.exists(final_res_dir):\n",
        "        os.makedirs(final_res_dir)\n",
        "    results_files = os.listdir(final_res_dir)\n",
        "    if len(results_files) == 8:\n",
        "        print(f\"Found all 8 results files for {model_short_name}. Skipping...\")\n",
        "        continue\n",
        "    print(f\"Found {len(results_files)} results files for {model_short_name}. Running...\")\n",
        "\n",
        "\n",
        "    if 'LoRa' in model_short_name:\n",
        "        MODEL, TOKENIZER = load_lora_model(model_long_name)\n",
        "    else:\n",
        "        MODEL, TOKENIZER = load_full_model(model_path=model_long_name, tokenizer_path='Salesforce/codegen-350M-mono')\n",
        "    MODEL.to(DEVICE)\n",
        "    MODEL.eval()\n",
        "    MAX_LENGTH = MODEL.config.n_positions\n",
        "    main(languages, stride, size, res_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rq0neeFbGtmq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lV_n0_azG4PS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "2GWZVM9_FA5k",
        "VcFVgyO9FED1"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
