{"text": [".x database records to a JSON file which\ncan then be imported by Akamatsu 2.x.\n\nModify database connection accordingly.\n\"\"\"\n\nimport json\nimport sys\n\nimport records\n\n\nNEW_ROLES = ('administrator', 'editor', 'blogger')\n\nQUERY_USERS = 'SELECT * FROM users'\nQUERY_USER_ROLES = (\n    'SELECT ro.name FROM roles ro '\n    'JOIN user_roles ur ON ro.id=ur.role_id '\n    'WHERE ur.user_id=:user_id'\n)\nQUERY_PAGES = 'SELECT * FROM pages'\nQUERY_POSTS = (\n    'SELECT p.*, u.username FROM posts p '\n    'JOIN users u ON u.id=p.author_id'\n)\nQUERY_POST_TAGS = (\n    'SELECT t.name FROM tags t '\n    'JOIN post_tags pt ON pt.tag_id=t.id '\n    'WHERE pt.post_id=:post_id'\n)\nQUERY_UPLOADS = 'SELECT * FROM uploads'\n\n\n\ndef export(db, output='./backup_akamatsu.json'):\n    result = {\n        'users': [],\n        'pages': [],\n        'posts': [],\n        'uploads': []\n    }\n\n    out = open(output, 'w', encoding='utf-8')\n\n    # Backup users\n    for r in db.query(QUERY_USERS):\n        user = {\n            'username': r['username'],\n            'password': r['password'],\n            'reset_password_token': r['reset_password_token'],\n            'email': r['email'],\n            'is_active': r['is_enabled'],\n            'first_name': r['first_name'],\n            'last_name': r['last_name'],\n            'personal_bio': r['personal_bio'],\n            'notify_login': r['notify_login'],\n            'roles': []\n        }\n\n        # Roles\n        for role in db.query(QUERY_USER_ROLES, user_id=r['id']):\n            if role['name'] == 'admin':\n                user['roles'].append('administrator')\n\n            elif role['name'] in NEW_ROLES:\n                user['roles'].append(role['name'])\n\n\n        out.write(json.dumps({'entity': 'user', 'data': user})+'\\n')\n\n\n    # Backup pages\n    for r in db.query(QUERY_PAGES):\n        page = {\n            'title': r['title'],\n            'mini': r['mini'],\n            'route': r['route'],\n            'custom_head': r['custom_head'],\n            'content': r['content'],\n            'is_published': r['is_published'],\n            'comments_enabled': r['comments_enabled'],\n            'ghosted': None, # Should be route\n            'last_updated': r['timestamp'].strftime('%Y-%m-%d %H:%M:%S'),\n        }\n\n        # Shim\n        if r['ghost']:\n            page['content'] = '[GHOST: {}]\\n\\n{}'.format(\n                r['ghost'],\n                page['content']\n            )\n\n\n        out.write(json.dumps({'entity': 'page', 'data': page})+'\\n')\n\n    # Backup posts\n    for r in db.query(QUERY_POSTS):\n        post = {\n            'title': r['title'],\n            'slug': r['slug'],\n            'content': r['content'],\n            'is_published': r['is_published'],\n            'comments_enabled': r['comments_enabled'],\n            'last_updated': r['timestamp'].strftime('%Y-%m-%d %H:%M:%S'),\n            'authors': [r['username'],],\n            'ghosted': None, # Should be slug\n            'tags': []\n        }\n\n        # Shim\n        if r['ghost']:\n            post['content'] = '[GHOST: {}]\\n\\n{}'.format(\n                r['ghost'],\n                post['content']\n            )\n\n        # Tags\n        for tag in db.query(QUERY_POST_TAGS, post_id=r['id']):\n            post['tags'].append(tag['name'])\n\n\n        out.write(json.dumps({'entity': 'post', 'data': post})+'\\n')\n\n    # Backup uploads\n    for r in db.query(QUERY_UPLOADS):\n        upload = {\n            'path': r['path'],\n            'description': r['description'],\n            'mime': 'UNKOWN',\n            'uploaded_at': r['timestamp'].strftime('%Y-%m-%d %H:%M:%S')\n        }\n\n        out.write(json.dumps({'entity': 'upload', 'data': upload})+'\\n')\n\n\n    out.close()\n\n\ndef main():\n    if len(sys.argv) != 3:\n        print('Usage: export_old_db.py <DB_URI> <OUT>')\n        return\n\n    db = records.Database(sys.argv[1])\n    out_path = sys.argv[2]\n\n    export(db, out_path)\n\n\nif __name__ == '__main__':\n    main()\n", "_metaclass__ = type\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport matplotlib.ticker as mticker\nfrom j24.tools import shift_edge, last_delta\n\n\nDEFAULT_DISCRETE_CMAP = 'tab20'\n\n\ndef class_color(cid, cm=None, cmap=DEFAULT_DISCRETE_CMAP, mapping=None,\n                default=(1, 1, 1)):\n    \"\"\"Pick a color for cid using optional mapping.\"\"\"\n    if cid < 0:\n        return default\n    cm = cm or plt.get_cmap(cmap)\n    if mapping is not None:\n        if cid in mapping.index:\n            return cm.colors[mapping[cid]]\n        return default\n    return cm.colors[cid]\n\n\ndef class_colors(classes, ymin=-0.2, ymax=0, ax=None, alpha=1, mapping=None,\n                 cmap=DEFAULT_DISCRETE_CMAP, displacement_factor=0.5, **kws):\n    \"\"\"Plot time series of color coding under x axis.\"\"\"\n    if not isinstance(classes, pd.Series):\n        classes = pd.Series(data=classes, index=classes)\n    if isinstance(classes.index, pd.DatetimeIndex):\n        t = classes.index\n        dt = last_delta(t)*displacement_factor\n        clss = classes.shift(freq=dt).fillna(-1).astype(int)\n    else:\n        clss = classes\n        dt = displacement_factor\n        clss.index = clss.index+dt\n    ax = ax or plt.gca()\n    cm = plt.get_cmap(cmap)\n    t0 = clss.index[0]-2*dt\n    for t1, cid in clss.iteritems():\n        color = class_color(cid, cm, mapping=mapping)\n        ax.axvspan(t0, t1, ymin=ymin, ymax=ymax, facecolor=color,\n                   alpha=alpha, clip_on=False, **kws)\n        t0 = t1\n\n\ndef heatmap(t, ax=None, **kws):\n    \"\"\"Plot DataFrame as a heatmap.\"\"\"\n    if ax is None:\n        fig, ax = plt.subplots()\n    x = shift_edge(t.columns.values)\n    y = shift_edge(t.index.values)\n    #y = t.index.values\n    mesh = ax.pcolormesh(x, y, t, **kws)\n    fig = ax.get_figure()\n    fig.colorbar(mesh)\n    return fig, ax\n\n\ndef contour(t, ax=None, **kws):\n    \"\"\"Plot DataFrame contours.\"\"\"\n    ax = ax or plt.gca()\n    x = t.columns.values\n    y = t.index.values\n    ax.contour(x, y, t, **kws)\n    return ax\n\n\ndef scatter_kde(x, y, ax=None, s=50, **kws):\n    \"\"\"Scatter plot colored by kernel density\n    Source: https://stackoverflow.com/questions/20105364\n    \"\"\"\n    from scipy.stats import gaussian_kde\n    ax = ax or plt.gca()\n    xy = np.vstack([x, y])\n    # The first call creates a new gaussian_kde object\n    # second call evaluates the estimated pdf on the set of points\n    # (shortcut for calling the evaluate method)\n    z = gaussian_kde(xy)(xy)\n    # Sort by density, such that the densest points are plotted last\n    idx = z.argsort()\n    x, y, z = x[idx], y[idx], z[idx]\n    return ax.scatter(x, y, c=z, s=s, edgecolor='')\n\n\ndef fmt_axis_date(axis, locator=None, datefmt='%b'):\n    \"\"\"Format date axis (e.g. ax.xaxis) ticks.\"\"\"\n    date_formatter = mdates.DateFormatter(datefmt)\n    locator = locator or mdates.MonthLocator()\n    axis.set_major_locator(locator)\n    axis.set_major_formatter(date_formatter)\n\n\ndef fmt_axis_str(axis, locations=None, fmt='{x}'):\n    \"\"\"Format ticks\"\"\"\n    if locations is not None:\n        locator = mticker.FixedLocator(locations)\n        axis.set_major_locator(locator)\n    formatter = mticker.StrMethodFormatter(fmt)\n    axis.set_major_formatter(formatter)\n\n\ndef axis_equal_3d(ax, dims='xyz'):\n    extents = np.array([getattr(ax, 'get_{}lim'.format(dim))() for dim in dims])\n    sz = extents[:,1] - extents[:,0]\n    centers = np.mean(extents, axis=1)\n    maxsize = max(abs(sz))\n    r = maxsize/2\n    for ctr, dim in zip(centers, dims):\n        getattr(ax, 'set_{}lim'.format(dim))(ctr - r, ctr + r)\n\n", "llenge_mgr\nfrom apps.widgets.notifications.models import NoticeTemplate, UserNotification\nfrom apps.admin.admin import challenge_designer_site, challenge_manager_site, developer_site\n\n\nclass UserNotificationAdmin(admin.ModelAdmin):\n    \"\"\"raffle admin\"\"\"\n    list_display = ('recipient', 'content_object', 'content_type', 'unread', 'level')\n    search_fields = ('recipient__username', 'content_type__name')\n\n\nadmin.site.register(NoticeTemplate)\nchallenge_designer_site.register(NoticeTemplate)\nchallenge_manager_site.register(NoticeTemplate)\ndeveloper_site.register(NoticeTemplate)\nadmin.site.register(UserNotification, UserNotificationAdmin)\nchallenge_designer_site.register(UserNotification, UserNotificationAdmin)\nchallenge_manager_site.register(UserNotification, UserNotificationAdmin)\ndeveloper_site.register(UserNotification, UserNotificationAdmin)\nchallenge_mgr.register_designer_challenge_info_model(\"Other Settings\", 3, NoticeTemplate, 3)\nchallenge_mgr.register_admin_challenge_info_model(\"Notifications\", 3, UserNotification, 2)\nchallenge_mgr.register_developer_challenge_info_model(\"Status\", 4, NoticeTemplate, 5)\nchallenge_mgr.register_developer_challenge_info_model(\"Status\", 4, UserNotification, 5)\n", "ANGO_SETTINGS_MODULE\", \"djangoproj.settings\")\n\n    from django.core.management import execute_from_command_line\n\n    execute_from_command_line(sys.argv)\n", "ght=None):\n#         self.val = val\n#         self.left = left\n#         self.right = right\nfrom typing import List\nfrom collections import defaultdict\nfrom local_packages.binary_tree import TreeNode\n\n\nclass Solution:\n    def verticalTraversal(self, root: TreeNode) -> List[List[int]]:\n        ddict = defaultdict(list)\n\n        def helper(node: TreeNode, col: int, row: int) -> None:\n            if node is None:\n                return\n            ddict[col].append((row, node.val))\n            helper(node.left, col - 1, row + 1)\n            helper(node.right, col + 1, row + 1)\n\n        helper(root, 0, 0)\n        return [[v for _, v in sorted(ddict[col])] for col in sorted(ddict)]\n\n\n# TESTS\nfor t, expected in [\n    (\"#\", []),\n    (\"1,#,#\", [[1]]),\n    (\"1,2,#,#,3,#,#\", [[2], [1], [3]]),\n    (\"3,9,#,#,20,15,#,#,7,#,#\", [[9], [3, 15], [20], [7]]),\n    (\"1,2,4,#,#,5,#,#,3,6,#,#,7,#,#\", [[4], [2], [1, 5, 6], [3], [7]]),\n    (\"0,8,#,#,1,3,#,4,#,7,#,#,2,5,6,#,#,#,#\", [[8], [0, 3, 6], [1, 4, 5], [2, 7]],),\n]:\n    sol = Solution()\n    actual = sol.verticalTraversal(TreeNode.deserialize(t))\n    print(\"Vertical traversal of\", t, \"->\", actual)\n    assert actual == expected\n", "rom time import gmtime, strftime\n\nfrom spockbot.mcp import datautils, mcdata\nfrom spockbot.mcp.bbuff import BoundBuffer, BufferUnderflowException\nfrom spockbot.mcp.mcdata import MC_VARINT\nfrom spockbot.mcp.mcpacket_extensions import hashed_extensions\n\n\nlogger = logging.getLogger('spockbot')\n\n\nclass PacketDecodeFailure(Exception):\n    def __init__(self, packet, pbuff, underflow=False):\n        self.packet = packet\n        self.pbuff = pbuff\n        self.underflow = underflow\n\n\nclass Packet(object):\n    def __init__(self,\n                 ident=[mcdata.HANDSHAKE_STATE, mcdata.CLIENT_TO_SERVER, 0x00],\n                 data=None\n                 ):\n        if isinstance(ident, basestring):\n            ident = mcdata.packet_str2ident[ident]\n        self.__ident = list(ident)\n        # Quick hack to fake default ident\n        if len(self.__ident) == 2:\n            self.__ident.append(0x00)\n        self.ident = tuple(self.__ident)\n        self.str_ident = mcdata.packet_ident2str[self.ident]\n        self.data = data if data else {}\n\n    def clone(self):\n        return Packet(self.ident, copy.deepcopy(self.data))\n\n    def new_ident(self, ident):\n        self.__init__(ident, self.data)\n\n    def decode(self, bbuff, proto_comp_state):\n        self.data = {}\n        packet_length = datautils.unpack(MC_VARINT, bbuff)\n        packet_data = bbuff.recv(packet_length)\n        pbuff = BoundBuffer(packet_data)\n        if proto_comp_state == mcdata.PROTO_COMP_ON:\n            body_length = datautils.unpack(MC_VARINT, pbuff)\n            if body_length > 0:\n                body_data = zlib.decompress(pbuff.flush(), zlib.MAX_WBITS)\n                pbuff.write(body_data)\n                pbuff.save()\n\n        try:\n            # Ident\n            self.__ident[2] = datautils.unpack(MC_VARINT, pbuff)\n            self.ident = tuple(self.__ident)\n            self.str_ident = mcdata.packet_ident2str[self.ident]\n            # Payload\n            for dtype, name in mcdata.hashed_structs[self.ident]:\n                self.data[name] = datautils.unpack(dtype, pbuff)\n            # Extension\n            if self.ident in hashed_extensions:\n                hashed_extensions[self.ident].decode_extra(self, pbuff)\n            if len(pbuff) > 0:\n                raise PacketDecodeFailure(self, pbuff)\n        except BufferUnderflowException:\n            raise PacketDecodeFailure(self, pbuff, True)\n        return self\n\n    def encode(self, proto_comp_state, proto_comp_threshold, comp_level=6):\n        # Ident\n        o = datautils.pack(MC_VARINT, self.ident[2])\n        # Payload\n        for dtype, name in mcdata.hashed_structs[self.ident]:\n            o += datautils.pack(dtype, self.data[name])\n        # Extension\n        if self.ident in hashed_extensions:\n            o += hashed_extensions[self.ident].encode_extra(self)\n\n        if proto_comp_state == mcdata.PROTO_COMP_ON:\n            uncompressed_len = len(o)\n            if uncompressed_len < proto_comp_threshold:\n                header = datautils.pack(MC_VARINT, uncompressed_len + 1)\n                header += datautils.pack(MC_VARINT, 0)\n            else:\n                o = zlib.compress(o, comp_level)\n                ulen_varint = datautils.pack(MC_VARINT, uncompressed_len)\n                header = datautils.pack(MC_VARINT,\n                                        len(o) + len(ulen_varint))\n                header += ulen_varint\n            return header + o\n        elif proto_comp_state == mcdata.PROTO_COMP_OFF:\n            return datautils.pack(MC_VARINT, len(o)) + o\n        else:\n            return None\n\n    def __repr__(self):\n        s = ('<<<', '>>>')[self.ident[1]]\n        f = \"[%s] %s (0x%02X, 0x%02X): %-\" + str(\n            max([len(i) for i in mcdata.hashed_names.values()]) + 1) + \"s%s\"\n        return f % (\n            strftime(\"%H:%M:%S\", gmtime()), s, self.ident[0], self.ident[2],\n            mcdata.hashed_names[self.ident],\n            str(self.data)\n        )\n", "name         = 'project',\n    version      = '1.0',\n    packages     = find_packages(),\n    entry_points = {'scrapy': ['settings = cwemail.settings']},\n)\n", "_)\n\n\ndef is_http_mock_started():\n    return http_mock.is_started()\n\n\ndef start_http_mock():\n    if not http_mock.is_started():\n        http_mock.start()\n        logger.debug('http mock started')\n        return True\n\n\ndef stop_http_mock():\n    if http_mock.is_started():\n        http_mock.stop()\n        logger.debug('http mock stopped')\n        return True\n", "ode_literals\n\nfrom django.db import migrations\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('familias', '0035_auto_20170621_2211'),\n        ('familias', '0034_add_oficio_estudiante'),\n    ]\n\n    operations = [\n    ]\n", "ils.functional import cached_property\nfrom django.utils.translation import ugettext_lazy as _\n\nfrom reviewboard.attachments.models import FileAttachment\nfrom reviewboard.reviews.models.base_comment import BaseComment\n\n\nclass FileAttachmentComment(BaseComment):\n    \"\"\"A comment on a file attachment.\"\"\"\n    anchor_prefix = \"fcomment\"\n    comment_type = \"file\"\n\n    file_attachment = models.ForeignKey(\n        FileAttachment,\n        verbose_name=_('file attachment'),\n        related_name=\"comments\")\n    diff_against_file_attachment = models.ForeignKey(\n        FileAttachment,\n        verbose_name=_('diff against file attachment'),\n        related_name=\"diffed_against_comments\",\n        null=True,\n        blank=True)\n\n    @cached_property\n    def review_ui(self):\n        \"\"\"Return a ReviewUI appropriate for this comment.\n\n        If a ReviewUI is available for this type of file, an instance of\n        one will be returned that's associated with this comment's\n        FileAttachment and the one being diffed against (if any).\n        \"\"\"\n        from reviewboard.reviews.ui.base import FileAttachmentReviewUI\n\n        # Note that we need to create our own instance here, so that we don't\n        # end up altering the state of another ReviewUI's file attachment\n        # (particularly with calling set_diff_against below).\n        review_ui = FileAttachmentReviewUI.for_type(self.file_attachment)\n\n        if not review_ui:\n            return None\n\n        if review_ui.supports_diffing and self.diff_against_file_attachment:\n            review_ui.set_diff_against(self.diff_against_file_attachment)\n\n        return review_ui\n\n    @property\n    def thumbnail(self):\n        \"\"\"Returns the thumbnail for this comment, if any, as HTML.\n\n        The thumbnail will be generated from the appropriate ReviewUI,\n        if there is one for this type of file.\n        \"\"\"\n        review_ui = self.review_ui\n\n        if review_ui:\n            try:\n                return review_ui.get_comment_thumbnail(self)\n            except Exception as e:\n                logging.error('Error when calling get_comment_thumbnail for '\n                              'FileAttachmentReviewUI %r: %s',\n                              review_ui, e, exc_info=1)\n        else:\n            return ''\n\n    def get_absolute_url(self):\n        \"\"\"Returns the URL for this comment.\"\"\"\n        review_ui = self.review_ui\n\n        if review_ui:\n            try:\n                return review_ui.get_comment_link_url(self)\n            except Exception as e:\n                logging.error('Error when calling get_comment_thumbnail for '\n                              'FileAttachmentReviewUI %r: %s',\n                              review_ui, e, exc_info=1)\n        else:\n            return self.file_attachment.get_absolute_url()\n\n    def get_link_text(self):\n        \"\"\"Returns the text for the link to the file.\"\"\"\n        review_ui = self.review_ui\n\n        if review_ui:\n            try:\n                return review_ui.get_comment_link_text(self)\n            except Exception as e:\n                logging.error('Error when calling get_comment_link_text for '\n                              'FileAttachmentReviewUI %r: %s',\n                              review_ui, e, exc_info=1)\n        else:\n            return self.file_attachment.display_name\n", "o.contrib.sites.models import Site\n\nfrom django.template import Context, loader\n\n\nregister = template.Library()\nAnalytics = models.get_model('googleanalytics', 'analytics')\n\ndef do_get_analytics(parser, token):\n    contents = token.split_contents()\n    tag_name = contents[0]\n    template_name = 'google_analytics/%s_template.html' % tag_name\n    if len(contents) == 2:\n        # split_contents() knows not to split quoted strings.\n        code = contents[1]\n    elif len(contents) == 1:\n        code = None\n    else:\n        raise template.TemplateSyntaxError, \"%r cannot take more than one argument\" % tag_name\n   \n    if not code:\n        current_site = Site.objects.get_current()\n    else:\n        if not (code[0] == code[-1] and code[0] in ('\"', \"'\")):\n            raise template.TemplateSyntaxError, \"%r tag's argument should be in quotes\" % tag_name\n        code = code[1:-1]\n        current_site = None\n\n    return AnalyticsNode(current_site, code, template_name)\n    \nclass AnalyticsNode(template.Node):\n    def __init__(self, site=None, code=None, template_name='google_analytics/analytics_template.html'):\n        self.site = site\n        self.code = code\n        self.template_name = template_name\n        \n    def render(self, context):\n        content = ''\n        if self.site:\n            code_set = self.site.analytics_set.all()\n            if code_set:\n                code = code_set[0].analytics_code\n            else:\n                return ''\n        elif self.code:\n            code = self.code\n        else:\n            return ''\n        \n        if code.strip() != '':\n            t = loader.get_template(self.template_name)\n            c = Context({\n                'analytics_code': code,\n                'track_page_load_time': getattr(settings,\n                                                \"GOOGLE_ANALYTICS_TRACK_PAGE_LOAD_TIME\",\n                                                False),\n            })\n            return t.render(c)\n        else:\n            return ''\n        \nregister.tag('analytics', do_get_analytics)\nregister.tag('analytics_async', do_get_analytics)\n", "ght (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n#\n# Code generated by Microsoft (R) AutoRest Code Generator.\n# Changes may cause incorrect behavior and will be lost if the code is\n# regenerated.\n# --------------------------------------------------------------------------\n\nfrom msrest.pipeline import ClientRawResponse\n\nfrom .. import models\n\n\nclass Datetimerfc1123(object):\n    \"\"\"Datetimerfc1123 operations.\n\n    :param client: Client for service requests.\n    :param config: Configuration of service client.\n    :param serializer: An object model serializer.\n    :param deserializer: An objec model deserializer.\n    \"\"\"\n\n    def __init__(self, client, config, serializer, deserializer):\n\n        self._client = client\n        self._serialize = serializer\n        self._deserialize = deserializer\n\n        self.config = config\n\n    def get_null(\n            self, custom_headers=None, raw=False, **operation_config):\n        \"\"\"Get null datetime value.\n\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: returns the direct response alongside the\n         deserialized response\n        :param operation_config: :ref:`Operation configuration\n         overrides<msrest:optionsforoperations>`.\n        :rtype: datetime\n        :rtype: :class:`ClientRawResponse<msrest.pipeline.ClientRawResponse>`\n         if raw=true\n        \"\"\"\n        # Construct URL\n        url = '/datetimerfc1123/null'\n\n        # Construct parameters\n        query_parameters = {}\n\n        # Construct headers\n        header_parameters = {}\n        header_parameters['Content-Type'] = 'application/json; charset=utf-8'\n        if custom_headers:\n            header_parameters.update(custom_headers)\n\n        # Construct and send request\n        request = self._client.get(url, query_parameters)\n        response = self._client.send(request, header_parameters, **operation_config)\n\n        if response.status_code not in [200]:\n            raise models.ErrorException(self._deserialize, response)\n\n        deserialized = None\n\n        if response.status_code == 200:\n            deserialized = self._deserialize('rfc-1123', response)\n\n        if raw:\n            client_raw_response = ClientRawResponse(deserialized, response)\n            return client_raw_response\n\n        return deserialized\n\n    def get_invalid(\n            self, custom_headers=None, raw=False, **operation_config):\n        \"\"\"Get invalid datetime value.\n\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: returns the direct response alongside the\n         deserialized response\n        :param operation_config: :ref:`Operation configuration\n         overrides<msrest:optionsforoperations>`.\n        :rtype: datetime\n        :rtype: :class:`ClientRawResponse<msrest.pipeline.ClientRawResponse>`\n         if raw=true\n        \"\"\"\n        # Construct URL\n        url = '/datetimerfc1123/invalid'\n\n        # Construct parameters\n        query_parameters = {}\n\n        # Construct headers\n        header_parameters = {}\n        header_parameters['Content-Type'] = 'application/json; charset=utf-8'\n        if custom_headers:\n            header_parameters.update(custom_headers)\n\n        # Construct and send request\n        request = self._client.get(url, query_parameters)\n        response = self._client.send(request, header_parameters, **operation_config)\n\n        if response.status_code not in [200]:\n            raise models.ErrorException(self._deserialize, response)\n\n        deserialized = None\n\n        if response.status_code == 200:\n            deserialized = self._deserialize('rfc-1123', response)\n\n        if raw:\n            client_raw_response = ClientRawResponse(deserialized, response)\n            return client_raw_response\n\n        return deserialized\n\n    def get_overflow(\n            self, custom_headers=None, raw=False, **operation_config):\n        \"\"\"Get overflow datetime value.\n\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: returns the direct response alongside the\n         deserialized response\n        :param operation_config: :ref:`Operation configuration\n         overrides<msrest:optionsforoperations>`.\n        :rtype: datetime\n        :rtype: :class:`ClientRawResponse<msrest.pipeline.ClientRawResponse>`\n         if raw=true\n        \"\"\"\n        # Construct URL\n        url = '/datetimerfc1123/overflow'\n\n        # Construct parameters\n        query_parameters = {}\n\n        # Construct headers\n        header_parameters = {}\n        header_parameters['Content-Type'] = 'application/json; charset=utf-8'\n        if custom_headers:\n            header_parameters.update(custom_headers)\n\n        # Construct and send request\n        request = self._client.get(url, query_parameters)\n        response = self._client.send(request, header_parameters, **operation_config)\n\n        if response.status_code not in [200]:\n            raise models.ErrorException(self._deserialize, response)\n\n        deserialized = None\n\n        if response.status_code == 200:\n            deserialized = self._deserialize('rfc-1123', response)\n\n        if raw:\n            client_raw_response = ClientRawResponse(deserialized, response)\n            return client_raw_response\n\n        return deserialized\n\n    def get_underflow(\n            self, custom_headers=None, raw=False, **operation_config):\n        \"\"\"Get underflow datetime value.\n\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: returns the direct response alongside the\n         deserialized response\n        :param operation_config: :ref:`Operation configuration\n         overrides<msrest:optionsforoperations>`.\n        :rtype: datetime\n        :rtype: :class:`ClientRawResponse<msrest.pipeline.ClientRawResponse>`\n         if raw=true\n        \"\"\"\n        # Construct URL\n        url = '/datetimerfc1123/underflow'\n\n        # Construct parameters\n        query_parameters = {}\n\n        # Construct headers\n        header_parameters = {}\n        header_parameters['Content-Type'] = 'application/json; charset=utf-8'\n        if custom_headers:\n            header_parameters.update(custom_headers)\n\n        # Construct and send request\n        request = self._client.get(url, query_parameters)\n        response = self._client.send(request, header_parameters, **operation_config)\n\n        if response.status_code not in [200]:\n            raise models.ErrorException(self._deserialize, response)\n\n        deserialized = None\n\n        if response.status_code == 200:\n            deserialized = self._deserialize('rfc-1123', response)\n\n        if raw:\n            client_raw_response = ClientRawResponse(deserialized, response)\n            return client_raw_response\n\n        return deserialized\n\n    def put_utc_max_date_time(\n            self, datetime_body, custom_headers=None, raw=False, **operation_config):\n        \"\"\"Put max datetime value Fri, 31 Dec 9999 23:59:59 GMT.\n\n        :param datetime_body:\n        :type datetime_body: datetime\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: returns the direct response alongside the\n         deserialized response\n        :param operation_config: :ref:`Operation configuration\n         overrides<msrest:optionsforoperations>`.\n        :rtype: None\n        :rtype: :class:`ClientRawResponse<msrest.pipeline.ClientRawResponse>`\n         if raw=true\n        \"\"\"\n        # Construct URL\n        url = '/datetimerfc1123/max'\n\n        # Construct parameters\n        query_parameters = {}\n\n        # Construct headers\n        header_parameters = {}\n        header_parameters['Content-Type'] = 'application/json; charset=utf-8'\n        if custom_headers:\n            header_parameters.update(custom_headers)\n\n        # Construct body\n        body_content = self._serialize.body(datetime_body, 'rfc-1123')\n\n        # Construct and send request\n        request = self._client.put(url, query_parameters)\n        response = self._client.send(\n            request, header_parameters, body_content, **operation_config)\n\n        if response.status_code not in [200]:\n            raise models.ErrorException(self._deserialize, response)\n\n        if raw:\n            client_raw_response = ClientRawResponse(None, response)\n            return client_raw_response\n\n    def get_utc_lowercase_max_date_time(\n            self, custom_headers=None, raw=False, **operation_config):\n        \"\"\"Get max datetime value fri, 31 dec 9999 23:59:59 gmt.\n\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: returns the direct response alongside the\n         deserialized response\n        :param operation_config: :ref:`Operation configuration\n         overrides<msrest:optionsforoperations>`.\n        :rtype: datetime\n        :rtype: :class:`ClientRawResponse<msrest.pipeline.ClientRawResponse>`\n         if raw=true\n        \"\"\"\n        # Construct URL\n        url = '/datetimerfc1123/max/lowercase'\n\n        # Construct parameters\n        query_parameters = {}\n\n        # Construct headers\n        header_parameters = {}\n        header_parameters['Content-Type'] = 'application/json; charset=utf-8'\n        if custom_headers:\n            header_parameters.update(custom_headers)\n\n        # Construct and send request\n        request = self._client.get(url, query_parameters)\n        response = self._client.send(request, header_parameters, **operation_config)\n\n        if response.status_code not in [200]:\n            raise models.ErrorException(self._deserialize, response)\n\n        deserialized = None\n\n        if response.status_code == 200:\n            deserialized = self._deserialize('rfc-1123', response)\n\n        if raw:\n            client_raw_response = ClientRawResponse(deserialized, response)\n            return client_raw_response\n\n        return deserialized\n\n    def get_utc_uppercase_max_date_time(\n            self, custom_headers=None, raw=False, **operation_config):\n        \"\"\"Get max datetime value FRI, 31 DEC 9999 23:59:59 GMT.\n\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: returns the direct response alongside the\n         deserialized response\n        :param operation_config: :ref:`Operation configuration\n         overrides<msrest:optionsforoperations>`.\n        :rtype: datetime\n        :rtype: :class:`ClientRawResponse<msrest.pipeline.ClientRawResponse>`\n         if raw=true\n        \"\"\"\n        # Construct URL\n        url = '/datetimerfc1123/max/uppercase'\n\n        # Construct parameters\n        query_parameters = {}\n\n        # Construct headers\n        header_parameters = {}\n        header_parameters['Content-Type'] = 'application/json; charset=utf-8'\n        if custom_headers:\n            header_parameters.update(custom_headers)\n\n        # Construct and send request\n        request = self._client.get(url, query_parameters)\n        response = self._client.send(request, header_parameters, **operation_config)\n\n        if response.status_code not in [200]:\n            raise models.ErrorException(self._deserialize, response)\n\n        deserialized = None\n\n        if response.status_code == 200:\n            deserialized = self._deserialize('rfc-1123', response)\n\n        if raw:\n            client_raw_response = ClientRawResponse(deserialized, response)\n            return client_raw_response\n\n        return deserialized\n\n    def put_utc_min_date_time(\n            self, datetime_body, custom_headers=None, raw=False, **operation_config):\n        \"\"\"Put min datetime value Mon, 1 Jan 0001 00:00:00 GMT.\n\n        :param datetime_body:\n        :type datetime_body: datetime\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: returns the direct response alongside the\n         deserialized response\n        :param operation_config: :ref:`Operation configuration\n         overrides<msrest:optionsforoperations>`.\n        :rtype: None\n        :rtype: :class:`ClientRawResponse<msrest.pipeline.ClientRawResponse>`\n         if raw=true\n        \"\"\"\n        # Construct URL\n        url = '/datetimerfc1123/min'\n\n        # Construct parameters\n        query_parameters = {}\n\n        # Construct headers\n        header_parameters = {}\n        header_parameters['Content-Type'] = 'application/json; charset=utf-8'\n        if custom_headers:\n            header_parameters.update(custom_headers)\n\n        # Construct body\n        body_content = self._serialize.body(datetime_body, 'rfc-1123')\n\n        # Construct and send request\n        request = self._client.put(url, query_parameters)\n        response = self._client.send(\n            request, header_parameters, body_content, **operation_config)\n\n        if response.status_code not in [200]:\n            raise models.ErrorException(self._deserialize, response)\n\n        if raw:\n            client_raw_response = ClientRawResponse(None, response)\n            return client_raw_response\n\n    def get_utc_min_date_time(\n            self, custom_headers=None, raw=False, **operation_config):\n        \"\"\"Get min datetime value Mon, 1 Jan 0001 00:00:00 GMT.\n\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: returns the direct response alongside the\n         deserialized response\n        :param operation_config: :ref:`Operation configuration\n         overrides<msrest:optionsforoperations>`.\n        :rtype: datetime\n        :rtype: :class:`ClientRawResponse<msrest.pipeline.ClientRawResponse>`\n         if raw=true\n        \"\"\"\n        # Construct URL\n        url = '/datetimerfc1123/min'\n\n        # Construct parameters\n        query_parameters = {}\n\n        # Construct headers\n        header_parameters = {}\n        header_parameters['Content-Type'] = 'application/json; charset=utf-8'\n        if custom_headers:\n            header_parameters.update(custom_headers)\n\n        # Construct and send request\n        request = self._client.get(url, query_parameters)\n        response = self._client.send(request, header_parameters, **operation_config)\n\n        if response.status_code not in [200]:\n            raise models.ErrorException(self._deserialize, response)\n\n        deserialized = None\n\n        if response.status_code == 200:\n            deserialized = self._deserialize('rfc-1123', response)\n\n        if raw:\n            client_raw_response = ClientRawResponse(deserialized, response)\n            return client_raw_response\n\n        return deserialized\n", "or):\n    def __init__(\n        self, plotly_name=\"xpad\", parent_name=\"scatterpolargl.marker.colorbar\", **kwargs\n    ):\n        super(XpadValidator, self).__init__(\n            plotly_name=plotly_name,\n            parent_name=parent_name,\n            edit_type=kwargs.pop(\"edit_type\", \"calc\"),\n            min=kwargs.pop(\"min\", 0),\n            **kwargs\n        )\n", "t time\nfrom commands import *\n\n\ndef main(stdscr):\n    cp = configparser.ConfigParser()\n    cp.read('config.cfg')\n    username = cp.get('credentials', 'username')\n    password = cp.get('credentials', 'password').encode('utf-8')\n\n    stdscr.clear()\n    ui = ChatUI(stdscr)\n\n    client = Client(username, password, ui)\n    client.subscribe_to_channel(client.current_channel)\n    client.subscribe_to_users(client.current_channel)\n    \n    message = ''\n    while message != '/quit':\n        try:\n            message = ui.wait_input().strip()\n            if message != \"\" and (message[0] != '/' or message[0:2] == '//'):\n                client.client.insert('messages', {'channel': client.current_channel, 'text': message})\n            else:\n                try:\n                    end = message.index(' ')\n                    command = message[1:end].strip()\n                    rest = message[end:].strip()\n                except ValueError:\n                    command = message[1:].strip()\n                    rest = None\n\n                if commands.get(command, False):\n                    commands[command][0](ui, client, rest)\n                else:\n                    ui.chatbuffer_add('Unknown command {}'.format(command))\n     \n        except KeyboardInterrupt:\n            client.logout()\n            time.sleep(1)\n\nwrapper(main)\n\n# Could not subscribe because a connection has not been etablished\n", " ['../statemachine', '../mouse']\nfrom State import State\nfrom StateMachine import StateMachine\nfrom MouseAction import MouseAction\n\nclass StateT(State):\n  def __init__(self):\n    self.transitions = None\n  def next(self, input):\n    if self.transitions.has_key(input):\n      return self.transitions[input]\n    else:\n      raise \"Input not supported for current state\"\n\nclass Waiting(StateT):\n  def run(self): \n    print \"Waiting: Broadcasting cheese smell\"\n  def next(self, input):\n    # Lazy initialization:\n    if not self.transitions:\n      self.transitions = { \n        MouseAction.appears : MouseTrap.luring \n      }\n    return StateT.next(self, input)\n\nclass Luring(StateT):\n  def run(self):\n    print \"Luring: Presenting Cheese, door open\"\n  def next(self, input):\n    # Lazy initialization:\n    if not self.transitions:\n      self.transitions = {\n        MouseAction.enters : MouseTrap.trapping,\n        MouseAction.runsAway : MouseTrap.waiting\n      }\n    return StateT.next(self, input)\n\nclass Trapping(StateT):\n  def run(self):\n    print \"Trapping: Closing door\"\n  def next(self, input):\n    # Lazy initialization:\n    if not self.transitions:\n      self.transitions = {\n        MouseAction.escapes : MouseTrap.waiting,\n        MouseAction.trapped : MouseTrap.holding\n      }\n    return StateT.next(self, input)\n\nclass Holding(StateT):\n  def run(self):\n    print \"Holding: Mouse caught\"\n  def next(self, input):\n    # Lazy initialization:\n    if not self.transitions:\n      self.transitions = {\n        MouseAction.removed : MouseTrap.waiting\n      }\n    return StateT.next(self, input)\n\nclass MouseTrap(StateMachine):\n  def __init__(self): \n    # Initial state\n    StateMachine.__init__(self, MouseTrap.waiting)\n\n# Static variable initialization:\nMouseTrap.waiting = Waiting()\nMouseTrap.luring = Luring()\nMouseTrap.trapping = Trapping()\nMouseTrap.holding = Holding()\n\nmoves = map(string.strip, \n  open(\"../mouse/MouseMoves.txt\").readlines())\nmouseMoves = map(MouseAction, moves)\nMouseTrap().runAll(mouseMoves)\n#<hr>\noutput = '''\nWaiting: Broadcasting cheese smell\nmouse appears\nLuring: Presenting Cheese, door open\nmouse runs away\nWaiting: Broadcasting cheese smell\nmouse appears\nLuring: Presenting Cheese, door open\nmouse enters trap\nTrapping: Closing door\nmouse escapes\nWaiting: Broadcasting cheese smell\nmouse appears\nLuring: Presenting Cheese, door open\nmouse enters trap\nTrapping: Closing door\nmouse trapped\nHolding: Mouse caught\nmouse removed\nWaiting: Broadcasting cheese smell\nmouse appears\nLuring: Presenting Cheese, door open\nmouse runs away\nWaiting: Broadcasting cheese smell\nmouse appears\nLuring: Presenting Cheese, door open\nmouse enters trap\nTrapping: Closing door\nmouse trapped\nHolding: Mouse caught\nmouse removed\nWaiting: Broadcasting cheese smell\n'''\n", "sAPI(JarvisAPI):\n    def __init__(self):\n        self.call_history = MockHistoryBuilder().\\\n            add_field('operation').\\\n            add_field('args').\\\n            add_field('return').\\\n            build()\n\n        self.say_history = MockHistoryBuilder().\\\n            add_field('text').\\\n            add_field('color').\\\n            build()\n\n        self.notification_history = MockHistoryBuilder().\\\n            add_field('msg').\\\n            add_field('time_seconds').\\\n            build()\n\n        self.schedule_history = MockHistoryBuilder().\\\n            add_field('time_seconds').\\\n            add_field('function').\\\n            add_field('args').\\\n            build()\n\n        self.data = {}\n        self._input_queue = []\n        self.is_voice_enabled = False\n\n    def say(self, text, color=\"\"):\n        # remove \\n\n        text = text.rstrip('\\n')\n        self.say_history.record(text, color)\n        self.call_history.record('say', (text, color), None)\n\n    def queue_input(self, text):\n        self._input_queue.append(text)\n\n    def input(self, prompt='', color=''):\n        if len(self._input_queue) == 0:\n            raise BaseException(\"MockJarvisAPI: No predefined answer in queue - add answer with 'self.queue_input(\\\"TEXT\\\")'\")\n        return self._input_queue.pop()\n\n    def input_number(self, prompt='', color='', rtype=float, rmin=None, rmax=None):\n        return JarvisAPI.input_number(self, prompt, color, rtype, rmin, rmax)\n\n    def connection_error(self):\n        self.call_history.record('connection_error', (), None)\n\n    def exit(self):\n        self.call_history.record('exit', (), None)\n\n    def notification(self, msg, time_seconds=0):\n        self.notification_history.record(msg, time_seconds)\n        self.call_history.record('notification', (msg, time_seconds), None)\n\n    def schedule(self, time_seconds, function, *args):\n        self.notification_history.record(time_seconds, function, *args)\n        self.call_history.record(\n            'schedule', (time_seconds, function, args), None)\n\n    def cancel(self, schedule_id):\n        self.call_history.record('cancel', (), None)\n\n    def enable_voice(self):\n        self.call_history.record('enable_voice', (), None)\n        self.is_voice_enabled = True\n\n    def disable_voice(self):\n        self.call_history.record('disable_voice', (), None)\n        self.is_voice_enabled = False\n\n    def is_voice_enabled(self):\n        self.call_history.record('is_voice_enabled', (), self.is_voice_enabled)\n        return self.is_voice_enabled\n\n    def get_data(self, key):\n        if key not in self.data:\n            value = None\n        else:\n            value = self.data[key]\n        self.call_history.record('get_data', (key), value)\n        return value\n\n    def add_data(self, key, value):\n        self.data[key] = value\n        self.call_history.record('add_data', (key, value), None)\n\n    def update_data(self, key, value):\n        self.data[key] = value\n        self.call_history.record('update_data', (key, value), None)\n\n    def del_data(self, key):\n        del self.data[key]\n        self.call_history.record('del_data', (key), None)\n\n    def eval(self, s):\n        self.call_history.record('eval', s)\n\n\nclass MockHistoryBuilder():\n    def __init__(self):\n        self._history = MockHistory()\n\n    def add_field(self, field):\n        self._history._storage_by_field[field] = []\n        self._history.__dict__[\n            'contains_{}'.format(field)] = partial(\n            self._history.contains, field)\n        self._history.__dict__[\n            'view_{}'.format(field)] = partial(\n            self._history.view, field)\n        self._history.__dict__[\n            'last_{}'.format(field)] = partial(\n            self._history.last, field)\n        self._history._field_list.append(field)\n        return self\n\n    def build(self):\n        return self._history\n\n\nclass MockHistory():\n    \"\"\"\n    Record/Output history.\n\n    Recorded data sets must contain fixed and predefined number of \"fields\" (e.g. text and color).\n\n    Note: For Methods with first parameter \"field\" method \"name_field\" exist.\n    So \"view('text')\" can be rewritten as \"view_text\".\n    \"\"\"\n\n    def __init__(self):\n        self._storage_by_field = {}\n        self._storage_by_index = []\n\n        self._field_list = []\n        self._counter = 0\n\n    def record(self, *args):\n        \"\"\"\n        Do not call manually!\n        \"\"\"\n        if len(self._field_list) != len(args):\n            raise ValueError(\n                \"Argument count miss-match: {} --- {}\".format(self._field_list, args))\n        for i, field in enumerate(self._field_list):\n            self._storage_by_field[field].append(args[i])\n        self._storage_by_index.append(args)\n        self._counter += 1\n\n    def contains(self, field=None, value=None):\n        \"\"\"\n        Check if value is recorded.\n        If field is None, value should be a tuple of values\n        for all fields.\n        \"\"\"\n        if field is None:\n            return value in self._storage_by_index\n        else:\n            return value in self._storage_by_field[field]\n\n    def view(self, field=None, index=None):\n        \"\"\"\n        Returns value.\n\n        If field is None, returns tuple of all values for all fields.\n        If index is None, returns all recorded values\n        \"\"\"\n        if field is None:\n            if index is None:\n                return self._storage_by_index\n            else:\n                return self._storage_by_index[index]\n        else:\n            if index is None:\n                return self._storage_by_field[field]\n            else:\n                return self._storage_by_field[field][index]\n\n    def last(self, field=None):\n        \"\"\"Shortcut for view with index -1\"\"\"\n        return self.view(field, -1)\n\n    def get_length(self):\n        \"\"\"Returns how many data sets were recorded\"\"\"\n        return self._counter\n\n\nclass PluginTest(unittest.TestCase):\n    def setUp(self):\n        self._setUp()\n\n    def _setUp(self):\n        if 'jarvis_api' not in self.__dict__ or self.jarvis_api is None:\n            self.jarvis_api = MockJarvisAPI()\n\n    def load_plugin(self, plugin_class):\n        \"\"\"\n        Returns Plugin Instance (object).\n        Works for both callable classes or methods.\n\n        Adds method run(string) - which execute plugin using mocked api.\n        \"\"\"\n        self._setUp()\n\n        plugin_backend = plugin_class()._backend[0]\n        plugin_backend.run = partial(plugin_backend, self.jarvis_api)\n        return plugin_backend\n\n    def tearDown(self):\n        self.jarvis_api = None\n\n    def queue_input(self, msg):\n        \"\"\"\n        Queue msg to be returned by 'jarvis.input()'\n        \"\"\"\n        self.jarvis_api.queue_input(msg)\n\n    def histroy_call(self):\n        \"\"\"\n        Returns MockHistory instance. Fields:\n\n        1. operation (string)\n        2. args (tuple)\n        3. return value\n        \"\"\"\n        return self.jarvis_api.call_history\n\n    def history_say(self):\n        \"\"\"\n        Returns MockHistory instance. Fields:\n\n        1. text (string)\n        2. color (colorama.Fore.*)\n        \"\"\"\n        return self.jarvis_api.say_history\n\n    def history_notification(self):\n        \"\"\"\n        Returns MockHistory instance. Fields:\n\n        1. msg (string)\n        2. time_seconds (int)\n        \"\"\"\n        return self.jarvis_api.notification_history\n\n    def history_schedule(self):\n        \"\"\"\n        Returns MockHistory instance. Fields:\n\n        1. time_seconds (int)\n        2. function\n        3. args (tuple)\n        \"\"\"\n        return self.jarvis_api.schedule_history\n", "test\n\nfrom rnglib import SimpleRNG\nfrom optionz import Option\n\n# pylint: disable=too-few-public-methods\n\n\nclass EmptyClass():\n    \"\"\" Stub class, for testing. \"\"\"\n    pass\n\n\n# pylint: disable=unused-argument\ndef simple_adder(self, a__, b__):\n    \"\"\" Simplest possible adder method. \"\"\"\n    return a__ + b__\n\n\nclass TestBaseOption(unittest.TestCase):\n    \"\"\" Test the [Base]Option class. \"\"\"\n\n    def setUp(self):\n        self.rng = SimpleRNG(time.time())\n\n    def tearDown(self):\n        pass\n\n    def test_constructor(self):\n        \"\"\" Test the constuctor. \"\"\"\n\n        opt_a = Option(name='fred', surname='jones')\n        # it's a dictionary\n        self.assertTrue('name' in opt_a)\n        self.assertTrue('surname'in opt_a)\n        self.assertFalse('fred' in opt_a)\n        # dots work\n        self.assertEqual(opt_a.name, 'fred')\n        # pylint: disable=no-member\n        self.assertEqual(opt_a.surname, 'jones')\n\n        opt_b = Option(name='fred', surname='jones')\n        self.assertEqual(opt_b.name, 'fred')\n        # pylint: disable=no-member\n        self.assertEqual(opt_b.surname, 'jones')\n\n        self.assertEqual(opt_a, opt_a)\n        self.assertEqual(opt_a, opt_b)\n\n        opt_c = Option(name='john', surname='smith')\n        # dictionary\n        self.assertTrue('name' in opt_c)\n        self.assertTrue('surname' in opt_c)\n        self.assertFalse('john' in opt_c)\n        self.assertFalse('smith' in opt_c)\n        # dots\n        self.assertEqual(opt_c.name, 'john')\n        # pylint: disable=no-member\n        self.assertEqual(opt_c.surname, 'smith')\n\n        self.assertNotEqual(opt_a, opt_c)\n\n        # assignment to dotted option\n        opt_b.name = 'george'\n        self.assertEqual(opt_b.name, 'george')\n\n\nif __name__ == '__main__':\n    unittest.main()\n", "t reduce\n\nimport numpy as np\nimport numpy.core.fromnumeric as fromnumeric\nfrom numpy import float_\nfrom numpy.testing.utils import build_err_msg\n\n# Fixme: this does not look right.\nnp.seterr(all='ignore')\n\npi = np.pi\n\n\nclass ModuleTester(object):\n    def __init__(self, module):\n        self.module = module\n        self.allequal = module.allequal\n        self.arange = module.arange\n        self.array = module.array\n        self.concatenate = module.concatenate\n        self.count = module.count\n        self.equal = module.equal\n        self.filled = module.filled\n        self.getmask = module.getmask\n        self.getmaskarray = module.getmaskarray\n        self.id = id\n        self.inner = module.inner\n        self.make_mask = module.make_mask\n        self.masked = module.masked\n        self.masked_array = module.masked_array\n        self.masked_values = module.masked_values\n        self.mask_or = module.mask_or\n        self.nomask = module.nomask\n        self.ones = module.ones\n        self.outer = module.outer\n        self.repeat = module.repeat\n        self.resize = module.resize\n        self.sort = module.sort\n        self.take = module.take\n        self.transpose = module.transpose\n        self.zeros = module.zeros\n        self.MaskType = module.MaskType\n        try:\n            self.umath = module.umath\n        except AttributeError:\n            self.umath = module.core.umath\n        self.testnames = []\n\n    def assert_array_compare(self, comparison, x, y, err_msg='', header='',\n                             fill_value=True):\n        \"\"\"\n        Assert that a comparison of two masked arrays is satisfied elementwise.\n\n        \"\"\"\n        xf = self.filled(x)\n        yf = self.filled(y)\n        m = self.mask_or(self.getmask(x), self.getmask(y))\n\n        x = self.filled(self.masked_array(xf, mask=m), fill_value)\n        y = self.filled(self.masked_array(yf, mask=m), fill_value)\n        if (x.dtype.char != \"O\"):\n            x = x.astype(float_)\n            if isinstance(x, np.ndarray) and x.size > 1:\n                x[np.isnan(x)] = 0\n            elif np.isnan(x):\n                x = 0\n        if (y.dtype.char != \"O\"):\n            y = y.astype(float_)\n            if isinstance(y, np.ndarray) and y.size > 1:\n                y[np.isnan(y)] = 0\n            elif np.isnan(y):\n                y = 0\n        try:\n            cond = (x.shape == () or y.shape == ()) or x.shape == y.shape\n            if not cond:\n                msg = build_err_msg([x, y],\n                                    err_msg\n                                    + '\\n(shapes %s, %s mismatch)' % (x.shape,\n                                                                      y.shape),\n                                    header=header,\n                                    names=('x', 'y'))\n                assert cond, msg\n            val = comparison(x, y)\n            if m is not self.nomask and fill_value:\n                val = self.masked_array(val, mask=m)\n            if isinstance(val, bool):\n                cond = val\n                reduced = [0]\n            else:\n                reduced = val.ravel()\n                cond = reduced.all()\n                reduced = reduced.tolist()\n            if not cond:\n                match = 100 - 100.0 * reduced.count(1) / len(reduced)\n                msg = build_err_msg([x, y],\n                                    err_msg\n                                    + '\\n(mismatch %s%%)' % (match,),\n                                    header=header,\n                                    names=('x', 'y'))\n                assert cond, msg\n        except ValueError:\n            msg = build_err_msg([x, y], err_msg, header=header, names=('x', 'y'))\n            raise ValueError(msg)\n\n    def assert_array_equal(self, x, y, err_msg=''):\n        \"\"\"\n        Checks the elementwise equality of two masked arrays.\n\n        \"\"\"\n        self.assert_array_compare(self.equal, x, y, err_msg=err_msg,\n                                  header='Arrays are not equal')\n\n    def test_0(self):\n        \"\"\"\n        Tests creation\n\n        \"\"\"\n        x = np.array([1., 1., 1., -2., pi / 2.0, 4., 5., -10., 10., 1., 2., 3.])\n        m = [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n        xm = self.masked_array(x, mask=m)\n        xm[0]\n\n    def test_1(self):\n        \"\"\"\n        Tests creation\n\n        \"\"\"\n        x = np.array([1., 1., 1., -2., pi / 2.0, 4., 5., -10., 10., 1., 2., 3.])\n        y = np.array([5., 0., 3., 2., -1., -4., 0., -10., 10., 1., 0., 3.])\n        m1 = [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n        m2 = [0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1]\n        xm = self.masked_array(x, mask=m1)\n        ym = self.masked_array(y, mask=m2)\n        xf = np.where(m1, 1.e+20, x)\n        xm.set_fill_value(1.e+20)\n\n        assert ((xm - ym).filled(0).any())\n        s = x.shape\n        assert (xm.size == reduce(lambda x, y: x * y, s))\n        assert (self.count(xm) == len(m1) - reduce(lambda x, y: x + y, m1))\n\n        for s in [(4, 3), (6, 2)]:\n            x.shape = s\n            y.shape = s\n            xm.shape = s\n            ym.shape = s\n            xf.shape = s\n            assert (self.count(xm) == len(m1) - reduce(lambda x, y: x + y, m1))\n\n    def test_2(self):\n        \"\"\"\n        Tests conversions and indexing.\n\n        \"\"\"\n        x1 = np.array([1, 2, 4, 3])\n        x2 = self.array(x1, mask=[1, 0, 0, 0])\n        x3 = self.array(x1, mask=[0, 1, 0, 1])\n        x4 = self.array(x1)\n        # test conversion to strings, no errors\n        str(x2)\n        repr(x2)\n        # tests of indexing\n        assert type(x2[1]) is type(x1[1])\n        assert x1[1] == x2[1]\n        x1[2] = 9\n        x2[2] = 9\n        self.assert_array_equal(x1, x2)\n        x1[1:3] = 99\n        x2[1:3] = 99\n        x2[1] = self.masked\n        x2[1:3] = self.masked\n        x2[:] = x1\n        x2[1] = self.masked\n        x3[:] = self.masked_array([1, 2, 3, 4], [0, 1, 1, 0])\n        x4[:] = self.masked_array([1, 2, 3, 4], [0, 1, 1, 0])\n        x1 = np.arange(5) * 1.0\n        x2 = self.masked_values(x1, 3.0)\n        x1 = self.array([1, 'hello', 2, 3], object)\n        x2 = np.array([1, 'hello', 2, 3], object)\n        # check that no error occurs.\n        x1[1]\n        x2[1]\n        assert x1[1:1].shape == (0,)\n        # Tests copy-size\n        n = [0, 0, 1, 0, 0]\n        m = self.make_mask(n)\n        m2 = self.make_mask(m)\n        assert (m is m2)\n        m3 = self.make_mask(m, copy=1)\n        assert (m is not m3)\n\n    def test_3(self):\n        \"\"\"\n        Tests resize/repeat\n\n        \"\"\"\n        x4 = self.arange(4)\n        x4[2] = self.masked\n        y4 = self.resize(x4, (8,))\n        assert self.allequal(self.concatenate([x4, x4]), y4)\n        assert self.allequal(self.getmask(y4), [0, 0, 1, 0, 0, 0, 1, 0])\n        y5 = self.repeat(x4, (2, 2, 2, 2), axis=0)\n        self.assert_array_equal(y5, [0, 0, 1, 1, 2, 2, 3, 3])\n        y6 = self.repeat(x4, 2, axis=0)\n        assert self.allequal(y5, y6)\n        y7 = x4.repeat((2, 2, 2, 2), axis=0)\n        assert self.allequal(y5, y7)\n        y8 = x4.repeat(2, 0)\n        assert self.allequal(y5, y8)\n\n    def test_4(self):\n        \"\"\"\n        Test of take, transpose, inner, outer products.\n\n        \"\"\"\n        x = self.arange(24)\n        y = np.arange(24)\n        x[5:6] = self.masked\n        x = x.reshape(2, 3, 4)\n        y = y.reshape(2, 3, 4)\n        assert self.allequal(np.transpose(y, (2, 0, 1)), self.transpose(x, (2, 0, 1)))\n        assert self.allequal(np.take(y, (2, 0, 1), 1), self.take(x, (2, 0, 1), 1))\n        assert self.allequal(np.inner(self.filled(x, 0), self.filled(y, 0)),\n                             self.inner(x, y))\n        assert self.allequal(np.outer(self.filled(x, 0), self.filled(y, 0)),\n                             self.outer(x, y))\n        y = self.array(['abc', 1, 'def', 2, 3], object)\n        y[2] = self.masked\n        t = self.take(y, [0, 3, 4])\n        assert t[0] == 'abc'\n        assert t[1] == 2\n        assert t[2] == 3\n\n    def test_5(self):\n        \"\"\"\n        Tests inplace w/ scalar\n\n        \"\"\"\n        x = self.arange(10)\n        y = self.arange(10)\n        xm = self.arange(10)\n        xm[2] = self.masked\n        x += 1\n        assert self.allequal(x, y + 1)\n        xm += 1\n        assert self.allequal(xm, y + 1)\n\n        x = self.arange(10)\n        xm = self.arange(10)\n        xm[2] = self.masked\n        x -= 1\n        assert self.allequal(x, y - 1)\n        xm -= 1\n        assert self.allequal(xm, y - 1)\n\n        x = self.arange(10) * 1.0\n        xm = self.arange(10) * 1.0\n        xm[2] = self.masked\n        x *= 2.0\n        assert self.allequal(x, y * 2)\n        xm *= 2.0\n        assert self.allequal(xm, y * 2)\n\n        x = self.arange(10) * 2\n        xm = self.arange(10) * 2\n        xm[2] = self.masked\n        x /= 2\n        assert self.allequal(x, y)\n        xm /= 2\n        assert self.allequal(xm, y)\n\n        x = self.arange(10) * 1.0\n        xm = self.arange(10) * 1.0\n        xm[2] = self.masked\n        x /= 2.0\n        assert self.allequal(x, y / 2.0)\n        xm /= self.arange(10)\n        self.assert_array_equal(xm, self.ones((10,)))\n\n        x = self.arange(10).astype(float_)\n        xm = self.arange(10)\n        xm[2] = self.masked\n        x += 1.\n        assert self.allequal(x, y + 1.)\n\n    def test_6(self):\n        \"\"\"\n        Tests inplace w/ array\n\n        \"\"\"\n        x = self.arange(10, dtype=float_)\n        y = self.arange(10)\n        xm = self.arange(10, dtype=float_)\n        xm[2] = self.masked\n        m = xm.mask\n        a = self.arange(10, dtype=float_)\n        a[-1] = self.masked\n        x += a\n        xm += a\n        assert self.allequal(x, y + a)\n        assert self.allequal(xm, y + a)\n        assert self.allequal(xm.mask, self.mask_or(m, a.mask))\n\n        x = self.arange(10, dtype=float_)\n        xm = self.arange(10, dtype=float_)\n        xm[2] = self.masked\n        m = xm.mask\n        a = self.arange(10, dtype=float_)\n        a[-1] = self.masked\n        x -= a\n        xm -= a\n        assert self.allequal(x, y - a)\n        assert self.allequal(xm, y - a)\n        assert self.allequal(xm.mask, self.mask_or(m, a.mask))\n\n        x = self.arange(10, dtype=float_)\n        xm = self.arange(10, dtype=float_)\n        xm[2] = self.masked\n        m = xm.mask\n        a = self.arange(10, dtype=float_)\n        a[-1] = self.masked\n        x *= a\n        xm *= a\n        assert self.allequal(x, y * a)\n        assert self.allequal(xm, y * a)\n        assert self.allequal(xm.mask, self.mask_or(m, a.mask))\n\n        x = self.arange(10, dtype=float_)\n        xm = self.arange(10, dtype=float_)\n        xm[2] = self.masked\n        m = xm.mask\n        a = self.arange(10, dtype=float_)\n        a[-1] = self.masked\n        x /= a\n        xm /= a\n\n    def test_7(self):\n        \"Tests ufunc\"\n        d = (self.array([1.0, 0, -1, pi / 2] * 2, mask=[0, 1] + [0] * 6),\n             self.array([1.0, 0, -1, pi / 2] * 2, mask=[1, 0] + [0] * 6),)\n        for f in ['sqrt', 'log', 'log10', 'exp', 'conjugate',\n                  #                  'sin', 'cos', 'tan',\n                  #                  'arcsin', 'arccos', 'arctan',\n                  #                  'sinh', 'cosh', 'tanh',\n                  #                  'arcsinh',\n                  #                  'arccosh',\n                  #                  'arctanh',\n                  #                  'absolute', 'fabs', 'negative',\n                  #                  # 'nonzero', 'around',\n                  #                  'floor', 'ceil',\n                  #                  # 'sometrue', 'alltrue',\n                  #                  'logical_not',\n                  #                  'add', 'subtract', 'multiply',\n                  #                  'divide', 'true_divide', 'floor_divide',\n                  #                  'remainder', 'fmod', 'hypot', 'arctan2',\n                  #                  'equal', 'not_equal', 'less_equal', 'greater_equal',\n                  #                  'less', 'greater',\n                  #                  'logical_and', 'logical_or', 'logical_xor',\n                  ]:\n            try:\n                uf = getattr(self.umath, f)\n            except AttributeError:\n                uf = getattr(fromnumeric, f)\n            mf = getattr(self.module, f)\n            args = d[:uf.nin]\n            ur = uf(*args)\n            mr = mf(*args)\n            self.assert_array_equal(ur.filled(0), mr.filled(0), f)\n            self.assert_array_equal(ur._mask, mr._mask)\n\n    def test_99(self):\n        # test average\n        ott = self.array([0., 1., 2., 3.], mask=[1, 0, 0, 0])\n        self.assert_array_equal(2.0, self.average(ott, axis=0))\n        self.assert_array_equal(2.0, self.average(ott, weights=[1., 1., 2., 1.]))\n        result, wts = self.average(ott, weights=[1., 1., 2., 1.], returned=1)\n        self.assert_array_equal(2.0, result)\n        assert (wts == 4.0)\n        ott[:] = self.masked\n        assert (self.average(ott, axis=0) is self.masked)\n        ott = self.array([0., 1., 2., 3.], mask=[1, 0, 0, 0])\n        ott = ott.reshape(2, 2)\n        ott[:, 1] = self.masked\n        self.assert_array_equal(self.average(ott, axis=0), [2.0, 0.0])\n        assert (self.average(ott, axis=1)[0] is self.masked)\n        self.assert_array_equal([2., 0.], self.average(ott, axis=0))\n        result, wts = self.average(ott, axis=0, returned=1)\n        self.assert_array_equal(wts, [1., 0.])\n        w1 = [0, 1, 1, 1, 1, 0]\n        w2 = [[0, 1, 1, 1, 1, 0], [1, 0, 0, 0, 0, 1]]\n        x = self.arange(6)\n        self.assert_array_equal(self.average(x, axis=0), 2.5)\n        self.assert_array_equal(self.average(x, axis=0, weights=w1), 2.5)\n        y = self.array([self.arange(6), 2.0 * self.arange(6)])\n        self.assert_array_equal(self.average(y, None), np.add.reduce(np.arange(6)) * 3. / 12.)\n        self.assert_array_equal(self.average(y, axis=0), np.arange(6) * 3. / 2.)\n        self.assert_array_equal(self.average(y, axis=1), [self.average(x, axis=0), self.average(x, axis=0) * 2.0])\n        self.assert_array_equal(self.average(y, None, weights=w2), 20. / 6.)\n        self.assert_array_equal(self.average(y, axis=0, weights=w2), [0., 1., 2., 3., 4., 10.])\n        self.assert_array_equal(self.average(y, axis=1), [self.average(x, axis=0), self.average(x, axis=0) * 2.0])\n        m1 = self.zeros(6)\n        m2 = [0, 0, 1, 1, 0, 0]\n        m3 = [[0, 0, 1, 1, 0, 0], [0, 1, 1, 1, 1, 0]]\n        m4 = self.ones(6)\n        m5 = [0, 1, 1, 1, 1, 1]\n        self.assert_array_equal(self.average(self.masked_array(x, m1), axis=0), 2.5)\n        self.assert_array_equal(self.average(self.masked_array(x, m2), axis=0), 2.5)\n        self.assert_array_equal(self.average(self.masked_array(x, m5), axis=0), 0.0)\n        self.assert_array_equal(self.count(self.average(self.masked_array(x, m4), axis=0)), 0)\n        z = self.masked_array(y, m3)\n        self.assert_array_equal(self.average(z, None), 20. / 6.)\n        self.assert_array_equal(self.average(z, axis=0), [0., 1., 99., 99., 4.0, 7.5])\n        self.assert_array_equal(self.average(z, axis=1), [2.5, 5.0])\n        self.assert_array_equal(self.average(z, axis=0, weights=w2), [0., 1., 99., 99., 4.0, 10.0])\n\n    def test_A(self):\n        x = self.arange(24)\n        x[5:6] = self.masked\n        x = x.reshape(2, 3, 4)\n\n\nif __name__ == '__main__':\n    setup_base = (\"from __main__ import ModuleTester \\n\"\n                  \"import numpy\\n\"\n                  \"tester = ModuleTester(module)\\n\")\n    setup_cur = \"import numpy.ma.core as module\\n\" + setup_base\n    (nrepeat, nloop) = (10, 10)\n\n    if 1:\n        for i in range(1, 8):\n            func = 'tester.test_%i()' % i\n            cur = timeit.Timer(func, setup_cur).repeat(nrepeat, nloop * 10)\n            cur = np.sort(cur)\n            print(\"#%i\" % i + 50 * '.')\n            print(eval(\"ModuleTester.test_%i.__doc__\" % i))\n            print(\"core_current : %.3f - %.3f\" % (cur[0], cur[1]))\n", "lidator):\n    def __init__(self, plotly_name=\"layer\", parent_name=\"layout.shape\", **kwargs):\n        super(LayerValidator, self).__init__(\n            plotly_name=plotly_name,\n            parent_name=parent_name,\n            edit_type=kwargs.pop(\"edit_type\", \"arraydraw\"),\n            values=kwargs.pop(\"values\", [\"below\", \"above\"]),\n            **kwargs\n        )\n", "Created: Wed Jul 10 16:07:22 2013\n#      by: PyQt4 UI code generator 4.9.6\n#\n# WARNING! All changes made in this file will be lost!\n\nfrom PyQt4 import QtCore, QtGui\n\ntry:\n    _fromUtf8 = QtCore.QString.fromUtf8\nexcept AttributeError:\n    def _fromUtf8(s):\n        return s\n\ntry:\n    _encoding = QtGui.QApplication.UnicodeUTF8\n    def _translate(context, text, disambig):\n        return QtGui.QApplication.translate(context, text, disambig, _encoding)\nexcept AttributeError:\n    def _translate(context, text, disambig):\n        return QtGui.QApplication.translate(context, text, disambig)\n\nclass Ui_batchadjform(object):\n    def setupUi(self, batchadjform):\n        batchadjform.setObjectName(_fromUtf8(\"batchadjform\"))\n        batchadjform.resize(543, 412)\n        icon = QtGui.QIcon()\n        icon.addPixmap(QtGui.QPixmap(_fromUtf8(\":/icons/settings\")), QtGui.QIcon.Normal, QtGui.QIcon.Off)\n        batchadjform.setWindowIcon(icon)\n        self.verticalLayout = QtGui.QVBoxLayout(batchadjform)\n        self.verticalLayout.setObjectName(_fromUtf8(\"verticalLayout\"))\n        self.frame_2 = QtGui.QFrame(batchadjform)\n        self.frame_2.setMinimumSize(QtCore.QSize(0, 40))\n        self.frame_2.setMaximumSize(QtCore.QSize(16777215, 40))\n        self.frame_2.setFrameShape(QtGui.QFrame.StyledPanel)\n        self.frame_2.setFrameShadow(QtGui.QFrame.Raised)\n        self.frame_2.setObjectName(_fromUtf8(\"frame_2\"))\n        self.horizontalLayout_3 = QtGui.QHBoxLayout(self.frame_2)\n        self.horizontalLayout_3.setObjectName(_fromUtf8(\"horizontalLayout_3\"))\n        self.label_2 = QtGui.QLabel(self.frame_2)\n        font = QtGui.QFont()\n        font.setFamily(_fromUtf8(\"Calibri\"))\n        font.setPointSize(18)\n        font.setBold(True)\n        font.setItalic(True)\n        font.setUnderline(True)\n        font.setWeight(75)\n        self.label_2.setFont(font)\n        self.label_2.setObjectName(_fromUtf8(\"label_2\"))\n        self.horizontalLayout_3.addWidget(self.label_2)\n        self.verticalLayout.addWidget(self.frame_2)\n        self.frame = QtGui.QFrame(batchadjform)\n        self.frame.setMinimumSize(QtCore.QSize(0, 45))\n        self.frame.setMaximumSize(QtCore.QSize(16777215, 45))\n        self.frame.setAutoFillBackground(False)\n        self.frame.setStyleSheet(_fromUtf8(\"border: none;\\n\"\n\"background: qlineargradient(x1: 0, y1: 0, x2: 0, y2: 1,\\n\"\n\"stop: 0 #BBBBBB, \\n\"\n\"stop: 0.4 #EEEEEE,\\n\"\n\"stop: 0.9 #CCCCCC,\\n\"\n\" stop: 1 #666666);\"))\n        self.frame.setFrameShape(QtGui.QFrame.Box)\n        self.frame.setFrameShadow(QtGui.QFrame.Raised)\n        self.frame.setObjectName(_fromUtf8(\"frame\"))\n        self.horizontalLayout_4 = QtGui.QHBoxLayout(self.frame)\n        self.horizontalLayout_4.setObjectName(_fromUtf8(\"horizontalLayout_4\"))\n        self.saveButton = QtGui.QPushButton(self.frame)\n        self.saveButton.setMinimumSize(QtCore.QSize(90, 0))\n        self.saveButton.setMaximumSize(QtCore.QSize(90, 16777215))\n        self.saveButton.setStyleSheet(_fromUtf8(\"QPushButton {\\n\"\n\"background-color: rgb(250, 250, 250);\\n\"\n\"color: #333;\\n\"\n\"border: 2px solid #555;\\n\"\n\"padding: 5px;}\\n\"\n\"QPushButton:hover {\\n\"\n\"background: qradialgradient(cx: 0.3, cy: -0.4,\\n\"\n\"fx: 0.3, fy: -0.4,\\n\"\n\"radius: 1.35, stop: 0 #fff, stop: 1 #ccc);}\\n\"\n\"QPushButton:pressed {\\n\"\n\"background: qradialgradient(cx: 0.4, cy: -0.1,\\n\"\n\"fx: 0.4, fy: -0.1,\\n\"\n\"radius: 1.35, stop: 0 #fff, stop: 1 #eee);}\"))\n        icon1 = QtGui.QIcon()\n        icon1.addPixmap(QtGui.QPixmap(_fromUtf8(\":/icons/save\")), QtGui.QIcon.Normal, QtGui.QIcon.Off)\n        self.saveButton.setIcon(icon1)\n        self.saveButton.setIconSize(QtCore.QSize(20, 20))\n        self.saveButton.setFlat(False)\n        self.saveButton.setObjectName(_fromUtf8(\"saveButton\"))\n        self.horizontalLayout_4.addWidget(self.saveButton)\n        self.clearButton = QtGui.QPushButton(self.frame)\n        self.clearButton.setMinimumSize(QtCore.QSize(90, 0))\n        self.clearButton.setMaximumSize(QtCore.QSize(90, 16777215))\n        self.clearButton.setStyleSheet(_fromUtf8(\"QPushButton {\\n\"\n\"background-color: rgb(250, 250, 250);\\n\"\n\"color: #333;\\n\"\n\"border: 2px solid #555;\\n\"\n\"padding: 5px;}\\n\"\n\"QPushButton:hover {\\n\"\n\"background: qradialgradient(cx: 0.3, cy: -0.4,\\n\"\n\"fx: 0.3, fy: -0.4,\\n\"\n\"radius: 1.35, stop: 0 #fff, stop: 1 #ccc);}\\n\"\n\"QPushButton:pressed {\\n\"\n\"background: qradialgradient(cx: 0.4, cy: -0.1,\\n\"\n\"fx: 0.4, fy: -0.1,\\n\"\n\"radius: 1.35, stop: 0 #fff, stop: 1 #eee);}\"))\n        icon2 = QtGui.QIcon()\n        icon2.addPixmap(QtGui.QPixmap(_fromUtf8(\":/icons/clear\")), QtGui.QIcon.Normal, QtGui.QIcon.Off)\n        self.clearButton.setIcon(icon2)\n        self.clearButton.setIconSize(QtCore.QSize(20, 20))\n        self.clearButton.setFlat(False)\n        self.clearButton.setObjectName(_fromUtf8(\"clearButton\"))\n        self.horizontalLayout_4.addWidget(self.clearButton)\n        self.refresh_pushButton = QtGui.QPushButton(self.frame)\n        self.refresh_pushButton.setMinimumSize(QtCore.QSize(90, 0))\n        self.refresh_pushButton.setMaximumSize(QtCore.QSize(90, 16777215))\n        self.refresh_pushButton.setStyleSheet(_fromUtf8(\"QPushButton {\\n\"\n\"background-color: rgb(250, 250, 250);\\n\"\n\"color: #333;\\n\"\n\"border: 2px solid #555;\\n\"\n\"padding: 5px;}\\n\"\n\"QPushButton:hover {\\n\"\n\"background: qradialgradient(cx: 0.3, cy: -0.4,\\n\"\n\"fx: 0.3, fy: -0.4,\\n\"\n\"radius: 1.35, stop: 0 #fff, stop: 1 #ccc);}\\n\"\n\"QPushButton:pressed {\\n\"\n\"background: qradialgradient(cx: 0.4, cy: -0.1,\\n\"\n\"fx: 0.4, fy: -0.1,\\n\"\n\"radius: 1.35, stop: 0 #fff, stop: 1 #eee);}\"))\n        icon3 = QtGui.QIcon()\n        icon3.addPixmap(QtGui.QPixmap(_fromUtf8(\":/icons/refresh\")), QtGui.QIcon.Normal, QtGui.QIcon.Off)\n        self.refresh_pushButton.setIcon(icon3)\n        self.refresh_pushButton.setIconSize(QtCore.QSize(20, 20))\n        self.refresh_pushButton.setFlat(False)\n        self.refresh_pushButton.setObjectName(_fromUtf8(\"refresh_pushButton\"))\n        self.horizontalLayout_4.addWidget(self.refresh_pushButton)\n        self.closeButton = QtGui.QPushButton(self.frame)\n        self.closeButton.setMinimumSize(QtCore.QSize(90, 0))\n        self.closeButton.setMaximumSize(QtCore.QSize(90, 16777215))\n        self.closeButton.setStyleSheet(_fromUtf8(\"QPushButton {\\n\"\n\"background-color: rgb(250, 250, 250);\\n\"\n\"color: #333;\\n\"\n\"border: 2px solid #555;\\n\"\n\"padding: 5px;}\\n\"\n\"QPushButton:hover {\\n\"\n\"background: qradialgradient(cx: 0.3, cy: -0.4,\\n\"\n\"fx: 0.3, fy: -0.4,\\n\"\n\"radius: 1.35, stop: 0 #fff, stop: 1 #ccc);}\\n\"\n\"QPushButton:pressed {\\n\"\n\"background: qradialgradient(cx: 0.4, cy: -0.1,\\n\"\n\"fx: 0.4, fy: -0.1,\\n\"\n\"radius: 1.35, stop: 0 #fff, stop: 1 #eee);}\"))\n        icon4 = QtGui.QIcon()\n        icon4.addPixmap(QtGui.QPixmap(_fromUtf8(\":/icons/exit\")), QtGui.QIcon.Normal, QtGui.QIcon.Off)\n        self.closeButton.setIcon(icon4)\n        self.closeButton.setIconSize(QtCore.QSize(20, 20))\n        self.closeButton.setFlat(False)\n        self.closeButton.setObjectName(_fromUtf8(\"closeButton\"))\n        self.horizontalLayout_4.addWidget(self.closeButton)\n        spacerItem = QtGui.QSpacerItem(34, 20, QtGui.QSizePolicy.Expanding, QtGui.QSizePolicy.Minimum)\n        self.horizontalLayout_4.addItem(spacerItem)\n        self.verticalLayout.addWidget(self.frame)\n        self.frame_3 = QtGui.QFrame(batchadjform)\n        self.frame_3.setFrameShape(QtGui.QFrame.StyledPanel)\n        self.frame_3.setFrameShadow(QtGui.QFrame.Raised)\n        self.frame_3.setObjectName(_fromUtf8(\"frame_3\"))\n        self.verticalLayout.addWidget(self.frame_3)\n        self.horizontalLayout = QtGui.QHBoxLayout()\n        self.horizontalLayout.setObjectName(_fromUtf8(\"horizontalLayout\"))\n        self.adj_tableView = QtGui.QTableView(batchadjform)\n        self.adj_tableView.setObjectName(_fromUtf8(\"adj_tableView\"))\n        self.horizontalLayout.addWidget(self.adj_tableView)\n        self.populate_pushButton = QtGui.QPushButton(batchadjform)\n        self.populate_pushButton.setText(_fromUtf8(\"\"))\n        icon5 = QtGui.QIcon()\n        icon5.addPixmap(QtGui.QPixmap(_fromUtf8(\":/icons/arrow-l\")), QtGui.QIcon.Normal, QtGui.QIcon.Off)\n        self.populate_pushButton.setIcon(icon5)\n        self.populate_pushButton.setIconSize(QtCore.QSize(50, 50))\n        self.populate_pushButton.setFlat(True)\n        self.populate_pushButton.setObjectName(_fromUtf8(\"populate_pushButton\"))\n        self.horizontalLayout.addWidget(self.populate_pushButton)\n        self.batchList_tableView = QtGui.QTableView(batchadjform)\n        self.batchList_tableView.setObjectName(_fromUtf8(\"batchList_tableView\"))\n        self.batchList_tableView.verticalHeader().setVisible(False)\n        self.horizontalLayout.addWidget(self.batchList_tableView)\n        self.verticalLayout.addLayout(self.horizontalLayout)\n\n        self.retranslateUi(batchadjform)\n        QtCore.QMetaObject.connectSlotsByName(batchadjform)\n\n    def retranslateUi(self, batchadjform):\n        batchadjform.setWindowTitle(_translate(\"batchadjform\", \"Batch Adjustment\", None))\n        self.label_2.setText(_translate(\"batchadjform\", \"Adjust Total Raw Materials Used:\", None))\n        self.saveButton.setText(_translate(\"batchadjform\", \"&Save\", None))\n        self.clearButton.setText(_translate(\"batchadjform\", \"Clear\", None))\n        self.refresh_pushButton.setText(_translate(\"batchadjform\", \"&Refresh\", None))\n        self.closeButton.setText(_translate(\"batchadjform\", \"Close\", None))\n\nimport images_rc\n", "iterals\n\nfrom . import generate_request_id, local, release_local\nfrom .conf import REQUEST_ID_HEADER\n\n\ndef get_request_id(request):\n    if hasattr(request, 'request_id'):\n        return request.request_id\n    elif REQUEST_ID_HEADER:\n        return request.META.get(REQUEST_ID_HEADER, '')\n    else:\n        return generate_request_id()\n\n\nclass RequestIdMiddleware(object):\n    def __init__(self, get_response=None):\n        self.get_response = get_response\n\n    def __call__(self, request):\n        request_id = get_request_id(request)\n        request.request_id = request_id\n        local.request_id = request_id\n\n        response = self.get_response(request)\n\n        release_local(local)\n\n        return response\n\n    # Compatibility methods for Django <1.10\n    def process_request(self, request):\n        request_id = get_request_id(request)\n        request.request_id = request_id\n        local.request_id = request_id\n\n    def process_response(self, request, response):\n        release_local(local)\n        return response\n", " return True\n    elif n % 2 == 0:\n        return False\n    for i in xrange(3, int(sqrt(n))+1, 2):\n        if n % i == 0:\n            return False\n    return True\n", "rgs):\n    if args:\n        trains_data_file = args[0]\n    else:\n        trains_data_file = \"trains.json\"\n    with open(trains_data_file) as f:\n        trains_data = f.read()\n    \n    if using_flask:\n        from train_data_service_flask import start\n    else:\n        from train_data_service_cherrypy import start\n    start(trains_data)\n        \n\nif __name__ == '__main__':\n        import sys\n        help_text = \"\"\" \n    Use this program to start a train data service:\n\n        python train_data_service.py\n\n    It will start a service on:\n\n        http://localhost:8081/data_for_train\n\n    You can pass on the command line the name of the json file to use as a data source. \n    It defaults to looking for \"trains.json\" in the current working directory.\n\n        python {0} trains.json\n        \"\"\".format(sys.argv[0])\n        if \"-help\" in sys.argv or \"--help\" in sys.argv or \"-h\" in sys.argv:\n            print(help_text)\n        else:\n            main(sys.argv[1:])\n\n", "lf, trip):\n        return trip.trip_short_name\n", "onse\n", "017-08-02 08:23:55Z aokada $\n\"\"\"\n\nfrom . import tools\n\ndef load_potisions(mode, config):\n\n    [section_in, section_out] = tools.get_section(mode)\n    header = tools.config_getboolean(config, section_in, \"header\")\n    \n    must = {}\n    opts = {}\n    for option in config.options(section_in):\n        param = \"\"\n        if option.startswith(\"col_\"):\n            param = option.replace(\"col_\", \"\")\n        \n        if len(param) > 0:\n            value = config.get(section_in, option)\n            if value == \"\":\n                continue\n            \n            if param.startswith(\"opt_\"):\n                if header == True:\n                    opts[param.replace(\"opt_\", \"\")] = value\n                else:\n                    opts[param.replace(\"opt_\", \"\")] = config.getint(section_in, option)\n            else:\n                if header == True:\n                    must[param] = value\n                else:\n                    must[param] = config.getint(section_in, option)\n                \n    return {\"must\": must, \"option\": opts}\n    \ndef _load_option(mode, config):\n\n    [section_in, section_out] = tools.get_section(mode)\n    \n    # data read\n    header = config.getboolean(section_in, \"header\")\n    if header < 0:\n        header = 0\n    sept = config.get(section_in, \"sept\").replace(\"\\\\t\", \"\\t\").replace(\"\\\\n\", \"\\n\").replace(\"\\\\r\", \"\\r\")\n    comment = tools.config_getstr(config, section_in, \"comment\")\n    lack = tools.config_getstr(config, section_out, \"lack_column_complement\")\n    suffix = tools.config_getstr(config, section_in, \"suffix\")\n    suffix_filt = tools.config_getstr(config, section_in, \"suffix_filt\")\n    \n    # return option dict\n    return {\"header\": header, \"sept\": sept, \"comment\": comment, \"lack\": lack, \"suffix\": suffix, \"suffix_filt\": suffix_filt}\n    \ndef _merge_metadata(files, option):\n\n    import os\n\n    # read all file's meta-data\n    meta_data = []\n    headers = []\n    for file_path in files:\n        if len(option[\"comment\"]) == 0:\n            break\n        \n        for line in open(file_path):\n            line = line.rstrip(\"\\r\\n\")\n            if len(line) == 0:\n                continue\n            \n            if line.find(option[\"comment\"]) == 0:\n                data = line.split(\":\")\n                if len(data) < 2:\n                    continue\n                \n                meta_data.append([data[0].replace(\" \", \"\"), data[1].strip(), file_path])\n                headers.append(data[0].replace(\" \", \"\"))\n            else:\n                break\n    \n    # merge meta-data\n    headers = list(set(headers))\n    headers.sort()\n    meta_text = \"\"\n    for header in headers:\n        values = {}\n        for meta in meta_data:\n            if meta[0] == header:\n                if (meta[1] in values) == False:\n                    values[meta[1]] = []\n                values[meta[1]].append(meta[2])\n                \n        for key in values:\n            meta_text += header + \":\" + key\n            if len(values[key]) != len(files):\n                f_text = \"\"\n                for f in values[key]:\n                    if len(f_text) > 0:\n                        f_text += \";\"\n                    f_text += os.path.basename(f).replace(option[\"suffix\"], \"\").replace(option[\"suffix_filt\"], \"\")\n                meta_text += \":\" + f_text\n            meta_text += \"\\n\"\n    \n    return meta_text\n\ndef _merge_title(files, mode, option, config):\n\n    if option[\"header\"] == False:\n        return []\n        \n    # titles\n    merged_title = []\n    for file_path in files:\n        title = []\n        for line in open(file_path):\n            \n            line = line.rstrip(\"\\r\\n\")\n            if len(line.replace(option[\"sept\"], \"\")) == 0:\n                continue\n            \n            if len(option[\"comment\"]) > 0 and line.find(option[\"comment\"]) == 0:\n                continue\n                \n            title = line.split(option[\"sept\"])\n            break\n        \n        for col in title:\n            if (col in merged_title) == False:\n                merged_title.append(col)\n    \n    return merged_title.sort()\n\ndef merge_result(files, ids, output_file, mode, config, extract = False):\n\n    [section_in, section_out] = tools.get_section(mode)\n    \n    if tools.config_getboolean(config, section_in, \"header\") == True:\n        return with_header(files, ids, output_file, mode, config, extract)\n    else:\n        return with_noheader(files, ids, output_file, mode, config, extract)\n    \ndef with_header(files, ids, output_file, mode, config, extract = False):\n\n    def calc_map(header, all_header):\n        mapper = [-1]*len(all_header)\n        for i in range(len(all_header)):\n            if all_header[i] in header:\n                mapper[i] = header.index(all_header[i])\n            else:\n                mapper[i] = -1\n        return mapper\n        \n    import os\n\n    if len(files) == 0:\n        return {}\n        \n    for file_path in files:\n        if os.path.exists(file_path) == False:\n            print (\"[ERROR] file is not exist. %s\" % file_path)\n            files.remove(file_path)\n            continue \n    \n    option = _load_option(mode, config)\n    if option[\"header\"] == False:\n        print (\"[ERROR] header is necessary for this function.\")\n        return {}\n        \n    meta_text = _merge_metadata(files, option)\n\n    positions = load_potisions(mode, config)\n    \n    if extract == False:\n        titles = _merge_title(files, mode, option, config)\n    else:\n        titles = []\n        for key in tools.dict_keys(positions[\"must\"]):\n            titles.append(positions[\"must\"][key])\n\n        for key in tools.dict_keys(positions[\"option\"]):\n            titles.append(positions[\"option\"][key])\n\n    # update positions to merged title\n    if (\"id\" in positions[\"option\"]) == False:\n        positions[\"option\"][\"id\"] = \"id\"\n    if (positions[\"option\"][\"id\"] in titles) == False:\n        titles.insert(0, positions[\"option\"][\"id\"])\n    \n    # write meta-data to file\n    f = open(output_file + \".tmp\", mode = \"w\")\n    f.write(meta_text)\n    f.write(option[\"sept\"].join(titles))\n    f.write(\"\\n\")\n    \n    for idx in range(len(files)):\n        file_path = files[idx]\n        \n        header = []\n        mapper = []\n        lines = []\n        lines_count = 0\n        for line in open(file_path):\n            line = line.rstrip(\"\\r\\n\")\n            if len(line.replace(option[\"sept\"], \"\")) == 0:\n                continue\n            \n            if len(option[\"comment\"]) > 0 and line.find(option[\"comment\"]) == 0:\n                continue\n            \n            if len(header) == 0:\n                header = line.split(option[\"sept\"])\n                mapper = calc_map(header, titles)\n                continue\n            \n            data = line.split(option[\"sept\"])\n            sort_data = []\n            for i in range(len(titles)):\n                if mapper[i] < 0:\n                    if titles[i] == positions[\"option\"][\"id\"]:\n                        sort_data.append(ids[idx])\n                    else:\n                        sort_data.append(option[\"lack\"])\n                else:\n                    sort_data.append(data[mapper[i]])\n            \n            lines.append(option[\"sept\"].join(sort_data) + \"\\n\")\n            lines_count += 1\n            \n            if (lines_count > 10000):\n                f.writelines(lines)\n                lines = []\n                lines_count = 0\n\n        if (lines_count > 0):\n            f.writelines(lines)\n        \n    f.close()\n\n    if os.path.exists(output_file):\n        os.remove(output_file)\n        \n    os.rename(output_file + \".tmp\", output_file)\n    return positions\n\ndef with_noheader(files, ids, output_file, mode, config, extract = False):\n\n    import os\n\n    if len(files) == 0:\n        return {}\n        \n    for file_path in files:\n        if os.path.exists(file_path) == False:\n            print (\"[ERROR] file is not exist. %s\" % file_path)\n            files.remove(file_path)\n            continue \n    \n    option = _load_option(mode, config)\n    if option[\"header\"] == True:\n        print (\"[ERROR] this is function for no-header data.\")\n        return {}\n        \n    meta_text = _merge_metadata(files, option)\n    positions = load_potisions(mode, config)\n\n    usecols = []\n    for key in positions[\"must\"]:\n        usecols.append(positions[\"must\"][key])\n            \n    for key in positions[\"option\"]:\n        usecols.append(positions[\"option\"][key])\n    \n    add_id = False\n    if (\"id\" in positions[\"option\"]) == False:\n        add_id = True\n    \n    \n    # write meta-data to file\n    f = open(output_file + \".tmp\", mode = \"w\")\n    f.write(meta_text)\n    \n    titles = []\n    for idx in range(len(files)):\n        file_path = files[idx]\n\n        lines = []\n        lines_count = 0\n        for line in open(file_path):\n            line = line.rstrip(\"\\r\\n\")\n            if len(line.replace(option[\"sept\"], \"\")) == 0:\n                continue\n            \n            if len(option[\"comment\"]) > 0 and line.find(option[\"comment\"]) == 0:\n                continue\n            \n            data = line.split(option[\"sept\"])\n            \n            # header\n            if titles == []:\n                if add_id == True:\n                    titles.append(\"id\")\n                    \n                for i in range(1, len(data)+1):\n                    if extract == True:\n                        if i in usecols:\n                            titles.append(\"v%d\" % i)\n                    else:\n                        titles.append(\"v%d\" % i)\n\n                lines.append(option[\"sept\"].join(titles) + \"\\n\")\n                \n            # add id\n            cat_data = []\n            if add_id == True:\n                cat_data.append(ids[idx])\n                \n            for i in range(1, len(data)+1):\n                if extract == True:\n                    if i in usecols:\n                        cat_data.append(data[i-1])\n                else:\n                    cat_data.append(data[i-1])\n            \n            lines.append(option[\"sept\"].join(cat_data) + \"\\n\")\n            lines_count += 1\n            \n            if (lines_count > 10000):\n                f.writelines(lines)\n                lines = []\n                lines_count = 0\n\n        if (lines_count > 0):\n            f.writelines(lines)\n        \n    f.close()\n    if os.path.exists(output_file):\n        os.remove(output_file)\n    os.rename(output_file + \".tmp\", output_file)\n    \n    # update positions\n    for key in positions[\"must\"]:\n        positions[\"must\"][key] = \"v%d\" % (positions[\"must\"][key])\n            \n    for key in positions[\"option\"]:\n        positions[\"option\"][key] = \"v%d\" % (positions[\"option\"][key])\n        \n    if (\"id\" in positions[\"option\"]) == False:\n        positions[\"option\"][\"id\"] = \"id\"\n\n    return positions\n\ndef position_to_dict(position):\n    di = {}\n    for key in position[\"must\"]:\n        di[key] = position[\"must\"][key]\n    for key in position[\"option\"]:\n        di[key] = position[\"option\"][key]\n    return di\n\n", "9 2009-01-07 19:09:28Z dkuhlman $\n# Author: Dave Kuhlman <dkuhlman@rexx.com>\n# Copyright: This module has been placed in the public domain.\n\n\"\"\"\nFix a word-processor-generated styles.odt for odtwriter use: Drop page size\nspecifications from styles.xml in STYLE_FILE.odt.\n\"\"\"\n\n#\n# Author: Michael Schutte <michi@uiae.at>\n\nfrom lxml import etree\nimport sys\nimport zipfile\nfrom tempfile import mkstemp\nimport shutil\nimport os\n\nNAMESPACES = {\n    \"style\": \"urn:oasis:names:tc:opendocument:xmlns:style:1.0\",\n    \"fo\": \"urn:oasis:names:tc:opendocument:xmlns:xsl-fo-compatible:1.0\"\n}\n\ndef prepstyle(filename):\n    \n    zin = zipfile.ZipFile(filename)\n    styles = zin.read(\"styles.xml\")\n    \n    root = etree.fromstring(styles)\n    for el in root.xpath(\"//style:page-layout-properties\", \n        namespaces=NAMESPACES):\n        for attr in el.attrib:\n            if attr.startswith(\"{%s}\" % NAMESPACES[\"fo\"]):\n                del el.attrib[attr]\n    \n    tempname = mkstemp()\n    zout = zipfile.ZipFile(os.fdopen(tempname[0], \"w\"), \"w\",\n        zipfile.ZIP_DEFLATED)\n    \n    for item in zin.infolist():\n        if item.filename == \"styles.xml\":\n            zout.writestr(item, etree.tostring(root))\n        else:\n            zout.writestr(item, zin.read(item.filename))\n    \n    zout.close()\n    zin.close()\n    shutil.move(tempname[1], filename)\n\n\ndef main():\n    args = sys.argv[1:]\n    if len(args) != 1:\n        print >> sys.stderr, __doc__\n        print >> sys.stderr, \"Usage: %s STYLE_FILE.odt\\n\" % sys.argv[0]\n        sys.exit(1)\n    filename = args[0]\n    prepstyle(filename)\n\nif __name__ == '__main__':\n    main()\n\n\n# vim:tw=78:sw=4:sts=4:et:\n", "tput.Output import Output\n\nclass Test(Output):\n    def emit_output(self, data):\n        self.data = data\n", "oes only contain a selection of the most common options. For a\n# full list see the documentation:\n# http://www.sphinx-doc.org/en/stable/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\nimport sphinx_bootstrap_theme\n\n\n# sys.path.insert(0, os.path.abspath('.'))\nsys.path.insert(0, '/home/eyurtsev/src/FlowCytometryTools')\n\n\n# -- Project information -----------------------------------------------------\n\nproject = u'FlowCytometryTools'\ncopyright = u'2018, Eugene Yurtsev and Jonathan Friedman'\nauthor = u'Eugene Yurtsev and Jonathan Friedman'\n\nimport FlowCytometryTools\n#\n# # The short X.Y version.\nversion = '%s' % (FlowCytometryTools.__version__)\n# The full version, including alpha/beta/rc tags.\nrelease = version\n\n## Google analytics\ngoogleanalytics_id = 'UA-45363835-3'\ngoogleanalytics_enabled = True\n\n# -- General configuration ---------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = '1.0'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n# ones.\nextensions = [\n    ##\n    'IPython.sphinxext.ipython_console_highlighting',\n    'IPython.sphinxext.ipython_directive',\n    'matplotlib.sphinxext.only_directives',\n    'matplotlib.sphinxext.plot_directive',\n    # On newer versions\n    'sphinx.ext.autodoc',\n    'sphinx.ext.autosummary',\n    'sphinx.ext.coverage',\n    'sphinx.ext.coverage',\n    'sphinx.ext.doctest',\n    'sphinx.ext.githubpages',\n    'sphinx.ext.ifconfig',\n    # 'sphinx.ext.linkcode',\n    'sphinx.ext.mathjax',\n    'sphinx.ext.todo',\n    'sphinx.ext.viewcode',\n]\n\nautosummary_generate = True\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = ['_templates', '_templates/autosummary']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = ['.rst', '.md']\nsource_suffix = '.rst'\n\n# The master toctree document.\nmaster_doc = 'index'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set \"language\" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path .\nexclude_patterns = []\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = 'sphinx'\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#html_theme = 'default'\n\n\nhtml_theme = 'bootstrap'\nhtml_theme_path = sphinx_bootstrap_theme.get_html_theme_path()\n\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#html_theme_options = {}\n\n# Theme options are theme-specific and customize the look and feel of a\n# theme further.\nhtml_theme_options = {\n    # Navigation bar title. (Default: ``project`` value)\n    #'navbar_title': \"Demo\",\n\n    # Tab name for entire site. (Default: \"Site\")\n    #'navbar_site_name': \"Site\",\n\n    # A list of tuples containing pages or urls to link to.\n    # Valid tuples should be in the following forms:\n    #    (name, page)                 # a link to a page\n    #    (name, \"/aa/bb\", 1)          # a link to an arbitrary relative url\n    #    (name, \"http://example.com\", True) # arbitrary absolute url\n    # Note the \"1\" or \"True\" value above as the third argument to indicate\n    # an arbitrary url.\n    'navbar_links': [\n        (\"Gallery\", 'gallery'),\n        (\"Install\", 'install'),\n        (\"Tutorial\", 'tutorial'),\n        (\"API\", \"api\"),\n    ],\n\n    # Render the next and previous page links in navbar. (Default: true)\n    'navbar_sidebarrel': False,\n\n    # Render the current pages TOC in the navbar. (Default: true)\n    'navbar_pagenav': True,\n\n    # Global TOC depth for \"site\" navbar tab. (Default: 1)\n    # Switching to -1 shows all levels.\n    'globaltoc_depth': -1,\n\n    # Include hidden TOCs in Site navbar?\n    #\n    # Note: If this is \"false\", you cannot have mixed ``:hidden:`` and\n    # non-hidden ``toctree`` directives in the same page, or else the build\n    # will break.\n    #\n    # Values: \"true\" (default) or \"false\"\n    'globaltoc_includehidden': \"true\",\n\n    # HTML navbar class (Default: \"navbar\") to attach to <div> element.\n    # For black navbar, do \"navbar navbar-inverse\"\n    #'navbar_class': \"navbar navbar-inverse\",\n    'navbar_class': \"navbar\",\n\n    # Fix navigation bar to top of page?\n    # Values: \"true\" (default) or \"false\"\n    'navbar_fixed_top': \"true\",\n\n    # Location of link to source.\n    # Options are \"nav\" (default), \"footer\" or anything else to exclude.\n    'source_link_position': \"nowhere\",\n\n    # Bootswatch (http://bootswatch.com/) theme.\n    #\n    # Options are nothing with \"\" (default) or the name of a valid theme\n    # such as \"amelia\" or \"cosmo\".\n    #\n    # Note that this is served off CDN, so won't be available offline.\n    'bootswatch_theme': \"yeti\",\n    #'bootswatch_theme': \"flatly\",\n    #'bootswatch_theme': \"united\",\n    #'bootswatch_theme': \"spacelab\",\n    #'bootswatch_theme' : 'simplex',\n\n    # Choose Bootstrap version.\n    # Values: \"3\" (default) or \"2\" (in quotes)\n    'bootstrap_version': \"3\",\n}\n\n# Add any paths that contain custom themes here, relative to this directory.\n#html_theme_path = []\n\n# The name for this set of Sphinx documents.  If None, it defaults to\n# \"<project> v<release> documentation\".\n#html_title = None\n\n# A shorter title for the navigation bar.  Default is the same as html_title.\n#html_short_title = None\n\n# The name of an image file (relative to this directory) to place at the top\n# of the sidebar.\n#html_logo = None\n\n# The name of an image file (within the static path) to use as favicon of the\n# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n# pixels large.\n#html_favicon = None\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\nhtml_static_path = ['_static']\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# The default sidebars (for documents that don't match any pattern) are\n# defined by theme itself.  Builtin themes are using these templates by\n# default: ``['localtoc.html', 'relations.html', 'sourcelink.html',\n# 'searchbox.html']``.\n#\ncustom_docs = ['localtoc.html']\nhtml_sidebars = {'tutorial': custom_docs, 'api' : custom_docs, 'gallery' : custom_docs}\n\n\n# -- Options for HTMLHelp output ---------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = 'FlowCytometryToolsdoc'\n\n# -- Options for LaTeX output ------------------------------------------------\n\nlatex_elements = {\n    # The paper size ('letterpaper' or 'a4paper').\n    #\n    # 'papersize': 'letterpaper',\n\n    # The font size ('10pt', '11pt' or '12pt').\n    #\n    # 'pointsize': '10pt',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # 'preamble': '',\n\n    # Latex figure (float) alignment\n    #\n    # 'figure_align': 'htbp',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, 'FlowCytometryTools.tex', u'FlowCytometryTools Documentation',\n     u'Eugene Yurtsev and Jonathan Friedman', 'manual'),\n]\n\n# -- Options for manual page output ------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, 'flowcytometrytools', u'FlowCytometryTools Documentation',\n     [author], 1)\n]\n\n# -- Options for Texinfo output ----------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, 'FlowCytometryTools', u'FlowCytometryTools Documentation',\n     author, 'FlowCytometryTools', 'One line description of project.',\n     'Miscellaneous'),\n]\n\n\n# -- Extension configuration -------------------------------------------------\n", "ss TestMyViewSuccessCondition(unittest.TestCase):\n    def setUp(self):\n        self.config = testing.setUp()\n        from sqlalchemy import create_engine\n        engine = create_engine('sqlite://')\n        from .models import (\n            Base,\n            MyModel,\n            )\n        DBSession.configure(bind=engine)\n        Base.metadata.create_all(engine)\n        with transaction.manager:\n            model = MyModel(name='one', value=55)\n            DBSession.add(model)\n\n    def tearDown(self):\n        DBSession.remove()\n        testing.tearDown()\n\n    def test_passing_view(self):\n        from .views import my_view\n        request = testing.DummyRequest()\n        info = my_view(request)\n        self.assertEqual(info['one'].name, 'one')\n        self.assertEqual(info['project'], 'scrom')\n\n\nclass TestMyViewFailureCondition(unittest.TestCase):\n    def setUp(self):\n        self.config = testing.setUp()\n        from sqlalchemy import create_engine\n        engine = create_engine('sqlite://')\n        from .models import (\n            Base,\n            MyModel,\n            )\n        DBSession.configure(bind=engine)\n\n    def tearDown(self):\n        DBSession.remove()\n        testing.tearDown()\n\n    def test_failing_view(self):\n        from .views import my_view\n        request = testing.DummyRequest()\n        info = my_view(request)\n        self.assertEqual(info.status_int, 500)", " 0\n    while num > 0:\n        ans += (num % 10) ** 2\n        num //= 10\n    return ans\n\n\ndef set_all(ends, members, value):\n    for j in members:\n        ends[j] = value\n\n\ndef prob_092():\n    # TODO Maybe Optimize - 53.801 sec\n    ends = [0] * 10000001\n    for temp in range(1, 10000000):\n        if ends[temp]:\n            continue\n        members = {temp}\n        while (temp - 89) and (temp - 1):\n            temp = sum_of_squares_of_digits(temp)\n            members.add(temp)\n            if ends[temp]:\n                set_all(ends, members, ends[temp])\n                break\n        else:\n            set_all(ends, members, temp)\n\n    return Counter(ends)[89]\n\n\nif __name__ == \"__main__\":\n    from project_euler import common\n\n    common.run_all(__name__)", "rom mqueue.admin import link_to_object, link_to_object_admin, MEventAdmin\n\n\nclass MqueueTestAdmin(MqueueBaseTest):\n    def test_admin(self):\n        instance, _ = MEvent.objects.get_or_create(\n            name=\"Event name\", url=\"http://url\", admin_url=\"http://admin_url\"\n        )\n        res = link_to_object(instance)\n        link = '<a href=\"http://url\" target=\"_blank\">http://url</a>'\n        self.assertEqual(link, res)\n        res = link_to_object_admin(instance)\n        link = '<a href=\"http://admin_url\" target=\"_blank\">http://admin_url</a>'\n        self.assertEqual(link, res)\n        request = self.factory.get(\"/\")\n\n        class TestAdminSite(admin.AdminSite):\n            pass\n\n        adm = MEventAdmin(MEvent, TestAdminSite)\n        res = adm.get_readonly_fields(request)\n        self.assertEqual(res, (\"notes\", \"request\"))\n", "ANGO_SETTINGS_MODULE\", \"jobsboard.settings\")\n\n    from django.core.management import execute_from_command_line\n\n    execute_from_command_line(sys.argv)\n", "a fn: \\\n        next((i for i, tn in enumerate(TABLE_NAMES) if tn in fn), None)\n    table_files = sorted(args[1:], key=index_of)\n\n    tables = {}\n    for i, table_file in enumerate(table_files):\n        curr_table = {}\n\n        with open(table_file, mode='r') as f:\n            for line in f:\n                tokens = line.split()\n                if tokens[0] == '-P':\n                    curr_table[tokens[1]] = {\n                            'default_policy': tokens[2],\n                            'rules': []\n                    }\n                elif tokens[0] == '-N':\n                    curr_table[tokens[1]] = {\n                            'rules': []\n                    }\n                elif tokens[0] == '-A':\n                    curr_table[tokens[1]]['rules'].append(' '.join(tokens[2:]))\n                else:\n                    raise Exception('Unknown iptables command!')\n        tables[TABLE_NAMES[i]] = curr_table\n\n    with open('iptables.out', mode='w') as f:\n        for table_name, chains in tables.items():\n            print('\\n<<{}>>'.format(table_name), file=f)\n            for chain_name, chain_data in chains.items():\n                if 'default_policy' in chain_data:\n                    print('\\t<{}:{}>'.format(chain_name,\n                                           chain_data['default_policy']),\n                          file=f)\n                else:\n                    print('\\t<{}>'.format(chain_name), file=f)\n                for rule in chain_data['rules']:\n                    print('\\t\\t' + rule, file=f)\n\n\nif __name__ == '__main__':\n    sys.exit(main(sys.argv))\n", "ght (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n#\n# Code generated by Microsoft (R) AutoRest Code Generator.\n# Changes may cause incorrect behavior and will be lost if the code is\n# regenerated.\n# --------------------------------------------------------------------------\n\nfrom .proxy_only_resource import ProxyOnlyResource\n\n\nclass VnetParameters(ProxyOnlyResource):\n    \"\"\"The required set of inputs to validate a VNET.\n\n    Variables are only populated by the server, and will be ignored when\n    sending a request.\n\n    :ivar id: Resource Id.\n    :vartype id: str\n    :ivar name: Resource Name.\n    :vartype name: str\n    :param kind: Kind of resource.\n    :type kind: str\n    :ivar type: Resource type.\n    :vartype type: str\n    :param vnet_resource_group: The Resource Group of the VNET to be validated\n    :type vnet_resource_group: str\n    :param vnet_name: The name of the VNET to be validated\n    :type vnet_name: str\n    :param vnet_subnet_name: The subnet name to be validated\n    :type vnet_subnet_name: str\n    \"\"\"\n\n    _validation = {\n        'id': {'readonly': True},\n        'name': {'readonly': True},\n        'type': {'readonly': True},\n    }\n\n    _attribute_map = {\n        'id': {'key': 'id', 'type': 'str'},\n        'name': {'key': 'name', 'type': 'str'},\n        'kind': {'key': 'kind', 'type': 'str'},\n        'type': {'key': 'type', 'type': 'str'},\n        'vnet_resource_group': {'key': 'properties.vnetResourceGroup', 'type': 'str'},\n        'vnet_name': {'key': 'properties.vnetName', 'type': 'str'},\n        'vnet_subnet_name': {'key': 'properties.vnetSubnetName', 'type': 'str'},\n    }\n\n    def __init__(self, kind=None, vnet_resource_group=None, vnet_name=None, vnet_subnet_name=None):\n        super(VnetParameters, self).__init__(kind=kind)\n        self.vnet_resource_group = vnet_resource_group\n        self.vnet_name = vnet_name\n        self.vnet_subnet_name = vnet_subnet_name\n", "q, open_accordion_section\nfrom ..utils.i_selenium import assert_tab, image_div\nfrom ..utils.i_selenium import wait_for_xpath_element\nfrom ..tests import TestWithDependency\nfrom selenium.common.exceptions import TimeoutException, NoSuchElementException\n\n__all__ = [\"numeric_q_all_incorrect\"]\n\n\n#####\n# Test : Numeric Questions Incorrect Value, Incorrect Unit\n#####\n@TestWithDependency(\"NUMERIC_Q_ALL_INCORRECT\", [\"NUMERIC_Q_ANSWER_CHANGE\"])\ndef numeric_q_all_incorrect(driver, ISAAC_WEB, WAIT_DUR, **kwargs):\n    \"\"\"Test numeric question behaviour on incorrect value and units.\n\n        - 'driver' should be a Selenium WebDriver.\n        - 'ISAAC_WEB' is the string URL of the Isaac website to be tested.\n        - 'WAIT_DUR' is the time in seconds to wait for JavaScript to run/load.\n    \"\"\"\n    assert_tab(driver, ISAAC_WEB + \"/questions/_regression_test_\")\n    time.sleep(WAIT_DUR)\n    try:\n        open_accordion_section(driver, 3)\n        num_question = driver.find_element_by_xpath(\"//div[@ng-switch-when='isaacNumericQuestion']\")\n    except NoSuchElementException:\n        log(ERROR, \"Can't find the numeric question; can't continue!\")\n        return False\n\n    log(INFO, \"Attempt to enter unknown incorrect value, to correct sig figs and incorrect units.\")\n    if not answer_numeric_q(num_question, \"4.33\", \"\\units{ m\\,s^{-1} }\", get_unit_wrong=True, wait_dur=WAIT_DUR):\n        log(ERROR, \"Couldn't answer Numeric Question; can't continue!\")\n        return False\n    time.sleep(WAIT_DUR)\n\n    try:\n        wait_for_xpath_element(driver, \"//div[@ng-switch-when='isaacNumericQuestion']//h2[text()='Incorrect']\")\n        log(INFO, \"An 'Incorrect' message was displayed as expected.\")\n        wait_for_xpath_element(driver, \"(//div[@ng-switch-when='isaacNumericQuestion']//p[text()='Check your working.'])[1]\")\n        log(INFO, \"The 'Check your working.' message was correctly shown.\")\n        wait_for_xpath_element(driver, \"//div[@ng-switch-when='isaacNumericQuestion']//h5[text()='Please try again.']\")\n        log(INFO, \"The 'Please try again.' message was correctly shown.\")\n        bg_colour1 = num_question.find_element_by_xpath(\"(.//div[@class='ru-answer-block-panel'])[1]\").value_of_css_property('background-color')\n        assert bg_colour1 in ['#be4c4c', 'rgba(190, 76, 76, 1)', 'rgb(190, 76, 76)']\n        log(INFO, \"Red highlighting shown around value box.\")\n        bg_colour2 = num_question.find_element_by_xpath(\"(.//div[@class='ru-answer-block-panel'])[2]\").value_of_css_property('background-color')\n        assert bg_colour2 in ['#be4c4c', 'rgba(190, 76, 76, 1)', 'rgb(190, 76, 76)']\n        log(INFO, \"Red highlighting shown around units box.\")\n        log(INFO, \"Avoid rate limiting: wait 1 minute.\")\n        time.sleep(60)\n        log(PASS, \"Numeric Question 'incorrect value, incorrect unit' behavior as expected.\")\n        return True\n    except TimeoutException:\n        image_div(driver, \"ERROR_numeric_q_all_incorrect\")\n        log(ERROR, \"The messages shown for an incorrect answer were not all displayed; see 'ERROR_numeric_q_all_incorrect.png'!\")\n        return False\n    except AssertionError:\n        image_div(driver, \"ERROR_numeric_q_all_incorrect\")\n        log(ERROR, \"The answer boxes were not highlighted red correctly; see 'ERROR_numeric_q_all_incorrect.png'!\")\n        return False\n", "======================+START SETUP+=============================\nMOTOR_L_F = 35\nMOTOR_L_B = 36\nMOTOR_R_F = 37\nMOTOR_R_B = 38\nLED_TEST = 3\n\npwmPins = {}\n\ndef setup():\n    print(\"-------------------------_SETTING UP GPIO_----------------------------\")\n    GPIO.setmode(GPIO.BOARD)\n\n    GPIO.setup(MOTOR_L_F, GPIO.OUT)\n    GPIO.setup(MOTOR_L_B, GPIO.OUT)\n    GPIO.setup(MOTOR_R_F, GPIO.OUT)\n    GPIO.setup(MOTOR_R_B, GPIO.OUT)\n    GPIO.setup(LED_TEST, GPIO.OUT)\n    # GPIO.setup(butPin, GPIO.IN, pull_up_down=GPIO.PUD_UP) # Button pin set as input w/ pull-up\n\n    GPIO.output(MOTOR_L_F, False)\n    GPIO.output(MOTOR_L_B, False)\n    GPIO.output(MOTOR_R_F, False)\n    GPIO.output(MOTOR_R_B, False)\n    GPIO.output(LED_TEST, False)\n    pwmPins.clear()\n    return\n\nsetup()\n\n#===============================+END SETUP+=============================\ndef write(pin, state):\n    \"\"\"\n    Write output GPIO pin to state MAKE SURE pin is setup before use\n    @param int pin\n    @param bool state\n    \"\"\"\n    if( GPIO.getmode() == None ):\n        return False\n    # print(\"GPIO WRITE\", pin, state, gpioState)\n    gpioState = GPIO.HIGH if state == True or state == 1 or state == \"1\" else GPIO.LOW\n    if( pwmPins.get(pin) == None ):\n        GPIO.output(pin, gpioState)\n        # pwmPins.get(pin).stop()\n    else:\n        GPIO.output(pin, gpioState)\n    return\n\ndef pwmPinsUpdate(pin, freq, dutyCycle):\n    \"\"\"\n    Start pwmPins on pin\n    @param int pin\n    @param float freq\n        in Hz\n    @param float dutyCycle\n        (0.0 <= dc <= 100.0)\n    \"\"\"\n    # print(\"pwmPinsUpdate\", pin, freq, dutyCycle, pwmPins.get(pin))\n    if( pwmPins.get(pin) == None ):\n        pwmPins[pin] = GPIO.PWM(pin, freq)\n        pwmPins.get(pin).start(dutyCycle)\n    else:\n        pwmPins.get(pin).ChangeDutyCycle(dutyCycle)\n        pwmPins.get(pin).ChangeFrequency(freq)\n\n    return\n\ndef pwmPinsStop(pin):\n    \"\"\"\n    Stop pwmPins then remove from dictionary\n    @param int pin\n    \"\"\"\n    if( pwmPins.get(pin) ):\n        # print(\"STOPPING\", pin)\n        pwmPins.get(pin).stop()\n        del pwmPins[pin]\n    return\n\ndef reset():\n    GPIO.cleanup()\n    setup()\n    return\n\nimport atexit\n@atexit.register\ndef shutdown():\n    \"\"\"\n    Shutdown GPIO\n    \"\"\"\n    print(\"My shutdown GPIO\")\n    GPIO.cleanup()\n", "rt constants as c\nfrom tkmodels import RootFrame\nimport bs4\nimport sys\nimport os\n\nPLATFORM = os.name\nEXIT_CODE_OK = 0\nEXIT_CODE_ERROR = 1\n\n\nclass Collector():\n    def __init__(self, file, dir_target):\n        self.verify_file(file)\n        self.path_file = os.path.realpath(file)\n        self.file_name = os.path.basename(self.path_file)\n        self.dir_root = os.path.dirname(self.path_file)\n        self.dir_base = os.path.dirname(self.dir_root)\n        self.media_found = list()\n        self.media_lost = list()\n        self.dir_target = dir_target\n        self.accepted_media_exts = [\n            '.cda', '.ivf', '.aif', '.aifc', '.aiff' '.asf', '.asx', '.wax', '.wm',\n            '.wma', '.wmd', '.wmv', '.wvx', '.wmp', '.wmx', '.avi', '.wav',\n            '.mpeg', '.mpg', '.m1v', '.mp2', '.mpa', '.mpe', '.mp2v*', '.mpv2'\n            '.mid', '.midi', '.rmi', '.au',  '.snd', '.mp3', '.m3u', '.vob'\n        ]\n\n    @property\n    def raw_data(self):\n        try:\n            with open(self.path_file, 'r') as f:\n                return f.read().replace('\\n', '')\n        except IOError:\n            sys.exit(EXIT_CODE_ERROR)\n\n    @classmethod\n    def verify_file(cls, file):\n        if os.path.exists(file) & os.path.isfile(file):\n            return True\n        raise FileNotFoundError\n\n    def as_html(self):\n        raw_data = str(self.raw_data)\n\n        data_replacements = [\n            ('<?wpl version=\"1.0\"?>', '<!DOCTYPE HTML>'),\n            ('<sml>', '<html>'), ('</sml', '</html>'),\n            ('<seq>', '<li>'), ('</seq>', '</li>'),\n            ('<media ', '<source ')\n        ]\n\n        for replacement in data_replacements:\n            old, new = replacement\n            raw_data = raw_data.replace(old, new)\n\n        return raw_data\n\n    def write_to_file(self, data, file):\n        self.verify_file(file)\n        try:\n            with open(file, 'w') as f:\n                f.write(data)\n        except IOError:\n            sys.exit(EXIT_CODE_ERROR)\n\n    def get_source_file_paths(self):\n        html_data = self.as_html()\n        soup = bs4.BeautifulSoup(html_data, 'html.parser')\n        list_src = [str(media['src']).encode(c.ENCODING_WPL) for media in soup.find_all('source')]\n        return list_src\n\n    def gather_media_at(self, target_dir):\n        #TODO remove possibility of duplicate tracks\n        if not os.path.isdir(target_dir):\n            os.mkdir(target_dir)\n\n        for path in self.get_source_file_paths():\n            if os.path.exists(path):\n                self.media_found.append(path)\n                file_name_full = os.path.basename(str(path.decode('utf-8')))\n                file_name, ext = os.path.splitext(file_name_full)\n\n                if ext in self.accepted_media_exts:\n                    path_target_full = os.path.join(target_dir, file_name_full)\n                    copyfile(path, path_target_full)\n                else:\n                    self.media_lost.append(path)\n            else:\n                self.media_lost.append(path)\n\n    def collect(self):\n        print(\"collecting data from %s @ %s\" % self.file_name, self.dir_target)\n        self.gather_media_at(self.dir_target)\n        print(\"total media gathered: %s\" % len(self.media_found))\n        print(\"total media lost: %s\" % len(self.media_lost))\n        print(\"lost media: %s\" % self.media_lost)\n\n\ndef main(file_wpl, dir_target):\n    media = Collector(file_wpl, dir_target)\n    media.collect()\n\n\nif __name__ == '__main__':\n    #wpl = sys.argv[1]\n    #target_dir = sys.argv[2]\n    root = RootFrame(None)\n    root.mainloop()\n    #main(wpl, target_dir)", "   $ python download_dataset.py\n# quoc_trinh\n\nimport tensorflow as tf\n\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\n\nimport os\nimport sys\nimport datetime\n\n# get current file_name as [0] of array\nfile_name =  os.path.splitext(os.path.basename(sys.argv[0]))[0]\nprint(\" File Name:\")\nprint(file_name)\nprint(\"\")\n# FLAG to know that whether this is traning process or not.\nFLAG = 'train'\nPOOL_X = 16\nPOOL_Y = 18\nN_HIDDEN_CONFIG = 32\n\nsave_path_name =  file_name + \"/model.ckpt\"\n\nprint(datetime.datetime.now())\n# Write to file: time to start, type, time to end\nf = open(file_name + '/time.txt', 'a+')\nf.write(\"------------- \\n\")\nf.write(\"This is time \\n\")\nf.write(\"Started at \\n\")\nf.write(str(datetime.datetime.now())+'\\n')\n\nif __name__ == \"__main__\":\n\n    # -----------------------------\n    # step1: load and prepare data\n    # -----------------------------\n    # Those are separate normalised input features for the neural network\n    INPUT_SIGNAL_TYPES = [\n        \"body_acc_x_\",\n        \"body_acc_y_\",\n        \"body_acc_z_\",\n        \"body_gyro_x_\",\n        \"body_gyro_y_\",\n        \"body_gyro_z_\",\n        \"total_acc_x_\",\n        \"total_acc_y_\",\n        \"total_acc_z_\"\n    ]\n\n    # Output classes to learn how to classify\n    LABELS = [\n        \"WALKING\",\n        \"WALKING_UPSTAIRS\",\n        \"WALKING_DOWNSTAIRS\",\n        \"SITTING\",\n        \"STANDING\",\n        \"LAYING\"\n    ]\n\n    DATA_PATH = \"../data/\"\n    DATASET_PATH = DATA_PATH + \"UCI HAR Dataset/\"\n    print(\"\\n\" + \"Dataset is now located at: \" + DATASET_PATH)\n    # Preparing data set:\n    TRAIN = \"train/\"\n    TEST = \"test/\"\n\n    # Load \"X\" (the neural network's training and testing inputs)\n    def load_X(X_signals_paths):\n        X_signals = []\n\n        for signal_type_path in X_signals_paths:\n            file = open(signal_type_path, 'rb')\n            # Read dataset from disk, dealing with text files' syntax\n            X_signals.append(\n                [np.array(serie, dtype=np.float32) for serie in [\n                    row.replace('  ', ' ').strip().split(' ') for row in file\n                    ]]\n            )\n            file.close()\n\n        \"\"\"Examples\n        --------\n        >> > x = np.arange(4).reshape((2, 2))\n        >> > x\n        array([[0, 1],\n               [2, 3]])\n\n        >> > np.transpose(x)\n        array([[0, 2],\n               [1, 3]])\n\n        >> > x = np.ones((1, 2, 3))\n        >> > np.transpose(x, (1, 0, 2)).shape\n        (2, 1, 3)\n        \"\"\"\n\n        return np.transpose(np.array(X_signals), (1, 2, 0))\n\n    X_train_signals_paths = [\n        DATASET_PATH + TRAIN + \"Inertial Signals/\" + signal + \"train.txt\" for signal in INPUT_SIGNAL_TYPES\n        ]\n    X_test_signals_paths = [\n        DATASET_PATH + TEST + \"Inertial Signals/\" + signal + \"test.txt\" for signal in INPUT_SIGNAL_TYPES\n        ]\n    X_train = load_X(X_train_signals_paths)  # [7352, 128, 9]\n    X_test = load_X(X_test_signals_paths)    # [7352, 128, 9]\n\n    # print(X_train)\n    print(len(X_train))  # 7352\n    print(len(X_train[0]))  # 128\n    print(len(X_train[0][0]))  # 9\n\n    print(type(X_train))\n\n    X_train = np.reshape(X_train, [-1, 32, 36])\n    X_test = np.reshape(X_test, [-1, 32, 36])\n\n    print(\"-----------------X_train---------------\")\n    # print(X_train)\n    print(len(X_train))  # 7352\n    print(len(X_train[0]))  # 32\n    print(len(X_train[0][0]))  # 36\n\n    print(type(X_train))\n    # exit()\n\n    y_train_path = DATASET_PATH + TRAIN + \"y_train.txt\"\n    y_test_path = DATASET_PATH + TEST + \"y_test.txt\"\n\n    def one_hot(label):\n        \"\"\"convert label from dense to one hot\n          argument:\n            label: ndarray dense label ,shape: [sample_num,1]\n          return:\n            one_hot_label: ndarray  one hot, shape: [sample_num,n_class]\n        \"\"\"\n        label_num = len(label)\n        new_label = label.reshape(label_num)  # shape : [sample_num]\n        # because max is 5, and we will create 6 columns\n        n_values = np.max(new_label) + 1\n        return np.eye(n_values)[np.array(new_label, dtype=np.int32)]\n\n    # Load \"y\" (the neural network's training and testing outputs)\n    def load_y(y_path):\n        file = open(y_path, 'rb')\n        # Read dataset from disk, dealing with text file's syntax\n        y_ = np.array(\n            [elem for elem in [\n                row.replace('  ', ' ').strip().split(' ') for row in file\n                ]],\n            dtype=np.int32\n        )\n        file.close()\n        # Subtract 1 to each output class for friendly 0-based indexing\n        return y_ - 1\n\n\n    y_train = one_hot(load_y(y_train_path))\n    y_test = one_hot(load_y(y_test_path))\n\n    print(\"---------y_train----------\")\n    # print(y_train)\n    print(len(y_train))  # 7352\n    print(len(y_train[0]))  # 6\n\n    # -----------------------------------\n    # step2: define parameters for model\n    # -----------------------------------\n    class Config(object):\n        \"\"\"\n        define a class to store parameters,\n        the input should be feature mat of training and testing\n        \"\"\"\n\n        def __init__(self, X_train, X_test):\n            # Input data\n            self.train_count = len(X_train)  # 7352 training series\n            self.test_data_count = len(X_test)  # 2947 testing series\n            self.n_steps = len(X_train[0])  # 128 time_steps per series\n\n            # Training\n            self.learning_rate = 0.0025\n            self.lambda_loss_amount = 0.0015\n            self.training_epochs = 300\n            self.batch_size = 1000\n\n            # LSTM structure\n            self.n_inputs = len(X_train[0][0])  # Features count is of 9: three 3D sensors features over time\n            self.n_hidden = N_HIDDEN_CONFIG  # nb of neurons inside the neural network\n            self.n_classes = 6  # Final output classes\n            self.W = {\n                'hidden': tf.Variable(tf.random_normal([self.n_inputs, self.n_hidden])),  # [9, 32]\n                'output': tf.Variable(tf.random_normal([self.n_hidden, self.n_classes]))  # [32, 6]\n            }\n            self.biases = {\n                'hidden': tf.Variable(tf.random_normal([self.n_hidden], mean=1.0)),  # [32]\n                'output': tf.Variable(tf.random_normal([self.n_classes]))  # [6]\n            }\n\n    config = Config(X_train, X_test)\n    # print(\"Some useful info to get an insight on dataset's shape and normalisation:\")\n    # print(\"features shape, labels shape, each features mean, each features standard deviation\")\n    # print(X_test.shape, y_test.shape,\n    #       np.mean(X_test), np.std(X_test))\n    # print(\"the dataset is therefore properly normalised, as expected.\")\n    #\n    #\n    # ------------------------------------------------------\n    # step3: Let's get serious and build the neural network\n    # ------------------------------------------------------\n    # [none, 128, 9]\n    X = tf.placeholder(tf.float32, [None, config.n_steps, config.n_inputs])\n    # [none, 6]\n    Y = tf.placeholder(tf.float32, [None, config.n_classes])\n\n    print(\"-------X Y----------\")\n    print(X)\n    X = tf.reshape(X, shape=[-1, 32, 36])\n    print(X)\n\n    print(Y)\n    Y = tf.reshape(Y, shape=[-1, 6])\n    print(Y)\n\n    # Weight Initialization\n    def weight_variable(shape):\n        # tra ve 1 gia tri random theo thuat toan truncated_ normal\n        initial = tf.truncated_normal(shape, mean=0.0, stddev=0.1, dtype=tf.float32)\n        return tf.Variable(initial)\n\n    def bias_varibale(shape):\n        initial = tf.constant(0.1, shape=shape, name='Bias')\n        return tf.Variable(initial)\n\n    # Convolution and Pooling\n    def conv2d(x, W):\n        # Must have `strides[0] = strides[3] = 1 `.\n        # For the most common case of the same horizontal and vertices strides, `strides = [1, stride, stride, 1] `.\n        return tf.nn.conv2d(input=x, filter=W, strides=[1, 1, 1, 1], padding='SAME', name='conv_2d')\n\n    def max_pool_2x2(x):\n        return tf.nn.max_pool(value=x, ksize=[1, 2, 2, 1],\n                              strides=[1, 2, 2, 1], padding='SAME', name='max_pool')\n\n    def LSTM_Network(feature_mat, config):\n        \"\"\"model a LSTM Network,\n          it stacks 2 LSTM layers, each layer has n_hidden=32 cells\n           and 1 output layer, it is a full connet layer\n          argument:\n            feature_mat: ndarray feature matrix, shape=[batch_size,time_steps,n_inputs]\n            config: class containing config of network\n          return:\n                  : matrix  output shape [batch_size,n_classes]\n        \"\"\"\n\n        W_conv1 = weight_variable([3, 3, 1, 32])\n        b_conv1 = bias_varibale([32])\n        # x_image = tf.reshape(x, shape=[-1, 28, 28, 1])\n        feature_mat_image = tf.reshape(feature_mat, shape=[-1, 32, 36, 1])\n        print(\"----feature_mat_image-----\")\n        print(feature_mat_image.get_shape())\n\n        h_conv1 = tf.nn.relu(conv2d(feature_mat_image, W_conv1) + b_conv1)\n        h_pool1 = h_conv1\n\n        # Second Convolutional Layer\n        W_conv2 = weight_variable([3, 3, 32, 32])\n        b_conv2 = weight_variable([32])\n        h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n        h_pool2 = h_conv2\n\n        # Third Convolutional Layer\n        W_conv3 = weight_variable([3, 3, 32, 32])\n        b_conv3 = weight_variable([32])\n        h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)\n        h_pool3 = max_pool_2x2(h_conv3)\n\n        # Forth Convolutional Layer\n        W_conv4 = weight_variable([3, 3, 32, 128])\n        b_conv4 = weight_variable([128])\n        h_conv4 = tf.nn.relu(conv2d(h_pool3, W_conv4) + b_conv4)\n        h_pool4 = (h_conv4)\n\n        # Fifth Convolutional Layer\n        W_conv5 = weight_variable([3, 3, 128, 128])\n        b_conv5 = weight_variable([128])\n        h_conv5 = tf.nn.relu(conv2d(h_pool4, W_conv5) + b_conv5)\n        h_pool5 = h_conv5\n\n        # Sixth Convolutional Layer\n        W_conv6 = weight_variable([3, 3, 128, 1])\n        b_conv6 = weight_variable([1])\n        h_conv6 = tf.nn.relu(conv2d(h_pool5, W_conv6) + b_conv6)\n        h_pool6 = h_conv6\n\n        h_pool6 = tf.reshape(h_pool6, shape=[-1, POOL_X, POOL_Y])\n        feature_mat = h_pool6\n        print(\"----feature_mat-----\")\n        print(feature_mat)\n        # exit()\n\n        # W_fc1 = weight_variable([8 * 9 * 1, 1024])\n        # b_fc1 = bias_varibale([1024])\n        # h_pool2_flat = tf.reshape(h_pool2, [-1, 8 * 9 * 1])\n        # h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n        # print(\"----h_fc1_drop-----\")\n        # print(h_fc1)\n        # exit()\n        #\n        # # keep_prob = tf.placeholder(tf.float32)\n        # keep_prob = tf.placeholder(1.0)\n        # h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob=keep_prob)\n        # print(\"----h_fc1_drop-----\")\n        # print(h_fc1_drop)\n        # exit()\n        #\n        # W_fc2 = weight_variable([1024, 10])\n        # b_fc2 = bias_varibale([10])\n        #\n        # y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n        # print(\"----y_conv-----\")\n        # print(y_conv)\n        # exit()\n\n        # Exchange dim 1 and dim 0\n        # Start at: [0,1,2] = [batch_size, 128, 9] => [batch_size, 32, 36]\n        feature_mat = tf.transpose(feature_mat, [1, 0, 2])\n        # New feature_mat's shape: [time_steps, batch_size, n_inputs] [128, batch_size, 9]\n        print(\"----feature_mat-----\")\n        print(feature_mat)\n        # exit()\n\n        # Temporarily crush the feature_mat's dimensions\n        feature_mat = tf.reshape(feature_mat, [-1, config.n_inputs])  # 9\n        # New feature_mat's shape: [time_steps*batch_size, n_inputs]  # 128 * batch_size, 9\n\n        # Linear activation, reshaping inputs to the LSTM's number of hidden:\n        hidden = tf.nn.relu(tf.matmul(\n            feature_mat, config.W['hidden']\n        ) + config.biases['hidden'])\n        # New feature_mat (hidden) shape: [time_steps*batch_size, n_hidden] [128*batch_size, 32]\n\n        print(\"--n_steps--\")\n        print(config.n_steps)\n        print(\"--hidden--\")\n        print(hidden)\n\n        # Split the series because the rnn cell needs time_steps features, each of shape:\n        hidden = tf.split(0, config.n_steps/4, hidden)  # (0, 128, [128*batch_size, 32])\n        # New hidden's shape: a list of length \"time_step\" containing tensors of shape [batch_size, n_hidden]\n\n        # Define LSTM cell of first hidden layer:\n        lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(config.n_hidden, forget_bias=1.0)\n\n        # Stack two LSTM layers, both layers has the same shape\n        lsmt_layers = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * 2)\n\n        # Get LSTM outputs, the states are internal to the LSTM cells,they are not our attention here\n        outputs, _ = tf.nn.rnn(lsmt_layers, hidden, dtype=tf.float32)\n        # outputs' shape: a list of lenght \"time_step\" containing tensors of shape [batch_size, n_hidden]\n\n        print(\"------------------list-------------------\")\n        print(outputs)\n        # Get last time step's output feature for a \"many to one\" style classifier,\n        # as in the image describing RNNs at the top of this page\n        lstm_last_output = outputs[-1]  # Get the last element of the array: [?, 32]\n\n        print(\"------------------last outputs-------------------\")\n        print (lstm_last_output)\n\n        # Linear activation\n        return tf.matmul(lstm_last_output, config.W['output']) + config.biases['output']\n\n    pred_Y = LSTM_Network(X, config)   # shape[?,6]\n    print(\"------------------pred_Y-------------------\")\n    print(pred_Y)\n\n    # Loss,train_step,evaluation\n    l2 = config.lambda_loss_amount * \\\n         sum(tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables())\n    # Softmax loss and L2\n    cost = tf.reduce_mean(\n        tf.nn.softmax_cross_entropy_with_logits(pred_Y, Y)) + l2\n    train_step = tf.train.AdamOptimizer(\n        learning_rate=config.learning_rate).minimize(cost)\n\n    correct_prediction = tf.equal(tf.argmax(pred_Y, 1), tf.argmax(Y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, dtype=tf.float32))\n\n    # --------------------------------------------\n    # step4: Hooray, now train the neural network\n    # --------------------------------------------\n    # Note that log_device_placement can be turned ON but will cause console spam.\n\n    # Initializing the variables\n    init = tf.initialize_all_variables()\n    # Add ops to save and restore all the variables.\n    saver = tf.train.Saver()\n    best_accuracy = 0.0\n\n    # sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=False))\nif (FLAG == 'train') : # If it is the training mode\n    with tf.Session() as sess:\n        # tf.initialize_all_variables().run()\n        sess.run(init)  # .run()\n        f.write(\"---Save model \\n\")\n\n        # Start training for each batch and loop epochs\n        for i in range(config.training_epochs):\n            for start, end in zip(range(0, config.train_count, config.batch_size),  # (0, 7352, 1500)\n                                range(config.batch_size, config.train_count + 1,\n                                        config.batch_size)):  # (1500, 7353, 1500)\n                print(start)\n                print(end)\n\n                sess.run(train_step, feed_dict={X: X_train[start:end],\n                                            Y: y_train[start:end]})\n            # Test completely at every epoch: calculate accuracy\n            pred_out, accuracy_out, loss_out = sess.run([pred_Y, accuracy, cost], feed_dict={\n                X: X_test, Y: y_test})\n            print(\"traing iter: {},\".format(i) + \\\n                \" test accuracy : {},\".format(accuracy_out) + \\\n                \" loss : {}\".format(loss_out))\n            best_accuracy = max(best_accuracy, accuracy_out)\n            \n            # Save the model in this session\n            save_path = saver.save(sess, file_name + \"/model.ckpt\")\n            print(\"Model saved in file: %s\" % save_path)\n\n        print(\"\")\n        print(\"final loss: {}\").format(loss_out)\n        print(\"final test accuracy: {}\".format(accuracy_out))\n        print(\"best epoch's test accuracy: {}\".format(best_accuracy))\n        print(\"\")\n        # Write all output to file\n        f.write(\"final loss:\" + str(format(loss_out)) +\" \\n\")        \n        f.write(\"final test accuracy:\" + str(format(accuracy_out)) +\" \\n\")\n        f.write(\"best epoch's test accuracy:\" + str(format(best_accuracy)) + \" \\n\")\nelse :\n    # Running a new session\n    print(\"Starting 2nd session...\")\n    with tf.Session() as sess:\n        # Initialize variables\n        sess.run(init)\n        f.write(\"---Restore model \\n\")\n\n        # Restore model weights from previously saved model\n        saver.restore(sess, file_name+ \"/model.ckpt\")\n        print(\"Model restored from file: %s\" % save_path_name)\n        # Test completely at every epoch: calculate accuracy\n        pred_out, accuracy_out, loss_out = sess.run([pred_Y, accuracy, cost], feed_dict={\n            X: X_test, Y: y_test})\n        # print(\"traing iter: {},\" + \\\n        #       \" test accuracy : {},\".format(accuracy_out) + \\\n        #       \" loss : {}\".format(loss_out))\n        best_accuracy = max(best_accuracy, accuracy_out)\n\n        print(\"\")\n        print(\"final loss: {}\").format(loss_out)\n        print(\"final test accuracy: {}\".format(accuracy_out))\n        print(\"best epoch's test accuracy: {}\".format(best_accuracy))\n        print(\"\")\n        # Write all output to file\n        f.write(\"final loss:\" + str(format(loss_out)) +\" \\n\")\n        f.write(\"final test accuracy:\" + str(format(accuracy_out)) +\" \\n\")\n        f.write(\"best epoch's test accuracy:\" + str(format(best_accuracy)) + \" \\n\")\n\n    #\n    # #------------------------------------------------------------------\n    # # step5: Training is good, but having visual insight is even better\n    # #------------------------------------------------------------------\n    # # The code is in the .ipynb\n    #\n    # #------------------------------------------------------------------\n    # # step6: And finally, the multi-class confusion matrix and metrics!\n    # #------------------------------------------------------------------\n    # # The code is in the .ipynb\n\nf.write(\"Ended at \\n\")\nf.write(str(datetime.datetime.now())+'\\n')\nf.write(\"------------- \\n\")\nf.close()", "om ..neuron import Neuron\n\n# Simple structure to define ON- and OFF- Bipolar cell types (polarity = -1 means depolarizing/inhibitory/ON, and vice-versa)\nBipolarType = namedtuple('BipolarType', ['name', 'polarity', 'occurrence'])\n\n\nclass BipolarCell(Neuron):\n  \"\"\"A bipolar cell model.\"\"\"\n  \n  # Bipolar cell types:-\n  # * By response:-\n  # ** Hyperpolarizing (H): Excitatory receptors, OFF-center\n  # ** Depolarizing (D): Inhibitory receptors, ON-center\n  # * By connectivity/morphological properties:-\n  # ** Midget (MB): Single cone input, mostly found in fovea\n  # *** Flat Midget (FMB)\n  # *** Invaginating Midget (IMB)\n  # ** Diffuse (DB): Cones only, 6-9 subtypes (DB1, DB2 ...) with axons terminating at gradually outer layers\n  # ** Rod (RB): Rods only, ON-center only, axons terminating in outermost layer\n  # ** Blue-cone (BB): Blue cone only\n  # Giant Bistratified (GBB): Axons terminating in two distinct layers - one inner, one outer\n  \n  # Simplified set of Bipolar types and their occurrence probabilities\n  bipolar_types = [ BipolarType('ON', -1.0, 0.5), BipolarType('OFF', 1.0, 0.5) ]\n  bipolar_probabilities = np.float32([ bipolar_type.occurrence for bipolar_type in bipolar_types ])\n  \n  # Electrophysiological parameters for Integrate-and-Fire method (model)\n  R = 300.0e06  # Ohms; membrane resistance (~30-700Mohm)\n  C = 3.0e-09  # Farads; membrane capacitance (~2-3nF)\n  tau = R * C  # seconds; time constant (~100-1000ms)\n  \n  # Miscellaneous parameters\n  potential_scale = 255 / abs(Neuron.resting_potential.mu / 2)  # factor used to convert cell potential to image pixel value\n  \n  def __init__(self, location, timeNow, retina, bipolarType=None):\n    Neuron.__init__(self, location, timeNow)\n    self.retina = retina\n    self.pixel = np.int_(location[:2])\n    #self.bipolarType = bipolarType if ((bipolarType is not None) and (bipolarType in self.bipolar_types)) else np.random.choice(self.bipolar_types, p=self.bipolar_probabilities)  # TODO debug this\n    self.bipolarType = self.bipolar_types[0]\n    # TODO Ensure Rod Bipolars are ON-center only?\n    #print \"Bipolar type:\", self.bipolarType\n    self.expDecayFactor = 0.0\n    self.pixelValue = 0\n  \n  def updatePotential(self):\n    # NOTE: Bipolar cells use graded potentials\n    # Differential equation solution, decay only (similar to Photoreceptor, Method 4)\n    self.expDecayFactor = exp(-self.deltaTime / self.tau)\n    self.potential = self.resting_potential.mu + ((self.potentialLastUpdated - self.resting_potential.mu) * self.expDecayFactor)  # V(t) = V_r + ((V(t') - V_r) * (e ^ (-(t - t') / tau)))\n    \n    # Accumulate/integrate incoming potentials (TODO hyperpolarize/depolarize based on polarity)\n    self.potential += self.bipolarType.polarity * self.potentialAccumulated  # integrate signals accumulated from neighbors\n    self.potentialAccumulated = 0.0  # reset accumulator (don't want to double count!)\n    \n    # Compute a value to render\n    self.pixelValue = int(np.clip(abs(self.potential - self.resting_potential.mu) * self.potential_scale, 0, 255))\n    \n    #self.sendGradedPotential()\n", "s import User\n\ndef in_upload_path(instance, filename):\n    \"\"\" Function to return upload path for test case input file\"\"\"\n    return \"/\".join([\"testcases\", str(instance.problem.id)]) + \".in\"\n\n\ndef out_upload_path(instance, filename):\n    \"\"\" Function to return upload path for test case output file\"\"\"\n    return \"/\".join([\"testcases\", str(instance.problem.id)]) + \".out\"\n\n\n# Model for users\nclass Coder(models.Model):\n    user = models.OneToOneField(User)\n    link = models.URLField()\n    score = models.DecimalField(default = 0, decimal_places = 3, max_digits = 100)\n    rank = models.IntegerField(default = -1)\n    problems_tried = models.ForeignKey('Problem', null = True, related_name = \"problems_tried\")\n    problems_ac = models.ForeignKey('Problem', null = True, related_name = \"problems_ac\")\n\n    def __unicode__(self):\n        return self.user.username\n\n\n# Model for the Problems to be uploaded on the judge\nclass Problem(models.Model):\n    # Problem name (example: life, the universe and everything)\n    name = models.CharField(max_length=255)\n    # Problem code (example: TEST)\n    code = models.CharField(max_length=20, unique = True)\n    # Problem link (example: poj.com/problems/TEST)\n    link = models.URLField()\n    # Problem statement\n    statement = models.TextField()\n\n    num_submissions = models.IntegerField(default = 0) # number of submissions\n    num_ac = models.IntegerField(default = 0)     # number of accepted submissions\n    num_wa = models.IntegerField(default = 0)          # number of wrong answers\n    num_re = models.IntegerField(default = 0)          # number of runtime errors\n    num_tle = models.IntegerField(default = 0)         # number of tles\n    num_ce = models.IntegerField(default = 0)\n    date_added = models.DateTimeField(auto_now_add = True) # When added\n    time_limit = models.IntegerField(default=1)         # Time Limit\n    source = models.CharField(max_length=255)\n    num_tests = models.IntegerField(default = 1)\n\n    author = models.ForeignKey('Coder', null = True)\n\n    def __unicode__(self):\n        return self.code\n\n\n# Model for Test Cases\nclass TestCase(models.Model):\n    problem = models.ForeignKey(Problem)\n    input_file = models.FileField(upload_to=in_upload_path)\n    output_file = models.FileField(upload_to=out_upload_path)\n\n    def __unicode__(self):\n        return self.problem.code\n\nLANGUAGES = (\n                (\"C\", \"GNU C\"),\n                (\"CPP\", \"GNU C++\"),\n                )\n\n\n# Model for problem submissions\nclass Submission(models.Model):\n    STATUSES = (\n                (\"NT\", \"Not tested\"),\n                (\"CE\", \"Compile Error\"),\n                (\"TL\", \"Time Limit Exceeded\"),\n                (\"RE\", \"Runtime Error\"),\n                (\"AC\", \"Accepted\")\n                )\n    submitter = models.ForeignKey(Coder, null = True)\n    problem = models.ForeignKey(Problem, default = None, null = True)\n    status = models.CharField(max_length = 2, default = \"NT\", choices = STATUSES)\n    lang = models.CharField(max_length = 4, default = \"C\", choices = LANGUAGES)\n    code = models.TextField(default=\"\")\n    created = models.DateTimeField(auto_now_add=True)\n    private = models.BooleanField(default = True)", "ort work_in_progress\nfrom rmgpy import settings\nfrom rmgpy.data.rmg import RMGDatabase\nfrom copy import copy, deepcopy\nfrom rmgpy.data.base import LogicOr\nfrom rmgpy.molecule import Group\n\nimport nose\nimport nose.tools\n\n\nclass TestDatabase():  # cannot inherit from unittest.TestCase if we want to use nose test generators\n    \"\"\"\n    Contains unit tests for the database for rigorous error checking.\n    \"\"\"\n    @classmethod\n    def setUpClass(cls):\n        \"\"\"\n        Load the database before running the tests.\n        \"\"\"\n        databaseDirectory = settings['database.directory']\n        cls.database = RMGDatabase()\n        cls.database.load(databaseDirectory, kineticsFamilies='all')\n    \n    # These are generators, that call the methods below.\n    def test_kinetics(self):\n        for family_name, family in self.database.kinetics.families.iteritems():\n\n            test = lambda x: self.kinetics_checkCorrectNumberofNodesInRules(family_name)\n            test_name = \"Kinetics family {0}: rules have correct number of nodes?\".format(family_name)\n            test.description = test_name\n            self.compat_func_name = test_name\n            yield test, None\n\n            test = lambda x: self.kinetics_checkNodesInRulesFoundInGroups(family_name)\n            test_name = \"Kinetics family {0}: rules' nodes exist in the groups?\".format(family_name)\n            test.description = test_name\n            self.compat_func_name = test_name\n            yield test, None\n\n            test = lambda x: self.kinetics_checkGroupsFoundInTree(family_name)\n            test_name = \"Kinetics family {0}: groups are in the tree with proper parents?\".format(family_name)\n            test.description = test_name\n            self.compat_func_name = test_name\n            yield test, None\n\n            test = lambda x: self.kinetics_checkGroupsNonidentical(family_name)\n            test_name = \"Kinetics family {0}: groups are not identical?\".format(family_name)\n            test.description = test_name\n            self.compat_func_name = test_name\n            yield test, family_name\n\n            test = lambda x: self.kinetics_checkChildParentRelationships(family_name)\n            test_name = \"Kinetics family {0}: parent-child relationships are correct?\".format(family_name)\n            test.description = test_name\n            self.compat_func_name = test_name\n            yield test, family_name\n\n            test = lambda x: self.kinetics_checkSiblingsForParents(family_name)\n            test_name = \"Kinetics family {0}: sibling relationships are correct?\".format(family_name)\n            test.description = test_name\n            self.compat_func_name = test_name\n            yield test, family_name\n\n            test = lambda x: self.kinetics_checkReactantAndProductTemplate(family_name)\n            test_name = \"Kinetics family {0}: reactant and product templates correctly defined?\".format(family_name)\n            test.description = test_name\n            self.compat_func_name = test_name\n            yield test, family_name\n            \n            for depository in family.depositories:\n                \n                test = lambda x: self.kinetics_checkAdjlistsNonidentical(depository)\n                test_name = \"Kinetics {1} Depository: check adjacency lists are nonidentical?\".format(family_name, depository.label)\n                test.description = test_name\n                self.compat_func_name = test_name\n                yield test, depository.label\n        \n        for library_name, library in self.database.kinetics.libraries.iteritems():\n            \n            test = lambda x: self.kinetics_checkAdjlistsNonidentical(library)\n            test_name = \"Kinetics library {0}: check adjacency lists are nonidentical?\".format(library_name)\n            test.description = test_name\n            self.compat_func_name = test_name\n            yield test, library_name\n        \n    def test_thermo(self):\n        for group_name, group in self.database.thermo.groups.iteritems():\n            test = lambda x: self.general_checkNodesFoundInTree(group_name, group)\n            test_name = \"Thermo groups {0}: nodes are in the tree with proper parents?\".format(group_name)\n            test.description = test_name\n            self.compat_func_name = test_name\n            yield test, group_name\n            \n            test = lambda x: self.general_checkGroupsNonidentical(group_name, group)\n            test_name = \"Thermo groups {0}: nodes are nonidentical?\".format(group_name)\n            test.description = test_name\n            self.compat_func_name = test_name\n            yield test, group_name\n            \n            test = lambda x: self.general_checkChildParentRelationships(group_name, group)\n            test_name = \"Thermo groups {0}: parent-child relationships are correct?\".format(group_name)\n            test.description = test_name\n            self.compat_func_name = test_name\n            yield test, group_name\n\n            test = lambda x: self.general_checkSiblingsForParents(group_name, group)\n            test_name = \"Thermo groups {0}: sibling relationships are correct?\".format(group_name)\n            test.description = test_name\n            self.compat_func_name = test_name\n            yield test, group_name\n\n            \n    def test_solvation(self):\n        for group_name, group in self.database.solvation.groups.iteritems():\n            test = lambda x: self.general_checkNodesFoundInTree(group_name, group)\n            test_name = \"Solvation groups {0}: nodes are in the tree with proper parents?\".format(group_name)\n            test.description = test_name\n            self.compat_func_name = test_name\n            yield test, group_name\n            \n            test = lambda x: self.general_checkGroupsNonidentical(group_name, group)\n            test_name = \"Solvation groups {0}: nodes are nonidentical?\".format(group_name)\n            test.description = test_name\n            self.compat_func_name = test_name\n            yield test, group_name\n\n            test = lambda x: self.general_checkChildParentRelationships(group_name, group)\n            test_name = \"Solvation groups {0}: parent-child relationships are correct?\".format(group_name)\n            test.description = test_name\n            self.compat_func_name = test_name\n            yield test, group_name\n\n            test = lambda x: self.general_checkSiblingsForParents(group_name, group)\n            test_name = \"Solvation groups {0}: sibling relationships are correct?\".format(group_name)\n            test.description = test_name\n            self.compat_func_name = test_name\n            yield test, group_name\n            \n    def test_statmech(self):\n        for group_name, group in self.database.statmech.groups.iteritems():\n            test = lambda x: self.general_checkNodesFoundInTree(group_name, group)\n            test_name = \"Statmech groups {0}: nodes are in the tree with proper parents?\".format(group_name)\n            test.description = test_name\n            self.compat_func_name = test_name\n            yield test, group_name\n            \n            test = lambda x: self.general_checkGroupsNonidentical(group_name, group)\n            test_name = \"Statmech groups {0}: nodes are nonidentical?\".format(group_name)\n            test.description = test_name\n            self.compat_func_name = test_name\n            yield test, group_name\n            \n            test = lambda x: self.general_checkChildParentRelationships(group_name, group)\n            test_name = \"Statmech groups {0}: parent-child relationships are correct?\".format(group_name)\n            test.description = test_name\n            self.compat_func_name = test_name\n            yield test, group_name\n\n            test = lambda x: self.general_checkSiblingsForParents(group_name, group)\n            test_name = \"Statmech groups {0}: sibling relationships are correct?\".format(group_name)\n            test.description = test_name\n            self.compat_func_name = test_name\n            yield test, group_name\n\n    def test_transport(self):\n        for group_name, group in self.database.transport.groups.iteritems():\n            test = lambda x: self.general_checkNodesFoundInTree(group_name, group)\n            test_name = \"Transport groups {0}: nodes are in the tree with proper parents?\".format(group_name)\n            test.description = test_name\n            self.compat_func_name = test_name\n            yield test, group_name\n\n            test = lambda x: self.general_checkGroupsNonidentical(group_name, group)\n            test_name = \"Transport groups {0}: nodes are nonidentical?\".format(group_name)\n            test.description = test_name\n            self.compat_func_name = test_name\n            yield test, group_name\n            \n            test = lambda x: self.general_checkChildParentRelationships(group_name, group)\n            test_name = \"Transport groups {0}: parent-child relationships are correct?\".format(group_name)\n            test.description = test_name\n            self.compat_func_name = test_name\n            yield test, group_name\n\n            test = lambda x: self.general_checkSiblingsForParents(group_name, group)\n            test_name = \"Transport groups {0}: sibling relationships are correct?\".format(group_name)\n            test.description = test_name\n            self.compat_func_name = test_name\n            yield test, group_name\n            \n    # These are the actual tests, that don't start with a \"test_\" name:\n    def kinetics_checkCorrectNumberofNodesInRules(self, family_name):\n        \"\"\"\n        This test ensures that each rate rule contains the proper number of nodes according to the family it originates.\n        \"\"\"\n        family = self.database.kinetics.families[family_name]\n        expectedNumberNodes = len(family.getRootTemplate())\n        for label, entries in family.rules.entries.iteritems():\n            for entry in entries:\n                nodes = label.split(';')\n                nose.tools.assert_equal(len(nodes), expectedNumberNodes, \"Wrong number of groups or semicolons in family {family} rule {entry}.  Should be {num_nodes}\".format(family=family_name, entry=entry, num_nodes=expectedNumberNodes))\n\n    def kinetics_checkNodesInRulesFoundInGroups(self, family_name):\n        \"\"\"\n        This test ensures that each rate rule contains nodes that exist in the groups and that they match the order of the forwardTemplate.\n        \"\"\"\n        family = self.database.kinetics.families[family_name]\n        \n        # List of the each top node's descendants (including the top node)\n        topDescendants = []\n        for topNode in family.getRootTemplate():\n            nodes = [topNode]\n            nodes.extend(family.groups.descendants(topNode))\n            topDescendants.append(nodes)\n            \n        topGroupOrder = ';'.join(topNode.label for topNode in family.getRootTemplate())\n        \n        for label, entries in family.rules.entries.iteritems():\n            for entry in entries:\n                nodes = label.split(';')\n                for i, node in enumerate(nodes):\n                    nose.tools.assert_true(node in family.groups.entries, \"In {family} family, no group definition found for label {label} in rule {entry}\".format(family=family_name, label=node, entry=entry))\n                    nose.tools.assert_true(family.groups.entries[node] in topDescendants[i], \"In {family} family, rule {entry} was found with groups out of order.  The correct order for a rule should be subgroups of {top}.\".format(family=family_name, entry=entry, top=topGroupOrder))\n                                        \n    def kinetics_checkGroupsFoundInTree(self, family_name):\n        \"\"\"\n        This test checks whether groups are found in the tree, with proper parents.\n        \"\"\"\n        family = self.database.kinetics.families[family_name]\n        for nodeName, nodeGroup in family.groups.entries.iteritems():\n            ascendParent = nodeGroup\n            # Check whether the node has proper parents unless it is the top reactant or product node\n            while ascendParent not in family.groups.top and ascendParent not in family.forwardTemplate.products:\n                child = ascendParent\n                ascendParent = ascendParent.parent\n                nose.tools.assert_true(ascendParent is not None, \"Group {group} in {family} family was found in the tree without a proper parent.\".format(group=child, family=family_name))\n                nose.tools.assert_true(child in ascendParent.children, \"Group {group} in {family} family was found in the tree without a proper parent.\".format(group=nodeName, family=family_name))\n\n    def kinetics_checkGroupsNonidentical(self, family_name):\n        \"\"\"\n        This test checks that the groups are non-identical.\n        \"\"\"\n        from rmgpy.data.base import Database\n        originalFamily = self.database.kinetics.families[family_name]\n        family = Database()\n        family.entries = originalFamily.groups.entries\n        entriesCopy = copy(family.entries)\n        for nodeName, nodeGroup in family.entries.iteritems():\n            del entriesCopy[nodeName]\n            for nodeNameOther, nodeGroupOther in entriesCopy.iteritems():\n                nose.tools.assert_false(family.matchNodeToNode(nodeGroup, nodeGroupOther), \"Group {group} in {family} family was found to be identical to group {groupOther}\".format(group=nodeName, family=family_name, groupOther=nodeNameOther))\n\n    def kinetics_checkChildParentRelationships(self, family_name):\n        \"\"\"\n        This test checks that groups' parent-child relationships are correct in the database.\n        \"\"\"\n        from rmgpy.data.base import Database\n        originalFamily = self.database.kinetics.families[family_name]\n        family = Database()\n        family.entries = originalFamily.groups.entries\n        for nodeName, childNode in family.entries.iteritems():\n            #top nodes and product nodes don't have parents by definition, so they get an automatic pass:\n            if childNode in originalFamily.groups.top or childNode in originalFamily.forwardTemplate.products: continue\n            parentNode = childNode.parent\n            # Check whether the node has proper parents unless it is the top reactant or product node\n            # The parent should be more general than the child\n            nose.tools.assert_true(family.matchNodeToChild(parentNode, childNode),\n                            \"In {family} family, group {parent} is not a proper parent of its child {child}.\".format(family=family_name, parent=parentNode, child=nodeName))\n\n            #check that parentNodes which are LogicOr do not have an ancestor that is a Group\n            #If it does, then the childNode must also be a child of the ancestor\n            if isinstance(parentNode, LogicOr):\n                ancestorNode = childNode\n                while ancestorNode not in originalFamily.groups.top and isinstance(ancestorNode, LogicOr):\n                    ancestorNode = ancestorNode.parent\n                if isinstance(ancestorNode, Group):\n                    nose.tools.assert_true(family.matchNodeToChild(ancestorNode, childNode),\n                                    \"In {family} family, group {ancestor} is not a proper ancestor of its child {child}.\".format(family=family_name, ancestor=ancestorNode, child=nodeName))\n\n    def kinetics_checkSiblingsForParents(self, family_name):\n        \"\"\"\n        This test checks that siblings in a tree are not actually parent/child\n        \"\"\"\n        from rmgpy.data.base import Database\n        originalFamily = self.database.kinetics.families[family_name]\n        family = Database()\n        family.entries = originalFamily.groups.entries\n        for nodeName, node in family.entries.iteritems():\n            #Some families also construct a 2-level trees for the products\n            #(root with all entries down one level) We don't care about this\n            #tree as it is not used in searching, so we ignore products\n            if node in originalFamily.forwardTemplate.products: continue\n            for index, child1 in enumerate(node.children):\n                for child2 in node.children[index+1:]:\n                    #Don't check a node against itself\n                    if child1 is child2: continue\n                    nose.tools.assert_false(family.matchNodeToChild(child1, child2),\n                                            \"In family {0}, node {1} is written as a sibling of {2}, when it is actually a parent.\".format(family_name, child1, child2))\n                    nose.tools.assert_false(family.matchNodeToChild(child2, child1),\n                                            \"In family {0}, node {1} is written as a sibling of {2}, when it is actually a parent.\".format(family_name, child2, child1))\n\n    def kinetics_checkAdjlistsNonidentical(self, database):\n        \"\"\"\n        This test checks whether adjacency lists of reactants in a KineticsDepository or KineticsLibrary database object are nonidentical.\n        \"\"\"\n        speciesDict = {}\n        entries = database.entries.values()\n        for entry in entries:\n            for reactant in entry.item.reactants:\n                if reactant.label not in speciesDict:\n                    speciesDict[reactant.label] = reactant\n                \n            for product in entry.item.products:\n                if product.label not in speciesDict:\n                    speciesDict[product.label] = product\n                    \n        # Go through all species to make sure they are nonidentical\n        speciesList = speciesDict.values()\n        labeledAtoms = [species.molecule[0].getLabeledAtoms() for species in speciesList]\n        for i in range(len(speciesList)):\n            for j in range(i+1,len(speciesList)):\n                    initialMap = {}\n                    try:\n                        for atomLabel in labeledAtoms[i]:\n                            initialMap[labeledAtoms[i][atomLabel]] = labeledAtoms[j][atomLabel]\n                    except KeyError:\n                        # atom labels did not match, therefore not a match\n                        continue\n                    \n                    nose.tools.assert_false(speciesList[i].molecule[0].isIsomorphic(speciesList[j].molecule[0], initialMap), \"Species {0} and species {1} in {2} database were found to be identical.\".format(speciesList[i].label,speciesList[j].label,database.label))\n\n    def kinetics_checkReactantAndProductTemplate(self, family_name):        \n        \"\"\"\n        This test checks whether the reactant and product templates within a family are correctly defined.\n        For a reversible family, the reactant and product templates must have matching labels.\n        For a non-reversible family, the reactant and product templates must have non-matching labels, otherwise overwriting may occur.\n        \"\"\"\n        family = self.database.kinetics.families[family_name]\n        if family.ownReverse:\n            nose.tools.assert_equal(family.forwardTemplate.reactants, family.forwardTemplate.products)\n        else:\n            reactant_labels = [reactant.label for reactant in family.forwardTemplate.reactants]\n            product_labels = [product.label for product in family.forwardTemplate.products]\n            for reactant_label in reactant_labels:\n                for product_label in product_labels:\n                    nose.tools.assert_false(reactant_label==product_label, \"Reactant label {0} matches that of product label {1} in a non-reversible family template.  Please rename product label.\".format(reactant_label,product_label))\n        \n    def general_checkNodesFoundInTree(self, group_name, group):\n        \"\"\"\n        This test checks whether nodes are found in the tree, with proper parents.\n        \"\"\"\n        for nodeName, nodeGroup in group.entries.iteritems():\n            ascendParent = nodeGroup\n            # Check whether the node has proper parents unless it is the top reactant or product node\n            while ascendParent not in group.top:\n                child = ascendParent\n                ascendParent = ascendParent.parent\n                nose.tools.assert_true(ascendParent is not None, \"Node {node} in {group} group was found in the tree without a proper parent.\".format(node=child, group=group_name))\n                nose.tools.assert_true(child in ascendParent.children, \"Node {node} in {group} group was found in the tree without a proper parent.\".format(node=nodeName, group=group_name))\n    \n    def general_checkGroupsNonidentical(self, group_name, group):\n        \"\"\"\n        This test checks whether nodes found in the group are nonidentical.\n        \"\"\"\n        entriesCopy = copy(group.entries)\n        for nodeName, nodeGroup in group.entries.iteritems():\n            del entriesCopy[nodeName]\n            for nodeNameOther, nodeGroupOther in entriesCopy.iteritems():\n                try: \n                    group.matchNodeToNode(nodeGroup,nodeGroupOther)\n                except:\n                    print nodeName\n                    print nodeNameOther\n                    pass\n                nose.tools.assert_false(group.matchNodeToNode(nodeGroup, nodeGroupOther), \"Node {node} in {group} group was found to be identical to node {nodeOther}\".format(node=nodeName, group=group_name, nodeOther=nodeNameOther))\n    \n    def general_checkChildParentRelationships(self, group_name, group):\n        \"\"\"\n        This test checks that nodes' parent-child relationships are correct in the database.\n        \"\"\"\n        for nodeName, childNode in group.entries.iteritems():\n            #top nodes and product nodes don't have parents by definition, so they get an automatic pass:\n            if childNode in group.top: continue\n            parentNode = childNode.parent\n            # Check whether the node has proper parents unless it is the top reactant or product node\n            # The parent should be more general than the child\n            nose.tools.assert_true(group.matchNodeToChild(parentNode, childNode),\n                            \"In {group} group, node {parent} is not a proper parent of its child {child}.\".format(group=group_name, parent=parentNode, child=nodeName))\n\n            #check that parentNodes which are LogicOr do not have an ancestor that is a Group\n            #If it does, then the childNode must also be a child of the ancestor\n            if isinstance(parentNode, LogicOr):\n                ancestorNode = childNode\n                while ancestorNode not in group.top and isinstance(ancestorNode, LogicOr):\n                    ancestorNode = ancestorNode.parent\n                if isinstance(ancestorNode, Group):\n                    nose.tools.assert_true(group.matchNodeToChild(ancestorNode, childNode),\n                                    \"In {group} group, node {ancestor} is not a proper ancestor of its child {child}.\".format(group=group_name, ancestor=ancestorNode, child=nodeName))\n\n    def general_checkSiblingsForParents(self, group_name, group):\n        \"\"\"\n        This test checks that siblings in a tree are not actually parent/child\n        \"\"\"\n        for nodeName, node in group.entries.iteritems():\n            for index, child1 in enumerate(node.children):\n                for child2 in node.children[index+1:]:\n                    #Don't check a node against itself\n                    if child1 is child2: continue\n                    nose.tools.assert_false(group.matchNodeToChild(child1, child2),\n                                            \"In {0} group, node {1} is written as a sibling of {2}, when it is actually a parent.\".format(group_name, child1, child2))\n                    nose.tools.assert_false(group.matchNodeToChild(child2, child1),\n                                            \"In {0} group, node {1} is written as a sibling of {2}, when it is actually a parent.\".format(group_name, child2, child1))\nif __name__ == '__main__':\n    nose.run(argv=[__file__, '-v', '--nologcapture'], defaultTest=__name__)\n", "\nfrom generativepy.drawing import make_image, setup\nfrom generativepy.color import Color\nfrom generativepy.geometry import Text\n\n\ndef draw_alpha(ctx, pixel_width, pixel_height, frame_no, frame_count):\n    setup(ctx, pixel_width, pixel_height, background=Color(1))\n\n    Text(ctx).of(\"Filled Times\", (100, 100)).font(\"Times\").size(40).fill(Color('blue'))\n    Text(ctx).of(\"Filled Arial\", (100, 150)).font(\"Arial\").size(40).fill(Color('red'))\n    Text(ctx).of(\"Small\", (100, 180)).font(\"Arial\").size(20).fill(Color('darkgreen'))\n    Text(ctx).of(\"Large\", (100, 240)).font(\"Arial\").size(60).fill(Color('magenta'))\n    Text(ctx).of(\"Stroke\", (100, 310)).font(\"Arial\").size(60).stroke(Color('black'), 4)\n    Text(ctx).of(\"Fill Stroke\", (100, 380)).font(\"Arial\").size(60)\\\n                                           .fill(Color('blue')).stroke(Color('red'), 2)\n    Text(ctx).of(\"Dashed\", (100, 450)).font(\"Arial\").size(60).stroke(Color('black'), 3, dash=[4])\n\n\nmake_image(\"text-drawing.png\", draw_alpha, 500, 500)", "ilog testbench to create a simulated memory\n# launch it with: python3 memory_init.py && cat memory.txt\nimport sys\n\ndef Itobin(i, l):\n\to = bin(i)[2:]\n\treturn '0'*(l-len(o))+o\ndef str2hex(S):\n\tout = ''\n\tfor i in range(0, len(S)):\n\t\tout+=hex(ord(S[i]))[2:]\n\treturn out+'\\n'\n\nopcodes = {\n\t'ADD'\t: '000000', # Add\t\tADD Rd,Ra,Rb\t\t\tRd := Rb + Ra\n\t'MUL'\t: '010000', # MUL\t\tMUL Rd,Ra,Rb \t\t\tRd := Ra * Rb\n\t'ADDI'\t: '001000', # ADDI immediate\tADDI Rd,Ra,Imm\t\t\tRd := s(Imm) + Ra\n\t'BSLLI'\t: '011001', # SHIFT\t\tBSLLI Rd,Ra,00000100000&Imm\tRd := (Ra << Imm5) & 0\n\n\t'LW'\t: '110010', # Load\t\tLW Rd,Ra,Rb \t\tAddr := Ra + Rb\t\tRd := *Addr\n\t'SW'\t: '110110', # Save\t\tSW Rd,Ra,Rb \t\tAddr := Ra + Rb\t\t*Addr := Rd\n\t#'LBU'\t: '110000' # Load 8\t\tLBU Rd,Ra,Rb \t\tAddr := Ra + Rb\t\tRd[0:23] := 0\t\tRd[24:31] := *Addr[0:7]\n\t#'LHU'\t: '110001' # Load 16\t\tLHU Rd,Ra,Rb \t\tAddr := Ra + Rb\t\tRd[0:15] := 0\t\tRd[16:31] := *Addr[0:15]\n}\n\n# Creating registers from 0 to 31\nregisters = {}\nfor i in range(0, 32):\n\ttobin=bin(i)[2:]\n\ttobin='0'*(5-len(tobin))+tobin\n\tregisters['R'+str(i)]=tobin\n'''\nDo not forget that:\n- R0 is equal to 0\n'''\n'''\n- Chargement dans R1 de A=5  stock\u00e9 \u00e0 l'adresse 0x1\n- Chargement dans R1 de B=10 stock\u00e9 \u00e0 l'adresse 0x2\n- R0 <- R0 + R1\n- Sauvegarde de R0 en m\u00e9moire principale\n'''\n\ndatas = {\n\t#ADDI R1,R0, <- Add(A)\n\t0 : ['ADDI R1,R0,1024',\t\t\"R1 <- Add(A)=0x100\"],\n\t1 : ['LW R2,R0,R1',\t\t\"Chargement dans R2 de A=5 stock\u00e9 \u00e0 l'adresse 0x100\"],\n\t2 : ['ADDI R1,R1,4',\t\t\"R1 <- Add(B)=Add(A)+4bytes\"],\n\t3 : ['LW R3,R0,R1',\t\t\"Chargement dans R2 de A=5 stock\u00e9 \u00e0 l'adresse 0x100\"],\n\t4 : ['ADD R2,R2,R3',\t\t\"R2<-5+1=6\"],\n\t5 : ['MUL R4,R2,R2',\t\t\"R2<-R2*R2\"],\n\t6 : ['SW R4,R0,R1',\t\t\"Sauvegarde de R2 \u00e0 l'adresse 0x100\"],\n\t\n\t256 : 5,\n\t257 : 1,\n}\n\nmemory_txt=''\nmemory_txt_comments=''\nkeyMax=max(datas.keys(), key=int)+1\nfor i in range(0, keyMax):\n\tif datas.get(i):\n\t\tif isinstance(datas.get(i), list):\n\t\t\tprint(datas[i][0])\nprint('\\n')\nfor i in range(0, keyMax):\n\tif datas.get(i):\n\t\tif isinstance(datas.get(i), list):\n\t\t\tprint(datas[i][1])\nprint('\\n')\n\t\t\t\nfor i in range(0, keyMax):\n\tif datas.get(i):\n\t\tsys.stdout.write(str(i)+' '*(len(str(keyMax))-len(str(i)))+' : '),\n\t\tif isinstance(datas.get(i), list):\n\t\t\tins=datas[i][0]\n\n\t\t\tinsT=ins\n\t\t\tins2bin=''\n\t\t\tj=0\n\t\t\tk=0\n\t\t\twhile j<=len(insT):\n\t\t\t\tif j==len(insT)-1 or (len(insT) > 1 and insT[j+1] in [' ',',']):\n\t\t\t\t\tif k==0:\n\t\t\t\t\t\tins2bin+=opcodes[insT[:j+1]]\n\t\t\t\t\telif insT[0]=='R':\n\t\t\t\t\t\tins2bin+=registers[insT[:j+1]]\n\t\t\t\t\telse: # decimal number\n\t\t\t\t\t\tins2bin+=Itobin(int(insT[:j+1]), 32-len(ins2bin))\n\t\t\t\t\tinsT=insT[(j+2):]\n\t\t\t\t\t\n\t\t\t\t\tj=0\n\t\t\t\t\tk+=1\n\t\t\t\telse:\n\t\t\t\t\tj+=1\n\t\t\tprint('- - - ')\n\t\t\tins2bin+='0'*(32-len(ins2bin))\n\t\t\tins2hex=str(hex(int(ins2bin, 2)))[2:]\n\t\t\tins2hex='0'*(8-len(ins2hex))+ins2hex\n\t\t\tprint(ins+' '*(32-len(ins))+'-> '+ins2bin+' -> '+ins2hex+' // '+datas[i][1])\n\t\t\t\n\t\t\tmemory_txt+=ins2hex+'\\n'\n\t\t\tmemory_txt_comments+=str2hex(datas[i][0])\n\t\telse:\n\t\t\ttohex=hex(datas.get(i))[2:]\n\t\t\ttohex='0'*(8-len(tohex))+tohex\n\t\t\tmemory_txt+=tohex+'\\n'\n\t\t\tprint(str(datas.get(i))+' -> 0x'+tohex)\n\telse:\n\t\tmemory_txt+='0'*8+'\\n'\nf = open('memory.txt', 'w')\t\nf.write(memory_txt)\nf.close()\nf = open('memory_comments.txt', 'w')\t\nf.write(memory_txt_comments)\nf.close()\n", "\nfrom __future__ import unicode_literals\n\nimport frappe\nfrom frappe import _\n\nfrom frappe.utils import now, cint\nfrom frappe.model import no_value_fields\nfrom frappe.model.document import Document\nfrom frappe.custom.doctype.property_setter.property_setter import make_property_setter\nfrom frappe.desk.notifications import delete_notification_count_for\nfrom frappe.modules import make_boilerplate\n\nform_grid_templates = {\n\t\"fields\": \"templates/form_grid/fields.html\"\n}\n\nclass DocType(Document):\n\t__doclink__ = \"https://frappe.io/docs/models/core/doctype\"\n\tdef get_feed(self):\n\t\treturn self.name\n\n\tdef validate(self):\n\t\t\"\"\"Validate DocType before saving.\n\n\t\t- Check if developer mode is set.\n\t\t- Validate series\n\t\t- Check fieldnames (duplication etc)\n\t\t- Clear permission table for child tables\n\t\t- Add `amended_from` and `ameneded_by` if Amendable\"\"\"\n\t\tif not frappe.conf.get(\"developer_mode\") and not self.custom:\n\t\t\tfrappe.throw(_(\"Not in Developer Mode! Set in site_config.json or make 'Custom' DocType.\"))\n\t\tfor c in [\".\", \"/\", \"#\", \"&\", \"=\", \":\", \"'\", '\"']:\n\t\t\tif c in self.name:\n\t\t\t\tfrappe.throw(_(\"{0} not allowed in name\").format(c))\n\t\tself.validate_series()\n\t\tself.scrub_field_names()\n\t\tself.validate_title_field()\n\t\tvalidate_fields(self)\n\n\t\tif self.istable:\n\t\t\t# no permission records for child table\n\t\t\tself.permissions = []\n\t\telse:\n\t\t\tvalidate_permissions(self)\n\n\t\tself.make_amendable()\n\n\tdef change_modified_of_parent(self):\n\t\t\"\"\"Change the timestamp of parent DocType if the current one is a child to clear caches.\"\"\"\n\t\tif frappe.flags.in_import:\n\t\t\treturn\n\t\tparent_list = frappe.db.sql(\"\"\"SELECT parent\n\t\t\tfrom tabDocField where fieldtype=\"Table\" and options=%s\"\"\", self.name)\n\t\tfor p in parent_list:\n\t\t\tfrappe.db.sql('UPDATE tabDocType SET modified=%s WHERE `name`=%s', (now(), p[0]))\n\n\tdef scrub_field_names(self):\n\t\t\"\"\"Sluggify fieldnames if not set from Label.\"\"\"\n\t\trestricted = ('name','parent','creation','modified','modified_by',\n\t\t\t'parentfield','parenttype',\"file_list\")\n\t\tfor d in self.get(\"fields\"):\n\t\t\tif d.fieldtype:\n\t\t\t\tif (not getattr(d, \"fieldname\", None)):\n\t\t\t\t\tif d.label:\n\t\t\t\t\t\td.fieldname = d.label.strip().lower().replace(' ','_')\n\t\t\t\t\t\tif d.fieldname in restricted:\n\t\t\t\t\t\t\td.fieldname = d.fieldname + '1'\n\t\t\t\t\telse:\n\t\t\t\t\t\td.fieldname = d.fieldtype.lower().replace(\" \",\"_\") + \"_\" + str(d.idx)\n\n\n\tdef validate_title_field(self):\n\t\t\"\"\"Throw exception if `title_field` is not a valid field.\"\"\"\n\t\tif self.title_field and \\\n\t\t\tself.title_field not in [d.fieldname for d in self.get(\"fields\")]:\n\t\t\tfrappe.throw(_(\"Title field must be a valid fieldname\"))\n\n\tdef validate_series(self, autoname=None, name=None):\n\t\t\"\"\"Validate if `autoname` property is correctly set.\"\"\"\n\t\tif not autoname: autoname = self.autoname\n\t\tif not name: name = self.name\n\n\t\tif not autoname and self.get(\"fields\", {\"fieldname\":\"naming_series\"}):\n\t\t\tself.autoname = \"naming_series:\"\n\n\t\tif autoname and (not autoname.startswith('field:')) \\\n\t\t\tand (not autoname.startswith('eval:')) \\\n\t\t\tand (not autoname in ('Prompt', 'hash')) \\\n\t\t\tand (not autoname.startswith('naming_series:')):\n\n\t\t\tprefix = autoname.split('.')[0]\n\t\t\tused_in = frappe.db.sql('select name from tabDocType where substring_index(autoname, \".\", 1) = %s and name!=%s', (prefix, name))\n\t\t\tif used_in:\n\t\t\t\tfrappe.throw(_(\"Series {0} already used in {1}\").format(prefix, used_in[0][0]))\n\n\tdef on_update(self):\n\t\t\"\"\"Update database schema, make controller templates if `custom` is not set and clear cache.\"\"\"\n\t\tfrom frappe.model.db_schema import updatedb\n\t\tupdatedb(self.name)\n\n\t\tself.change_modified_of_parent()\n\t\tmake_module_and_roles(self)\n\n\t\tfrom frappe import conf\n\t\tif not self.custom and not (frappe.flags.in_import or frappe.flags.in_test) and conf.get('developer_mode'):\n\t\t\tself.export_doc()\n\t\t\tself.make_controller_template()\n\n\t\t# update index\n\t\tif not self.custom:\n\t\t\tself.run_module_method(\"on_doctype_update\")\n\t\t\tif self.flags.in_insert:\n\t\t\t\tself.run_module_method(\"after_doctype_insert\")\n\n\t\tdelete_notification_count_for(doctype=self.name)\n\t\tfrappe.clear_cache(doctype=self.name)\n\n\tdef run_module_method(self, method):\n\t\tfrom frappe.modules import load_doctype_module\n\t\tmodule = load_doctype_module(self.name, self.module)\n\t\tif hasattr(module, method):\n\t\t\tgetattr(module, method)()\n\n\n\tdef before_rename(self, old, new, merge=False):\n\t\t\"\"\"Throw exception if merge. DocTypes cannot be merged.\"\"\"\n\t\tif merge:\n\t\t\tfrappe.throw(_(\"DocType can not be merged\"))\n\n\tdef after_rename(self, old, new, merge=False):\n\t\t\"\"\"Change table name using `RENAME TABLE` if table exists. Or update\n\t\t`doctype` property for Single type.\"\"\"\n\t\tif self.issingle:\n\t\t\tfrappe.db.sql(\"\"\"update tabSingles set doctype=%s where doctype=%s\"\"\", (new, old))\n\t\telse:\n\t\t\tfrappe.db.sql(\"rename table `tab%s` to `tab%s`\" % (old, new))\n\n\tdef before_reload(self):\n\t\t\"\"\"Preserve naming series changes in Property Setter.\"\"\"\n\t\tif not (self.issingle and self.istable):\n\t\t\tself.preserve_naming_series_options_in_property_setter()\n\n\tdef preserve_naming_series_options_in_property_setter(self):\n\t\t\"\"\"Preserve naming_series as property setter if it does not exist\"\"\"\n\t\tnaming_series = self.get(\"fields\", {\"fieldname\": \"naming_series\"})\n\n\t\tif not naming_series:\n\t\t\treturn\n\n\t\t# check if atleast 1 record exists\n\t\tif not (frappe.db.table_exists(self.name) and frappe.db.sql(\"select name from `tab{}` limit 1\".format(self.name))):\n\t\t\treturn\n\n\t\texisting_property_setter = frappe.db.get_value(\"Property Setter\", {\"doc_type\": self.name,\n\t\t\t\"property\": \"options\", \"field_name\": \"naming_series\"})\n\n\t\tif not existing_property_setter:\n\t\t\tmake_property_setter(self.name, \"naming_series\", \"options\", naming_series[0].options, \"Text\", validate_fields_for_doctype=False)\n\t\t\tif naming_series[0].default:\n\t\t\t\tmake_property_setter(self.name, \"naming_series\", \"default\", naming_series[0].default, \"Text\", validate_fields_for_doctype=False)\n\n\tdef export_doc(self):\n\t\t\"\"\"Export to standard folder `[module]/doctype/[name]/[name].json`.\"\"\"\n\t\tfrom frappe.modules.export_file import export_to_files\n\t\texport_to_files(record_list=[['DocType', self.name]])\n\n\tdef import_doc(self):\n\t\t\"\"\"Import from standard folder `[module]/doctype/[name]/[name].json`.\"\"\"\n\t\tfrom frappe.modules.import_module import import_from_files\n\t\timport_from_files(record_list=[[self.module, 'doctype', self.name]])\n\n\tdef make_controller_template(self):\n\t\t\"\"\"Make boilderplate controller template.\"\"\"\n\t\tmake_boilerplate(\"controller.py\", self)\n\n\t\tif not (self.istable or self.issingle):\n\t\t\tmake_boilerplate(\"test_controller.py\", self)\n\n\tdef make_amendable(self):\n\t\t\"\"\"If is_submittable is set, add amended_from docfields.\"\"\"\n\t\tif self.is_submittable:\n\t\t\tif not frappe.db.sql(\"\"\"select name from tabDocField\n\t\t\t\twhere fieldname = 'amended_from' and parent = %s\"\"\", self.name):\n\t\t\t\t\tself.append(\"fields\", {\n\t\t\t\t\t\t\"label\": \"Amended From\",\n\t\t\t\t\t\t\"fieldtype\": \"Link\",\n\t\t\t\t\t\t\"fieldname\": \"amended_from\",\n\t\t\t\t\t\t\"options\": self.name,\n\t\t\t\t\t\t\"read_only\": 1,\n\t\t\t\t\t\t\"print_hide\": 1,\n\t\t\t\t\t\t\"no_copy\": 1\n\t\t\t\t\t})\n\n\tdef get_max_idx(self):\n\t\t\"\"\"Returns the highest `idx`\"\"\"\n\t\tmax_idx = frappe.db.sql(\"\"\"select max(idx) from `tabDocField` where parent = %s\"\"\",\n\t\t\tself.name)\n\t\treturn max_idx and max_idx[0][0] or 0\n\ndef validate_fields_for_doctype(doctype):\n\tvalidate_fields(frappe.get_meta(doctype))\n\n# this is separate because it is also called via custom field\ndef validate_fields(meta):\n\t\"\"\"Validate doctype fields. Checks\n\n\t1. There are no illegal characters in fieldnames\n\t2. If fieldnames are unique.\n\t3. Fields that do have database columns are not mandatory.\n\t4. `Link` and `Table` options are valid.\n\t5. **Hidden** and **Mandatory** are not set simultaneously.\n\t7. `Check` type field has default as 0 or 1.\n\t8. `Dynamic Links` are correctly defined.\n\t9. Precision is set in numeric fields and is between 1 & 6.\n\t10. Fold is not at the end (if set).\n\t11. `search_fields` are valid.\n\n\t:param meta: `frappe.model.meta.Meta` object to check.\"\"\"\n\tdef check_illegal_characters(fieldname):\n\t\tfor c in ['.', ',', ' ', '-', '&', '%', '=', '\"', \"'\", '*', '$',\n\t\t\t'(', ')', '[', ']', '/']:\n\t\t\tif c in fieldname:\n\t\t\t\tfrappe.throw(_(\"{0} not allowed in fieldname {1}\").format(c, fieldname))\n\n\tdef check_unique_fieldname(fieldname):\n\t\tduplicates = filter(None, map(lambda df: df.fieldname==fieldname and str(df.idx) or None, fields))\n\t\tif len(duplicates) > 1:\n\t\t\tfrappe.throw(_(\"Fieldname {0} appears multiple times in rows {1}\").format(fieldname, \", \".join(duplicates)))\n\n\tdef check_illegal_mandatory(d):\n\t\tif (d.fieldtype in no_value_fields) and d.fieldtype!=\"Table\" and d.reqd:\n\t\t\tfrappe.throw(_(\"Field {0} of type {1} cannot be mandatory\").format(d.label, d.fieldtype))\n\n\tdef check_link_table_options(d):\n\t\tif d.fieldtype in (\"Link\", \"Table\"):\n\t\t\tif not d.options:\n\t\t\t\tfrappe.throw(_(\"Options requried for Link or Table type field {0} in row {1}\").format(d.label, d.idx))\n\t\t\tif d.options==\"[Select]\" or d.options==d.parent:\n\t\t\t\treturn\n\t\t\tif d.options != d.parent:\n\t\t\t\toptions = frappe.db.get_value(\"DocType\", d.options, \"name\")\n\t\t\t\tif not options:\n\t\t\t\t\tfrappe.throw(_(\"Options must be a valid DocType for field {0} in row {1}\").format(d.label, d.idx))\n\t\t\t\telse:\n\t\t\t\t\t# fix case\n\t\t\t\t\td.options = options\n\n\tdef check_hidden_and_mandatory(d):\n\t\tif d.hidden and d.reqd and not d.default:\n\t\t\tfrappe.throw(_(\"Field {0} in row {1} cannot be hidden and mandatory without default\").format(d.label, d.idx))\n\n\tdef check_width(d):\n\t\tif d.fieldtype == \"Currency\" and cint(d.width) < 100:\n\t\t\tfrappe.throw(_(\"Max width for type Currency is 100px in row {0}\").format(d.idx))\n\n\tdef check_in_list_view(d):\n\t\tif d.in_list_view and d.fieldtype!=\"Image\" and (d.fieldtype in no_value_fields):\n\t\t\tfrappe.throw(_(\"'In List View' not allowed for type {0} in row {1}\").format(d.fieldtype, d.idx))\n\n\tdef check_dynamic_link_options(d):\n\t\tif d.fieldtype==\"Dynamic Link\":\n\t\t\tdoctype_pointer = filter(lambda df: df.fieldname==d.options, fields)\n\t\t\tif not doctype_pointer or (doctype_pointer[0].fieldtype!=\"Link\") \\\n\t\t\t\tor (doctype_pointer[0].options!=\"DocType\"):\n\t\t\t\tfrappe.throw(_(\"Options 'Dynamic Link' type of field must point to another Link Field with options as 'DocType'\"))\n\n\tdef check_illegal_default(d):\n\t\tif d.fieldtype == \"Check\" and d.default and d.default not in ('0', '1'):\n\t\t\tfrappe.throw(_(\"Default for 'Check' type of field must be either '0' or '1'\"))\n\t\tif d.fieldtype == \"Select\" and d.default and (d.default not in d.options.split(\"\\n\")):\n\t\t\tfrappe.throw(_(\"Default for {0} must be an option\").format(d.fieldname))\n\n\tdef check_precision(d):\n\t\tif d.fieldtype in (\"Currency\", \"Float\", \"Percent\") and d.precision is not None and not (1 <= cint(d.precision) <= 6):\n\t\t\tfrappe.throw(_(\"Precision should be between 1 and 6\"))\n\n\tdef check_unique_and_text(d):\n\t\tif getattr(d, \"unique\", False) and d.fieldtype in (\"Text\", \"Long Text\", \"Small Text\", \"Code\", \"Text Editor\"):\n\t\t\tfrappe.throw(_(\"Fieldtype {0} for {1} cannot be unique\").format(d.fieldtype, d.label))\n\n\t\tif d.search_index and d.fieldtype in (\"Text\", \"Long Text\", \"Small Text\", \"Code\", \"Text Editor\"):\n\t\t\tfrappe.throw(_(\"Fieldtype {0} for {1} cannot be indexed\").format(d.fieldtype, d.label))\n\n\tdef check_fold(fields):\n\t\tfold_exists = False\n\t\tfor i, f in enumerate(fields):\n\t\t\tif f.fieldtype==\"Fold\":\n\t\t\t\tif fold_exists:\n\t\t\t\t\tfrappe.throw(_(\"There can be only one Fold in a form\"))\n\t\t\t\tfold_exists = True\n\t\t\t\tif i < len(fields)-1:\n\t\t\t\t\tnxt = fields[i+1]\n\t\t\t\t\tif nxt.fieldtype != \"Section Break\":\n\t\t\t\t\t\tfrappe.throw(_(\"Fold must come before a Section Break\"))\n\t\t\t\telse:\n\t\t\t\t\tfrappe.throw(_(\"Fold can not be at the end of the form\"))\n\n\tdef check_search_fields(meta):\n\t\tif not meta.search_fields:\n\t\t\treturn\n\n\t\tfieldname_list = [d.fieldname for d in fields]\n\t\tfor fieldname in (meta.search_fields or \"\").split(\",\"):\n\t\t\tfieldname = fieldname.strip()\n\t\t\tif fieldname not in fieldname_list:\n\t\t\t\tfrappe.throw(_(\"Search Fields should contain valid fieldnames\"))\n\n\n\n\tfields = meta.get(\"fields\")\n\tfor d in fields:\n\t\tif not d.permlevel: d.permlevel = 0\n\t\tif not d.fieldname:\n\t\t\tfrappe.throw(_(\"Fieldname is required in row {0}\").format(d.idx))\n\t\td.fieldname = d.fieldname.lower()\n\t\tcheck_illegal_characters(d.fieldname)\n\t\tcheck_unique_fieldname(d.fieldname)\n\t\tcheck_illegal_mandatory(d)\n\t\tcheck_link_table_options(d)\n\t\tcheck_dynamic_link_options(d)\n\t\tcheck_hidden_and_mandatory(d)\n\t\tcheck_in_list_view(d)\n\t\tcheck_illegal_default(d)\n\t\tcheck_unique_and_text(d)\n\n\tcheck_fold(fields)\n\tcheck_search_fields(meta)\n\ndef validate_permissions_for_doctype(doctype, for_remove=False):\n\t\"\"\"Validates if permissions are set correctly.\"\"\"\n\tdoctype = frappe.get_doc(\"DocType\", doctype)\n\n\tif frappe.conf.developer_mode and not frappe.flags.in_test:\n\t\t# save doctype\n\t\tdoctype.save()\n\n\telse:\n\t\tvalidate_permissions(doctype, for_remove)\n\n\t\t# save permissions\n\t\tfor perm in doctype.get(\"permissions\"):\n\t\t\tperm.db_update()\n\ndef validate_permissions(doctype, for_remove=False):\n\tpermissions = doctype.get(\"permissions\")\n\tif not permissions:\n\t\tfrappe.throw(_('Enter at least one permission row'), frappe.MandatoryError)\n\tissingle = issubmittable = isimportable = False\n\tif doctype:\n\t\tissingle = cint(doctype.issingle)\n\t\tissubmittable = cint(doctype.is_submittable)\n\t\tisimportable = cint(doctype.allow_import)\n\n\tdef get_txt(d):\n\t\treturn _(\"For {0} at level {1} in {2} in row {3}\").format(d.role, d.permlevel, d.parent, d.idx)\n\n\tdef check_atleast_one_set(d):\n\t\tif not d.read and not d.write and not d.submit and not d.cancel and not d.create:\n\t\t\tfrappe.throw(_(\"{0}: No basic permissions set\").format(get_txt(d)))\n\n\tdef check_double(d):\n\t\thas_similar = False\n\t\tfor p in permissions:\n\t\t\tif (p.role==d.role and p.permlevel==d.permlevel\n\t\t\t\tand p.apply_user_permissions==d.apply_user_permissions and p!=d):\n\t\t\t\thas_similar = True\n\t\t\t\tbreak\n\n\t\tif has_similar:\n\t\t\tfrappe.throw(_(\"{0}: Only one rule allowed with the same Role, Level and Apply User Permissions\").format(get_txt(d)))\n\n\tdef check_level_zero_is_set(d):\n\t\tif cint(d.permlevel) > 0 and d.role != 'All':\n\t\t\thas_zero_perm = False\n\t\t\tfor p in permissions:\n\t\t\t\tif p.role==d.role and (p.permlevel or 0)==0 and p!=d:\n\t\t\t\t\thas_zero_perm = True\n\t\t\t\t\tbreak\n\n\t\t\tif not has_zero_perm:\n\t\t\t\tfrappe.throw(_(\"{0}: Permission at level 0 must be set before higher levels are set\").format(get_txt(d)))\n\n\t\t\tfor invalid in (\"create\", \"submit\", \"cancel\", \"amend\"):\n\t\t\t\tif d.get(invalid): d.set(invalid, 0)\n\n\tdef check_permission_dependency(d):\n\t\tif d.cancel and not d.submit:\n\t\t\tfrappe.throw(_(\"{0}: Cannot set Cancel without Submit\").format(get_txt(d)))\n\n\t\tif (d.submit or d.cancel or d.amend) and not d.write:\n\t\t\tfrappe.throw(_(\"{0}: Cannot set Submit, Cancel, Amend without Write\").format(get_txt(d)))\n\t\tif d.amend and not d.write:\n\t\t\tfrappe.throw(_(\"{0}: Cannot set Amend without Cancel\").format(get_txt(d)))\n\t\tif d.get(\"import\") and not d.create:\n\t\t\tfrappe.throw(_(\"{0}: Cannot set Import without Create\").format(get_txt(d)))\n\n\tdef remove_rights_for_single(d):\n\t\tif not issingle:\n\t\t\treturn\n\n\t\tif d.report:\n\t\t\tfrappe.msgprint(_(\"Report cannot be set for Single types\"))\n\t\t\td.report = 0\n\t\t\td.set(\"import\", 0)\n\t\t\td.set(\"export\", 0)\n\n\t\tfor ptype, label in (\n\t\t\t(\"set_user_permissions\", _(\"Set User Permissions\")),\n\t\t\t(\"apply_user_permissions\", _(\"Apply User Permissions\"))):\n\t\t\tif d.get(ptype):\n\t\t\t\td.set(ptype, 0)\n\t\t\t\tfrappe.msgprint(_(\"{0} cannot be set for Single types\").format(label))\n\n\tdef check_if_submittable(d):\n\t\tif d.submit and not issubmittable:\n\t\t\tfrappe.throw(_(\"{0}: Cannot set Assign Submit if not Submittable\").format(get_txt(d)))\n\t\telif d.amend and not issubmittable:\n\t\t\tfrappe.throw(_(\"{0}: Cannot set Assign Amend if not Submittable\").format(get_txt(d)))\n\n\tdef check_if_importable(d):\n\t\tif d.get(\"import\") and not isimportable:\n\t\t\tfrappe.throw(_(\"{0}: Cannot set import as {1} is not importable\").format(get_txt(d), doctype))\n\n\tfor d in permissions:\n\t\tif not d.permlevel:\n\t\t\td.permlevel=0\n\t\tcheck_atleast_one_set(d)\n\t\tif not for_remove:\n\t\t\tcheck_double(d)\n\t\t\tcheck_permission_dependency(d)\n\t\t\tcheck_if_submittable(d)\n\t\t\tcheck_if_importable(d)\n\t\tcheck_level_zero_is_set(d)\n\t\tremove_rights_for_single(d)\n\ndef make_module_and_roles(doc, perm_fieldname=\"permissions\"):\n\t\"\"\"Make `Module Def` and `Role` records if already not made. Called while installing.\"\"\"\n\ttry:\n\t\tif not frappe.db.exists(\"Module Def\", doc.module):\n\t\t\tm = frappe.get_doc({\"doctype\": \"Module Def\", \"module_name\": doc.module})\n\t\t\tm.app_name = frappe.local.module_app[frappe.scrub(doc.module)]\n\t\t\tm.flags.ignore_mandatory = m.flags.ignore_permissions = True\n\t\t\tm.insert()\n\n\t\tdefault_roles = [\"Administrator\", \"Guest\", \"All\"]\n\t\troles = [p.role for p in doc.get(\"permissions\") or []] + default_roles\n\n\t\tfor role in list(set(roles)):\n\t\t\tif not frappe.db.exists(\"Role\", role):\n\t\t\t\tr = frappe.get_doc({\"doctype\": \"Role\", \"role_name\": role})\n\t\t\t\tr.role_name = role\n\t\t\t\tr.flags.ignore_mandatory = r.flags.ignore_permissions = True\n\t\t\t\tr.insert()\n\texcept frappe.DoesNotExistError, e:\n\t\tpass\n\texcept frappe.SQLError, e:\n\t\tif e.args[0]==1146:\n\t\t\tpass\n\t\telse:\n\t\t\traise\n\ndef init_list(doctype):\n\t\"\"\"Make boilerplate list views.\"\"\"\n\tdoc = frappe.get_meta(doctype)\n\tmake_boilerplate(\"controller_list.js\", doc)\n\tmake_boilerplate(\"controller_list.html\", doc)\n\n", "ort time\n\n\nclass Config(object):\n    \"\"\"Config to be given to an instance of translator to do the Authorization.\"\"\"\n\n    def __init__(self, translator_client_id, translator_client_secret):\n        assert translator_client_id is not None\n        assert type(translator_client_id) is str\n\n        assert translator_client_secret is not None\n        assert type(translator_client_secret) is str\n\n        super(Config, self).__init__()\n        self.translator_client_id = translator_client_id\n        self.translator_client_secret = translator_client_secret\n\n\nclass Translator(object):\n    \"\"\"An instance of this class can be used to detect language and translate text.\"\"\"\n\n    def __init__(self, config):\n        assert isinstance(config, Config) is True\n        super(Translator, self).__init__()\n        self.config = config\n        self.access_token = self.token_expiry = None\n\n    def _get_access_token(self):\n        data = {\n            \"client_id\": self.config.translator_client_id,\n            \"client_secret\": self.config.translator_client_secret,\n            \"scope\": 'http://api.microsofttranslator.com',\n            \"grant_type\": 'client_credentials'\n        }\n        resp = requests.post(url='https://datamarket.accesscontrol.windows.net/v2/OAuth2-13',\n                             data=urllib.urlencode(data))\n        if not resp.ok:\n            resp.raise_for_status()\n\n        resp_content = resp.json()\n        token_expiry = time.time() + int(resp_content[\"expires_in\"])  # in seconds\n        access_token = resp_content[\"access_token\"]\n\n        return (access_token, token_expiry)\n\n    def _authorization_header(self):\n        # Auth tokens are only valid for a limited number of seconds.\n        # Token expiry is sent by the server at the time of obtaining the\n        # token and can be used to auto-refresh the token before making a\n        # new request, post expiry time.\n        if (not self.token_expiry) or (self.token_expiry < time.time()):\n            self.access_token, self.token_expiry = self._get_access_token()\n\n        return \"Bearer\" + \" \" + self.access_token\n\n    def detect_language(self, text):\n        text = text.encode('utf-8')\n        authorization_header = self._authorization_header()\n        headers = {\"Authorization\": authorization_header}\n        data = {\"text\": text}\n        resp = requests.get(url='http://api.microsofttranslator.com/v2/Http.svc/Detect',\n                            params=data, headers=headers)\n        if not resp.ok:\n            resp.raise_for_status()\n\n        try:\n            t = resp.text.encode('utf-8')\n            # Different unicodes for different languages are not parsed\n            # correctly with xml module.\n            detected_language_code = t.split('>')[1].split('<')[0]\n            return detected_language_code\n        except Exception as e:\n            sys.stderr.write(e)\n            raise\n\n    def translate(self, text, from_language, to_language):\n        text = text.encode('utf-8')\n        authorization_header = self._authorization_header()\n        headers = {\"Authorization\": authorization_header}\n        data = {\n            \"text\": text,\n            \"from\": from_language,\n            \"to\": to_language\n        }\n        resp = requests.get(url='http://api.microsofttranslator.com/v2/Http.svc/Translate',\n                            params=data, headers=headers)\n\n        if not resp.ok:\n            resp.raise_for_status()\n\n        try:\n            t = resp.text.encode('utf-8')\n            translated_text = t.split('>')[1].split('<')[0]\n            return translated_text\n        except Exception:\n            sys.stderr.write(u\"Could not parse XML {0}\".format(resp.text))\n            raise\n", "\n    \n    \n        \"\"\"\n        Creates the himesis graph representing the DSLTrans rule EReference.\n        \"\"\"\n        # Flag this instance as compiled now\n        self.is_compiled = True\n        \n        super(HEReference_2, self).__init__(name='HEReference_2', num_nodes=0, edges=[])\n        \n        \n        # Set the graph attributes\n        self[\"mm__\"] = ['HimesisMM']\n        \n        self[\"name\"] = \"\"\"EReference\"\"\"\n        self[\"GUID__\"] = uuid.uuid3(uuid.NAMESPACE_DNS,'EReference')\n        \n        # match model. We only support one match model\n        self.add_node()\n        self.vs[0][\"mm__\"] = \"\"\"MatchModel\"\"\"\n        \n        # apply model node\n        self.add_node()\n        self.vs[1][\"mm__\"] = \"\"\"ApplyModel\"\"\"\n        \n        # paired with relation between match and apply models\n        self.add_node()\n        self.vs[2][\"mm__\"] = \"\"\"paired_with\"\"\"\n \n        \n        # match class EReference() node\n        self.add_node()\n\n        self.vs[3][\"mm__\"] = \"\"\"EReference\"\"\" \n        self.vs[3][\"attr1\"] = \"\"\"+\"\"\" \n        # match_contains node for class EReference()\n        self.add_node()\n        self.vs[4][\"mm__\"] = \"\"\"match_contains\"\"\"\n        \n        \n        # apply class EReference() node\n        self.add_node()\n\n        self.vs[5][\"mm__\"] = \"\"\"EReference\"\"\" \n        self.vs[5][\"attr1\"] = \"\"\"1\"\"\"\n        # apply_contains node for class EReference()\n        self.add_node()\n        self.vs[6][\"mm__\"] = \"\"\"apply_contains\"\"\"\n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        # Add the edges\n        self.add_edges([\n                (0,4), # matchmodel -> match_contains\n                (4,3), # match_contains -> match_class EReference()\n                (1,6), # applymodel -> apply_contains\n                (6,5), # apply_contains -> apply_class EReference()\n                (0,2), # matchmodel -> pairedwith\n                (2,1) # pairedwith -> applyModel\t\t\t\t\n\t\t])\n\n        # Add the attribute equations\n        self[\"equations\"] = [((5,'name'),(3,'name')), ((5,'ordered'),(3,'ordered')), ((5,'unique'),(3,'unique')), ((5,'lowerBound'),(3,'lowerBound')), ((5,'upperBound'),(3,'upperBound')), ((5,'changeable'),(3,'changeable')), ((5,'volatile'),(3,'volatile')), ((5,'transient'),(3,'transient')), ((5,'defaultValueLiteral'),(3,'defaultValueLiteral')), ((5,'unsettable'),(3,'unsettable')), ((5,'derived'),(3,'derived')), ((5,'containment'),(3,'containment')), ((5,'resolveProxies'),(3,'resolveProxies')), ((5,'ApplyAttribute'),('constant','solveRef')), ]\n\n        \n", "g\n# See: http://doc.scrapy.org/en/latest/topics/item-pipeline.html\n\nfrom sqlalchemy.orm import sessionmaker\nfrom models import OxfordPicture, db_connect, create_oxford_picture_table\nfrom scrapy.selector import Selector\nimport urllib2\n\nclass ExpressionFilterPipeline(object):\n\tdef _get_picture(self, oxford_picture, url):\n\t\ttry:\n\t\t\thdr = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n\t\t\t\t\t'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n\t\t\t\t\t'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n\t\t\t\t\t'Accept-Encoding': 'none',\n\t\t\t\t\t'Accept-Language': 'en-US,en;q=0.8',\n\t\t\t\t\t'Connection': 'keep-alive'}\n\t\t\treq = urllib2.Request(url, headers=hdr)\n\t\t\tresp = urllib2.urlopen(req, timeout=15)\n\t\t\toxford_picture.picture = resp.read()\n\t\texcept Exception as error:\n\t\t\tprint error\n\n\tdef process_item(self, item, spider):\n\t\tcontent = item['content']\n\t\twriter = OxfordPictureWriter()\n\t\toxford_picture = OxfordPicture()\n\t\tsel = Selector(text=content)\n\t\tword = ''.join(sel.xpath('//div[@class=\"webtop-g\"]/h2//text()').extract()).strip()\n\t\tprint word\n\t\toxford_picture.word = word\n\t\tpicture_url = sel.xpath('//div[@id=\"ox-enlarge\"]/a/img/@src').extract()[0]\n\t\tprint picture_url\n\t\toxford_picture.img_url = picture_url \n\t\tself._get_picture(oxford_picture, oxford_picture.img_url)\n\t\toxford_picture.imgref_url = item['url']\n\t\twriter.write(oxford_picture)\n\t\treturn item\n\nclass OxfordPictureWriter(object):\n\tdef __init__(self):\n\t\tengine = db_connect()\n\t\tcreate_oxford_picture_table(engine)\n\t\tself.Session = sessionmaker(bind=engine)\n\n\tdef write(self, photographic):\n\t\tsession = self.Session()\n\n\t\ttry:\n\t\t\tsession.add(photographic)\n\t\t\tsession.commit()\n\t\texcept:\n\t\t\tsession.rollback()\n\t\t\traise\n\t\tfinally:\n\t\t\tsession.close()\n", " redistribute it and/or\n# modify it under the terms of the MIT License; see the\n# LICENSE file for more details.\n\nfrom operator import attrgetter\n\nfrom flask import flash, request, session\nfrom sqlalchemy.orm import joinedload, subqueryload\n\nfrom indico.core.db import db\nfrom indico.modules.events.abstracts.models.abstracts import Abstract, AbstractState\nfrom indico.modules.events.abstracts.models.fields import AbstractFieldValue\nfrom indico.modules.events.abstracts.models.reviews import AbstractReview\nfrom indico.modules.events.abstracts.util import can_create_invited_abstracts\nfrom indico.modules.events.contributions.models.fields import ContributionField\nfrom indico.modules.events.tracks.models.tracks import Track\nfrom indico.modules.events.util import ListGeneratorBase\nfrom indico.util.i18n import _\nfrom indico.web.flask.templating import get_template_module\n\n\nclass AbstractListGeneratorBase(ListGeneratorBase):\n    \"\"\"Listing and filtering actions in an abstract list.\"\"\"\n\n    show_contribution_fields = True\n\n    def __init__(self, event):\n        super().__init__(event)\n\n        self.default_list_config = {\n            'items': (),\n            'filters': {'fields': {}, 'items': {}, 'extra': {}}\n        }\n        track_empty = {None: _('No track')}\n        type_empty = {None: _('No type')}\n        track_choices = {str(t.id): t.title for t in sorted(self.event.tracks, key=attrgetter('title'))}\n        type_choices = {str(t.id): t.name for t in sorted(self.event.contribution_types, key=attrgetter('name'))}\n        self.static_items = {\n            'state': {'title': _('State'), 'filter_choices': {state.value: state.title for state in AbstractState}},\n            'submitter': {'title': _('Submitter')},\n            'authors': {'title': _('Primary authors')},\n            'accepted_track': {'title': _('Accepted track'), 'filter_choices': track_empty | track_choices},\n            'submitted_for_tracks': {'title': _('Submitted for tracks'), 'filter_choices': track_empty | track_choices},\n            'reviewed_for_tracks': {'title': _('Reviewed for tracks'), 'filter_choices': track_empty | track_choices},\n            'accepted_contrib_type': {'title': _('Accepted type'), 'filter_choices': type_empty | type_choices},\n            'submitted_contrib_type': {'title': _('Submitted type'), 'filter_choices': type_empty | type_choices},\n            'score': {'title': _('Score')},\n            'submitted_dt': {'title': _('Submission date')},\n            'modified_dt': {'title': _('Modification date')},\n            'description': {'title': _('Content')}\n        }\n        self.extra_filters = {}\n        self.list_config = self._get_config()\n\n    def _get_static_columns(self, ids):\n        \"\"\"\n        Retrieve information needed for the header of the static columns.\n\n        :return: a list of {'id': ..., 'caption': ...} dicts\n        \"\"\"\n        return [{'id': id_, 'caption': self.static_items[id_]['title']} for id_ in self.static_items if id_ in ids]\n\n    def get_all_contribution_fields(self):\n        \"\"\"Return the list of contribution fields for the event.\"\"\"\n        return self.event.contribution_fields.all() if self.show_contribution_fields else []\n\n    def _get_sorted_contribution_fields(self, item_ids):\n        \"\"\"\n        Return the contribution fields ordered by their position in\n        the abstract form.\n        \"\"\"\n\n        if not item_ids or not self.show_contribution_fields:\n            return []\n        return (ContributionField.query\n                .with_parent(self.event)\n                .filter(ContributionField.id.in_(item_ids))\n                .order_by(ContributionField.position)\n                .all())\n\n    def _get_filters_from_request(self):\n        filters = super()._get_filters_from_request()\n        for field in self.event.contribution_fields:\n            if field.field_type == 'single_choice':\n                options = [x if x != 'None' else None for x in request.form.getlist(f'field_{field.id}')]\n                if options:\n                    filters['fields'][str(field.id)] = options\n        return filters\n\n    def _build_query(self):\n        return (Abstract.query\n                .with_parent(self.event)\n                .options(joinedload('submitter'),\n                         joinedload('accepted_track'),\n                         joinedload('accepted_contrib_type'),\n                         joinedload('submitted_contrib_type'),\n                         joinedload('contribution').load_only('id', 'event_id'),\n                         subqueryload('field_values'),\n                         subqueryload('submitted_for_tracks'),\n                         subqueryload('reviewed_for_tracks'),\n                         subqueryload('person_links'),\n                         subqueryload('reviews').joinedload('ratings'))\n                .order_by(Abstract.friendly_id))\n\n    def _filter_list_entries(self, query, filters):\n        criteria = []\n        field_filters = filters.get('fields')\n        item_filters = filters.get('items')\n        extra_filters = filters.get('extra')\n\n        if not (field_filters or item_filters or extra_filters):\n            return query\n\n        if field_filters:\n            for field_id, field_values in field_filters.items():\n                field_values = set(field_values)\n\n                # Support filtering by 'No selection' in single-choice abstract fields.\n                field_criteria = []\n                if None in field_values:\n                    # Handle the case when there is no value in\n                    # 'Abstract.field_values' matching the 'field_id'.\n                    # This can happen when custom fields are added after the\n                    # abstract had already been submitted or when submitting as a regular\n                    # user who cannot see a field that is only editable by managers.\n                    # In these cases, we still want to show the abstracts.\n                    field_values.discard(None)\n                    field_criteria += [\n                        ~Abstract.field_values.any(AbstractFieldValue.contribution_field_id == field_id),\n                        Abstract.field_values.any(db.and_(\n                            AbstractFieldValue.contribution_field_id == field_id,\n                            AbstractFieldValue.data.op('#>>')('{}').is_(None)\n                        ))\n                    ]\n                if field_values:\n                    field_criteria.append(Abstract.field_values.any(db.and_(\n                        AbstractFieldValue.contribution_field_id == field_id,\n                        AbstractFieldValue.data.op('#>>')('{}').in_(field_values)\n                    )))\n\n                criteria.append(db.or_(*field_criteria))\n\n        if item_filters:\n            static_filters = {\n                'accepted_track': Abstract.accepted_track_id,\n                'accepted_contrib_type': Abstract.accepted_contrib_type_id,\n                'submitted_contrib_type': Abstract.submitted_contrib_type_id,\n                'submitted_for_tracks': Abstract.submitted_for_tracks,\n                'reviewed_for_tracks': Abstract.reviewed_for_tracks\n            }\n            for key, column in static_filters.items():\n                ids = set(item_filters.get(key, ()))\n                if not ids:\n                    continue\n                column_criteria = []\n                if '_for_tracks' in key:\n                    if None in ids:\n                        column_criteria.append(~column.any())\n                        ids.discard(None)\n                    if ids:\n                        column_criteria.append(column.any(Track.id.in_(ids)))\n                else:\n                    if None in ids:\n                        column_criteria.append(column.is_(None))\n                        ids.discard(None)\n                    if ids:\n                        column_criteria.append(column.in_(ids))\n                criteria.append(db.or_(*column_criteria))\n            if 'state' in item_filters:\n                states = [AbstractState(int(state)) for state in item_filters['state']]\n                criteria.append(Abstract.state.in_(states))\n        if extra_filters:\n            if extra_filters.get('multiple_tracks'):\n                submitted_for_count = (db.select([db.func.count()])\n                                       .scalar_subquery()\n                                       .where(Abstract.submitted_for_tracks.prop.primaryjoin))\n                criteria.append(submitted_for_count > 1)\n            if extra_filters.get('comments'):\n                criteria.append(Abstract.submission_comment != '')\n        return query.filter(db.and_(*criteria))\n\n    def get_list_kwargs(self):\n        list_config = self._get_config()\n        abstracts_query = self._build_query()\n        total_entries = abstracts_query.count()\n        abstracts = self._filter_list_entries(abstracts_query, list_config['filters']).all()\n        dynamic_item_ids, static_item_ids = self._split_item_ids(list_config['items'], 'dynamic')\n        static_columns = self._get_static_columns(static_item_ids)\n        dynamic_columns = self._get_sorted_contribution_fields(dynamic_item_ids)\n        return {\n            'abstracts': abstracts,\n            'total_abstracts': total_entries,\n            'static_columns': static_columns,\n            'dynamic_columns': dynamic_columns,\n            'filtering_enabled': total_entries != len(abstracts)\n        }\n\n    def get_list_export_config(self):\n        list_config = self._get_config()\n        static_item_ids, dynamic_item_ids = self._split_item_ids(list_config['items'], 'static')\n        return {\n            'static_item_ids': static_item_ids,\n            'dynamic_items': self._get_sorted_contribution_fields(dynamic_item_ids)\n        }\n\n    def render_list(self, abstract=None):\n        list_kwargs = self.get_list_kwargs()\n        tpl = get_template_module('events/abstracts/management/_abstract_list.html')\n        filtering_enabled = list_kwargs.pop('filtering_enabled')\n        tpl_lists = get_template_module('events/management/_lists.html')\n        filter_statistics = tpl_lists.render_displayed_entries_fragment(len(list_kwargs['abstracts']),\n                                                                        list_kwargs['total_abstracts'])\n        return {\n            'html': tpl.render_abstract_list(**list_kwargs,\n                                             can_create_invited_abstracts=can_create_invited_abstracts(self.event)),\n            'filtering_enabled': filtering_enabled,\n            'filter_statistics': filter_statistics,\n            'hide_abstract': abstract not in list_kwargs['abstracts'] if abstract else None\n        }\n\n    def flash_info_message(self, abstract):\n        flash(_(\"The abstract '{}' is not displayed in the list due to the enabled filters\")\n              .format(abstract.title), 'info')\n\n\nclass AbstractListGeneratorManagement(AbstractListGeneratorBase):\n    \"\"\"\n    Listing and filtering actions in the abstract list in the\n    management view.\n    \"\"\"\n\n    list_link_type = 'abstract_management'\n    endpoint = '.manage_abstract_list'\n\n    def __init__(self, event):\n        super().__init__(event)\n        self.default_list_config['items'] = ('submitted_contrib_type', 'accepted_contrib_type', 'state')\n        if event.tracks:\n            self.default_list_config['items'] += ('submitted_for_tracks', 'reviewed_for_tracks', 'accepted_track')\n        self.extra_filters = {\n            'multiple_tracks': {'title': _('Proposed for multiple tracks'), 'type': 'bool'},\n            'comments': {'title': _('Must have comments'), 'type': 'bool'}\n        }\n\n\nclass AbstractListGeneratorDisplay(AbstractListGeneratorBase):\n    \"\"\"\n    Listing and filtering actions in the abstract list in the display view.\n    \"\"\"\n\n    list_link_type = 'abstract_display'\n    endpoint = '.display_reviewable_track_abstracts'\n    show_contribution_fields = False\n\n    def __init__(self, event, track):\n        super().__init__(event)\n        self.track = track\n        self.default_list_config['items'] = ('accepted_contrib_type', 'state')\n        items = {'submitted_contrib_type', 'submitter', 'accepted_contrib_type', 'state'}\n        if self.track.can_convene(session.user):\n            items.add('score')\n        self.static_items = {key: value for key, value in self.static_items.items() if key in items}\n\n    def _build_query(self):\n        return (super()._build_query()\n                .filter(Abstract.state != AbstractState.invited,\n                        Abstract.reviewed_for_tracks.contains(self.track)))\n\n    def get_user_reviewed_abstracts_for_track(self, user, track):\n        return (Abstract.query\n                .join(Abstract.reviews)\n                .filter(AbstractReview.user == user,\n                        Abstract.state != AbstractState.invited,\n                        Abstract.reviewed_for_tracks.contains(track),\n                        ~Abstract.is_deleted)\n                .all())\n\n    def get_list_kwargs(self):\n        kwargs = super().get_list_kwargs()\n        kwargs['reviewed_abstracts'] = self.get_user_reviewed_abstracts_for_track(session.user, self.track)\n        return kwargs\n", "o the database\n        \n        Args:\n            variant (dict): A variant dictionary\n        \"\"\"\n        raise NotImplementedError\n    \n    def get_variant(self, variant):\n        \"\"\"Return a variant from the database\n        \n        Args:\n            variant (dict): A variant dictionary\n    \n        Returns:\n            variant (dict): A variant dictionary\n        \"\"\"\n        raise NotImplementedError\n    \n    def delete_variant(self, variant):\n        \"\"\"Remove variant from database\n            \n            This means that we take down the observations variable with one.\n            If 'observations' == 1 we remove the variant. If variant was homozygote\n            we decrease 'homozygote' with one.\n            \n            Args:\n                variant (dict): A variant dictionary            \n        \"\"\"\n        raise NotImplementedError\n\n    def add_bulk(self, variants):\n        \"\"\"Add a bulk of variants to the database\n        \n        Args:\n            variants (Iterable(dict)): An iterable with variant dictionaries\n        \"\"\"\n        raise NotImplementedError\n    ", "in.site.register(Way)\n", "s in ascii_lowercase:\n    if not c[s]:\n        print('not pangram')\n        break\nelse:\n    print('pangram')\n", ".\n\"\"\"\n\n# File: connector.py\n# Author: Muse\n# Create: 2017.01.27 23:40\n# Modify: 2017.02.04 09:30\n\n\nimport pdb\nimport event\n\nfrom logicsocket import LogicSocket\nfrom spiderconf import SpiderConf as Config\n\n\ndef initialize():\n    \"\"\"\n    Called from c layer, bind to giving addr, as a data recv server.\n    \"\"\"\n    if \"Connectoraddr\" in Config and \"Capability\" in Config:\n        connector = LogicSocket( \\\n            event.CONNECTOR_EVENT, connaddr = Config[\"Connectoraddr\"])\n\n        connector.psend((\"add\", Config[\"Taskaddr\"], Config[\"Capability\"]))\n        connector.psend((\"fuck\", ))\n\n        return  connector\n\ndef destroy(connector):\n    connector.destroy()\n\n    return  True\n\ndef process(connector, fd, events):\n    if events & event.EPOLLRDHUP:\n        print(\"watcher closed\")\n\n", " raise ValueError(\"Please use python 3.\")\n\nfrom ._version import get_versions\n\nfrom .ISRSpectrum import Specinit, ioncheck, getionmass\n\nfrom .mathutils import *\n\n__version__ = get_versions()[\"version\"]\n\ndel get_versions\n", "n: 2.7\n    Description: Pivoting API for various REST services for rapid intel gathering\n    Copyright (c) 2015 Rick Correa\n\n    The MIT License (MIT)\n\n    Permission is hereby granted, free of charge, to any person obtaining a copy\n    of this software and associated documentation files (the \"Software\"), to deal\n    in the Software without restriction, including without limitation the rights\n    to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n    copies of the Software, and to permit persons to whom the Software is\n    furnished to do so, subject to the following conditions:\n\n    The above copyright notice and this permission notice shall be included in\n    all copies or substantial portions of the Software.\n\n    THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n    IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n    FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n    LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n    OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n    THE SOFTWARE.\n'''\n\n\n\n__author__ = 'rickcorrea'\n\nimport pivotEngine\nimport collections\nimport peEngine\nimport hashlib\n\n\nclass Hopper(object):\n    def __init__(self):\n        self.hash_container = collections.defaultdict(list)\n        self.domain_container = collections.defaultdict(list)\n        self.url_container = collections.defaultdict(list)\n        self.ip_container = collections.defaultdict(list)\n        self.email_container = collections.defaultdict(list)\n        self.import_table_container = collections.defaultdict(list)\n        self.section_hash_container = collections.defaultdict(list)\n\n\n    def __str__(self):\n        output = \"\"\n        output += \"%s\\n\" %self.hash_container.keys()\n        output += \"%s\\n\" %self.domain_container.keys()\n        output += \"%s\\n\" %self.url_container.keys()\n        output += \"%s\\n\" %self.ip_container.keys()\n        output += \"%s\\n\" %self.email_container.keys()\n        output += \"%s\\n\" %self.import_table_container.keys()\n        output += \"%s\\n\" %self.section_hash_container.keys()\n        return output\n\n\n    def __add__(self, other):\n        output = Hopper()\n\n        if self == None:\n            if other == None:\n                return None\n            else:\n                return other\n\n        if type(other) != Hopper:\n            raise Exception, \"Only Hopper Instances allowed to be added.  Got: [%s]\" %type(other)\n\n        temp = self.hash_container.copy()\n        temp.update(other.hash_container)\n        output.hash_container = temp\n\n        temp = self.domain_container.copy()\n        temp.update(other.domain_container)\n        output.domain_container = temp\n\n        temp = self.url_container.copy()\n        temp.update(other.url_container)\n        output.url_container = temp\n\n        temp = self.ip_container.copy()\n        temp.update(other.ip_container)\n        output.ip_container = temp\n\n        temp = self.email_container.copy()\n        temp.update(other.email_container)\n        output.email_container = temp\n\n        temp = self.domain_container.copy()\n        temp.update(other.domain_container)\n        output.domain_container = temp\n\n        temp = self.import_table_container.copy()\n        temp.update(other.import_table_container)\n        output.import_table_container = temp\n\n        temp = self.section_hash_container.copy()\n        temp.update(other.section_hash_container)\n        output.section_hash_container = temp\n\n        return output\n\n\ndef parseVTIP(dump):\n    h = Hopper()\n\n    if dump[\"response_code\"] == 0:\n        return h\n    #TODO: There might be good stuff in \"undetected_downloaded_samples\"\n    if dump.has_key(\"detected_downloaded_samples\"):\n        samples = dump[\"detected_downloaded_samples\"]\n        for sample in samples:\n            h.hash_container[sample[\"sha256\"]].append(\"VT\")\n        samples = dump[\"detected_urls\"]\n        for sample in samples:\n            h.url_container[sample[\"url\"]].append(\"VT\")\n        samples = dump[\"resolutions\"]\n        for sample in samples:\n            h.domain_container[sample[\"hostname\"]].append(\"VT\")\n        samples = dump[\"detected_communicating_samples\"]\n        for sample in samples:\n            h.hash_container[sample[\"sha256\"]].append(\"VT\")\n\n    return h\n\n\ndef parseVTDomain(dump):\n    h = Hopper()\n\n    if dump[\"response_code\"] == -1:\n        return h\n\n    if dump[\"verbose_msg\"] == \"Domain not found in dataset\":\n        return h\n\n    if dump.has_key(\"resolutions\"):\n        resolutions = dump[\"resolutions\"]\n        for res in resolutions:\n            h.ip_container[res[\"ip_address\"]].append(\"VT Domain\")\n    if dump.has_key(\"detected_urls\"):\n        detected_urls = dump[\"detected_urls\"]\n        for res in detected_urls:\n            h.url_container[res[\"url\"]].append(\"VT Domain\")\n    if dump.has_key(\"domain_siblings\"):  # TODO: Test, not sure if this works since it's not well documented in VT\n        dSiblings = dump[\"domain_siblings\"]\n        for sibling in dSiblings:\n            h.domain_container[sibling].append(\"VT Sibling Domain\")\n    if dump.has_key(\"subdomains\"):\n        subdomains = dump[\"subdomains\"]\n        for subd in subdomains:\n            h.domain_container[subd].append(\"VT Subdomains\")\n\n    return h\n\n\n# fullScan = True does a pivot search on section and import hashes, both are slow and expensive and have a different\n#  daily limit (50k)\ndef parseVTFile(dump, fullScan = False):\n    h = Hopper()\n\n    if dump[\"response_code\"] == 0:\n        return h\n\n\n    add_info = dump[\"additional_info\"]\n\n    impTblHash = add_info[\"pe-imphash\"]\n    h.import_table_container[impTblHash].append(\"VT-import-table\")\n    \n    if fullScan == True:\n        out = pivotEngine.pivotVTFile(impTblHash, \"imphash\")\n        relatedHashes = __parseVTHashes__(out)\n        if relatedHashes != None:\n            for entry in relatedHashes:\n                h.hash_container[entry].append(\"VT-import-table\")\n\n\n    if add_info.has_key(\"behaviour-v1\"):\n        behavior = add_info[\"behaviour-v1\"]\n        net = behavior[\"network\"]\n        udp = net[\"udp\"]\n        tcp = net[\"tcp\"]\n        dns = net[\"dns\"]\n        http = net[\"http\"]\n\n        for i in udp:\n            base = i.split(\":\")[0]\n            if base == u\"<MACHINE_DNS_SERVER>\":\n                continue\n            h.ip_container[base].append(\"VT-behavior\")\n        for i in tcp:\n            base = i.split(\":\")[0]\n            h.ip_container[base].append(\"VT-behavior\")\n        for i in dns:\n            h.ip_container[i[\"ip\"]].append(\"VT-behavior\")\n            h.domain_container[i[\"hostname\"]].append(\"VT-behavior\")\n        for i in http:\n            h.url_container[i[\"url\"]].append(\"VT-behavior\")\n\n    # full scans are a really expensive VT operation.  Make it optional\n    if fullScan == True:\n        sections = add_info[\"sections\"]\n        for i in sections:\n            sname = i[0]\n            shash = i[5]\n            h.section_hash_container[shash].append(\"VT-section-%s\" %sname)\n            out = pivotEngine.pivotVTFile(shash, \"sectionmd5\")\n            relatedHashes = __parseVTHashes__(out)\n            if relatedHashes == None:\n                continue\n            for entry in relatedHashes:\n                h.hash_container[entry].append(\"VT-section-%s\" %sname)\n\n    h.hash_container[dump[\"md5\"]].append(\"VT-Hash\")\n\n    return h\n\n\ndef __parseVTHashes__(dump):\n    if dump[\"response_code\"] == 0:\n        return\n    else:\n        return dump[\"hashes\"]\n\n\ndef isEmpty(field):\n    res = False\n    if field == None:\n        res = True\n    elif len(field) == 0:\n        res = True\n    else:\n        res = False\n\n    #print \"isEmpty(%s)->%s\" %(field, res)\n    return res\n\n    \ndef parseOpenDNSWhoIs(resultslist):\n    h = Hopper()\n    if type(resultslist) == dict:\n        results = resultslist\n        if results.has_key(\"errorMessage\"):\n            if results[\"errorMessage\"] == \"Not found\":\n                return h\n        if results.has_key(\"administrativeContactEmail\"):\n            aemail = results[\"administrativeContactEmail\"]\n            if isEmpty(aemail) == False:\n                h.email_container[aemail].append(\"OpenDNS-AdminEmail\")\n        if results.has_key(\"technicalContactEmail\"):\n            temail = results[\"technicalContactEmail\"]\n            if isEmpty(temail) == False:\n                h.email_container[temail].append(\"OpenDNS-TechEmail\")\n        if results.has_key(\"registrantEmail\"):\n            remail = results[\"registrantEmail\"]\n            if isEmpty(remail) == False:\n                h.email_container[remail].append(\"OpenDNS-RegEmail\")\n        for email in results[\"emails\"]:\n            if isEmpty(email) == False:\n                h.email_container[email].append(\"OpenDNS-GenEmail\")\n    elif type(resultslist) == list:\n        for results in resultslist:\n            if results.has_key(\"errorMessage\"):\n                if results[\"errorMessage\"] == \"Not found\":\n                    return h\n            if results.has_key(\"administrativeContactEmail\"):\n                aemail = results[\"administrativeContactEmail\"]\n                if isEmpty(aemail) == False:\n                    h.email_container[aemail].append(\"OpenDNS-AdminEmail\")\n            if results.has_key(\"technicalContactEmail\"):\n                temail = results[\"technicalContactEmail\"]\n                if isEmpty(temail) == False:\n                    h.email_container[temail].append(\"OpenDNS-TechEmail\")\n            if results.has_key(\"registrantEmail\"):\n                remail = results[\"registrantEmail\"]\n                if isEmpty(remail) == False:\n                    h.email_container[remail].append(\"OpenDNS-RegEmail\")\n            for email in results[\"emails\"]:\n                if isEmpty(email) == False:\n                    h.email_container[email].append(\"OpenDNS-GenEmail\")\n    else:\n        raise \"Unsupported type\"\n    return h\n\n\ndef parseOpenDNSRelDoms(results):\n    h = Hopper()\n    if len(results) == 0:\n        return h\n\n    if results[\"found\"] == True:\n        for dom, thresh in results[\"tb1\"]:\n            # TODO: Check threshold\n            h.domain_container[dom].append(\"OpenDNS-RelatedDomains\")\n    return h\n\n\ndef parseOpenDNSCoOccurance(results):\n    h = Hopper()\n    if len(results) == 0:\n        return h\n\n    if results[\"found\"] == True:\n        for dom, scores in results[\"pfs2\"]:\n            # TODO: Check scores\n            h.domain_container[dom].append(\"OpenDNS-RelatedDomains\")\n    return h\n\n\ndef parseOpenDNSARecords(results):\n    h = Hopper()\n\n    # sometimes it's a list\n    # TODO: Check if the list has good data\n    if type(results) != dict:\n        return h\n        \n    for i in results[\"rrs\"]:\n        dom = i[\"rr\"]\n        if dom.endswith(\".\"):\n            dom=dom[:-1]\n        h.domain_container[dom].append(\"OpenDNS-ARecordsOnIP\")\n    return h\n\n\ndef parseOpenDNSMalOnIP(results):\n    h = Hopper()\n\n    for i in results:\n        dom = i[\"name\"]\n        if dom.endswith(\".\"):\n            dom=dom[:-1]\n        h.domain_container[dom].append(\"OpenDNS-MalOnIP\")\n    return h\n\n\ndef parseOpenDNSEmail(results):\n    h = Hopper()\n\n    emails = results.keys()\n\n    for email in emails:\n        entry = results[email]\n        doms = entry[\"domains\"]\n\n        for domEntry in doms:\n            domain = domEntry[\"domain\"]\n            h.domain_container[domain].append(\"OpenDNS-DNSEmail\")\n\n    return h\n\n\ndef pivotIP(ipAddr):\n    h1 = parseVTIP(pivotEngine.queryVT(ipAddr, \"ip-address\"))\n    h2 =  parseOpenDNSARecords(pivotEngine.pivotOpenDns(ipAddr, \"grabDomainsOnIP\"))\n    h3 =  parseOpenDNSARecords(pivotEngine.pivotOpenDns(ipAddr, \"malwareOnIP\"))\n\n    return h1 + h2 + h3\n\n\ndef pivotDomain(domain):\n    h1 = parseVTDomain(pivotEngine.queryVT(domain, \"domain\"))\n    h2 = parseOpenDNSWhoIs(pivotEngine.pivotOpenDns(domain, \"whois\"))\n    h3 = parseOpenDNSRelDoms(pivotEngine.pivotOpenDns(domain, \"relatedDomains\"))\n    h4 = parseOpenDNSCoOccurance(pivotEngine.pivotOpenDns(domain, \"coOccurance\"))\n\n    return h1 + h2 + h3 + h4\n\n\ndef pivotHash(hash):\n    h1 = parseVTFile(pivotEngine.queryVT(hash, \"file\"), fullScan=True)\n    return h1\n\n\ndef pivotEmail(email):\n    h1 = parseOpenDNSEmail(pivotEngine.pivotOpenDns(email, \"whoisEmail\"))\n    return h1\n\n\ndef pivotFile(fileHandle):\n       \n    file = peEngine.REFile(fileHandle)\n    \n    h1 = pivotHash(file.md5)\n\n    sectionHashes = []\n    for section in file.pe.sections:\n        smd5 = section.get_hash_md5()\n        h1.section_hash_container[smd5].append(\"GroundTruth\")\n        \n    impHash  = file.pe.get_imphash()\n    h1.import_table_container[impHash].append(\"GroundTruth\")\n\n    #TODO\n    # Build lookup for import table and section hashes\n    # i.e. return h1+h2+h3\n    \n    return h1 \n\n\n\n\n\n", "https://developers.google.com/youtube/2.0/developers_guide_protocol_video_feeds#Videos_feed\n\nnote: you might need to do `sudo pip install requests' before running this script\n\neg: \t./008-yt-feed.py \"skateboarding dog\"\n\"\"\"\nimport argparse\nimport subprocess\nimport os\nimport json\nimport requests\n\ndef main():\n\n\tparser = argparse.ArgumentParser(description='Download some videos')\n\tparser.add_argument('--dest', type=str, help='output destination', default='work')\n\tparser.add_argument('--num', type=int, help='number of videos', default=10)\n\tparser.add_argument('keyword', type=str, nargs=1, help='A keyword used to search YouTube')\t\n\targs = parser.parse_args()\n\n\tif not os.path.exists(args.dest):\n\t\tos.makedirs(args.dest)\n\n\tfeed_url = \"http://gdata.youtube.com/feeds/api/videos?q={0}&max-results={1}&v=2&alt=json\".format(args.keyword[0], args.num)\n\tr = requests.get(feed_url)\n\tfeed = json.loads( r.text )\n\n\tfor i, entry in enumerate(feed[\"feed\"][\"entry\"]):\n\t\t#title = entry[\"title\"][\"$t\"]\n\t\turl = entry[\"link\"][0][\"href\"]\n\t\tcmd = 'youtube-dl -o \"{0}/{1}.%(ext)s\" {2}'.format(args.dest, i, url)\n\t\tsubprocess.call(cmd, shell=True)\n\n\nif __name__ == '__main__':\n\tmain()", "g locally. Gunicorn is used to run the\r\n    # application on Google App Engine. See entrypoint in app.yaml.\r\n    app.run(host='127.0.0.1', port=8080, debug=True)", " {\n    \"DEFAULT_DIRNAME\": \"/var/run/global_counter\",\n    \"DEFAULT_OFFSET\": 1,\n    \"DEFAULT_TOKENS\": 256,\n    \"DEFAULT_TIMEOUT\": 10,\n    \"DEFAULT_TIMEWAIT\": 0.2,\n    \"DEFAULT_RESERVE_SIZE\": 32,\n    \"DEFAULT_INIT_POSITION\": None\n}\n\n\nclass GlobalCounterError(Exception):\n    pass\n\n\nclass Counters(dict):\n    def __missing__(self, name):\n        raise GlobalCounterError('Counter not registered: %s' % name)\n\n\nclass GlobalCounter(object):\n    def __init__(self,\n                 name=\"default\",\n                 dirname=config['DEFAULT_DIRNAME'],\n                 offset=config['DEFAULT_OFFSET'],\n                 tokens=config['DEFAULT_TOKENS'],\n                 timewait=config['DEFAULT_TIMEWAIT'],\n                 timeout=config['DEFAULT_TIMEOUT'],\n                 reserve_size=config['DEFAULT_RESERVE_SIZE'],\n                 init_position=config['DEFAULT_INIT_POSITION']):\n        self.filename = \"%s/%s.%d.count\" % (dirname, name, offset)\n        self.offset = offset\n        self.tokens = tokens\n        self.timeout = timeout\n        self.timewait = timewait\n        self.reserve_size = reserve_size\n\n        self.queue = Queue.Queue()\n\n        if not os.path.isfile(self.filename):\n            if init_position is None:\n                raise GlobalCounterError('Could not find counter file, please specify init_position=POSITION if you wish to restart the counter')\n            with open(self.filename, 'w') as fp:\n                fp.write(\"%d\" % init_position)\n\n    def step(self):\n        while True:\n            try:\n                return self.offset + self.queue.get_nowait() * self.tokens\n            except Queue.Empty:\n                self.reserve(self.reserve_size)\n\n    def reserve(self, reserve_size):\n        time_waited = 0\n        while True:\n            try:\n                with open(self.filename, 'r+') as fp:\n                    fcntl.flock(fp, fcntl.LOCK_EX | fcntl.LOCK_NB)\n                    current_id = int(fp.read())\n                    last_id = current_id + reserve_size\n                    fp.seek(0)\n                    fp.write(\"%d\" % last_id)\n                    fp.truncate()\n\n                    for i_id in range(current_id+1, last_id+1):\n                        self.queue.put_nowait(i_id)\n                break\n\n            except IOError as e:\n                if e.errno == 11:\n                    if time_waited+self.timewait >= self.timeout:\n                        raise GlobalCounterError('Timeout for counter lock: %d' % self.timeout)\n\n                    sleep(self.timewait)\n                    time_waited += self.timewait\n\n                else:\n                    raise\n\n\ncounters = Counters()\n\n\ndef register(name, *args, **kwargs):\n    counters[name] = GlobalCounter(name, *args, **kwargs)\n\n\ndef step(name):\n    return counters[name].step()\n", "lob import glob\nfrom PIL import Image\nfrom io import BytesIO\nimport discord\nimport logging\nimport os\n\nlogger = logging.getLogger(__name__)\n\n\nclass LocalImage(object):\n    def __init__(self, location: str, *, max_width: int = 1920, max_height: int = 1920):\n        self.location = location.strip()\n        if not self.location.endswith('/'):\n            self.location += '/'\n        self.max_width = max_width\n        self.max_height = max_height\n\n    def get_random_image(self):\n        \"\"\"\n        Picks a random .jpg image from the given location\n\n        @return: Full path of the image\n        @rtype str\n        \"\"\"\n        if self.location:\n            return choice(glob(self.location + '*.jpg'))\n        else:\n            raise ConfigException('Location value is missing')\n\n    async def send_image(self, channel: discord.abc.Messageable, count: int = 1):\n        \"\"\"\n        Wrapper for sending the picture\n\n        @param channel: Channel where the image should be sent\n        @param count: Number of images to send\n        \"\"\"\n        try:\n            files = []\n            bytes_sent = 0\n            for i in range(count):\n                path = self.get_random_image()\n                image = Image.open(path)\n                image.thumbnail((self.max_width, self.max_height))\n                bytes_io = BytesIO()\n                image.save(bytes_io, \"JPEG\")\n                _, filename = os.path.split(path)\n                bytes_sent += len(bytes_io.getvalue())\n                bytes_io.seek(0)\n                files.append(discord.File(bytes_io, filename))\n            logger.info(\"Trying to send {} KB\".format(bytes_sent / 1024))\n            await channel.send(files=files)\n        except discord.errors.Forbidden as e:\n            if isinstance(channel, discord.abc.GuildChannel):\n                logger.warning(\"[{}:{}][{}]: {}\".format(str(channel.guild), channel.guild.id, str(channel), str(e)))\n            elif isinstance(channel, discord.abc.PrivateChannel):\n                logger.warning(\"[{}]: {}\".format(str(channel), str(e)))\n            try:\n                await channel.send(\"No permission to send an attachment\")\n            except discord.errors.Forbidden as e:\n                if isinstance(channel, discord.abc.GuildChannel):\n                    logger.warning(\"[{}:{}][{}]: {}\".format(str(channel.guild), channel.guild.id, str(channel), str(e)))\n                elif isinstance(channel, discord.abc.PrivateChannel):\n                    logger.warning(\"[{}]: {}\".format(str(channel), str(e)))\n\n\nclass WebImage(object):\n    \"\"\"\n    @todo make a class that allows getting images from the web using aiohttp (for example get image from danbooru api from given keyword)\n    \"\"\"\n    def __init__(self):\n        pass\n", "  app.run(debug=DEBUG_MODE)\n", "ng and extracting data-files from the internet.\n#\n# Implemented in Python 3.5\n#\n########################################################################\n#\n# This file is part of the TensorFlow Tutorials available at:\n#\n# https://github.com/Hvass-Labs/TensorFlow-Tutorials\n#\n# Published under the MIT License. See the file LICENSE for details.\n#\n# Copyright 2016 by Magnus Erik Hvass Pedersen\n#\n########################################################################\n\nimport sys\nimport os\nimport urllib.request\nimport tarfile\nimport zipfile\n\n########################################################################\n\n\ndef _print_download_progress(count, block_size, total_size):\n    \"\"\"\n    Function used for printing the download progress.\n    Used as a call-back function in maybe_download_and_extract().\n    \"\"\"\n\n    # Percentage completion.\n    pct_complete = float(count * block_size) / total_size\n\n    # Limit it because rounding errors may cause it to exceed 100%.\n    pct_complete = min(1.0, pct_complete)\n\n    # Status-message. Note the \\r which means the line should overwrite itself.\n    msg = \"\\r- Download progress: {0:.1%}\".format(pct_complete)\n\n    # Print it.\n    sys.stdout.write(msg)\n    sys.stdout.flush()\n\n\n########################################################################\n\ndef download(base_url, filename, download_dir):\n    \"\"\"\n    Download the given file if it does not already exist in the download_dir.\n\n    :param base_url: The internet URL without the filename.\n    :param filename: The filename that will be added to the base_url.\n    :param download_dir: Local directory for storing the file.\n    :return: Nothing.\n    \"\"\"\n\n    # Path for local file.\n    save_path = os.path.join(download_dir, filename)\n\n    # Check if the file already exists, otherwise we need to download it now.\n    if not os.path.exists(save_path):\n        # Check if the download directory exists, otherwise create it.\n        if not os.path.exists(download_dir):\n            os.makedirs(download_dir)\n\n        print(\"Downloading\", filename, \"...\")\n\n        # Download the file from the internet.\n        url = base_url + filename\n        file_path, _ = urllib.request.urlretrieve(url=url,\n                                                  filename=save_path,\n                                                  reporthook=_print_download_progress)\n\n        print(\" Done!\")\n\n\ndef maybe_download_and_extract(url, download_dir):\n    \"\"\"\n    Download and extract the data if it doesn't already exist.\n    Assumes the url is a tar-ball file.\n\n    :param url:\n        Internet URL for the tar-file to download.\n        Example: \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n\n    :param download_dir:\n        Directory where the downloaded file is saved.\n        Example: \"data/CIFAR-10/\"\n\n    :return:\n        Nothing.\n    \"\"\"\n\n    # Filename for saving the file downloaded from the internet.\n    # Use the filename from the URL and add it to the download_dir.\n    filename = url.split('/')[-1]\n    file_path = os.path.join(download_dir, filename)\n\n    # Check if the file already exists.\n    # If it exists then we assume it has also been extracted,\n    # otherwise we need to download and extract it now.\n    if not os.path.exists(file_path):\n        # Check if the download directory exists, otherwise create it.\n        if not os.path.exists(download_dir):\n            os.makedirs(download_dir)\n\n        # Download the file from the internet.\n        file_path, _ = urllib.request.urlretrieve(url=url,\n                                                  filename=file_path,\n                                                  reporthook=_print_download_progress)\n\n        print()\n        print(\"Download finished. Extracting files.\")\n\n        if file_path.endswith(\".zip\"):\n            # Unpack the zip-file.\n            zipfile.ZipFile(file=file_path, mode=\"r\").extractall(download_dir)\n        elif file_path.endswith((\".tar.gz\", \".tgz\")):\n            # Unpack the tar-ball.\n            tarfile.open(name=file_path, mode=\"r:gz\").extractall(download_dir)\n\n        print(\"Done.\")\n    else:\n        print(\"Data has apparently already been downloaded and unpacked.\")\n\n\n########################################################################\n", "ght (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for license information.\n# Code generated by Microsoft (R) AutoRest Code Generator.\n# Changes may cause incorrect behavior and will be lost if the code is regenerated.\n# --------------------------------------------------------------------------\nfrom typing import Any, AsyncIterable, Callable, Dict, Generic, Optional, TypeVar, Union\nimport warnings\n\nfrom azure.core.async_paging import AsyncItemPaged, AsyncList\nfrom azure.core.exceptions import ClientAuthenticationError, HttpResponseError, ResourceExistsError, ResourceNotFoundError, map_error\nfrom azure.core.pipeline import PipelineResponse\nfrom azure.core.pipeline.transport import AsyncHttpResponse, HttpRequest\nfrom azure.core.polling import AsyncLROPoller, AsyncNoPolling, AsyncPollingMethod\nfrom azure.mgmt.core.exceptions import ARMErrorFormat\nfrom azure.mgmt.core.polling.async_arm_polling import AsyncARMPolling\n\nfrom ... import models as _models\n\nT = TypeVar('T')\nClsType = Optional[Callable[[PipelineResponse[HttpRequest, AsyncHttpResponse], T, Dict[str, Any]], Any]]\n\nclass VirtualRoutersOperations:\n    \"\"\"VirtualRoutersOperations async operations.\n\n    You should not instantiate this class directly. Instead, you should create a Client instance that\n    instantiates it for you and attaches it as an attribute.\n\n    :ivar models: Alias to model classes used in this operation group.\n    :type models: ~azure.mgmt.network.v2019_09_01.models\n    :param client: Client for service requests.\n    :param config: Configuration of service client.\n    :param serializer: An object model serializer.\n    :param deserializer: An object model deserializer.\n    \"\"\"\n\n    models = _models\n\n    def __init__(self, client, config, serializer, deserializer) -> None:\n        self._client = client\n        self._serialize = serializer\n        self._deserialize = deserializer\n        self._config = config\n\n    async def _delete_initial(\n        self,\n        resource_group_name: str,\n        virtual_router_name: str,\n        **kwargs: Any\n    ) -> None:\n        cls = kwargs.pop('cls', None)  # type: ClsType[None]\n        error_map = {\n            401: ClientAuthenticationError, 404: ResourceNotFoundError, 409: ResourceExistsError\n        }\n        error_map.update(kwargs.pop('error_map', {}))\n        api_version = \"2019-09-01\"\n        accept = \"application/json\"\n\n        # Construct URL\n        url = self._delete_initial.metadata['url']  # type: ignore\n        path_format_arguments = {\n            'resourceGroupName': self._serialize.url(\"resource_group_name\", resource_group_name, 'str'),\n            'virtualRouterName': self._serialize.url(\"virtual_router_name\", virtual_router_name, 'str'),\n            'subscriptionId': self._serialize.url(\"self._config.subscription_id\", self._config.subscription_id, 'str'),\n        }\n        url = self._client.format_url(url, **path_format_arguments)\n\n        # Construct parameters\n        query_parameters = {}  # type: Dict[str, Any]\n        query_parameters['api-version'] = self._serialize.query(\"api_version\", api_version, 'str')\n\n        # Construct headers\n        header_parameters = {}  # type: Dict[str, Any]\n        header_parameters['Accept'] = self._serialize.header(\"accept\", accept, 'str')\n\n        request = self._client.delete(url, query_parameters, header_parameters)\n        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)\n        response = pipeline_response.http_response\n\n        if response.status_code not in [200, 202, 204]:\n            map_error(status_code=response.status_code, response=response, error_map=error_map)\n            error = self._deserialize.failsafe_deserialize(_models.Error, response)\n            raise HttpResponseError(response=response, model=error, error_format=ARMErrorFormat)\n\n        if cls:\n            return cls(pipeline_response, None, {})\n\n    _delete_initial.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Network/virtualRouters/{virtualRouterName}'}  # type: ignore\n\n    async def begin_delete(\n        self,\n        resource_group_name: str,\n        virtual_router_name: str,\n        **kwargs: Any\n    ) -> AsyncLROPoller[None]:\n        \"\"\"Deletes the specified Virtual Router.\n\n        :param resource_group_name: The name of the resource group.\n        :type resource_group_name: str\n        :param virtual_router_name: The name of the Virtual Router.\n        :type virtual_router_name: str\n        :keyword callable cls: A custom type or function that will be passed the direct response\n        :keyword str continuation_token: A continuation token to restart a poller from a saved state.\n        :keyword polling: By default, your polling method will be AsyncARMPolling.\n         Pass in False for this operation to not poll, or pass in your own initialized polling object for a personal polling strategy.\n        :paramtype polling: bool or ~azure.core.polling.AsyncPollingMethod\n        :keyword int polling_interval: Default waiting time between two polls for LRO operations if no Retry-After header is present.\n        :return: An instance of AsyncLROPoller that returns either None or the result of cls(response)\n        :rtype: ~azure.core.polling.AsyncLROPoller[None]\n        :raises ~azure.core.exceptions.HttpResponseError:\n        \"\"\"\n        polling = kwargs.pop('polling', True)  # type: Union[bool, AsyncPollingMethod]\n        cls = kwargs.pop('cls', None)  # type: ClsType[None]\n        lro_delay = kwargs.pop(\n            'polling_interval',\n            self._config.polling_interval\n        )\n        cont_token = kwargs.pop('continuation_token', None)  # type: Optional[str]\n        if cont_token is None:\n            raw_result = await self._delete_initial(\n                resource_group_name=resource_group_name,\n                virtual_router_name=virtual_router_name,\n                cls=lambda x,y,z: x,\n                **kwargs\n            )\n\n        kwargs.pop('error_map', None)\n        kwargs.pop('content_type', None)\n\n        def get_long_running_output(pipeline_response):\n            if cls:\n                return cls(pipeline_response, None, {})\n\n        path_format_arguments = {\n            'resourceGroupName': self._serialize.url(\"resource_group_name\", resource_group_name, 'str'),\n            'virtualRouterName': self._serialize.url(\"virtual_router_name\", virtual_router_name, 'str'),\n            'subscriptionId': self._serialize.url(\"self._config.subscription_id\", self._config.subscription_id, 'str'),\n        }\n\n        if polling is True: polling_method = AsyncARMPolling(lro_delay, lro_options={'final-state-via': 'location'}, path_format_arguments=path_format_arguments,  **kwargs)\n        elif polling is False: polling_method = AsyncNoPolling()\n        else: polling_method = polling\n        if cont_token:\n            return AsyncLROPoller.from_continuation_token(\n                polling_method=polling_method,\n                continuation_token=cont_token,\n                client=self._client,\n                deserialization_callback=get_long_running_output\n            )\n        else:\n            return AsyncLROPoller(self._client, raw_result, get_long_running_output, polling_method)\n    begin_delete.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Network/virtualRouters/{virtualRouterName}'}  # type: ignore\n\n    async def get(\n        self,\n        resource_group_name: str,\n        virtual_router_name: str,\n        expand: Optional[str] = None,\n        **kwargs: Any\n    ) -> \"_models.VirtualRouter\":\n        \"\"\"Gets the specified Virtual Router.\n\n        :param resource_group_name: The name of the resource group.\n        :type resource_group_name: str\n        :param virtual_router_name: The name of the Virtual Router.\n        :type virtual_router_name: str\n        :param expand: Expands referenced resources.\n        :type expand: str\n        :keyword callable cls: A custom type or function that will be passed the direct response\n        :return: VirtualRouter, or the result of cls(response)\n        :rtype: ~azure.mgmt.network.v2019_09_01.models.VirtualRouter\n        :raises: ~azure.core.exceptions.HttpResponseError\n        \"\"\"\n        cls = kwargs.pop('cls', None)  # type: ClsType[\"_models.VirtualRouter\"]\n        error_map = {\n            401: ClientAuthenticationError, 404: ResourceNotFoundError, 409: ResourceExistsError\n        }\n        error_map.update(kwargs.pop('error_map', {}))\n        api_version = \"2019-09-01\"\n        accept = \"application/json\"\n\n        # Construct URL\n        url = self.get.metadata['url']  # type: ignore\n        path_format_arguments = {\n            'resourceGroupName': self._serialize.url(\"resource_group_name\", resource_group_name, 'str'),\n            'virtualRouterName': self._serialize.url(\"virtual_router_name\", virtual_router_name, 'str'),\n            'subscriptionId': self._serialize.url(\"self._config.subscription_id\", self._config.subscription_id, 'str'),\n        }\n        url = self._client.format_url(url, **path_format_arguments)\n\n        # Construct parameters\n        query_parameters = {}  # type: Dict[str, Any]\n        query_parameters['api-version'] = self._serialize.query(\"api_version\", api_version, 'str')\n        if expand is not None:\n            query_parameters['$expand'] = self._serialize.query(\"expand\", expand, 'str')\n\n        # Construct headers\n        header_parameters = {}  # type: Dict[str, Any]\n        header_parameters['Accept'] = self._serialize.header(\"accept\", accept, 'str')\n\n        request = self._client.get(url, query_parameters, header_parameters)\n        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)\n        response = pipeline_response.http_response\n\n        if response.status_code not in [200]:\n            map_error(status_code=response.status_code, response=response, error_map=error_map)\n            error = self._deserialize.failsafe_deserialize(_models.Error, response)\n            raise HttpResponseError(response=response, model=error, error_format=ARMErrorFormat)\n\n        deserialized = self._deserialize('VirtualRouter', pipeline_response)\n\n        if cls:\n            return cls(pipeline_response, deserialized, {})\n\n        return deserialized\n    get.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Network/virtualRouters/{virtualRouterName}'}  # type: ignore\n\n    async def _create_or_update_initial(\n        self,\n        resource_group_name: str,\n        virtual_router_name: str,\n        parameters: \"_models.VirtualRouter\",\n        **kwargs: Any\n    ) -> \"_models.VirtualRouter\":\n        cls = kwargs.pop('cls', None)  # type: ClsType[\"_models.VirtualRouter\"]\n        error_map = {\n            401: ClientAuthenticationError, 404: ResourceNotFoundError, 409: ResourceExistsError\n        }\n        error_map.update(kwargs.pop('error_map', {}))\n        api_version = \"2019-09-01\"\n        content_type = kwargs.pop(\"content_type\", \"application/json\")\n        accept = \"application/json\"\n\n        # Construct URL\n        url = self._create_or_update_initial.metadata['url']  # type: ignore\n        path_format_arguments = {\n            'resourceGroupName': self._serialize.url(\"resource_group_name\", resource_group_name, 'str'),\n            'virtualRouterName': self._serialize.url(\"virtual_router_name\", virtual_router_name, 'str'),\n            'subscriptionId': self._serialize.url(\"self._config.subscription_id\", self._config.subscription_id, 'str'),\n        }\n        url = self._client.format_url(url, **path_format_arguments)\n\n        # Construct parameters\n        query_parameters = {}  # type: Dict[str, Any]\n        query_parameters['api-version'] = self._serialize.query(\"api_version\", api_version, 'str')\n\n        # Construct headers\n        header_parameters = {}  # type: Dict[str, Any]\n        header_parameters['Content-Type'] = self._serialize.header(\"content_type\", content_type, 'str')\n        header_parameters['Accept'] = self._serialize.header(\"accept\", accept, 'str')\n\n        body_content_kwargs = {}  # type: Dict[str, Any]\n        body_content = self._serialize.body(parameters, 'VirtualRouter')\n        body_content_kwargs['content'] = body_content\n        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)\n        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)\n        response = pipeline_response.http_response\n\n        if response.status_code not in [200, 201]:\n            map_error(status_code=response.status_code, response=response, error_map=error_map)\n            error = self._deserialize.failsafe_deserialize(_models.Error, response)\n            raise HttpResponseError(response=response, model=error, error_format=ARMErrorFormat)\n\n        if response.status_code == 200:\n            deserialized = self._deserialize('VirtualRouter', pipeline_response)\n\n        if response.status_code == 201:\n            deserialized = self._deserialize('VirtualRouter', pipeline_response)\n\n        if cls:\n            return cls(pipeline_response, deserialized, {})\n\n        return deserialized\n    _create_or_update_initial.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Network/virtualRouters/{virtualRouterName}'}  # type: ignore\n\n    async def begin_create_or_update(\n        self,\n        resource_group_name: str,\n        virtual_router_name: str,\n        parameters: \"_models.VirtualRouter\",\n        **kwargs: Any\n    ) -> AsyncLROPoller[\"_models.VirtualRouter\"]:\n        \"\"\"Creates or updates the specified Virtual Router.\n\n        :param resource_group_name: The name of the resource group.\n        :type resource_group_name: str\n        :param virtual_router_name: The name of the Virtual Router.\n        :type virtual_router_name: str\n        :param parameters: Parameters supplied to the create or update Virtual Router.\n        :type parameters: ~azure.mgmt.network.v2019_09_01.models.VirtualRouter\n        :keyword callable cls: A custom type or function that will be passed the direct response\n        :keyword str continuation_token: A continuation token to restart a poller from a saved state.\n        :keyword polling: By default, your polling method will be AsyncARMPolling.\n         Pass in False for this operation to not poll, or pass in your own initialized polling object for a personal polling strategy.\n        :paramtype polling: bool or ~azure.core.polling.AsyncPollingMethod\n        :keyword int polling_interval: Default waiting time between two polls for LRO operations if no Retry-After header is present.\n        :return: An instance of AsyncLROPoller that returns either VirtualRouter or the result of cls(response)\n        :rtype: ~azure.core.polling.AsyncLROPoller[~azure.mgmt.network.v2019_09_01.models.VirtualRouter]\n        :raises ~azure.core.exceptions.HttpResponseError:\n        \"\"\"\n        polling = kwargs.pop('polling', True)  # type: Union[bool, AsyncPollingMethod]\n        cls = kwargs.pop('cls', None)  # type: ClsType[\"_models.VirtualRouter\"]\n        lro_delay = kwargs.pop(\n            'polling_interval',\n            self._config.polling_interval\n        )\n        cont_token = kwargs.pop('continuation_token', None)  # type: Optional[str]\n        if cont_token is None:\n            raw_result = await self._create_or_update_initial(\n                resource_group_name=resource_group_name,\n                virtual_router_name=virtual_router_name,\n                parameters=parameters,\n                cls=lambda x,y,z: x,\n                **kwargs\n            )\n\n        kwargs.pop('error_map', None)\n        kwargs.pop('content_type', None)\n\n        def get_long_running_output(pipeline_response):\n            deserialized = self._deserialize('VirtualRouter', pipeline_response)\n\n            if cls:\n                return cls(pipeline_response, deserialized, {})\n            return deserialized\n\n        path_format_arguments = {\n            'resourceGroupName': self._serialize.url(\"resource_group_name\", resource_group_name, 'str'),\n            'virtualRouterName': self._serialize.url(\"virtual_router_name\", virtual_router_name, 'str'),\n            'subscriptionId': self._serialize.url(\"self._config.subscription_id\", self._config.subscription_id, 'str'),\n        }\n\n        if polling is True: polling_method = AsyncARMPolling(lro_delay, lro_options={'final-state-via': 'azure-async-operation'}, path_format_arguments=path_format_arguments,  **kwargs)\n        elif polling is False: polling_method = AsyncNoPolling()\n        else: polling_method = polling\n        if cont_token:\n            return AsyncLROPoller.from_continuation_token(\n                polling_method=polling_method,\n                continuation_token=cont_token,\n                client=self._client,\n                deserialization_callback=get_long_running_output\n            )\n        else:\n            return AsyncLROPoller(self._client, raw_result, get_long_running_output, polling_method)\n    begin_create_or_update.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Network/virtualRouters/{virtualRouterName}'}  # type: ignore\n\n    def list_by_resource_group(\n        self,\n        resource_group_name: str,\n        **kwargs: Any\n    ) -> AsyncIterable[\"_models.VirtualRouterListResult\"]:\n        \"\"\"Lists all Virtual Routers in a resource group.\n\n        :param resource_group_name: The name of the resource group.\n        :type resource_group_name: str\n        :keyword callable cls: A custom type or function that will be passed the direct response\n        :return: An iterator like instance of either VirtualRouterListResult or the result of cls(response)\n        :rtype: ~azure.core.async_paging.AsyncItemPaged[~azure.mgmt.network.v2019_09_01.models.VirtualRouterListResult]\n        :raises: ~azure.core.exceptions.HttpResponseError\n        \"\"\"\n        cls = kwargs.pop('cls', None)  # type: ClsType[\"_models.VirtualRouterListResult\"]\n        error_map = {\n            401: ClientAuthenticationError, 404: ResourceNotFoundError, 409: ResourceExistsError\n        }\n        error_map.update(kwargs.pop('error_map', {}))\n        api_version = \"2019-09-01\"\n        accept = \"application/json\"\n\n        def prepare_request(next_link=None):\n            # Construct headers\n            header_parameters = {}  # type: Dict[str, Any]\n            header_parameters['Accept'] = self._serialize.header(\"accept\", accept, 'str')\n\n            if not next_link:\n                # Construct URL\n                url = self.list_by_resource_group.metadata['url']  # type: ignore\n                path_format_arguments = {\n                    'resourceGroupName': self._serialize.url(\"resource_group_name\", resource_group_name, 'str'),\n                    'subscriptionId': self._serialize.url(\"self._config.subscription_id\", self._config.subscription_id, 'str'),\n                }\n                url = self._client.format_url(url, **path_format_arguments)\n                # Construct parameters\n                query_parameters = {}  # type: Dict[str, Any]\n                query_parameters['api-version'] = self._serialize.query(\"api_version\", api_version, 'str')\n\n                request = self._client.get(url, query_parameters, header_parameters)\n            else:\n                url = next_link\n                query_parameters = {}  # type: Dict[str, Any]\n                request = self._client.get(url, query_parameters, header_parameters)\n            return request\n\n        async def extract_data(pipeline_response):\n            deserialized = self._deserialize('VirtualRouterListResult', pipeline_response)\n            list_of_elem = deserialized.value\n            if cls:\n                list_of_elem = cls(list_of_elem)\n            return deserialized.next_link or None, AsyncList(list_of_elem)\n\n        async def get_next(next_link=None):\n            request = prepare_request(next_link)\n\n            pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)\n            response = pipeline_response.http_response\n\n            if response.status_code not in [200]:\n                error = self._deserialize.failsafe_deserialize(_models.Error, response)\n                map_error(status_code=response.status_code, response=response, error_map=error_map)\n                raise HttpResponseError(response=response, model=error, error_format=ARMErrorFormat)\n\n            return pipeline_response\n\n        return AsyncItemPaged(\n            get_next, extract_data\n        )\n    list_by_resource_group.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Network/virtualRouters'}  # type: ignore\n\n    def list(\n        self,\n        **kwargs: Any\n    ) -> AsyncIterable[\"_models.VirtualRouterListResult\"]:\n        \"\"\"Gets all the Virtual Routers in a subscription.\n\n        :keyword callable cls: A custom type or function that will be passed the direct response\n        :return: An iterator like instance of either VirtualRouterListResult or the result of cls(response)\n        :rtype: ~azure.core.async_paging.AsyncItemPaged[~azure.mgmt.network.v2019_09_01.models.VirtualRouterListResult]\n        :raises: ~azure.core.exceptions.HttpResponseError\n        \"\"\"\n        cls = kwargs.pop('cls', None)  # type: ClsType[\"_models.VirtualRouterListResult\"]\n        error_map = {\n            401: ClientAuthenticationError, 404: ResourceNotFoundError, 409: ResourceExistsError\n        }\n        error_map.update(kwargs.pop('error_map', {}))\n        api_version = \"2019-09-01\"\n        accept = \"application/json\"\n\n        def prepare_request(next_link=None):\n            # Construct headers\n            header_parameters = {}  # type: Dict[str, Any]\n            header_parameters['Accept'] = self._serialize.header(\"accept\", accept, 'str')\n\n            if not next_link:\n                # Construct URL\n                url = self.list.metadata['url']  # type: ignore\n                path_format_arguments = {\n                    'subscriptionId': self._serialize.url(\"self._config.subscription_id\", self._config.subscription_id, 'str'),\n                }\n                url = self._client.format_url(url, **path_format_arguments)\n                # Construct parameters\n                query_parameters = {}  # type: Dict[str, Any]\n                query_parameters['api-version'] = self._serialize.query(\"api_version\", api_version, 'str')\n\n                request = self._client.get(url, query_parameters, header_parameters)\n            else:\n                url = next_link\n                query_parameters = {}  # type: Dict[str, Any]\n                request = self._client.get(url, query_parameters, header_parameters)\n            return request\n\n        async def extract_data(pipeline_response):\n            deserialized = self._deserialize('VirtualRouterListResult', pipeline_response)\n            list_of_elem = deserialized.value\n            if cls:\n                list_of_elem = cls(list_of_elem)\n            return deserialized.next_link or None, AsyncList(list_of_elem)\n\n        async def get_next(next_link=None):\n            request = prepare_request(next_link)\n\n            pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)\n            response = pipeline_response.http_response\n\n            if response.status_code not in [200]:\n                error = self._deserialize.failsafe_deserialize(_models.Error, response)\n                map_error(status_code=response.status_code, response=response, error_map=error_map)\n                raise HttpResponseError(response=response, model=error, error_format=ARMErrorFormat)\n\n            return pipeline_response\n\n        return AsyncItemPaged(\n            get_next, extract_data\n        )\n    list.metadata = {'url': '/subscriptions/{subscriptionId}/providers/Microsoft.Network/virtualRouters'}  # type: ignore\n", "self, auction_helper):\n        self.base_url = \"http://www.wieckauction.com\"\n        self.url = \"http://www.wieckauction.com/auctions/\"\n        self.AuctionHelper = auction_helper\n        self.require_js = False\n\n    def determine_auction_type(self, text):\n        return self.AuctionHelper.determine_auction_type(text)\n\n    def get_direct_link_soup(self, link):\n        \"\"\" Takes a link and returns a processed soup \"\"\"\n        soup = self.AuctionHelper.get_direct_link_soup(link)\n        return soup\n\n    def clean_up(self, text):\n        \"\"\" Takes a string and encodes it to utf-8, gets rid of excess spaces/tabs \"\"\"\n        return \" \".join(text.encode(\"utf-8\").strip().split())\n\n    def process_page(self, soup):\n        print \"Processing {}\".format(self.url)\n        auctions = []\n\n        auction_cards = soup.find(\"ul\", {\"class\": \"cards\"}).find_all(\"li\", recursive=False)\n\n        for auction in auction_cards:\n            print \"----------------------------------------\"\n\n            more_info_link = auction.find(\"a\").get(\"href\")\n\n            # Title and Description\n            _details = auction.find(\"div\", {\"class\": \"auction-details\"})\n            auction_title = self.clean_up(_details.find(\"h5\").text)\n            description = self.clean_up(_details.find(\"h6\").text)\n\n            # Time and location\n            auction_time = self.clean_up(_details.find(\"span\", {\"class\": \"date\"}).text)\n            auction_location = self.clean_up(_details.find(\"span\", {\"class\": \"location\"}).text)\n\n\n            # Find the type of auction\n            _soup = self.get_direct_link_soup(more_info_link)\n            _info_block = self.clean_up(_soup.find(\"div\", {\"id\": \"editable\"}).text)\n            auction_type = self.determine_auction_type(_info_block)\n\n            print \"\"\n            print \"Title: {}\".format(auction_title)\n            print \"Type: {}\".format(auction_type)\n            print \"More info link: {}\".format(more_info_link)\n            print \"Description: {}\".format(description)\n            print \"Auction Location: {}\".format(auction_location)\n            print \"Auction Time: {}\".format(auction_time)\n            print \"\"\n\n            struct = {\n                \"title\": auction_title,\n                \"type\": auction_type,\n                \"more_info_link\": more_info_link,\n                \"description\": description,\n                \"auction_location\": auction_location,\n                \"auction_time\": auction_time,\n            }\n            # Convert all the values away from unicode\n            struct = {k: str(v) for k,v in struct.iteritems()}\n\n            auctions.append(struct)\n\n            print \"----------------------------------------\"\n\n        print \"Finished processing auctions for {}\".format(self.base_url)\n        return auctions\n", "and manipulate\n    **physical quantities**: the product of a numerical value and a\n    unit of measurement. It allows arithmetic operations between them\n    and conversions from and to different units.\n\n    :copyright: (c) 2012 by Hernan E. Grecco.\n    :license: BSD, see LICENSE for more details.\n\"\"\"\nfrom __future__ import with_statement\nimport os\nimport subprocess\nimport pkg_resources\nfrom .formatting import formatter\nfrom .unit import UnitRegistry, DimensionalityError, UndefinedUnitError, LazyRegistry\nfrom .util import pi_theorem, logger\n\nfrom .context import Context\n\n_DEFAULT_REGISTRY = LazyRegistry()\n\n__version__ = \"unknown\"\ntry:                    # pragma: no cover\n    # try to grab the commit version of our package\n    __version__ = (subprocess.check_output([\"git\", \"describe\"],\n                                           stderr=subprocess.STDOUT,\n                                           cwd=os.path.dirname(os.path.abspath(__file__)))).strip()\nexcept:                 # pragma: no cover\n    # on any error just try to grab the version that is installed on the system\n    try:\n        __version__ = pkg_resources.get_distribution('pint').version\n    except:             # pragma: no cover\n        pass  # we seem to have a local copy without any repository control or installed without setuptools\n              # so the reported version will be __unknown__\n\ndef _build_quantity(value, units):\n    return _DEFAULT_REGISTRY.Quantity(value, units)\n\n\ndef run_pyroma(data):   # pragma: no cover\n    import sys\n    from zest.releaser.utils import ask\n    if not ask(\"Run pyroma on the package before uploading?\"):\n        return\n    try:\n        from pyroma import run\n        result = run(data['tagdir'])\n        if result != 10:\n            if not ask(\"Continue?\"):\n                sys.exit(1)\n    except ImportError:\n        if not ask(\"pyroma not available. Continue?\"):\n            sys.exit(1)\n", "------------------------------------------------------------\n# Copyright (C) 2013 The IPython and Julia Development Teams.\n#\n# Distributed under the terms of the BSD License. The full license is in\n# the file COPYING, distributed as part of this software.\n# ----------------------------------------------------------------------------\n\n# ----------------------------------------------------------------------------\n# Imports\n# ----------------------------------------------------------------------------\n\nfrom __future__ import absolute_import, print_function\n\nimport atexit\nimport ctypes\nimport ctypes.util\nimport logging as _logging  # see `.logger`\nimport os\nimport sys\nimport textwrap\nimport warnings\nfrom ctypes import c_char_p, c_void_p\nfrom logging import getLogger  # see `.logger`\nfrom types import ModuleType  # this is python 3.3 specific\n\nfrom .find_libpython import find_libpython, linked_libpython\nfrom .juliainfo import JuliaInfo\nfrom .libjulia import UNBOXABLE_TYPES, LibJulia, get_inprocess_libjulia, get_libjulia\nfrom .options import JuliaOptions, options_docs\nfrom .release import __version__\nfrom .utils import PYCALL_PKGID, is_windows\n\ntry:\n    from shutil import which\nexcept ImportError:\n    # For Python < 3.3; it should behave more-or-less similar to\n    # shutil.which when used with single argument.\n    from distutils.spawn import find_executable as which\n\ntry:\n    FutureWarning\nexcept NameError:\n    # Python 2\n    FutureWarning = DeprecationWarning\n\ntry:\n    string_types = (basestring,)\nexcept NameError:\n    string_types = (str,)\n\n# ----------------------------------------------------------------------------\n# Classes and funtions\n# ----------------------------------------------------------------------------\npython_version = sys.version_info\n\n\nlogger = getLogger(\"julia\")\n\"\"\"\nImplementation notes: We are not importing `logging` module at the top\nlevel so that using `logging.debug` instead of `logger.debug` becomes\nan error.\n\"\"\"\n\n_loghandler = None\n\n\ndef get_loghandler():\n    \"\"\"\n    Get `logging.StreamHandler` private to PyJulia.\n    \"\"\"\n    global _loghandler\n    if _loghandler is None:\n        formatter = _logging.Formatter(\"%(levelname)s %(message)s\")\n\n        _loghandler = _logging.StreamHandler()\n        _loghandler.setFormatter(formatter)\n\n        logger.addHandler(_loghandler)\n    return _loghandler\n\n\ndef set_loglevel(level):\n    get_loghandler()\n    logger.setLevel(getattr(_logging, level, level))\n\n\ndef enable_debug():\n    set_loglevel(\"DEBUG\")\n\n    handler = get_loghandler()\n    handler.setFormatter(_logging.Formatter(\"%(levelname)s (%(process)d) %(message)s\"))\n\n    logger.debug(\"\")  # flush whatever in the line\n    logger.debug(\"Debug-level logging is enabled for PyJulia.\")\n    logger.debug(\"PyJulia version: %s\", __version__)\n\n\nclass JuliaError(Exception):\n    \"\"\"\n    Wrapper for Julia exceptions.\n    \"\"\"\n\n\n# fmt: off\n\n\nclass JuliaNotFound(RuntimeError):\n    def __init__(self, executable, kwargname):\n        self.executable = executable\n        self.kwargname = kwargname\n\n    def __str__(self):\n        return \"\"\"\\\nJulia executable `{}` cannot be found.\n\nIf you have installed Julia, make sure Julia executable is in the\nsystem PATH.  Alternatively, specify file path to the Julia executable\nusing `{}` keyword argument.\n\nIf you have not installed Julia, download Julia from\nhttps://julialang.org/downloads/ and install it.\n\"\"\".format(self.executable, self.kwargname)\n\n\ndef remove_prefix(string, prefix):\n    return string[len(prefix):] if string.startswith(prefix) else string\n\n\ndef jl_name(name):\n    if name.endswith('_b'):\n        return name[:-2] + '!'\n    return name\n\n\ndef py_name(name):\n    if name.endswith('!'):\n        return name[:-1] + '_b'\n    return name\n\n\nclass JuliaModule(ModuleType):\n\n    def __init__(self, loader, *args, **kwargs):\n        super(JuliaModule, self).__init__(*args, **kwargs)\n        self._julia = loader.julia\n        self.__loader__ = loader\n\n    @property\n    def __all__(self):\n        juliapath = remove_prefix(self.__name__, \"julia.\")\n        names = set(self._julia.eval(\"names({})\".format(juliapath)))\n        names.discard(juliapath.rsplit('.', 1)[-1])\n        return [py_name(n) for n in names if is_accessible_name(n)]\n\n    def __dir__(self):\n        if python_version.major == 2:\n            names = set()\n        else:\n            names = set(super(JuliaModule, self).__dir__())\n        names.update(self.__all__)\n        return list(names)\n    # Override __dir__ method so that completing member names work\n    # well in Python REPLs like IPython.\n\n    __path__ = ()\n    # Declare that `JuliaModule` is a Python module since any Julia\n    # module can have sub-modules.\n    # See: https://docs.python.org/3/reference/import.html#package-path-rules\n\n    def __getattr__(self, name):\n        try:\n            return self.__try_getattr(name)\n        except AttributeError:\n            if name.endswith(\"_b\"):\n                try:\n                    return self.__try_getattr(jl_name(name))\n                except AttributeError:\n                    pass\n            raise\n\n    def __try_getattr(self, name):\n        jl_module = remove_prefix(self.__name__, \"julia.\")\n        jl_fullname = \".\".join((jl_module, name))\n\n        if self._julia.isamodule(jl_fullname):\n            realname = self._julia.fullname(self._julia.eval(jl_fullname))\n            if self._julia.isdefined(realname):\n                return self.__loader__.load_module(\"julia.\" + realname)\n            # Otherwise, it may be, e.g., \"Main.anonymous\", created by\n            # Module().\n\n        if self._julia._isdefined(jl_module, name):\n            return self._julia.eval(jl_fullname)\n\n        raise AttributeError(name)\n\n\nclass JuliaMainModule(JuliaModule):\n\n    def __setattr__(self, name, value):\n        if name.startswith('_'):\n            super(JuliaMainModule, self).__setattr__(name, value)\n        else:\n            juliapath = remove_prefix(self.__name__, \"julia.\")\n            setter = '''\n            PyCall.pyfunctionret(\n                (x) -> Base.eval({}, :({} = $x)),\n                Any,\n                PyCall.PyAny)\n            '''.format(juliapath, jl_name(name))\n            self._julia.eval(setter)(value)\n\n    help = property(lambda self: self._julia.help)\n    eval = property(lambda self: self._julia.eval)\n    using = property(lambda self: self._julia.using)\n\n\n# add custom import behavior for the julia \"module\"\nclass JuliaImporter(object):\n\n    # find_module was deprecated in v3.4\n    def find_module(self, fullname, path=None):\n        if fullname.startswith(\"julia.\"):\n            filename = fullname.split(\".\", 2)[1]\n            filepath = os.path.join(os.path.dirname(__file__), filename)\n            if os.path.isfile(filepath + \".py\") or os.path.isdir(filepath):\n                return\n            return JuliaModuleLoader()\n\n\nclass JuliaModuleLoader(object):\n\n    @property\n    def julia(self):\n        self.__class__.julia = julia = Julia()\n        return julia\n\n    # load module was deprecated in v3.4\n    def load_module(self, fullname):\n        juliapath = remove_prefix(fullname, \"julia.\")\n        if juliapath == 'Main':\n            return sys.modules.setdefault(fullname,\n                                          JuliaMainModule(self, fullname))\n        elif self.julia.isafunction(juliapath):\n            return self.julia.eval(juliapath)\n\n        try:\n            self.julia.eval(\"import {}\".format(juliapath.split(\".\", 1)[0]))\n        except JuliaError:\n            pass\n        else:\n            if self.julia.isamodule(juliapath):\n                return sys.modules.setdefault(fullname,\n                                              JuliaModule(self, fullname))\n\n        raise ImportError(\"{} not found\".format(juliapath))\n\n\ndef ismacro(name):\n    \"\"\" Is the name a macro?\n\n    >>> ismacro('@time')\n    True\n    >>> ismacro('sum')\n    False\n    \"\"\"\n    return name.startswith(\"@\")\n\n\ndef isoperator(name):\n    return not name[0].isalpha()\n\n\ndef isprotected(name):\n    return name.startswith(\"_\")\n\n\ndef notascii(name):\n    try:\n        name.encode(\"ascii\")\n        return False\n    except:\n        return True\n\n\ndef is_accessible_name(name):\n    \"\"\"\n    Check if a Julia variable `name` is (easily) accessible from Python.\n\n    Return `True` if `name` can be safely converted to a Python\n    identifier using `py_name` function.  For example,\n\n    >>> is_accessible_name('A_mul_B!')\n    True\n\n    Since it can be accessed as `A_mul_B_b` in Python.\n    \"\"\"\n    return not (ismacro(name) or\n                isoperator(name) or\n                isprotected(name) or\n                notascii(name))\n\n# fmt: on\n\n\ndef determine_if_statically_linked():\n    \"\"\"Determines if this python executable is statically linked\"\"\"\n    return linked_libpython() is None\n\n\n_unsupported_error_common_header = \"\"\"\\\nIt seems your Julia and PyJulia setup are not supported.\n\nJulia executable:\n    {runtime}\nPython interpreter and libpython used by PyCall.jl:\n    {jlinfo.python}\n    {jl_libpython}\nPython interpreter used to import PyJulia and its libpython.\n    {sys.executable}\n    {py_libpython}\n\"\"\"\n\n\n_unsupported_error_common_footer = \"\"\"\nFor more information, see:\n\n    https://pyjulia.readthedocs.io/en/latest/troubleshooting.html\n\"\"\"\n\n\n_unsupported_error_statically_linked = \"\"\"\nYour Python interpreter \"{sys.executable}\"\nis statically linked to libpython.  Currently, PyJulia does not fully\nsupport such Python interpreter.\n\nThe easiest workaround is to pass `compiled_modules=False` to `Julia`\nconstructor.  To do so, first *reboot* your Python REPL (if this happened\ninside an interactive session) and then evaluate:\n\n    >>> from julia.api import Julia\n    >>> jl = Julia(compiled_modules=False)\n\nAnother workaround is to run your Python script with `python-jl`\ncommand bundled in PyJulia.  You can simply do:\n\n    $ python-jl PATH/TO/YOUR/SCRIPT.py\n\nSee `python-jl --help` for more information.\n\"\"\"\n\n\n_unsupported_error_incompatible_libpython = \"\"\"\nIn Julia >= 0.7, above two paths to `libpython` have to match exactly\nin order for PyJulia to work out-of-the-box.  To configure PyCall.jl to use\nPython interpreter \"{sys.executable}\",\nrun the following code in the Python REPL:\n\n    >>> import julia\n    >>> julia.install()\n\"\"\"\n\n\nclass UnsupportedPythonError(Exception):\n    def __init__(self, jlinfo):\n        self.jlinfo = jlinfo\n        self.statically_linked = determine_if_statically_linked()\n\n    def __str__(self):\n        template = _unsupported_error_common_header\n        if self.statically_linked:\n            template += _unsupported_error_statically_linked\n        else:\n            template += _unsupported_error_incompatible_libpython\n        template += _unsupported_error_common_footer\n        return template.format(\n            runtime=self.jlinfo.julia,\n            jlinfo=self.jlinfo,\n            py_libpython=find_libpython(),\n            jl_libpython=self.jlinfo.libpython_path,\n            sys=sys,\n        )\n\n\nclass Julia(object):\n    \"\"\"\n    Implements a bridge to the Julia runtime.\n    This uses the Julia PyCall module to perform type conversions and allow\n    full access to the entire Julia runtime.\n    \"\"\"\n\n    # fmt: off\n\n    def __init__(self, init_julia=True, jl_init_path=None, runtime=None,\n                 jl_runtime_path=None, debug=False, **julia_options):\n        \"\"\"\n        Create a Python object that represents a live Julia runtime.\n\n        Note: Use `LibJulia` to fully control the initialization of\n        the Julia runtime.\n\n        Parameters\n        ==========\n\n        init_julia : bool\n            If True, try to initialize the Julia runtime. If this code is\n            being called from inside an already running Julia, the flag should\n            be passed as False so the interpreter isn't re-initialized.\n\n            Note that it is safe to call this class constructor twice in the\n            same process with `init_julia` set to True, as a global reference\n            is kept to avoid re-initializing it. The purpose of the flag is\n            only to manage situations when Julia was initialized from outside\n            this code.\n\n        runtime : str\n            Custom Julia binary, e.g. \"/usr/local/bin/julia\" or \"julia-1.0.0\".\n\n        debug : bool\n            If True, print some debugging information to STDERR\n        \"\"\"\n        # Note: `options_docs` is appended below (top level)\n\n        if debug:\n            enable_debug()\n\n        if jl_runtime_path is not None:\n            warnings.warn(\n                \"`jl_runtime_path` is deprecated. Please use `runtime`.\", FutureWarning\n            )\n\n        if not init_julia and runtime is None and is_windows:\n            warnings.warn(\n                \"It is recommended to pass `runtime` when `init_julia=False` in Windows\"\n            )\n\n        if runtime is None:\n            if jl_runtime_path is None:\n                runtime = \"julia\"\n            else:\n                runtime = jl_runtime_path\n        else:\n            if jl_runtime_path is None:\n                jl_runtime_path = which(runtime)\n                if jl_runtime_path is None:\n                    raise JuliaNotFound(runtime, kwargname=\"runtime\")\n            else:\n                raise TypeError(\n                    \"Both `runtime` and `jl_runtime_path` are specified.\")\n\n        if jl_init_path:\n            warnings.warn(\n                \"`jl_init_path` is deprecated. Please use `bindir`.\", FutureWarning\n            )\n            if \"bindir\" in julia_options:\n                raise TypeError(\"Both `jl_init_path` and `bindir` are specified.\")\n\n        logger.debug(\"\")  # so that debug message is shown nicely w/ pytest\n\n        if get_libjulia():\n            # Use pre-existing `LibJulia`.\n            self.api = get_libjulia()\n        elif init_julia:\n            jlinfo = JuliaInfo.load(runtime)\n            if jlinfo.version_info < (0, 7):\n                raise RuntimeError(\"PyJulia does not support Julia < 0.7 anymore\")\n\n            self.api = LibJulia.from_juliainfo(jlinfo)\n\n            if jl_init_path:\n                self.api.bindir = jl_init_path\n\n            options = JuliaOptions(**julia_options)\n\n            is_compatible_python = jlinfo.is_compatible_python()\n            logger.debug(\"is_compatible_python = %r\", is_compatible_python)\n            use_custom_sysimage = options.sysimage is not None\n            logger.debug(\"use_custom_sysimage = %r\", use_custom_sysimage)\n            logger.debug(\"compiled_modules = %r\", options.compiled_modules)\n            if not (\n                options.compiled_modules == \"no\"\n                or is_compatible_python\n                or use_custom_sysimage\n            ):\n                raise UnsupportedPythonError(jlinfo)\n\n            self.api.init_julia(options)\n\n            # We are assuming that `jl_is_initialized()` was true only\n            # if this process was a Julia process (hence PyCall had\n            # already called `atexit(Py_Finalize)`).  This is not true\n            # if `libjulia` is initialized in a Python process with\n            # other mechanisms.  Julia's atexit hooks will not be\n            # called if this happens.  As it's not clear what should\n            # be done for such cases (the other mechanisms may or may\n            # not register the atexit hook), let's play on the safer\n            # side for now.\n            if not self.api.was_initialized:  # = jl_is_initialized()\n                atexit.register(self.api.jl_atexit_hook, 0)\n        else:\n            self.api = get_inprocess_libjulia(julia=runtime)\n\n        # Currently, PyJulia assumes that `Main.PyCall` exsits.  Thus, we need\n        # to import `PyCall` again here in case `init_julia=False` is passed:\n        if debug:\n            self._call(\"\"\"\n            const PyCall = try\n                Base.require({0})\n            catch err\n                @error \"Failed to import PyCall\" exception = (err, catch_backtrace())\n                rethrow()\n            end\n            \"\"\".format(PYCALL_PKGID))\n        else:\n            self._call(\"const PyCall = Base.require({0})\".format(PYCALL_PKGID))\n\n        self._call(u\"using .PyCall\")\n\n        # Whether we initialized Julia or not, we MUST create at least one\n        # instance of PyObject and the convert function. Since these will be\n        # needed on every call, we hold them in the Julia object itself so\n        # they can survive across reinitializations.\n        self._PyObject = self._call(\"PyCall.PyObject\")\n        self._convert = self._call(\"convert\")\n\n        self.sprint = self.eval('sprint')\n        self.showerror = self.eval('showerror')\n\n        if self.eval('VERSION >= v\"0.7-\"'):\n            self.eval(\"@eval Main import Base.MainInclude: eval, include\")\n            # https://github.com/JuliaLang/julia/issues/28825\n\n        if not self._isdefined(\"Main\", \"_PyJuliaHelper\"):\n            self.eval(\"include\")(\n                os.path.join(\n                    os.path.dirname(os.path.realpath(__file__)), \"pyjulia_helper.jl\"\n                )\n            )\n\n    def _call(self, src):\n        \"\"\"\n        Low-level call to execute a snippet of Julia source.\n\n        This only raises an exception if Julia itself throws an error, but it\n        does NO type conversion into usable Python objects nor any memory\n        management. It should never be used for returning the result of Julia\n        expressions, only to execute statements.\n        \"\"\"\n        # logger.debug(\"_call(%s)\", src)\n        ans = self.api.jl_eval_string(src.encode('utf-8'))\n        self.check_exception(src)\n\n        return ans\n\n    @staticmethod\n    def _check_unboxable(c_type):\n        if c_type not in UNBOXABLE_TYPES:\n            raise ValueError(\"Julia value cannot be unboxed as c_type={!r}.\\n\"\n                             \"c_type supported by PyJulia are:\\n\"\n                             \"{}\".format(c_type, \"\\n\".join(UNBOXABLE_TYPES)))\n\n    def _is_unboxable_as(self, pointer, c_type):\n        self._check_unboxable(c_type)\n        jl_type = getattr(self.api, 'jl_{}_type'.format(c_type))\n        desired = ctypes.cast(jl_type, ctypes.POINTER(ctypes.c_void_p))[0]\n        actual = self.api.jl_typeof(pointer)\n        return actual == desired\n\n    # `_unbox_as` was added for communicating with Julia runtime before\n    # initializing PyCal:\n    # * Fail with a helpful message if separate cache is not supported\n    #   https://github.com/JuliaPy/pyjulia/pull/186\n    # However, this is not used anymore at the moment. Maybe clean this up?\n    def _unbox_as(self, pointer, c_type):\n        self._check_unboxable(c_type)\n        jl_unbox = getattr(self.api, 'jl_unbox_{}'.format(c_type))\n        if self._is_unboxable_as(pointer, c_type):\n            return jl_unbox(pointer)\n        else:\n            raise TypeError(\"Cannot unbox pointer {} as {}\"\n                            .format(pointer, c_type))\n\n    def check_exception(self, src=\"<unknown code>\"):\n        exoc = self.api.jl_exception_occurred()\n        logger.debug(\"exception occured? %s\", str(exoc))\n        if not exoc:\n            # logger.debug(\"No Exception\")\n            self.api.jl_exception_clear()\n            return\n\n        # If, theoretically, an exception happens in early stage of\n        # self.__init__, showerror and sprint as below does not work.\n        # Let's use jl_typeof_str in such case.\n        try:\n            sprint = self.sprint\n            showerror = self.showerror\n        except AttributeError:\n            res = None\n        else:\n            res = self.api.jl_call2(self._convert, self._PyObject, exoc)\n        if res is None:\n            exception = self.api.jl_typeof_str(exoc).decode('utf-8')\n        else:\n            exception = sprint(showerror, self._as_pyobj(res))\n        raise JuliaError(u'Exception \\'{}\\' occurred while calling julia code:\\n{}'\n                         .format(exception, src))\n\n    def _typeof_julia_exception_in_transit(self):\n        exception = c_void_p.in_dll(self.api, 'jl_exception_in_transit')\n        msg = self.api.jl_typeof_str(exception)\n        return c_char_p(msg).value\n\n    def help(self, name):\n        \"\"\" Return help string for function by name. \"\"\"\n        if name is None:\n            return None\n        return self.eval('Markdown.plain(@doc(\"{}\"))'.format(name))\n\n    def eval(self, src):\n        \"\"\" Execute code in Julia, then pull some results back to Python. \"\"\"\n        if src is None:\n            return None\n        ans = self._call(src)\n        if not ans:\n            return None\n        res = self.api.jl_call2(self._convert, self._PyObject, ans)\n\n        if res is None:\n            self.check_exception(\"convert(PyCall.PyObject, {})\".format(src))\n        return self._as_pyobj(res)\n\n    def _as_pyobj(self, res):\n        if res == 0:\n            return None\n        boxed_obj = self.api.jl_get_field(res, b'o')\n        pyobj = self.api.jl_unbox_voidpointer(boxed_obj)\n        # make sure we incref it before returning it,\n        # as this is a borrowed reference\n        ctypes.pythonapi.Py_IncRef(ctypes.py_object(pyobj))\n        return pyobj\n\n    # fmt: on\n\n    def using(self, module):\n        \"\"\"Load module in Julia by calling the `using module` command\"\"\"\n        self.eval(\"using %s\" % module)\n\n    def fullname(self, module):\n        if isinstance(module, JuliaModule):\n            assert module.__name__.startswith(\"julia.\")\n            return module.__name__[len(\"julia.\") :]\n\n        from .Main._PyJuliaHelper import fullnamestr\n\n        return fullnamestr(module)\n\n    def isdefined(self, fullname):\n        from .Main._PyJuliaHelper import isdefinedstr\n\n        if not isinstance(fullname, string_types):\n            raise ValueError(\"`julia.isdefined(name)` requires string `name`\")\n        if \".\" not in fullname:\n            raise ValueError(\n                \"`julia.isdefined(name)` requires at least one dot in `name`.\"\n            )\n        parent, member = fullname.rsplit(\".\", 1)\n\n        if isinstance(parent, string_types):\n            parent = self.eval(parent)\n        return isdefinedstr(parent, member)\n\n    def _isdefined(self, parent, member):\n        # `_isdefined` is used in context that `isdefined` is not available\n        return self.eval(\"isdefined({}, :({}))\".format(parent, member))\n\n    def isamodule(self, julia_name):\n        try:\n            return self.eval(\"isa({}, Module)\".format(julia_name))\n        except JuliaError:\n            return False  # assuming this is an `UndefVarError`\n\n    def isafunction(self, julia_name):\n        code = \"isa({}, Function)\".format(julia_name)\n        try:\n            return self.eval(code)\n        except Exception:\n            return False\n\n\nif sys.version_info[0] > 2:\n    Julia.__init__.__doc__ = textwrap.dedent(Julia.__init__.__doc__) + options_docs\n\n\nclass LegacyJulia(object):\n    __doc__ = Julia.__doc__\n\n    def __init__(self, *args, **kwargs):\n        self.__julia = Julia(*args, **kwargs)\n\n    __init__.__doc__ = Julia.__init__.__doc__\n\n    def __getattr__(self, name):\n        from julia import Main\n\n        warnings.warn(\n            \"Accessing `Julia().<name>` to obtain Julia objects is\"\n            \" deprecated.  Use `from julia import Main; Main.<name>` or\"\n            \" `jl = Julia(); jl.eval('<name>')`.\",\n            FutureWarning,\n        )\n        try:\n            return getattr(self.__julia, name)\n        except AttributeError:\n            return getattr(Main, name)\n\n\nsys.meta_path.append(JuliaImporter())\n", "1\nshearmodulus = 1\nbulkmodulus = 1\n# Create a packer, see packers directory for options\ncubic = pyck.CubicPacker([10.0, 10.0, 10.0], h)\n# pack = pyck.Pack(cubic); # do not create the cubic packer in this\n# function call as it will be destroyed, blame SWIG developers\npack = pyck.StructuredPack(cubic)\n\n# Create some shapes, see shapes directory for options and reference\n# First argument is always a tag for these particles\n# Mapping operations are applied sequentially\ntri = pyck.TriPrism(1, [1.5, 3.0, 0.0], [3, 0, 0], [0, 0, 0], 2)\n\n# Map the shapes and generate the pack\n# pack.AddShape(cube); # As with creating the cubic packer, do not create the shapes within the function call here\n# pack.AddShape(sphere);\npack.AddShape(tri)\npack.Process()\n\n# Create a new model from the pack\nmodel = pyck.Model(pack)\n\n# Create a new field of n-dimensional integers\n# Arguments are CreateIntField(label,dimensions)\n# label - label for this field in the vtp file\n# dimensions - dimensionality of this field, doesnt have to correspond to model dimensions\n# Create field of doubles in the same way with CreateDoubleField\nstateField = model.CreateIntField(\"State\", 1)\n\n# Arguments are SetIntField(field,tag,value(s))\n# field - returned from CreateIntField\n# tag - tag applied to particles during shape Mapping\n# value(s) - singular value or array of values [v1, v2,...vn] to set of particles with a matching tag\n# model.SetIntField(stateField,1,10);\n# model.SetIntField(stateField,2,20);\nmodel.SetIntField(stateField, 1, 30)\n\n# Overwrite some parameters\n# Arguments are SetParameter(Label,Value)\nmodel.SetParameter(\"MaxSteps\", \"100\")\nmodel.SetParameter(\"Mass\", \"0.5\")\n\n# Or use a python dictionary to overwrite parameters\nparameters = pyck.Parameters({'ViscAlpha': '0.1', 'ViscBeta': '0.2'})\nmodel.SetParameters(parameters)\n\n# Create a file writer, in this case VTP according to spark format\nwriter = pyck.SparkWriter()\n\n# Write the VTP file\nmodel.Serialize(\"tri_test.vtp\", writer)\n", "nctools\nimport cPickle as pickle\n## 3rd party\nimport numpy as np\nimport pandas as pd\nimport scipy.stats as stats\n#import mixture\nimport dill\nfrom pathos.multiprocessing import ProcessingPool\n# amplication\nfrom QSIPCython import GC2BD\nfrom Genome import Genome\nfrom SimFrags import SimFrags\nimport Utils\n\n# logging\nlogging.basicConfig(format='%(levelname)s:%(message)s', level=logging.DEBUG)\n\n\n\ndef main(args):\n    \"\"\"\n    Parmeters\n    ---------\n    args : dict\n        See ``fragments`` subcommand\n    \"\"\"\n    # list of genome files\n    genomeList =  Utils.parseGenomeList(args['<genomeList>'], \n                                        filePath=args['--fp'])\n        \n    # analyzing each genome (in parallel)    \n    pfunc = functools.partial(by_genome, args=args)\n    \n    # difussion calc in parallel\n    pool = ProcessingPool(nodes=int(args['--np']))\n    if args['--debug']:\n        fragList = map(pfunc, genomeList)\n    else:\n        fragList = pool.map(pfunc, genomeList)\n\n    # writing out table\n    if args['--tbl']:\n        write_fragList(fragList)\n    else:\n        dill.dump(fragList, sys.stdout)\n\n\ndef load_frags_table(inFH, sep='\\t'):\n    \"\"\"Load frag info table as a dict of dicts of 2d lists.\n    \n    Parameters\n    ----------\n    inFH : file handle\n    sep : str\n        value delimiter\n    \n    Returns\n    -------\n    frags : dict\n        {taxon_name : {scaffold : [fragStart, fragEnd, GC]}}\n    \"\"\"    \n    header_vals = set(['taxon_name','scaffoldID','fragStart',\n                       'fragLength','fragGC'])\n    \n    d = dict()\n    lineNum = 0\n    for line in inFH.readlines():\n        lineNum += 1\n        line = line.rstrip().split(sep)\n\n        #header\n        if lineNum == 1:            \n            if not (header_vals == set(line) or header_vals < set(line)):\n                msg = 'The fragGC table does not have all'\\\n                      ' required columns:\\n\\t{}'\\\n                      .format(','.join(header_vals))\n                raise IOError(msg)\n            header_idx = {line[i]:i for i in xrange(len(line))}\n        # body            \n        else:\n            taxon_name = line[header_idx['taxon_name']]\n            try:\n                type(d[taxon_name])\n            except KeyError:\n                d[taxon_name] = dict()\n                d[taxon_name]['fragLength'] = []\n                d[taxon_name]['fragGC'] = []\n\n            # fragment length\n            fragLength = line[header_idx['fragLength']]\n            try:\n                fragLength = int(fragLength)\n            except ValueError:\n                continue\n\n            # fragment GC content\n            fragGC = line[header_idx['fragGC']]\n            try:\n                fragGC = float(fragGC)\n            except ValueError:\n                continue\n\n            # adding to dict\n            d[taxon_name]['fragLength'].append(fragLength)\n            d[taxon_name]['fragGC'].append(fragGC)\n\n    return d\n\n            \ndef load_frags_pickle(inFH):\n    \"\"\"Load frag GC info assuming a pickled python object\n    produced by SIPSim fragGC.\n    \n    Parameters\n    ----------\n    inFH : file handle\n\n    Returns\n    -------\n    d : dict\n        {taxon_name : {info : [values]}}\n    \"\"\"\n    fojb =  pickle.load(inFH)\n\n    d = dict()\n    for x in fojb:\n        taxon_name = x[0]\n        if d.has_key(taxon_name):\n            msg =  'WARNING: {} was found multiple times\\n'\n            sys.stderr.write(msg.format(taxon_name))\n        d[taxon_name] = dict()\n        d[taxon_name]['fragLength'] = []\n        d[taxon_name]['fragGC'] = []\n            \n        for scaf,v in x[1].items():            \n            for z in v:\n                # fragStart, fragLength, fragGC\n                d[taxon_name]['fragLength'].append(z[1])\n                d[taxon_name]['fragGC'].append(z[2])              \n    return d\n\n\ndef load_frags(fileName):\n    \"\"\"Load fragment data (pickled) table.\n    \n    Parameters\n    ----------\n    fileName : str\n        name of the fragment data table\n\n    Returns\n    -------\n    d : dict{dict} \n        {taxon_name:{key:value}}\n    \"\"\"\n    try:\n        inFH = open(fileName, 'r')\n    except IOError:\n        inFH = sys.stdin\n\n    try:\n        frag_data = load_frags_pickle(inFH)\n    except (pickle.UnpicklingError, EOFError):\n        try:\n            inFH.seek(0)\n        except IOError:\n            msg = ('Illegal seek; either you piped in a non-pickled table or'\n                   'your file name is incorrect')\n            raise IOError(msg)\n        frag_data = load_frags_table(inFH)            \n\n    inFH.close()\n    \n    return frag_data\n\n\ndef fit_kde(frag_data, bw_method=None):\n    \"\"\"Returns multivariate KernelDensity function fit to\n    fragment buoyant density (calculated from G+C) \n    and fragment lengths.\n    Bandwidth selection based on bandwidth attribute.\n    \n    Parameters\n    ----------\n    frag_data : dict\n       dict of lists {fragment info)\n    bw_method -- passed to stats.gaussian_kde\n\n    Returns\n    -------\n    d : dict \n        Dict of KDE objects {taxon_name:KDE}\n    \"\"\"\n    try:\n        bw_method = float(bw_method)\n    except TypeError:\n        pass\n    \n    kdes = dict()\n    for taxon_name,data in frag_data.items():\n        # getting GC & length values\n        try:\n            frag_GC = data['fragGC']\n        except KeyError:\n            msg = 'Taxon: {}: cannot find \"fragGC\"'            \n            raise KeyError, msg.format(taxon_name)\n        try:\n            frag_len = data['fragLength']\n        except KeyError:\n            msg = 'Taxon: {}: cannot find \"fragLength\"'            \n            raise KeyError, msg.format(taxon_name)\n\n        # GC2BD\n        frag_BD = GC2BD(np.array(frag_GC))\n\n        # kde fitting\n        try:\n            kdes[taxon_name] = stats.gaussian_kde([frag_BD, frag_len], \n                                                  bw_method=bw_method)\n        except ValueError:\n            kdes[taxon_name] = None\n\n    return kdes\n\n     \ndef by_genome(x, args):\n    \"\"\"All processing conducted per genome.\n\n    Parameters\n    ----------\n    x : list\n        [inFile,taxonName]\n        inFile -- genome sequence file name\n        taxonName -- taxon name of genome\n    args : dict\n       user-provided args \n\n    Returns\n    -------\n    l2d -- list of lists\n        for each fragment: [taxonName,scaf,start,end,GC]\n    \"\"\"\n    taxonName,inFile = x\n    # status\n    sys.stderr.write('Processing: \"{}\"\\n'.format(taxonName))\n\n    # making genome object\n    assert '--fr' in args, '\"--fr\" must be provided in args'\n    genome = Genome(inFile, taxonName, args['--fr'])\n    \n    # MFEprimer.py executable\n    MFEprimerExe = args['--MFE']\n    \n    # sequenced read template location: amplicons\n    if genome.primerFile is not None:\n        # in-silico PCR\n        assert '--rtr' in args, '\"--rtr\" must be in args'\n        genome.callMFEprimer(rtr=args['--rtr'], MFEprimerExe=MFEprimerExe)\n    \n        # filtering overlapping in-silico amplicons\n        genome.filterOverlaps()\n                \n    # simulating fragments    \n    simFO = SimFrags(fld=args['--fld'], flr=args['--flr'], rtl=args['--rtl'])\n    nFragsMade = 0\n    fragList = dict()\n    ## if no amplicons\n    if genome.nAmplicons == 0:\n        pass\n    ## if using coverage\n    elif args['--nf'].endswith('X') or args['--nf'].endswith('x'):\n        coverage = float(args['--nf'].rstrip('xX'))\n        fragLenCov = genome.length * coverage\n        fragLenTotal = 0\n        while 1:\n            (scaf,fragStart,fragLen,fragGC) = simFO.simFrag(genome)\n            try:\n                type(fragList[scaf])\n            except KeyError:\n                fragList[scaf] = []\n                                \n            if fragStart == \"NA\":\n                break\n            elif fragLenTotal > fragLenCov:\n                break\n            fragLenTotal += fragLen \n\n            nFragsMade += 1\n            fragList[scaf].append([fragStart, fragLen, fragGC])            \n    ## if using fixed number of fragments\n    else:            \n        for i in xrange(int(args['--nf'])):\n            (scaf,fragStart,fragLen,fragGC) = simFO.simFrag(genome)\n\n            try:\n                type(fragList[scaf])\n            except KeyError:\n                fragList[scaf] = []\n\n            if fragStart == \"NA\":\n                break\n\n            nFragsMade += 1\n            fragList[scaf].append([fragStart, fragLen, fragGC])\n                \n    # status\n    sys.stderr.write('  Genome name: {}\\n'.format(genome.taxonName))                \n    sys.stderr.write('  Genome length (bp): {}\\n'.format(genome.length))\n    if args['--nf']:\n        msg = '  Number of amplicons: {}\\n'\n        sys.stderr.write(msg.format(genome.nAmplicons))\n    msg = '  Number of fragments simulated: {}\\n'\n    sys.stderr.write(msg.format(nFragsMade))\n                \n    return [genome.taxonName, fragList]\n\n\ndef write_fragList(fragList):\n    \"\"\"Write out fragList as a tab-delim table.\"\"\"\n    print '\\t'.join(['taxon_name','scaffoldID','fragStart',\n                     'fragLength','fragGC'])            \n    for x in fragList:\n        taxon_name = x[0]\n        for scaf,v in x[1].items():\n            for y in v:                \n                print '\\t'.join([taxon_name, scaf] + [str(i) for i in y])\n\n\n        \n\n", ")]\n\nfor pin in rgb:\n    pin.mode = pingo.OUT\n    pin.low()\n\nwhile True:\n    for pin in rgb:\n        pin.low()\n        print(pin, pin.state)\n        time.sleep(.5)\n        pin.high()\n    break\n", "strap import Bootstrap\n\napp = Flask(__name__)\napp.secret_key = 'itmard_khastas:D'\nBootstrap(app)\n\n@app.route('/')\ndef index():\n    login_status = session.get('logged_in')\n    return render_template('index.html', login_status=login_status)\n\n@app.route('/login/', methods=['GET', 'POST'])\ndef login():\n    error = None\n    if request.method == 'POST':\n        if request.form['username'] != 'admin' and request.form['password'] != 'admin':\n            error = 'Invalid credentials'\n        else:\n            session['logged_in'] = True\n            flash('You were successfully logged in')\n            return redirect(url_for('index'))\n    return render_template('login.html', error=error)\n\n@app.route('/logout/')\ndef logout():\n    if session.get('logged_in'):\n        session['logged_in'] = False\n        return redirect(url_for('index'))\n\nif __name__ == \"__main__\":\n    app.run(debug=True)", "u_too_hot'\nBASE_URL = 'https://maker.ifttt.com/trigger/'\nKEY = 'cyR3vPNFlP9K32W4NZB9cd'\n\ndef send_notification(temp):\n    data = urllib.urlencode({'value1' : str(temp)})\n    url = BASE_URL + EVENT + '/with/key/' + KEY\n    response = urllib2.urlopen(url=url, data=data)\n    print(response.read())\n\n\ndef cpu_temp():\n    dev = os.popen('/opt/vc/bin/vcgencmd measure_temp')\n    cpu_temp = dev.read()[5:-3]\n    return float(cpu_temp)\n    \nwhile True:\n    temp = cpu_temp()\n    print(\"CPU Temp (C): \" + str(temp))\n    if temp > MAX_TEMP:\n        print(\"CPU TOO HOT!\")\n        send_notification(temp)\n        print(\"No more notifications for: \" + str(MIN_T_BETWEEN_WARNINGS) + \" mins\")\n        time.sleep(MIN_T_BETWEEN_WARNINGS * 60)\n    time.sleep(1)\n", "pypi.python.org/pypi/PyDispatcher/2.0.1\nSee license.txt for original license.\n\nHeavily modified for Django's purposes.\n\"\"\"\n\nfrom django.dispatch.dispatcher import Signal, receiver  # NOQA\n", " distlib.locators import Locator, SimpleScrapingLocator\nfrom distlib.util import split_filename\nfrom six.moves.urllib.parse import urlparse  # pylint: disable=F0401,E0611\n\n\nLOG = logging.getLogger(__name__)\nALL_EXTENSIONS = Locator.source_extensions + Locator.binary_extensions\n\n\ndef parse_filename(filename, name=None):\n    \"\"\" Parse a name and version out of a filename \"\"\"\n    version = None\n    for ext in ALL_EXTENSIONS:\n        if filename.endswith(ext):\n            trimmed = filename[:-len(ext)]\n            parsed = split_filename(trimmed, name)\n            if parsed is None:\n                break\n            else:\n                parsed_name, version = parsed[:2]\n            break\n    if version is None:\n        raise ValueError(\"Cannot parse package file '%s'\" % filename)\n    if name is None:\n        name = parsed_name\n    return normalize_name(name), version\n\n\ndef normalize_name(name):\n    \"\"\" Normalize a python package name \"\"\"\n    # Lifted directly from PEP503:\n    # https://www.python.org/dev/peps/pep-0503/#id4\n    return re.sub(r\"[-_.]+\", \"-\", name).lower()\n\n\nclass BetterScrapingLocator(SimpleScrapingLocator):\n\n    \"\"\" Layer on top of SimpleScrapingLocator that allows preferring wheels \"\"\"\n    prefer_wheel = True\n\n    def __init__(self, *args, **kw):\n        kw['scheme'] = 'legacy'\n        super(BetterScrapingLocator, self).__init__(*args, **kw)\n\n    def locate(self, requirement, prereleases=False, wheel=True):\n        self.prefer_wheel = wheel\n        return super(BetterScrapingLocator, self).locate(requirement, prereleases)\n\n    def score_url(self, url):\n        t = urlparse(url)\n        filename = posixpath.basename(t.path)\n        return (\n            t.scheme == 'https',\n            not (self.prefer_wheel ^ filename.endswith('.whl')),\n            'pypi.python.org' in t.netloc,\n            filename,\n        )\n\n    def _get_project(self, name):\n        # We're overriding _get_project so that we can wrap the name with the\n        # NormalizeNameHackString. This is hopefully temporary. See this PR for\n        # more details:\n        # https://bitbucket.org/vinay.sajip/distlib/pull-requests/7/update-name-comparison-to-match-pep-503\n        return super(BetterScrapingLocator, self)._get_project(NormalizeNameHackString(name))\n\n\n# Distlib checks if wheels are compatible before returning them.\n# This is useful if you are attempting to install on the system running\n# distlib, but we actually want ALL wheels so we can display them to the\n# clients.  So we have to monkey patch the method. I'm sorry.\ndef is_compatible(wheel, tags=None):\n    \"\"\" Hacked function to monkey patch into distlib \"\"\"\n    return True\n\ndistlib.locators.is_compatible = is_compatible\n\n\nclass NormalizeNameHackString(six.text_type):\n    \"\"\"\n    Super hacked wrapper around a string that runs normalize_name before doing\n    equality comparisons\n\n    \"\"\"\n\n    def lower(self):\n        # lower() needs to return another NormalizeNameHackString in order to\n        # plumb this hack far enough into distlib.\n        lower = super(NormalizeNameHackString, self).lower()\n        return NormalizeNameHackString(lower)\n\n    def __eq__(self, other):\n        if isinstance(other, six.string_types):\n            return normalize_name(self) == normalize_name(other)\n        else:\n            return False\n\n\ndef getdefaults(settings, *args):\n    \"\"\"\n    Attempt multiple gets from a dict, returning a default value if none of the\n    keys are found.\n\n    \"\"\"\n    assert len(args) >= 3\n    args, default = args[:-1], args[-1]\n    canonical = args[0]\n    for key in args:\n        if key in settings:\n            if key != canonical:\n                LOG.warn(\"Using deprecated option '%s' \"\n                         \"(replaced by '%s')\", key, canonical)\n            return settings[key]\n    return default\n\n\ndef create_matcher(queries, query_type):\n    \"\"\"\n    Create a matcher for a list of queries\n\n    Parameters\n    ----------\n    queries : list\n        List of queries\n\n    query_type: str\n        Type of query to run: [\"or\"|\"and\"]\n\n    Returns\n    -------\n        Matcher function\n\n    \"\"\"\n    queries = [query.lower() for query in queries]\n    if query_type == 'or':\n        return lambda x: any((q in x.lower() for q in queries))\n    else:\n        return lambda x: all((q in x.lower() for q in queries))\n", "ingIO import StringIO\nfrom Bio import SeqIO\nfrom biotool import FastaStats\n\nclass TestFastaStats(unittest.TestCase):\n    '''Unit tests for FastaStats'''\n    def do_test(self, input_str, minlen, expected):\n        \"Wrapper function for testing FastaStats\"\n        result = FastaStats().from_file(StringIO(input_str), minlen)\n        self.assertEqual(expected, result)\n\n    def test_zero_byte_input(self):\n        \"Test input containing zero bytes\"\n        expected = FastaStats(num_seqs=0,\n                              num_bases=0,\n                              min_len=None,\n                              max_len=None,\n                              average=None)\n        self.do_test('', 0, expected)\n\n    def test_single_newline_input(self):\n        \"Test input containing a newline (\\n) character\"\n        expected = FastaStats(num_seqs=0,\n                              num_bases=0,\n                              min_len=None,\n                              max_len=None,\n                              average=None)\n        self.do_test('\\n', 0, expected)\n\n    def test_single_greater_than_input(self):\n        \"Test input containing a single greater-than (>) character\"\n        expected = FastaStats(num_seqs=1,\n                              num_bases=0,\n                              min_len=0,\n                              max_len=0,\n                              average=0)\n        self.do_test('>', 0, expected)\n\n    def test_one_sequence(self):\n        \"Test input containing one sequence\"\n        expected = FastaStats(num_seqs=1,\n                              num_bases=5,\n                              min_len=5,\n                              max_len=5,\n                              average=5)\n        self.do_test(\">header\\nATGC\\nA\", 0, expected)\n\n    def test_two_sequences(self):\n        \"Test input containing two sequences\"\n        expected = FastaStats(num_seqs=2,\n                              num_bases=9,\n                              min_len=2,\n                              max_len=7,\n                              average=4)\n        self.do_test(\">header1\\nATGC\\nAGG\\n>header2\\nTT\\n\", 0, expected)\n\n    def test_no_header(self):\n        \"Test input containing sequence without preceding header\"\n        expected = FastaStats(num_seqs=0,\n                              num_bases=0,\n                              min_len=None,\n                              max_len=None,\n                              average=None)\n        self.do_test(\"no header\\n\", 0, expected)\n\n    def test_minlen_less_than_all(self):\n        \"Test input when --minlen is less than 2 out of 2 sequences\"\n        expected = FastaStats(num_seqs=2,\n                              num_bases=9,\n                              min_len=2,\n                              max_len=7,\n                              average=4)\n        self.do_test(\">header1\\nATGC\\nAGG\\n>header2\\nTT\\n\", 2, expected)\n\n    def test_minlen_greater_than_one(self):\n        \"Test input when --minlen is less than 1 out of 2 sequences\"\n        expected = FastaStats(num_seqs=1,\n                              num_bases=7,\n                              min_len=7,\n                              max_len=7,\n                              average=7)\n        self.do_test(\">header1\\nATGC\\nAGG\\n>header2\\nTT\\n\", 3, expected)\n\n    def test_minlen_greater_than_all(self):\n        \"Test input when --minlen is greater than 2 out of 2 sequences\"\n        expected = FastaStats(num_seqs=0,\n                              num_bases=0,\n                              min_len=None,\n                              max_len=None,\n                              average=None)\n        self.do_test(\">header1\\nATGC\\nAGG\\n>header2\\nTT\\n\", 8, expected)\n\n\nif __name__ == '__main__':\n    unittest.main()\n", ".coding import for_argument\nfrom satella.coding.structures import Singleton\nfrom satella.coding.transforms import percentile\nfrom satella.time import parse_time_string\n\nDEFAULT_REFRESH_EACH = '30m'\nDEFAULT_WINDOW_SECONDS = '5m'\n\n\n@Singleton\nclass CPUProfileBuilderThread(threading.Thread):\n    \"\"\"\n    A CPU profile builder thread and a core singleton object to use.\n\n    :param window_seconds: the amount of seconds for which to collect data.\n        Generally, this should be the interval during which your system cycles through all of\n        it's load, eg. if it asks it's devices each 5 minutes, the interval should be 300 seconds.\n        Or a time string.\n    :param refresh_each: time of seconds to sleep between rebuilding of profiles, or a time string.\n    \"\"\"\n\n    def __init__(self, window_seconds: tp.Union[str, int] = DEFAULT_WINDOW_SECONDS,\n                 refresh_each: tp.Union[str, int] = DEFAULT_REFRESH_EACH,\n                 percentiles_requested: tp.Sequence[float] = (0.9,)):\n        super().__init__(name='CPU profile builder', daemon=True)\n        self.window_size = int(parse_time_string(window_seconds))\n        self.refresh_each = parse_time_string(refresh_each)\n        self.data = []\n        self.percentiles_requested = list(percentiles_requested)\n        self.percentile_values = []\n        self.percentiles_regenerated = False\n        self.start()\n\n    def request_percentile(self, percent: float) -> None:\n        if percent not in self.percentiles_requested:\n            self.percentiles_requested.append(percent)\n            self.percentiles_regenerated = False\n\n    def percentile(self, percent: float) -> float:\n        if not self.data:\n            return 0\n        if percent in self.percentiles_requested and self.percentiles_regenerated:\n            return self.percentile_values[self.percentiles_requested.index(percent)]\n        else:\n            return percentile(self.data, percent)\n\n    def is_done(self) -> bool:\n        return bool(self.data)\n\n    def recalculate(self) -> None:\n        data = []\n        calculate_occupancy_factor()  # as first values tend to be a bit wonky\n        for _ in range(int(self.window_size)):\n            time.sleep(1)\n            data.append(calculate_occupancy_factor())\n        percentiles = []\n        data.sort()\n        for percent in self.percentiles_requested:\n            percentiles.append(percentile(data, percent))\n        self.percentile_values = percentiles\n        self.percentiles_regenerated = True\n        self.data = data\n\n    def run(self):\n        while True:\n            time.sleep(self.refresh_each)\n            self.recalculate()\n\n\nclass CPUTimeManager:\n    @staticmethod\n    def percentile(percent: float) -> float:\n        \"\"\"\n        Return given percentile of current CPU time's profile\n        :param percent: float between 0 and 1\n        :return: the value of the percentile\n        \"\"\"\n        return CPUProfileBuilderThread().percentile(percent)\n\n    @staticmethod\n    def set_window_size(window_size: float) -> None:\n        \"\"\"\n        Set the time that should be observed in order to build an execution profile.\n\n        :param window_size: time, in seconds\n        \"\"\"\n        CPUProfileBuilderThread().window_size = window_size\n\n\n@for_argument(parse_time_string)\ndef sleep_cpu_aware(seconds: tp.Union[str, float], of_below: tp.Optional[float] = None,\n                    of_above: tp.Optional[float] = None,\n                    check_each: float = 1) -> bool:\n    \"\"\"\n    Sleep for specified number of seconds.\n\n    Quit earlier if the occupancy factor goes below of_below or above of_above\n    :param seconds: time to sleep in seconds, or a time string\n    :param of_below: occupancy factor below which the sleep will return\n    :param of_above: occupancy factor above which the sleep will return\n    :param check_each: amount of seconds to sleep at once\n    :return: whether was awoken due to CPU time condition\n    \"\"\"\n    v = False\n    if of_below is None and of_above is None:\n        time.sleep(seconds)\n    else:\n        calculate_occupancy_factor()  # prime the counter\n        while seconds > 0:\n            time_to_sleep = min(seconds, check_each)\n            time.sleep(time_to_sleep)\n            of = calculate_occupancy_factor()\n\n            if of_above is not None:\n                if of > of_above:\n                    v = True\n                    break\n            if of_below is not None:\n                if of < of_below:\n                    v = True\n                    break\n            seconds -= time_to_sleep\n            if seconds <= 0:\n                break\n    return v\n\n\nprevious_cf = None  # type: float\nprevious_timestamp = None  # type: float\n\n\ndef _calculate_occupancy_factor() -> float:\n    c = psutil.cpu_times()\n    try:\n        try:\n            try:\n                used = c.user + c.nice + c.system + c.irq + c.softirq + c.steal + c.guest + c.guest_nice\n            except AttributeError:\n                # Linux?\n                used = c.user + c.nice + c.system + c.irq + c.softirq\n        except AttributeError:\n            # UNIX ?\n            used = c.user + c.nice + c.system\n    except AttributeError:\n        # windows?\n        used = c.user + c.system + c.interrupt + c.dpc\n    cur_time = time.monotonic()\n    occupation_factor = used / multiprocessing.cpu_count()\n    global previous_timestamp, previous_cf\n    if previous_timestamp is None:\n        previous_cf = occupation_factor\n        previous_timestamp = cur_time\n        return\n    delta = cur_time - previous_timestamp\n    if delta == 0:\n        return\n    of = (occupation_factor - previous_cf) / delta\n    previous_cf = occupation_factor\n    previous_timestamp = cur_time\n    return of\n\n\ndef calculate_occupancy_factor() -> float:\n    \"\"\"\n    Get the average load between now and the time it was last called as a float,\n    where 0.0 is LA=0 and 1.0 is LA=max_cores.\n\n    This will be the average between now and the time it was last called.\n\n    .. warning:: This in rare cases (being called the first or the second time) may block for\n                 up to 0.1 seconds\n\n    :return: a float between 0 and 1 telling you how occupied CPU-wise is your system.\n    \"\"\"\n    c = _calculate_occupancy_factor()\n    while c is None:\n        time.sleep(0.1)\n        c = _calculate_occupancy_factor()\n    return c\n", "ium.webdriver.common.mobileby import MobileBy\n\n\nclass ContestoBy(By):\n    SIZZLE = \"sizzle\"\n\n\nclass Locator(dict):\n    def __init__(self, by, value):\n        super(Locator, self).__init__()\n        self['by'] = by\n        self['value'] = value\n\n\nclass JavaUiSelector(str):\n    def __new__(cls, data=None):\n        if data is None:\n            data = str(\"new UiSelector()\")\n        return str.__new__(cls, data)\n\n    def description(self, desc):\n        return JavaUiSelector(self + '.description(\"%s\")' % desc)\n\n    def description_contains(self, desc):\n        return JavaUiSelector(self + '.descriptionContains(\"%s\")' % desc)\n\n    def description_matches(self, regex):\n        return JavaUiSelector(self + '.descriptionMatches(\"%s\")' % regex)\n\n    def child_selector(self, selector):\n        return JavaUiSelector(self + '.childSelector(%s)' % selector)\n\n    def index(self, index):\n        return JavaUiSelector(self + '.index(%s)' % index)\n\n    def instance(self, instance):\n        return JavaUiSelector(self + '.instance(%s)' % instance)\n\n    def resource_id(self, id):\n        return JavaUiSelector(self + '.resourceId(\"%s\")' % id)\n\n\nby_id = partial(Locator, By.ID)\nby_xpath = partial(Locator, By.XPATH)\nby_link_text = partial(Locator, By.LINK_TEXT)\nby_partial_link_text = partial(Locator, By.PARTIAL_LINK_TEXT)\nby_name = partial(Locator, By.NAME)\nby_tag_name = partial(Locator, By.TAG_NAME)\nby_class_name = partial(Locator, By.CLASS_NAME)\nby_css_selector = partial(Locator, By.CSS_SELECTOR)\n\nby_uiautomator = partial(Locator, MobileBy.ANDROID_UIAUTOMATOR)\nby_uiautomation = partial(Locator, MobileBy.IOS_UIAUTOMATION)\nby_accessibility_id = partial(Locator, MobileBy.ACCESSIBILITY_ID)\n\nby_sizzle = partial(Locator, ContestoBy.SIZZLE)\n\n\nclass by:\n    id = partial(Locator, By.ID)\n    xpath = partial(Locator, By.XPATH)\n    link_text = partial(Locator, By.LINK_TEXT)\n    partial_link_text = partial(Locator, By.PARTIAL_LINK_TEXT)\n    name = partial(Locator, By.NAME)\n    tag_name = partial(Locator, By.TAG_NAME)\n    class_name = partial(Locator, By.CLASS_NAME)\n    css_selector = partial(Locator, By.CSS_SELECTOR)\n\n    uiautomator = partial(Locator, MobileBy.ANDROID_UIAUTOMATOR)\n    uiautomation = partial(Locator, MobileBy.IOS_UIAUTOMATION)\n    accessibility_id = partial(Locator, MobileBy.ACCESSIBILITY_ID)\n\n    sizzle = partial(Locator, ContestoBy.SIZZLE)\n", "\nsettings.media_root = path('media/') if arg('debug') else '/var/media/'\n\nprint settings\n", "sion to and from strings:\n\tord(c) -> integer\n\tchr(i) -> character \n'''\n\n\t\nclass DecodeError(Exception):\n    pass\n\n\ndef encode(in_bytes):\n    '''Encode a string using Consistent Overhead Byte Stuffing (COBS).\n    \n    Input is any byte string. Output is also a byte string.\n    \n    Encoding guarantees no zero bytes in the output. The output\n    string will be expanded slightly, by a predictable amount.\n    \n    An empty string is encoded to \"\\\\x01\"'''\n    final_zero = True\n    out_bytes = []\n    idx = 0\n    search_start_idx = 0\n    for in_char in in_bytes:\n        if in_char == '\\x00':\n            final_zero = True\n            out_bytes.append(chr(idx - search_start_idx + 1))\n            out_bytes.append(in_bytes[search_start_idx:idx])\n            search_start_idx = idx + 1\n        else:\n            if idx - search_start_idx == 0xFD:\n                final_zero = False\n                out_bytes.append('\\xFF')\n                out_bytes.append(in_bytes[search_start_idx:idx+1])\n                search_start_idx = idx + 1\n        idx += 1\n    if idx != search_start_idx or final_zero:\n        out_bytes.append(chr(idx - search_start_idx + 1))\n        out_bytes.append(in_bytes[search_start_idx:idx])\n    return ''.join(out_bytes)\n\n\ndef decode(in_bytes):\n    '''Decode a string using Consistent Overhead Byte Stuffing (COBS).\n    \n    Input should be a byte string that has been COBS encoded. Output\n    is also a byte string.\n    \n    A cobs.DecodeError exception will be raised if the encoded data\n    is invalid.'''\n    out_bytes = []\n    idx = 0\n\n    if len(in_bytes) > 0:\n        while True:\n            length = ord(in_bytes[idx])\n            if length == 0:\n                raise DecodeError(\"zero byte found in input\")\n            idx += 1\n            end = idx + length - 1\n            copy_bytes = in_bytes[idx:end]\n            if '\\x00' in copy_bytes:\n                raise DecodeError(\"zero byte found in input\")\n            out_bytes.append(copy_bytes)\n            idx = end\n            if idx > len(in_bytes):\n                raise DecodeError(\"not enough input bytes for length code\")\n            if idx < len(in_bytes):\n                if length < 0xFF:\n                    out_bytes.append('\\x00')\n            else:\n                break\n    return ''.join(out_bytes)\n", "lBridge): class for subl bridge\n\"\"\"\nimport imp\n\nfrom os import path\n\nfrom EasyClangComplete.tests.gui_test_wrapper import GuiTestWrapper\nfrom EasyClangComplete.plugin.utils import action_request\nfrom EasyClangComplete.plugin.utils.subl import row_col\n\nimp.reload(action_request)\nimp.reload(row_col)\n\nActionRequest = action_request.ActionRequest\nZeroIndexedRowCol = row_col.ZeroIndexedRowCol\nOneIndexedRowCol = row_col.OneIndexedRowCol\n\n\nclass test_action_request(GuiTestWrapper):\n    \"\"\"Test other things.\"\"\"\n\n    def test_setup_view(self):\n        \"\"\"Test that setup view correctly sets up the view.\"\"\"\n        file_name = path.join(path.dirname(__file__),\n                              'test_files',\n                              'test.cpp')\n        self.check_view(file_name)\n\n    def test_round_trip(self):\n        \"\"\"Test that we can create another location from rowcol of the view.\"\"\"\n        file_name = path.join(path.dirname(__file__),\n                              'test_files',\n                              'test.cpp')\n        query_pos = ZeroIndexedRowCol.from_one_indexed(\n            OneIndexedRowCol(5, 9))\n        self.set_up_view(file_path=file_name, cursor_position=query_pos)\n        self.assertEqual(self.get_row(query_pos.row), \"  void foo(double a);\")\n        equal_pos = ZeroIndexedRowCol.from_1d_location(\n            self.view, query_pos.as_1d_location(self.view))\n        self.assertEqual(equal_pos.row, query_pos.row)\n        self.assertEqual(equal_pos.col, query_pos.col)\n        self.assertEqual(equal_pos.as_1d_location(self.view),\n                         query_pos.as_1d_location(self.view))\n\n    def test_create(self):\n        \"\"\"Test creation.\"\"\"\n        self.set_up_view()\n        expected_trigger_position = 42\n        action_request = ActionRequest(self.view, expected_trigger_position)\n        self.assertEqual(expected_trigger_position,\n                         action_request.get_trigger_position())\n        self.assertEqual(self.view.buffer_id(),\n                         action_request.get_view().buffer_id())\n        expected_identifier = (self.view.buffer_id(), expected_trigger_position)\n        self.assertEqual(expected_identifier, action_request.get_identifier())\n\n    def test_suitable(self):\n        \"\"\"Test that we detect if the view is suitable.\"\"\"\n        file_path = path.join(path.dirname(__file__),\n                              'test_files',\n                              'test.cpp')\n        query_pos = ZeroIndexedRowCol.from_one_indexed(\n            OneIndexedRowCol(5, 9))\n        self.set_up_view(file_path=file_path, cursor_position=query_pos)\n        self.assertEqual(self.get_row(query_pos.row), \"  void foo(double a);\")\n        trigger_position = self.view.text_point(query_pos.row, query_pos.col)\n        current_word = self.view.substr(self.view.word(trigger_position))\n        self.assertEqual(current_word, \"foo\")\n\n        action_request = ActionRequest(self.view, trigger_position)\n        self.assertTrue(action_request.is_suitable_for_view(self.view))\n\n    def test_not_suitable_location(self):\n        \"\"\"Test that we detect if the view is suitable.\"\"\"\n        file_path = path.join(path.dirname(__file__),\n                              'test_files',\n                              'test.cpp')\n        query_pos = ZeroIndexedRowCol.from_one_indexed(\n            OneIndexedRowCol(5, 9))\n        self.set_up_view(file_path=file_path, cursor_position=query_pos)\n        self.assertEqual(self.get_row(query_pos.row), \"  void foo(double a);\")\n        trigger_position = self.view.text_point(query_pos.row, query_pos.col)\n        current_word = self.view.substr(self.view.word(trigger_position))\n        self.assertEqual(current_word, \"foo\")\n\n        wrong_trigger_position = 42\n        action_request = ActionRequest(self.view, wrong_trigger_position)\n        self.assertFalse(action_request.is_suitable_for_view(self.view))\n\n    def test_not_suitable_view(self):\n        \"\"\"Test that we detect if the view is suitable.\"\"\"\n        self.set_up_view()\n\n        default_trigger_position = 0\n        action_request = ActionRequest(self.view, default_trigger_position)\n        self.assertTrue(action_request.is_suitable_for_view(self.view))\n\n        self.tearDown()\n        self.setUp()\n        self.set_up_view()\n        self.assertFalse(action_request.is_suitable_for_view(self.view))\n", "port socket\nimport unittest\nimport requests_mock\nimport random\nfrom nose.tools import raises\nfrom mock import patch\nimport warnings\nimport mock\n\nfrom influxdb.influxdb08 import InfluxDBClient\nfrom influxdb.influxdb08.client import session\n\nimport sys\nif sys.version < '3':\n    import codecs\n\n    def u(x):\n        return codecs.unicode_escape_decode(x)[0]\nelse:\n    def u(x):\n        return x\n\n\ndef _build_response_object(status_code=200, content=\"\"):\n    resp = requests.Response()\n    resp.status_code = status_code\n    resp._content = content.encode(\"utf8\")\n    return resp\n\n\ndef _mocked_session(method=\"GET\", status_code=200, content=\"\"):\n\n    method = method.upper()\n\n    def request(*args, **kwargs):\n        c = content\n\n        # Check method\n        assert method == kwargs.get('method', 'GET')\n\n        if method == 'POST':\n            data = kwargs.get('data', None)\n\n            if data is not None:\n                # Data must be a string\n                assert isinstance(data, str)\n\n                # Data must be a JSON string\n                assert c == json.loads(data, strict=True)\n\n                c = data\n\n        # Anyway, Content must be a JSON string (or empty string)\n        if not isinstance(c, str):\n            c = json.dumps(c)\n\n        return _build_response_object(status_code=status_code, content=c)\n\n    mocked = patch.object(\n        session,\n        'request',\n        side_effect=request\n    )\n\n    return mocked\n\n\nclass TestInfluxDBClient(unittest.TestCase):\n\n    def setUp(self):\n        # By default, raise exceptions on warnings\n        warnings.simplefilter('error', FutureWarning)\n\n        self.dummy_points = [\n            {\n                \"points\": [\n                    [\"1\", 1, 1.0],\n                    [\"2\", 2, 2.0]\n                ],\n                \"name\": \"foo\",\n                \"columns\": [\"column_one\", \"column_two\", \"column_three\"]\n            }\n        ]\n\n        self.dsn_string = 'influxdb://uSr:pWd@host:1886/db'\n\n    def test_scheme(self):\n        cli = InfluxDBClient('host', 8086, 'username', 'password', 'database')\n        self.assertEqual(cli._baseurl, 'http://host:8086')\n\n        cli = InfluxDBClient(\n            'host', 8086, 'username', 'password', 'database', ssl=True\n        )\n        self.assertEqual(cli._baseurl, 'https://host:8086')\n\n    def test_dsn(self):\n        cli = InfluxDBClient.from_DSN(self.dsn_string)\n        self.assertEqual('http://host:1886', cli._baseurl)\n        self.assertEqual('uSr', cli._username)\n        self.assertEqual('pWd', cli._password)\n        self.assertEqual('db', cli._database)\n        self.assertFalse(cli.use_udp)\n\n        cli = InfluxDBClient.from_DSN('udp+' + self.dsn_string)\n        self.assertTrue(cli.use_udp)\n\n        cli = InfluxDBClient.from_DSN('https+' + self.dsn_string)\n        self.assertEqual('https://host:1886', cli._baseurl)\n\n        cli = InfluxDBClient.from_DSN('https+' + self.dsn_string,\n                                      **{'ssl': False})\n        self.assertEqual('http://host:1886', cli._baseurl)\n\n    def test_switch_database(self):\n        cli = InfluxDBClient('host', 8086, 'username', 'password', 'database')\n        cli.switch_database('another_database')\n        self.assertEqual(cli._database, 'another_database')\n\n    @raises(FutureWarning)\n    def test_switch_db_deprecated(self):\n        cli = InfluxDBClient('host', 8086, 'username', 'password', 'database')\n        cli.switch_db('another_database')\n        self.assertEqual(cli._database, 'another_database')\n\n    def test_switch_user(self):\n        cli = InfluxDBClient('host', 8086, 'username', 'password', 'database')\n        cli.switch_user('another_username', 'another_password')\n        self.assertEqual(cli._username, 'another_username')\n        self.assertEqual(cli._password, 'another_password')\n\n    def test_write(self):\n        with requests_mock.Mocker() as m:\n            m.register_uri(\n                requests_mock.POST,\n                \"http://localhost:8086/write\"\n            )\n            cli = InfluxDBClient(database='db')\n            cli.write(\n                {\"database\": \"mydb\",\n                 \"retentionPolicy\": \"mypolicy\",\n                 \"points\": [{\"name\": \"cpu_load_short\",\n                             \"tags\": {\"host\": \"server01\",\n                                      \"region\": \"us-west\"},\n                             \"timestamp\": \"2009-11-10T23:00:00Z\",\n                             \"values\": {\"value\": 0.64}}]}\n            )\n\n            self.assertEqual(\n                json.loads(m.last_request.body),\n                {\"database\": \"mydb\",\n                 \"retentionPolicy\": \"mypolicy\",\n                 \"points\": [{\"name\": \"cpu_load_short\",\n                             \"tags\": {\"host\": \"server01\",\n                                      \"region\": \"us-west\"},\n                             \"timestamp\": \"2009-11-10T23:00:00Z\",\n                             \"values\": {\"value\": 0.64}}]}\n            )\n\n    def test_write_points(self):\n        with requests_mock.Mocker() as m:\n            m.register_uri(\n                requests_mock.POST,\n                \"http://localhost:8086/db/db/series\"\n            )\n\n            cli = InfluxDBClient(database='db')\n            cli.write_points(\n                self.dummy_points\n            )\n\n            self.assertListEqual(\n                json.loads(m.last_request.body),\n                self.dummy_points\n            )\n\n    def test_write_points_string(self):\n        with requests_mock.Mocker() as m:\n            m.register_uri(\n                requests_mock.POST,\n                \"http://localhost:8086/db/db/series\"\n            )\n\n            cli = InfluxDBClient(database='db')\n            cli.write_points(\n                str(json.dumps(self.dummy_points))\n            )\n\n            self.assertListEqual(\n                json.loads(m.last_request.body),\n                self.dummy_points\n            )\n\n    def test_write_points_batch(self):\n        with requests_mock.Mocker() as m:\n            m.register_uri(requests_mock.POST,\n                           \"http://localhost:8086/db/db/series\")\n            cli = InfluxDBClient('localhost', 8086,\n                                 'username', 'password', 'db')\n            cli.write_points(data=self.dummy_points, batch_size=2)\n        self.assertEqual(1, m.call_count)\n\n    def test_write_points_batch_invalid_size(self):\n        with requests_mock.Mocker() as m:\n            m.register_uri(requests_mock.POST,\n                           \"http://localhost:8086/db/db/series\")\n            cli = InfluxDBClient('localhost', 8086,\n                                 'username', 'password', 'db')\n            cli.write_points(data=self.dummy_points, batch_size=-2)\n        self.assertEqual(1, m.call_count)\n\n    def test_write_points_batch_multiple_series(self):\n        dummy_points = [\n            {\"points\": [[\"1\", 1, 1.0], [\"2\", 2, 2.0], [\"3\", 3, 3.0],\n                        [\"4\", 4, 4.0], [\"5\", 5, 5.0]],\n             \"name\": \"foo\",\n             \"columns\": [\"val1\", \"val2\", \"val3\"]},\n            {\"points\": [[\"1\", 1, 1.0], [\"2\", 2, 2.0], [\"3\", 3, 3.0],\n                        [\"4\", 4, 4.0], [\"5\", 5, 5.0], [\"6\", 6, 6.0],\n                        [\"7\", 7, 7.0], [\"8\", 8, 8.0]],\n             \"name\": \"bar\",\n             \"columns\": [\"val1\", \"val2\", \"val3\"]},\n        ]\n        expected_last_body = [{'points': [['7', 7, 7.0], ['8', 8, 8.0]],\n                               'name': 'bar',\n                               'columns': ['val1', 'val2', 'val3']}]\n        with requests_mock.Mocker() as m:\n            m.register_uri(requests_mock.POST,\n                           \"http://localhost:8086/db/db/series\")\n            cli = InfluxDBClient('localhost', 8086,\n                                 'username', 'password', 'db')\n            cli.write_points(data=dummy_points, batch_size=3)\n        self.assertEqual(m.call_count, 5)\n        self.assertEqual(expected_last_body, m.request_history[4].json())\n\n    def test_write_points_udp(self):\n        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n        port = random.randint(4000, 8000)\n        s.bind(('0.0.0.0', port))\n\n        cli = InfluxDBClient(\n            'localhost', 8086, 'root', 'root',\n            'test', use_udp=True, udp_port=port\n        )\n        cli.write_points(self.dummy_points)\n\n        received_data, addr = s.recvfrom(1024)\n\n        self.assertEqual(self.dummy_points,\n                         json.loads(received_data.decode(), strict=True))\n\n    def test_write_bad_precision_udp(self):\n        cli = InfluxDBClient(\n            'localhost', 8086, 'root', 'root',\n            'test', use_udp=True, udp_port=4444\n        )\n\n        with self.assertRaisesRegexp(\n                Exception,\n                \"InfluxDB only supports seconds precision for udp writes\"\n        ):\n            cli.write_points(\n                self.dummy_points,\n                time_precision='ms'\n            )\n\n    @raises(Exception)\n    def test_write_points_fails(self):\n        with _mocked_session('post', 500):\n            cli = InfluxDBClient('host', 8086, 'username', 'password', 'db')\n            cli.write_points([])\n\n    def test_write_points_with_precision(self):\n        with _mocked_session('post', 200, self.dummy_points):\n            cli = InfluxDBClient('host', 8086, 'username', 'password', 'db')\n            self.assertTrue(cli.write_points(self.dummy_points))\n\n    def test_write_points_bad_precision(self):\n        cli = InfluxDBClient()\n        with self.assertRaisesRegexp(\n            Exception,\n            \"Invalid time precision is given. \\(use 's', 'm', 'ms' or 'u'\\)\"\n        ):\n            cli.write_points(\n                self.dummy_points,\n                time_precision='g'\n            )\n\n    @raises(Exception)\n    def test_write_points_with_precision_fails(self):\n        with _mocked_session('post', 500):\n            cli = InfluxDBClient('host', 8086, 'username', 'password', 'db')\n            cli.write_points_with_precision([])\n\n    def test_delete_points(self):\n        with _mocked_session('delete', 204) as mocked:\n            cli = InfluxDBClient('host', 8086, 'username', 'password', 'db')\n            self.assertTrue(cli.delete_points(\"foo\"))\n\n            self.assertEqual(len(mocked.call_args_list), 1)\n            args, kwds = mocked.call_args_list[0]\n\n            self.assertEqual(kwds['params'],\n                             {'u': 'username', 'p': 'password'})\n            self.assertEqual(kwds['url'], 'http://host:8086/db/db/series/foo')\n\n    @raises(Exception)\n    def test_delete_points_with_wrong_name(self):\n        with _mocked_session('delete', 400):\n            cli = InfluxDBClient('host', 8086, 'username', 'password', 'db')\n            cli.delete_points(\"nonexist\")\n\n    @raises(NotImplementedError)\n    def test_create_scheduled_delete(self):\n        cli = InfluxDBClient('host', 8086, 'username', 'password', 'db')\n        cli.create_scheduled_delete([])\n\n    @raises(NotImplementedError)\n    def test_get_list_scheduled_delete(self):\n        cli = InfluxDBClient('host', 8086, 'username', 'password', 'db')\n        cli.get_list_scheduled_delete()\n\n    @raises(NotImplementedError)\n    def test_remove_scheduled_delete(self):\n        cli = InfluxDBClient('host', 8086, 'username', 'password', 'db')\n        cli.remove_scheduled_delete(1)\n\n    def test_query(self):\n        data = [\n            {\n                \"name\": \"foo\",\n                \"columns\": [\"time\", \"sequence_number\", \"column_one\"],\n                \"points\": [\n                    [1383876043, 16, \"2\"], [1383876043, 15, \"1\"],\n                    [1383876035, 14, \"2\"], [1383876035, 13, \"1\"]\n                ]\n            }\n        ]\n        with _mocked_session('get', 200, data):\n            cli = InfluxDBClient('host', 8086, 'username', 'password', 'db')\n            result = cli.query('select column_one from foo;')\n            self.assertEqual(len(result[0]['points']), 4)\n\n    def test_query_chunked(self):\n        cli = InfluxDBClient(database='db')\n        example_object = {\n            'points': [\n                [1415206250119, 40001, 667],\n                [1415206244555, 30001, 7],\n                [1415206228241, 20001, 788],\n                [1415206212980, 10001, 555],\n                [1415197271586, 10001, 23]\n            ],\n            'name': 'foo',\n            'columns': [\n                'time',\n                'sequence_number',\n                'val'\n            ]\n        }\n        example_response = \\\n            json.dumps(example_object) + json.dumps(example_object)\n\n        with requests_mock.Mocker() as m:\n            m.register_uri(\n                requests_mock.GET,\n                \"http://localhost:8086/db/db/series\",\n                text=example_response\n            )\n\n            self.assertListEqual(\n                cli.query('select * from foo', chunked=True),\n                [example_object, example_object]\n            )\n\n    def test_query_chunked_unicode(self):\n        cli = InfluxDBClient(database='db')\n        example_object = {\n            'points': [\n                [1415206212980, 10001, u('unicode-\\xcf\\x89')],\n                [1415197271586, 10001, u('more-unicode-\\xcf\\x90')]\n            ],\n            'name': 'foo',\n            'columns': [\n                'time',\n                'sequence_number',\n                'val'\n            ]\n        }\n        example_response = \\\n            json.dumps(example_object) + json.dumps(example_object)\n\n        with requests_mock.Mocker() as m:\n            m.register_uri(\n                requests_mock.GET,\n                \"http://localhost:8086/db/db/series\",\n                text=example_response\n            )\n\n            self.assertListEqual(\n                cli.query('select * from foo', chunked=True),\n                [example_object, example_object]\n            )\n\n    @raises(Exception)\n    def test_query_fail(self):\n        with _mocked_session('get', 401):\n            cli = InfluxDBClient('host', 8086, 'username', 'password', 'db')\n            cli.query('select column_one from foo;')\n\n    def test_query_bad_precision(self):\n        cli = InfluxDBClient()\n        with self.assertRaisesRegexp(\n            Exception,\n            \"Invalid time precision is given. \\(use 's', 'm', 'ms' or 'u'\\)\"\n        ):\n            cli.query('select column_one from foo', time_precision='g')\n\n    def test_create_database(self):\n        with _mocked_session('post', 201, {\"name\": \"new_db\"}):\n            cli = InfluxDBClient('host', 8086, 'username', 'password', 'db')\n            self.assertTrue(cli.create_database('new_db'))\n\n    @raises(Exception)\n    def test_create_database_fails(self):\n        with _mocked_session('post', 401):\n            cli = InfluxDBClient('host', 8086, 'username', 'password', 'db')\n            cli.create_database('new_db')\n\n    def test_delete_database(self):\n        with _mocked_session('delete', 204):\n            cli = InfluxDBClient('host', 8086, 'username', 'password', 'db')\n            self.assertTrue(cli.delete_database('old_db'))\n\n    @raises(Exception)\n    def test_delete_database_fails(self):\n        with _mocked_session('delete', 401):\n            cli = InfluxDBClient('host', 8086, 'username', 'password', 'db')\n            cli.delete_database('old_db')\n\n    def test_get_list_database(self):\n        data = [\n            {\"name\": \"a_db\"}\n        ]\n        with _mocked_session('get', 200, data):\n            cli = InfluxDBClient('host', 8086, 'username', 'password')\n            self.assertEqual(len(cli.get_list_database()), 1)\n            self.assertEqual(cli.get_list_database()[0]['name'], 'a_db')\n\n    @raises(Exception)\n    def test_get_list_database_fails(self):\n        with _mocked_session('get', 401):\n            cli = InfluxDBClient('host', 8086, 'username', 'password')\n            cli.get_list_database()\n\n    @raises(FutureWarning)\n    def test_get_database_list_deprecated(self):\n        data = [\n            {\"name\": \"a_db\"}\n        ]\n        with _mocked_session('get', 200, data):\n            cli = InfluxDBClient('host', 8086, 'username', 'password')\n            self.assertEqual(len(cli.get_database_list()), 1)\n            self.assertEqual(cli.get_database_list()[0]['name'], 'a_db')\n\n    def test_delete_series(self):\n        with _mocked_session('delete', 204):\n            cli = InfluxDBClient('host', 8086, 'username', 'password', 'db')\n            cli.delete_series('old_series')\n\n    @raises(Exception)\n    def test_delete_series_fails(self):\n        with _mocked_session('delete', 401):\n            cli = InfluxDBClient('host', 8086, 'username', 'password', 'db')\n            cli.delete_series('old_series')\n\n    def test_get_series_list(self):\n        cli = InfluxDBClient(database='db')\n\n        with requests_mock.Mocker() as m:\n            example_response = \\\n                '[{\"name\":\"list_series_result\",\"columns\":' \\\n                '[\"time\",\"name\"],\"points\":[[0,\"foo\"],[0,\"bar\"]]}]'\n\n            m.register_uri(\n                requests_mock.GET,\n                \"http://localhost:8086/db/db/series\",\n                text=example_response\n            )\n\n            self.assertListEqual(\n                cli.get_list_series(),\n                ['foo', 'bar']\n            )\n\n    def test_get_continuous_queries(self):\n        cli = InfluxDBClient(database='db')\n\n        with requests_mock.Mocker() as m:\n\n            # Tip: put this in a json linter!\n            example_response = '[ { \"name\": \"continuous queries\", \"columns\"' \\\n                               ': [ \"time\", \"id\", \"query\" ], \"points\": [ [ ' \\\n                               '0, 1, \"select foo(bar,95) from \\\\\"foo_bar' \\\n                               's\\\\\" group by time(5m) into response_times.' \\\n                               'percentiles.5m.95\" ], [ 0, 2, \"select perce' \\\n                               'ntile(value,95) from \\\\\"response_times\\\\\" g' \\\n                               'roup by time(5m) into response_times.percen' \\\n                               'tiles.5m.95\" ] ] } ]'\n\n            m.register_uri(\n                requests_mock.GET,\n                \"http://localhost:8086/db/db/series\",\n                text=example_response\n            )\n\n            self.assertListEqual(\n                cli.get_list_continuous_queries(),\n                [\n                    'select foo(bar,95) from \"foo_bars\" group '\n                    'by time(5m) into response_times.percentiles.5m.95',\n\n                    'select percentile(value,95) from \"response_times\" group '\n                    'by time(5m) into response_times.percentiles.5m.95'\n                ]\n            )\n\n    def test_get_list_cluster_admins(self):\n        pass\n\n    def test_add_cluster_admin(self):\n        with requests_mock.Mocker() as m:\n            m.register_uri(\n                requests_mock.POST,\n                \"http://localhost:8086/cluster_admins\"\n            )\n\n            cli = InfluxDBClient(database='db')\n            cli.add_cluster_admin(\n                new_username='paul',\n                new_password='laup'\n            )\n\n            self.assertDictEqual(\n                json.loads(m.last_request.body),\n                {\n                    'name': 'paul',\n                    'password': 'laup'\n                }\n            )\n\n    def test_update_cluster_admin_password(self):\n        with requests_mock.Mocker() as m:\n            m.register_uri(\n                requests_mock.POST,\n                \"http://localhost:8086/cluster_admins/paul\"\n            )\n\n            cli = InfluxDBClient(database='db')\n            cli.update_cluster_admin_password(\n                username='paul',\n                new_password='laup'\n            )\n\n            self.assertDictEqual(\n                json.loads(m.last_request.body),\n                {'password': 'laup'}\n            )\n\n    def test_delete_cluster_admin(self):\n        with requests_mock.Mocker() as m:\n            m.register_uri(\n                requests_mock.DELETE,\n                \"http://localhost:8086/cluster_admins/paul\",\n                status_code=200,\n            )\n\n            cli = InfluxDBClient(database='db')\n            cli.delete_cluster_admin(username='paul')\n\n            self.assertIsNone(m.last_request.body)\n\n    def test_set_database_admin(self):\n        pass\n\n    def test_unset_database_admin(self):\n        pass\n\n    def test_alter_database_admin(self):\n        with requests_mock.Mocker() as m:\n            m.register_uri(\n                requests_mock.POST,\n                \"http://localhost:8086/db/db/users/paul\"\n            )\n\n            cli = InfluxDBClient(database='db')\n            cli.alter_database_admin(\n                username='paul',\n                is_admin=False\n            )\n\n            self.assertDictEqual(\n                json.loads(m.last_request.body),\n                {\n                    'admin': False\n                }\n            )\n\n    @raises(NotImplementedError)\n    def test_get_list_database_admins(self):\n        cli = InfluxDBClient('host', 8086, 'username', 'password', 'db')\n        cli.get_list_database_admins()\n\n    @raises(NotImplementedError)\n    def test_add_database_admin(self):\n        cli = InfluxDBClient('host', 8086, 'username', 'password', 'db')\n        cli.add_database_admin('admin', 'admin_secret_password')\n\n    @raises(NotImplementedError)\n    def test_update_database_admin_password(self):\n        cli = InfluxDBClient('host', 8086, 'username', 'password', 'db')\n        cli.update_database_admin_password('admin', 'admin_secret_password')\n\n    @raises(NotImplementedError)\n    def test_delete_database_admin(self):\n        cli = InfluxDBClient('host', 8086, 'username', 'password', 'db')\n        cli.delete_database_admin('admin')\n\n    def test_get_database_users(self):\n        cli = InfluxDBClient('localhost', 8086, 'username', 'password', 'db')\n\n        example_response = \\\n            '[{\"name\":\"paul\",\"isAdmin\":false,\"writeTo\":\".*\",\"readFrom\":\".*\"},'\\\n            '{\"name\":\"bobby\",\"isAdmin\":false,\"writeTo\":\".*\",\"readFrom\":\".*\"}]'\n\n        with requests_mock.Mocker() as m:\n            m.register_uri(\n                requests_mock.GET,\n                \"http://localhost:8086/db/db/users\",\n                text=example_response\n            )\n            users = cli.get_database_users()\n\n        self.assertEqual(json.loads(example_response), users)\n\n    def test_add_database_user(self):\n        with requests_mock.Mocker() as m:\n            m.register_uri(\n                requests_mock.POST,\n                \"http://localhost:8086/db/db/users\"\n            )\n            cli = InfluxDBClient(database='db')\n            cli.add_database_user(\n                new_username='paul',\n                new_password='laup',\n                permissions=('.*', '.*')\n            )\n\n            self.assertDictEqual(\n                json.loads(m.last_request.body),\n                {\n                    'writeTo': '.*',\n                    'password': 'laup',\n                    'readFrom': '.*',\n                    'name': 'paul'\n                }\n            )\n\n    def test_add_database_user_bad_permissions(self):\n        cli = InfluxDBClient()\n\n        with self.assertRaisesRegexp(\n                Exception,\n                \"'permissions' must be \\(readFrom, writeTo\\) tuple\"\n        ):\n            cli.add_database_user(\n                new_password='paul',\n                new_username='paul',\n                permissions=('hello', 'hello', 'hello')\n            )\n\n    def test_alter_database_user_password(self):\n        with requests_mock.Mocker() as m:\n            m.register_uri(\n                requests_mock.POST,\n                \"http://localhost:8086/db/db/users/paul\"\n            )\n\n            cli = InfluxDBClient(database='db')\n            cli.alter_database_user(\n                username='paul',\n                password='n3wp4ss!'\n            )\n\n            self.assertDictEqual(\n                json.loads(m.last_request.body),\n                {\n                    'password': 'n3wp4ss!'\n                }\n            )\n\n    def test_alter_database_user_permissions(self):\n        with requests_mock.Mocker() as m:\n            m.register_uri(\n                requests_mock.POST,\n                \"http://localhost:8086/db/db/users/paul\"\n            )\n\n            cli = InfluxDBClient(database='db')\n            cli.alter_database_user(\n                username='paul',\n                permissions=('^$', '.*')\n            )\n\n            self.assertDictEqual(\n                json.loads(m.last_request.body),\n                {\n                    'readFrom': '^$',\n                    'writeTo': '.*'\n                }\n            )\n\n    def test_alter_database_user_password_and_permissions(self):\n        with requests_mock.Mocker() as m:\n            m.register_uri(\n                requests_mock.POST,\n                \"http://localhost:8086/db/db/users/paul\"\n            )\n\n            cli = InfluxDBClient(database='db')\n            cli.alter_database_user(\n                username='paul',\n                password='n3wp4ss!',\n                permissions=('^$', '.*')\n            )\n\n            self.assertDictEqual(\n                json.loads(m.last_request.body),\n                {\n                    'password': 'n3wp4ss!',\n                    'readFrom': '^$',\n                    'writeTo': '.*'\n                }\n            )\n\n    def test_update_database_user_password_current_user(self):\n        cli = InfluxDBClient(\n            username='root',\n            password='hello',\n            database='database'\n        )\n        with requests_mock.Mocker() as m:\n            m.register_uri(\n                requests_mock.POST,\n                \"http://localhost:8086/db/database/users/root\"\n            )\n\n            cli.update_database_user_password(\n                username='root',\n                new_password='bye'\n            )\n\n            self.assertEqual(cli._password, 'bye')\n\n    def test_delete_database_user(self):\n        with requests_mock.Mocker() as m:\n            m.register_uri(\n                requests_mock.DELETE,\n                \"http://localhost:8086/db/db/users/paul\"\n            )\n\n            cli = InfluxDBClient(database='db')\n            cli.delete_database_user(username='paul')\n\n            self.assertIsNone(m.last_request.body)\n\n    @raises(NotImplementedError)\n    def test_update_permission(self):\n        cli = InfluxDBClient('host', 8086, 'username', 'password', 'db')\n        cli.update_permission('admin', [])\n\n    @mock.patch('requests.Session.request')\n    def test_request_retry(self, mock_request):\n        \"\"\"Tests that two connection errors will be handled\"\"\"\n\n        class CustomMock(object):\n            i = 0\n\n            def connection_error(self, *args, **kwargs):\n                self.i += 1\n\n                if self.i < 3:\n                    raise requests.exceptions.ConnectionError\n                else:\n                    r = requests.Response()\n                    r.status_code = 200\n                    return r\n\n        mock_request.side_effect = CustomMock().connection_error\n\n        cli = InfluxDBClient(database='db')\n        cli.write_points(\n            self.dummy_points\n        )\n\n    @mock.patch('requests.Session.request')\n    def test_request_retry_raises(self, mock_request):\n        \"\"\"Tests that three connection errors will not be handled\"\"\"\n\n        class CustomMock(object):\n            i = 0\n\n            def connection_error(self, *args, **kwargs):\n                self.i += 1\n\n                if self.i < 4:\n                    raise requests.exceptions.ConnectionError\n                else:\n                    r = requests.Response()\n                    r.status_code = 200\n                    return r\n\n        mock_request.side_effect = CustomMock().connection_error\n\n        cli = InfluxDBClient(database='db')\n\n        with self.assertRaises(requests.exceptions.ConnectionError):\n            cli.write_points(self.dummy_points)\n", "\n\nclass KeyTableTests(unittest.TestCase):\n    def setUp(self):\n        self.key_table_file = tempfile.mkstemp()[1]\n\n    def tearDown(self):\n        os.unlink(self.key_table_file)\n\n    def write_key_file_contents(self, keyfile):\n        with open(self.key_table_file, 'w') as f:\n            for short_name in keyfile:\n                line = '{}  {}:{}:{}\\n'.format(\n                    short_name,\n                    keyfile[short_name][KeyTable.DOMAIN],\n                    keyfile[short_name][KeyTable.SELECTOR],\n                    keyfile[short_name][KeyTable.PRIVATE_KEY])\n                f.write(line)\n\n    def test_simple_load(self):\n        key_dir = tempfile.mkdtemp()\n\n        try:\n            keyfile = {\n                'example': {\n                    KeyTable.DOMAIN: 'example.com',\n                    KeyTable.SELECTOR: '20170101',\n                    KeyTable.PRIVATE_KEY: key_dir + '/example.private'\n                },\n                'replacehandsaw': {\n                    KeyTable.DOMAIN: 'replacehandsaw.test',\n                    KeyTable.SELECTOR: '20170201',\n                    KeyTable.PRIVATE_KEY: key_dir + '/replacehandsaw.private'\n                }}\n\n            self.write_key_file_contents(keyfile)\n            keytable = KeyTable(self.key_table_file)\n\n            self.assertEqual(2, len(keytable))\n\n            for short_name, values in keytable:\n                self.assertTrue(short_name in keyfile)\n                self.assertEqual(keyfile[short_name][KeyTable.DOMAIN],\n                                 values[KeyTable.DOMAIN])\n                self.assertEqual(keyfile[short_name][KeyTable.SELECTOR],\n                                 values[KeyTable.SELECTOR])\n                self.assertEqual(keyfile[short_name][KeyTable.PRIVATE_KEY],\n                                 values[KeyTable.PRIVATE_KEY])\n        finally:\n            shutil.rmtree(key_dir)\n\n    def test_sorted_by_short_name(self):\n        key_dir = tempfile.mkdtemp()\n\n        try:\n            keyfile = {\n                'foo': {\n                    KeyTable.DOMAIN: 'foo.test',\n                    KeyTable.SELECTOR: '20170101',\n                    KeyTable.PRIVATE_KEY: key_dir + '/foo.test'\n                },\n                'bar': {\n                    KeyTable.DOMAIN: 'bar.test',\n                    KeyTable.SELECTOR: '20170201',\n                    KeyTable.PRIVATE_KEY: key_dir + '/bar.test'\n                },\n                'apple': {\n                    KeyTable.DOMAIN: 'apple.test',\n                    KeyTable.SELECTOR: '20170201',\n                    KeyTable.PRIVATE_KEY: key_dir + '/apple.test'\n                }}\n\n            self.write_key_file_contents(keyfile)\n            keytable = KeyTable(self.key_table_file)\n\n            self.assertEqual(3, len(keytable))\n\n            self.assertEqual(keyfile['apple'][KeyTable.DOMAIN],\n                             keytable[0][KeyTable.DOMAIN])\n            self.assertEqual(keyfile['bar'][KeyTable.DOMAIN],\n                             keytable[1][KeyTable.DOMAIN])\n            self.assertEqual(keyfile['foo'][KeyTable.DOMAIN],\n                             keytable[2][KeyTable.DOMAIN])\n        finally:\n            shutil.rmtree(key_dir)\n\n    def test_short_name_padding(self):\n        key_dir = tempfile.mkdtemp()\n\n        try:\n            keyfile = {\n                'unitedmonkey': {\n                    KeyTable.DOMAIN: 'foo.test',\n                    KeyTable.SELECTOR: '20170101',\n                    KeyTable.PRIVATE_KEY: key_dir + '/foo.test'\n                },\n                'bar': {\n                    KeyTable.DOMAIN: 'bar.test',\n                    KeyTable.SELECTOR: '20170201',\n                    KeyTable.PRIVATE_KEY: key_dir + '/bar.test'\n                },\n                'apple': {\n                    KeyTable.DOMAIN: 'apple.test',\n                    KeyTable.SELECTOR: '20170201',\n                    KeyTable.PRIVATE_KEY: key_dir + '/apple.test'\n                }}\n\n            self.write_key_file_contents(keyfile)\n            keytable = KeyTable(self.key_table_file)\n\n            self.assertEqual(3, len(keytable))\n\n            keytable.save_changes()\n\n            selector_format = '{:' + \\\n                              str(len('unitedmonkey') + KeyTable.SELECTOR_PADDING) + \\\n                              '}{}'\n\n            with open(self.key_table_file) as f:\n                lines = f.read().splitlines()\n\n            for selector, i in [('apple', 0), ('bar', 1), ('unitedmonkey', 2)]:\n                beginningtext = selector_format.format(selector, keyfile[selector][KeyTable.DOMAIN])\n                self.assertTrue(lines[i].startswith(beginningtext))\n        finally:\n            shutil.rmtree(key_dir)\n\n    def test_update_selector(self):\n        key_dir = tempfile.mkdtemp()\n\n        try:\n            keyfile = {\n                'orangeauto': {\n                    KeyTable.DOMAIN: 'orangeauto.test',\n                    KeyTable.SELECTOR: '20170101',\n                    KeyTable.PRIVATE_KEY: key_dir + '/orangeauto.test'\n                }}\n\n            self.write_key_file_contents(keyfile)\n            keytable = KeyTable(self.key_table_file)\n\n            self.assertEqual(1, len(keytable))\n            self.assertEqual('20170101', keytable.entries['orangeauto'][KeyTable.SELECTOR])\n\n            keytable.update_selector('orangeauto', '20170201')\n            self.assertEqual('20170201', keytable.entries['orangeauto'][KeyTable.SELECTOR])\n        finally:\n            shutil.rmtree(key_dir)\n\n", "ode_literals\n\nfrom django.db import migrations, models\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('blog', '0023_auto_20170625_2127'),\n    ]\n\n    operations = [\n        migrations.AlterField(\n            model_name='user',\n            name='avatar',\n            field=models.ImageField(default='avatar/default.png', upload_to='avatar', verbose_name='\u5934\u50cf'),\n        ),\n    ]\n", "_maze\nfrom sys import argv\n\nH_WIDTH = 80\nH_HEIGHT = 3\nV_WIDTH = 3\nV_HEIGHT = 80\nCOLOUR = [0, 50, 200]\nCELL_WIDTH = 80\n\ndef init(alex):\n    alex.pubsub.publish('wall_horizontal/wall_horizontal.json',\n                        {'width': H_WIDTH, 'height': H_HEIGHT,\n                         'colour': COLOUR})\n    alex.pubsub.publish('wall_vertical/wall_vertical.json',\n                        {'width': V_WIDTH, 'height': V_HEIGHT,\n                         'colour': COLOUR})\n\ndef draw_wall(index, wall, tick, alex):\n    position = (wall[0] * CELL_WIDTH, wall[1] * CELL_WIDTH)\n    send_movement(position, orientation(wall), index, tick, alex)\n\ndef orientation(wall):\n    if wall[1] == wall[3]:\n        return 'horizontal'\n    else:\n        return 'vertical'\n\ndef draw(walls, world, alex):\n    number_of_walls = len(walls)\n    walls_in_world = filter(lambda(e): e.startswith('wall_'),\n                            world['entities'].keys())\n    if len(walls_in_world) >= number_of_walls:\n        return\n    indexes_in_world = sorted(map(lambda(w): int(w.partition('al_')[2]),\n                                  walls_in_world))\n    index = [i for i in range(number_of_walls) if i not in indexes_in_world][0]\n    draw_wall(index, walls[index], world['tick'], alex)\n\ndef send_movement(position, orientation, index, tick, alex):\n    entity = 'wall_%s' % orientation\n    name = '%s_%d' % (entity, index,)\n    movement = {'tick': tick,\n                'entity': entity, 'index': index,\n                'from': position, 'to': position}\n    alex.pubsub.publish('movement.' + name, movement)\n\nalex = Alexandra(argv[1])\ninit(alex)\nwalls = list(make_maze(alex.config['field_width']/CELL_WIDTH,\n                       alex.config['field_height']/CELL_WIDTH))\nalex.pubsub.consume_topic('world', lambda w: draw(walls, w, alex))\n", "e = [900, 600]\n    screen = pygame.display.set_mode(size)\n    \n    title = \"PONG: First to 11 points wins!                                          PRESS ANY KEY TO START\"\n    \n    pygame.display.set_caption(title)\n    \n    white = (255,255,255)\n    grey =  (169,169,169)\n    black = (  0,  0,  0)\n\n    cycle1 = 1\n    cycle2 = 2\n    \n    clock = pygame.time.Clock()    \n    \n    while True:\n\n        for event in pygame.event.get():\n            \n            if event.type == pygame.QUIT:\n                pygame.quit()\n                sys.exit(0)\n            \n            if event.type == pygame.KEYDOWN:\n                if event.key == pygame.K_ESCAPE:\n                    pygame.quit()\n                    sys.exit(0)\n                else:\n                    way = random.randint(1,2)\n                    pygame.mixer.init()\n                    beep = pygame.mixer.Sound(\"beep.ogg\")\n                    peep = pygame.mixer.Sound(\"peep.ogg\")\n                    beep.play()\n                    time.sleep(0.4)\n                    beep.play()\n                    time.sleep(0.3)\n                    peep.play()\n                    \n                    pygame.mixer.music.load(\"Ping_Pong.ogg\")\n    \n                    pygame.mixer.music.set_volume(1.0)\n                    \n                    play(0, 0, 250, 250, way)\n            \n        lPaddle = pygame.Rect((10,250),(20,100))\n        rPaddle = pygame.Rect((870,250),(20,100))\n        ball = pygame.Rect((440,290),(20,20))\n\n        pygame.draw.rect(screen, white, lPaddle)\n        pygame.draw.rect(screen, white, rPaddle)\n        pygame.draw.rect(screen, white, ball)\n        \n        for i in range(0,900,50):\n            line = pygame.Rect((445,i+12),(10,25))\n            pygame.draw.rect(screen, grey, line)\n\n        lSlatewha = pygame.Rect((250,30),(60,100))\n        lSlatebla = pygame.Rect((260,40),(40,80))\n        lSlateonea = pygame.Rect((250,30),(50,100))\n        lSlatemida = pygame.Rect((260,75),(40,10))\n        lSlatewhb = pygame.Rect((330,30),(60,100))\n        lSlateblb = pygame.Rect((340,40),(40,80))\n        lSlateoneb = pygame.Rect((330,30),(50,100))\n        lSlatemidb = pygame.Rect((340,75),(40,10))\n        rSlatewha = pygame.Rect((510,30),(60,100))\n        rSlatebla = pygame.Rect((520,40),(40,80))\n        rSlateonea = pygame.Rect((510,30),(50,100))\n        rSlatemida = pygame.Rect((520,75),(40,10))\n        rSlatewhb = pygame.Rect((590,30),(60,100))\n        rSlateblb = pygame.Rect((600,40),(40,80))\n        rSlateoneb = pygame.Rect((590,30),(50,100))\n        rSlatemidb = pygame.Rect((600,75),(40,10))\n\n        pygame.draw.rect(screen, grey, lSlatewha)\n        pygame.draw.rect(screen, grey, lSlatewhb)\n        pygame.draw.rect(screen, grey, rSlatewha)\n        pygame.draw.rect(screen, grey, rSlatewhb)\n        pygame.draw.rect(screen, black, lSlatebla)\n        pygame.draw.rect(screen, black, lSlateblb)\n        pygame.draw.rect(screen, black, rSlatebla)\n        pygame.draw.rect(screen, black, rSlateblb)\n        pygame.draw.rect(screen, grey, lSlatemidb)\n        pygame.draw.rect(screen, grey, rSlatemidb)\n        \n        if cycle1 == 1:                       #0,1\n            pygame.draw.rect(screen, black, lSlateblb)\n            pygame.draw.rect(screen, black, rSlateoneb)\n        \n        elif cycle1 == 2:                     #8,0\n            pygame.draw.rect(screen, black, rSlateoneb)\n        \n        elif cycle1 == 3:                     #1,8\n            pygame.draw.rect(screen, black, lSlateblb)\n\n        elif cycle1 == 4:                     #0,1\n            pygame.draw.rect(screen, black, lSlateblb)\n            pygame.draw.rect(screen, black, rSlateoneb)\n            \n        elif cycle1 == 5:                     #8,1\n            pygame.draw.rect(screen, black, rSlateoneb)\n            \n        elif cycle1 == 6:                     #0,8\n            pygame.draw.rect(screen, black, lSlateblb)\n\n        if cycle2 == 1:                       #0,0\n            pass\n        \n        elif cycle2 == 2:                     #1,0\n            pygame.draw.rect(screen, black, lSlateonea)\n        \n        elif cycle2 == 3:                     #1,1\n            pygame.draw.rect(screen, black, lSlateonea)\n            pygame.draw.rect(screen, black, rSlateonea)\n            \n        elif cycle2 == 4:                     #0,1\n            pygame.draw.rect(screen, black, rSlateonea)\n        \n        if cycle1 < 6:\n            cycle1 += 1\n        else:\n            cycle1 = 1\n\n        if cycle2 < 4:\n            cycle2 += 1\n        else:\n            cycle2 = 1\n            \n        pygame.display.flip()\n        clock.tick(30)\n        \ndef play(p1, p2, y2, y3, way):\n\n    size = [900, 600]\n    screen = pygame.display.set_mode(size)\n    \n    title = \"PONG: First to 11 points wins!                                            PRESS 'R' TO RESTART\"\n    title_pause = \"                                                                                     Game paused\"\n    \n    pygame.display.set_caption(title)\n\n    pygame.mixer.music.play(-1, 11.0)\n    \n    white = (255,255,255)\n    grey =  (169,169,169)\n    black = (  0,  0,  0)\n    \n    x1 = 440 # Coordinates of Ball\n    y1 = 290\n    \n    x2 = 10  # x coordinate of Left Paddle\n    \n    x3 = 870 # x coordinate of Right Paddle\n    \n    d2 = ''\n    d3 = ''\n    \n    if way == 1:\n        way = 'left'\n    elif way == 2:\n        way = 'right'\n    \n    increment = 5\n        \n    height = ''\n    \n    clock = pygame.time.Clock()    \n    \n    while True:\n\n        for event in pygame.event.get():\n            \n            if event.type == pygame.QUIT:\n                pygame.quit()\n                sys.exit(0)\n            \n            if event.type == pygame.KEYDOWN:\n                if event.key == pygame.K_UP:\n                    pygame.mixer.music.set_volume(pygame.mixer.music.get_volume() - 0.05)\n                if event.key == pygame.K_DOWN:\n                    pygame.mixer.music.set_volume(pygame.mixer.music.get_volume() - 0.05)\n                if event.key == pygame.K_r:\n                    start()\n                if event.key == pygame.K_ESCAPE:\n                    pygame.quit()\n                    sys.exit(0)\n                if event.key == pygame.K_SPACE:\n                    \n                    loop = 0\n                    \n                    size = [900, 600]\n                    screen = pygame.display.set_mode(size)\n                    \n                    pygame.display.set_caption(title_pause)\n\n                    pygame.mixer.music.pause()\n                    \n                    while loop == 0:\n                        for event in pygame.event.get():\n                            if event.type == pygame.QUIT:\n                                pygame.quit()\n                                sys.exit()\n                            if event.type == pygame.KEYDOWN:\n                                if event.key == pygame.K_r:\n                                    start()\n                                if event.key == pygame.K_ESCAPE:\n                                    pygame.quit()\n                                    sys.exit(0)\n                                if event.key == pygame.K_SPACE:\n                                    pygame.display.set_caption(title)\n                                    pygame.mixer.music.unpause()\n                                    loop = 1\n\n            if event.type == pygame.KEYUP:\n                if event.key == pygame.K_w or event.key == pygame.K_s:\n                    d2 = ''\n                if event.key == pygame.K_i or event.key == pygame.K_k:\n                    d3 = ''\n\n        key = pygame.key.get_pressed()\n        \n        if key[pygame.K_w]:\n            if y2 == 0:\n                d2 = ''\n            else:\n                d2 = 'up'\n        elif key[pygame.K_s]:\n            if y2 == 500:\n                d2 = ''\n            else:\n                d2 = 'down'\n        \n        if key[pygame.K_i]:\n            if y3 == 0:\n                d3 = ''\n            else:\n                d3 = 'up'\n        elif key[pygame.K_k]:\n            if y3 == 500:\n                d3 = ''\n            else:\n                d3 = 'down'\n\n        screen.fill((0,0,0))\n\n        # Gameplay Stuffs\n        \n        lPaddle = pygame.Rect((x2,y2),(20,100))\n        lHitbox = pygame.Rect((x2+20,y2-9),(20,138))\n        rPaddle = pygame.Rect((x3,y3),(20,100))\n        rHitbox = pygame.Rect((x3-20,y3-9),(20,138))\n        ball = pygame.Rect((x1,y1),(20,20))\n        \n        pygame.draw.rect(screen, white, lPaddle)\n        pygame.draw.rect(screen, white, rPaddle)\n        \n        for i in range(0,900,50):\n            line = pygame.Rect((445,i+12),(10,25))\n            pygame.draw.rect(screen, grey, line)\n        \n        # Blank Scoreboard\n        \n        lSlatewha = pygame.Rect((250,30),(60,100))\n        lSlatebla = pygame.Rect((260,40),(40,80))\n        lSlateonea = pygame.Rect((250,30),(50,100))\n        lSlatemida = pygame.Rect((260,75),(40,10))\n        \n        lSlatewhb = pygame.Rect((330,30),(60,100))\n        lSlateblb = pygame.Rect((340,40),(40,80))\n        lSlatetopb = pygame.Rect((340,30),(40,10))\n        lSlateoneb = pygame.Rect((330,30),(50,100))\n        lSlatemidb = pygame.Rect((340,75),(40,10))\n        lSlatebotb = pygame.Rect((340,120),(40,10))\n        lSlatele1b = pygame.Rect((330,40),(10,35))\n        lSlatele2b = pygame.Rect((330,85),(10,35))\n        lSlateri1b = pygame.Rect((380,40),(10,35))\n        lSlateri2b = pygame.Rect((380,85),(10,35))\n        lSlatecmb = pygame.Rect((330,75),(10,10))\n        lSlatecbb = pygame.Rect((330,120),(10,10))\n        \n        rSlatewha = pygame.Rect((510,30),(60,100))\n        rSlatebla = pygame.Rect((520,40),(40,80))\n        rSlateonea = pygame.Rect((510,30),(50,100))\n        rSlatemida = pygame.Rect((520,75),(40,10))\n        \n        rSlatewhb = pygame.Rect((590,30),(60,100))\n        rSlateblb = pygame.Rect((600,40),(40,80))\n        rSlatetopb = pygame.Rect((600,30),(40,10))\n        rSlateoneb = pygame.Rect((590,30),(50,100))\n        rSlatemidb = pygame.Rect((600,75),(40,10))\n        rSlatebotb = pygame.Rect((600,120),(40,10))\n        rSlatele1b = pygame.Rect((590,40),(10,35))\n        rSlatele2b = pygame.Rect((590,85),(10,35))\n        rSlateri1b = pygame.Rect((640,40),(10,35))\n        rSlateri2b = pygame.Rect((640,85),(10,35))\n        rSlatecmb = pygame.Rect((590,75),(10,10))\n        rSlatecbb = pygame.Rect((590,120),(10,10))\n        \n        pygame.draw.rect(screen, grey, lSlatewha)\n        pygame.draw.rect(screen, black, lSlatebla)\n        pygame.draw.rect(screen, grey, lSlatemida)\n        \n        pygame.draw.rect(screen, grey, lSlatewhb)\n        pygame.draw.rect(screen, black, lSlateblb)\n        pygame.draw.rect(screen, grey, lSlatemidb)\n        \n        pygame.draw.rect(screen, grey, rSlatewha)\n        pygame.draw.rect(screen, black, rSlatebla)\n        pygame.draw.rect(screen, grey, rSlatemida)\n        \n        pygame.draw.rect(screen, grey, rSlatewhb)\n        pygame.draw.rect(screen, black, rSlateblb)\n        pygame.draw.rect(screen, grey, rSlatemidb)\n        \n        # Display Score\n        \n        if p1 < 10:\n            pygame.draw.rect(screen, black, lSlatebla)\n            if p1 == 0:\n                pygame.draw.rect(screen, black, lSlateblb)\n            elif p1 == 1:\n                pygame.draw.rect(screen, black, lSlateoneb)\n            elif p1 == 2:\n                pygame.draw.rect(screen, black, lSlatele1b)\n                pygame.draw.rect(screen, black, lSlateri2b)\n            elif p1 == 3:\n                pygame.draw.rect(screen, black, lSlatele1b)\n                pygame.draw.rect(screen, black, lSlatele2b)\n            elif p1 == 4:\n                pygame.draw.rect(screen, black, lSlatetopb)\n                pygame.draw.rect(screen, black, lSlatele2b)\n                pygame.draw.rect(screen, black, lSlatebotb)\n                pygame.draw.rect(screen, black, lSlatecbb)\n            elif p1 == 5:\n                pygame.draw.rect(screen, black, lSlateri1b)\n                pygame.draw.rect(screen, black, lSlatele2b)\n            elif p1 == 6:\n                pygame.draw.rect(screen, black, lSlateri1b)\n            elif p1 == 7:\n                pygame.draw.rect(screen, black, lSlateblb)\n                pygame.draw.rect(screen, black, lSlatele1b)\n                pygame.draw.rect(screen, black, lSlatele2b)\n                pygame.draw.rect(screen, black, lSlatebotb)\n                pygame.draw.rect(screen, black, lSlatecmb)\n                pygame.draw.rect(screen, black, lSlatecbb)\n            elif p1 == 8:\n                pass\n            elif p1 == 9:\n                pygame.draw.rect(screen, black, lSlatele2b)\n                pygame.draw.rect(screen, black, lSlatebotb)\n                pygame.draw.rect(screen, black, lSlatecbb)\n        elif p1 == 10:\n            pygame.draw.rect(screen, black, lSlateonea)\n            pygame.draw.rect(screen, black, lSlateblb)\n        elif p1 == 11:\n            pygame.draw.rect(screen, black, lSlateonea)\n            pygame.draw.rect(screen, black, lSlateoneb)\n                \n        if p2 < 10:\n            pygame.draw.rect(screen, black, rSlatebla)\n            if p2 == 0:\n                pygame.draw.rect(screen, black, rSlateblb)\n            elif p2 == 1:\n                pygame.draw.rect(screen, black, rSlateoneb)\n            elif p2 == 2:\n                pygame.draw.rect(screen, black, rSlatele1b)\n                pygame.draw.rect(screen, black, rSlateri2b)\n            elif p2 == 3:\n                pygame.draw.rect(screen, black, rSlatele1b)\n                pygame.draw.rect(screen, black, rSlatele2b)\n            elif p2 == 4:\n                pygame.draw.rect(screen, black, rSlatetopb)\n                pygame.draw.rect(screen, black, rSlatele2b)\n                pygame.draw.rect(screen, black, rSlatebotb)\n                pygame.draw.rect(screen, black, rSlatecbb)\n            elif p2 == 5:\n                pygame.draw.rect(screen, black, rSlateri1b)\n                pygame.draw.rect(screen, black, rSlatele2b)\n            elif p2 == 6:\n                pygame.draw.rect(screen, black, rSlateri1b)\n            elif p2 == 7:\n                pygame.draw.rect(screen, black, rSlateblb)\n                pygame.draw.rect(screen, black, rSlatele1b)\n                pygame.draw.rect(screen, black, rSlatele2b)\n                pygame.draw.rect(screen, black, rSlatebotb)\n                pygame.draw.rect(screen, black, rSlatecmb)\n                pygame.draw.rect(screen, black, rSlatecbb)\n            elif p2 == 8:\n                pass\n            elif p2 == 9:\n                pygame.draw.rect(screen, black, rSlatele2b)\n                pygame.draw.rect(screen, black, rSlatebotb)\n                pygame.draw.rect(screen, black, rSlatecbb)\n        elif p2 == 10:\n            pygame.draw.rect(screen, black, rSlateonea)\n            pygame.draw.rect(screen, black, rSlateblb)\n        elif p2 == 11:\n            pygame.draw.rect(screen, black, rSlateonea)\n            pygame.draw.rect(screen, black, rSlateoneb)\n\n        pygame.draw.rect(screen, white, ball)\n\n        if p1 == 11 or p2 == 11:\n            finish(p1,p2,y2,y3)\n        \n        if d2 == '':\n            pass\n        elif d2 == 'up':\n            if y2 > 1:\n                y2 -= 7\n            else:\n                d2 = ''\n        elif d2 == 'down':\n            if y2 < 500:\n                y2 += 7\n            else:\n                d2 = ''\n\n        if d3 == '':\n            pass\n        elif d3 == 'up':\n            if y3 > 1:\n                y3 -= 7\n            else:\n                d3 = ''\n        elif d3 == 'down':\n            if y3 < 500:\n                y3 += 7\n        \n        if way == 'right':\n            if x1 + 20 != x3:\n                x1 += 5\n            elif not(ball.colliderect(rHitbox)):\n                x1 += 5\n            else:\n                beep = pygame.mixer.Sound(\"beep.ogg\")\n                beep.play()\n                x1 -= 5\n                if increment > 12:\n                    increment -= random.randint(1,2)\n                elif increment > 9:\n                    increment -= random.randint(-1,2)\n                elif increment > 4:\n                    increment += random.randint(-1,2)\n                else:\n                    increment += random.randint(1,2)\n                way = 'left'\n                        \n        elif way == 'left':\n            if x1 - 20 != x2:\n                x1 -= 5\n            elif not(ball.colliderect(lHitbox)):\n                x1 -= 5\n            else:\n                beep = pygame.mixer.Sound(\"beep.ogg\")\n                beep.play()\n                x1 += 5\n                if increment > 12:\n                    increment -= random.randint(1,2)\n                elif increment > 9:\n                    increment -= random.randint(-1,2)\n                elif increment > 4:\n                    increment += random.randint(-1,2)\n                else:\n                    increment += random.randint(1,2)\n                way = 'right'\n\n        if height == '':\n            if not(ball.colliderect(lHitbox) or ball.colliderect(rHitbox)):\n                pass\n            else:\n                i = random.randint(0,1)\n                if i == 0:\n                    height = 'up'\n                elif i == 1:\n                    height = 'down'\n        \n        elif height == 'down':\n            if y1 < 580:\n                y1 += increment\n            else:\n                plop = pygame.mixer.Sound(\"plop.ogg\")\n                plop.play()\n                y1 -= increment\n                height = 'up'\n                                \n        elif height == 'up':\n            if y1 > 0:\n                y1 -= increment\n            else:\n                plop = pygame.mixer.Sound(\"plop.ogg\")\n                plop.play()\n                y1 += increment\n                height = 'down'\n        \n        if x1 == -20 or x1 == 900:\n            \n            if x1 < 0:\n                p2 += 1\n                time.sleep(0.5)\n                play(p1,p2,y2,250,2)\n                \n            else:\n                p1 += 1\n                time.sleep(0.5)\n                play(p1,p2,250,y3,1)\n                \n        pygame.display.flip()\n        clock.tick(120)\n\ndef finish(p1,p2,y2,y3):\n    \n    size = [900, 600]\n    screen = pygame.display.set_mode(size)\n    \n    if p1 > p2:\n        win = 1\n    else:\n        win = 2\n    \n    title = \"PONG: First to 11 points wins!                                        PRESS 'R' FOR A REMATCH!\"\n    \n    pygame.display.set_caption(title)\n    \n    white = (255,255,255)\n    grey = (169,169,169)\n    black = (  0,  0,  0)\n\n    d2 = ''\n    d3 = ''\n    \n    done = False\n    clock = pygame.time.Clock()\n    \n    while True:\n\n        for event in pygame.event.get():\n            \n            if event.type == pygame.QUIT:\n                pygame.quit()\n                sys.exit(0)\n\n            if event.type == pygame.KEYDOWN:\n                if event.key == pygame.K_r:\n                    if win == 1:\n                        play(0, 0, 250, y3, 2)\n                    else:\n                        play(0, 0, y2, 250, 1)\n                if event.key == pygame.K_ESCAPE:\n                    pygame.quit()\n                    sys.exit(0)\n            \n            if event.type == pygame.KEYUP:\n                if event.key == pygame.K_w or event.key == pygame.K_s:\n                    d2 = ''\n                if event.key == pygame.K_i or event.key == pygame.K_k:\n                    d3 = ''\n\n        key = pygame.key.get_pressed()\n        \n        if key[pygame.K_w]:\n            if y2 == 0:\n                d2 = ''\n            else:\n                d2 = 'up'\n        elif key[pygame.K_s]:\n            if y2 == 500:\n                d2 = ''\n            else:\n                d2 = 'down'\n        \n        if key[pygame.K_i]:\n            if y3 == 0:\n                d3 = ''\n            else:\n                d3 = 'up'\n        elif key[pygame.K_k]:\n            if y3 == 500:\n                d3 = ''\n            else:\n                d3 = 'down'\n\n        screen.fill((0,0,0))\n        \n        lPaddle = pygame.Rect((10,y2),(20,100))\n        rPaddle = pygame.Rect((870,y3),(20,100))\n        \n        pygame.draw.rect(screen, white, lPaddle)\n        pygame.draw.rect(screen, white, rPaddle)\n\n        for i in range(0,900,50):\n            line = pygame.Rect((445,i+12),(10,25))\n            pygame.draw.rect(screen, grey, line)        \n\n        if p1 > p2:\n            x = 0\n        else:\n            x = 425\n            \n        pi = math.pi\n        \n        pygame.draw.line(screen, grey, [175+x,250], [150+x,220], 5)              # Y\n        pygame.draw.line(screen, grey, [175+x,250], [200+x,220], 5)\n        pygame.draw.line(screen, grey, [175+x,250], [175+x,290], 5)\n        \n        pygame.draw.circle(screen, grey, [235+x, 255], 36, 5)                    # O\n        \n        pygame.draw.arc(screen, grey, [275+x, 155, 60, 135],  pi, 3 * pi / 2, 5) # U\n        pygame.draw.arc(screen, grey, [275+x, 155, 60, 135], 3 * pi / 2, 2 * pi, 5)\n\n        pygame.draw.line(screen, grey, [150+x,320], [165+x,390], 5)              # W\n        pygame.draw.line(screen, grey, [165+x,390], [175+x,320], 5)\n        pygame.draw.line(screen, grey, [175+x,320], [185+x,390], 5)\n        pygame.draw.line(screen, grey, [185+x,390], [200+x,320], 5)\n\n        pygame.draw.line(screen, grey, [230+x,320], [230+x,390], 5)              # I\n        \n        pygame.draw.line(screen, grey, [265+x,320], [265+x,390], 5)              # N\n        pygame.draw.line(screen, grey, [265+x,320], [300+x,390], 5)\n        pygame.draw.line(screen, grey, [300+x,320], [300+x,390], 5)\n        \n        pygame.draw.line(screen, grey, [325+x,320], [325+x,370], 5)              # !\n        pygame.draw.circle(screen, grey, [327+x,385], 5, 5)\n            \n        # Blank Scoreboard\n        \n        lSlatewha = pygame.Rect((250,30),(60,100))\n        lSlatebla = pygame.Rect((260,40),(40,80))\n        lSlateonea = pygame.Rect((250,30),(50,100))\n        lSlatemida = pygame.Rect((260,75),(40,10))\n        \n        lSlatewhb = pygame.Rect((330,30),(60,100))\n        lSlateblb = pygame.Rect((340,40),(40,80))\n        lSlatetopb = pygame.Rect((340,30),(40,10))\n        lSlateoneb = pygame.Rect((330,30),(50,100))\n        lSlatemidb = pygame.Rect((340,75),(40,10))\n        lSlatebotb = pygame.Rect((340,120),(40,10))\n        lSlatele1b = pygame.Rect((330,40),(10,35))\n        lSlatele2b = pygame.Rect((330,85),(10,35))\n        lSlateri1b = pygame.Rect((380,40),(10,35))\n        lSlateri2b = pygame.Rect((380,85),(10,35))\n        lSlatecmb = pygame.Rect((330,75),(10,10))\n        lSlatecbb = pygame.Rect((330,120),(10,10))\n        \n        rSlatewha = pygame.Rect((510,30),(60,100))\n        rSlatebla = pygame.Rect((520,40),(40,80))\n        rSlateonea = pygame.Rect((510,30),(50,100))\n        rSlatemida = pygame.Rect((520,75),(40,10))\n        \n        rSlatewhb = pygame.Rect((590,30),(60,100))\n        rSlateblb = pygame.Rect((600,40),(40,80))\n        rSlatetopb = pygame.Rect((600,30),(40,10))\n        rSlateoneb = pygame.Rect((590,30),(50,100))\n        rSlatemidb = pygame.Rect((600,75),(40,10))\n        rSlatebotb = pygame.Rect((600,120),(40,10))\n        rSlatele1b = pygame.Rect((590,40),(10,35))\n        rSlatele2b = pygame.Rect((590,85),(10,35))\n        rSlateri1b = pygame.Rect((640,40),(10,35))\n        rSlateri2b = pygame.Rect((640,85),(10,35))\n        rSlatecmb = pygame.Rect((590,75),(10,10))\n        rSlatecbb = pygame.Rect((590,120),(10,10))\n        \n        pygame.draw.rect(screen, grey, lSlatewha)\n        pygame.draw.rect(screen, black, lSlatebla)\n        pygame.draw.rect(screen, grey, lSlatemida)\n        \n        pygame.draw.rect(screen, grey, lSlatewhb)\n        pygame.draw.rect(screen, black, lSlateblb)\n        pygame.draw.rect(screen, grey, lSlatemidb)\n        \n        pygame.draw.rect(screen, grey, rSlatewha)\n        pygame.draw.rect(screen, black, rSlatebla)\n        pygame.draw.rect(screen, grey, rSlatemida)\n        \n        pygame.draw.rect(screen, grey, rSlatewhb)\n        pygame.draw.rect(screen, black, rSlateblb)\n        pygame.draw.rect(screen, grey, rSlatemidb)\n        \n        # Display Score\n        \n        if p1 < 10:\n            pygame.draw.rect(screen, black, lSlatebla)\n            if p1 == 0:\n                pygame.draw.rect(screen, black, lSlateblb)\n            elif p1 == 1:\n                pygame.draw.rect(screen, black, lSlateoneb)\n            elif p1 == 2:\n                pygame.draw.rect(screen, black, lSlatele1b)\n                pygame.draw.rect(screen, black, lSlateri2b)\n            elif p1 == 3:\n                pygame.draw.rect(screen, black, lSlatele1b)\n                pygame.draw.rect(screen, black, lSlatele2b)\n            elif p1 == 4:\n                pygame.draw.rect(screen, black, lSlatetopb)\n                pygame.draw.rect(screen, black, lSlatele2b)\n                pygame.draw.rect(screen, black, lSlatebotb)\n                pygame.draw.rect(screen, black, lSlatecbb)\n            elif p1 == 5:\n                pygame.draw.rect(screen, black, lSlateri1b)\n                pygame.draw.rect(screen, black, lSlatele2b)\n            elif p1 == 6:\n                pygame.draw.rect(screen, black, lSlateri1b)\n            elif p1 == 7:\n                pygame.draw.rect(screen, black, lSlateblb)\n                pygame.draw.rect(screen, black, lSlatele1b)\n                pygame.draw.rect(screen, black, lSlatele2b)\n                pygame.draw.rect(screen, black, lSlatebotb)\n                pygame.draw.rect(screen, black, lSlatecmb)\n                pygame.draw.rect(screen, black, lSlatecbb)\n            elif p1 == 8:\n                pass\n            elif p1 == 9:\n                pygame.draw.rect(screen, black, lSlatele2b)\n                pygame.draw.rect(screen, black, lSlatebotb)\n                pygame.draw.rect(screen, black, lSlatecbb)\n        elif p1 == 10:\n            pygame.draw.rect(screen, black, lSlateonea)\n            pygame.draw.rect(screen, black, lSlateblb)\n        elif p1 == 11:\n            pygame.draw.rect(screen, black, lSlateonea)\n            pygame.draw.rect(screen, black, lSlateoneb)\n                \n        if p2 < 10:\n            pygame.draw.rect(screen, black, rSlatebla)\n            if p2 == 0:\n                pygame.draw.rect(screen, black, rSlateblb)\n            elif p2 == 1:\n                pygame.draw.rect(screen, black, rSlateoneb)\n            elif p2 == 2:\n                pygame.draw.rect(screen, black, rSlatele1b)\n                pygame.draw.rect(screen, black, rSlateri2b)\n            elif p2 == 3:\n                pygame.draw.rect(screen, black, rSlatele1b)\n                pygame.draw.rect(screen, black, rSlatele2b)\n            elif p2 == 4:\n                pygame.draw.rect(screen, black, rSlatetopb)\n                pygame.draw.rect(screen, black, rSlatele2b)\n                pygame.draw.rect(screen, black, rSlatebotb)\n                pygame.draw.rect(screen, black, rSlatecbb)\n            elif p2 == 5:\n                pygame.draw.rect(screen, black, rSlateri1b)\n                pygame.draw.rect(screen, black, rSlatele2b)\n            elif p2 == 6:\n                pygame.draw.rect(screen, black, rSlateri1b)\n            elif p2 == 7:\n                pygame.draw.rect(screen, black, rSlateblb)\n                pygame.draw.rect(screen, black, rSlatele1b)\n                pygame.draw.rect(screen, black, rSlatele2b)\n                pygame.draw.rect(screen, black, rSlatebotb)\n                pygame.draw.rect(screen, black, rSlatecmb)\n                pygame.draw.rect(screen, black, rSlatecbb)\n            elif p2 == 8:\n                pass\n            elif p2 == 9:\n                pygame.draw.rect(screen, black, rSlatele2b)\n                pygame.draw.rect(screen, black, rSlatebotb)\n                pygame.draw.rect(screen, black, rSlatecbb)\n        elif p2 == 10:\n            pygame.draw.rect(screen, black, rSlateonea)\n            pygame.draw.rect(screen, black, rSlateblb)\n        elif p2 == 11:\n            pygame.draw.rect(screen, black, rSlateonea)\n            pygame.draw.rect(screen, black, rSlateoneb)\n            \n        if d2 == '':\n            pass\n        elif d2 == 'up':\n            if y2 > 1:\n                y2 -= 10\n            else:\n                d2 = ''\n        elif d2 == 'down':\n            if y2 < 500:\n                y2 += 10\n            else:\n                d2 = ''\n\n        if d3 == '':\n            pass\n        elif d3 == 'up':\n            if y3 > 1:\n                y3 -= 10\n            else:\n                d3 = ''\n        elif d3 == 'down':\n            if y3 < 500:\n                y3 += 10\n        \n        pygame.display.flip()\n        clock.tick(60)\n\nstart()\n", "mport desc\nfrom itsdangerous import TimedJSONWebSignatureSerializer as Serializer\nfrom itsdangerous import SignatureExpired, BadSignature\nfrom werkzeug.security import generate_password_hash, check_password_hash\n\nfrom app.models import Post\nfrom app.utils import markdown_renderer\nfrom app import app, db, login_manager\n\nSYNTAX_HIGHLIGHTER_CHOICES = [\n    'autumn.css', 'borland.css', 'bw.css', 'colorful.css', 'default.css', 'emacs.css', 'friendly.css', 'fruity.css',\n    'github.css', 'manni.css', 'monokai.css', 'murphy.css', 'native.css', 'pastie.css', 'perldoc.css', 'tango.css',\n    'trac.css', 'vim.css', 'vs.css', 'zenburn.css'\n]\nSYNTAX_HIGHLIGHTER_TUPLE = [(raw, raw.capitalize()[:-4]) for raw in SYNTAX_HIGHLIGHTER_CHOICES]\n\n\nclass User(db.Model):\n    \"\"\"\n    User with blog model. Integrates with Flask-Login.\n\n    id: The ID of the user.\n    username: The username of the user (can contain any characters)\n    password: Encrypted password\n    superuser: Boolean to tell if the user is a superuser\n    active: Boolean to tell if the user is active (ability to login and operate on the app)\n    register_date: The date the user registered\n    last_login: The date the user last logged in the app\n\n    blog_slug: The slugified username. Used as subdomain\n    blog_title: The title of the blog that can be set in the user's settings\n    blog_description: The description of the blog that can be set in the user's settings\n    blog_image: The image (avatar) of the blog that can be set in the user's settings using an URL\n    blog_image_rounded: Tells whether the image of the blog should be rounded or not (user setting)\n\n    blog_bg: The URL of the background of the user\n    blog_bg_public: Tells if the background should be displayed to everyone or only the user\n    blog_bg_repeat: Should the background be repeated ? Useful for small images\n    blog_bg_everywhere: Use the background image everywhere on markdownblog instead of the default one\n    blog_bg_override: Use the background everywhere even on other blogs\n\n    posts: The posts associated to this blog/user\n    \"\"\"\n    id = db.Column(db.Integer, primary_key=True)\n    username = db.Column(db.String(50), unique=True)\n    password = db.Column(db.String(54))\n    superuser = db.Column(db.Boolean())\n    active = db.Column(db.Boolean())\n    register_date = db.Column(db.DateTime())\n    last_login = db.Column(db.DateTime())\n    linkedin_url = db.Column(db.String(200), default=\"\")\n    gplus_url = db.Column(db.String(200), default=\"\")\n    github_url = db.Column(db.String(200), default=\"\")\n    twitter_url = db.Column(db.String(200), default=\"\")\n\n    # Blog Related Informations\n    blog_slug = db.Column(db.String(50), unique=True)\n    blog_title = db.Column(db.String(50), default=\"Untitled Blog\")\n    blog_description = db.Column(db.String(200), default=\"No Description\")\n    blog_public = db.Column(db.Boolean(), default=True)\n\n    # Blog Images Related\n    blog_image = db.Column(db.String(200))\n    blog_image_rounded = db.Column(db.Boolean(), default=False)\n    blog_bg = db.Column(db.String(200))\n    blog_bg_public = db.Column(db.Boolean(), default=False)\n    blog_bg_repeat = db.Column(db.Boolean(), default=False)\n    blog_bg_everywhere = db.Column(db.Boolean(), default=False)\n    blog_bg_override = db.Column(db.Boolean(), default=False)\n\n    # Blog Pagination\n    blog_paginate = db.Column(db.Boolean(), default=False)\n    blog_paginate_by = db.Column(db.Integer(), default=10)\n\n    # Blog Design\n    blog_truncate_posts = db.Column(db.Boolean(), default=False)\n    blog_syntax_highlighter_css = db.Column(db.Enum(*SYNTAX_HIGHLIGHTER_CHOICES), default='monokai.css')\n\n    posts = db.relationship('Post', backref='user', lazy='dynamic')\n\n    def __init__(self, active=True, superuser=False, api_purpose=False, **kwargs):\n        \"\"\"\n        :param username: The username of the user, will become the blog subdomain once slugified.\n        :param password: The raw password to be encrypted and stored.\n        :param active: To change once postfix is setup and app can send mails.\n        :param superuser: Set if the user is a superuser (currently no use for that)\n        :param api_purpose: If using this with Marshmallow, do not bother to generate a password.\n        \"\"\"\n        super(User, self).__init__(active=active, superuser=superuser, **kwargs)\n        now = datetime.utcnow()\n        if not api_purpose:\n            self.set_password(self.password)\n        self.register_date = now\n        self.last_login = now\n        self.blog_slug = slugify(self.username)\n\n    @property\n    def total_pages(self):\n        \"\"\"\n        Property that returns the minimum number of pages to display all the articles (posts)\n        :return: int\n        \"\"\"\n        count = self.posts.count()\n        if count % self.blog_paginate_by == 0:\n            return int(count / self.blog_paginate_by)\n        else:\n            return int(count / self.blog_paginate_by) + 1\n\n    @property\n    def pages_as_list(self):\n        \"\"\"\n        Returns a list of integers representing the available pages\n        \"\"\"\n        return list(range(1, self.total_pages + 1))\n\n    def get_page(self, page):\n        if not page < self.total_pages:\n            return None\n        else:\n            return list(self.posts.order_by(desc(Post.pub_date)).all())[self.blog_paginate_by * page: self.blog_paginate_by * (page+1)]\n\n    def get_all_posts(self):\n        return list(self.posts.order_by(desc(Post.pub_date)).all())\n\n    def save(self):\n        db.session.add(self)\n        try:\n            db.session.commit()\n        except Exception as e:\n            app.logger.exception(\"Something went wrong while saving a user\")\n            db.session.rollback()\n            return False\n        return True\n\n    def delete(self):\n        db.session.delete(self)\n        try:\n            db.session.commit()\n        except Exception as e:\n            app.logger.exception(\"Something went wrong while deleting a user\")\n            db.session.rollback()\n            return False\n        return True\n\n    def is_superuser(self):\n        return self.superuser\n\n    def set_password(self, password):\n        self.password = generate_password_hash(password)\n\n    def check_password(self, password):\n        return check_password_hash(self.password, password)\n\n    @property\n    def is_authenticated(self):\n        return True\n\n    @property\n    def is_active(self):\n        return self.active\n\n    @property\n    def is_anonymous(self):\n        return False\n\n    def get_id(self):\n        return self.id\n\n    def generate_auth_token(self, expiration=6000):\n        s = Serializer(app.config['SECRET_KEY'], expires_in=expiration)\n        return s.dumps({'id': self.id})\n\n    @staticmethod\n    def verify_auth_token(token):\n        s = Serializer(app.config['SECRET_KEY'])\n        try:\n            data = s.loads(token)\n        except SignatureExpired:\n            return None\n        except BadSignature:\n            return None\n        user = User.query.get(data['id'])\n        return user\n\n    def description_as_html(self):\n        return markdown_renderer.render(self.blog_description)\n\n    def __repr__(self):\n        return self.username\n\n    def __str__(self):\n        return self.username\n\n\n@login_manager.user_loader\ndef load_user(user_id):\n    return db.session.query(User).get(user_id)\n", "rt numpy as np\r\n\r\n#from sklearn.preprocessing import Imputer\r\nfrom sklearn.ensemble import ExtraTreesRegressor, ExtraTreesClassifier\r\nfrom numpy import sqrt\r\n\r\n#*************************** STATICS ************************************\r\n#input files\r\norig_trainData = \"D:/DataScience/train_values.csv\"\r\norig_trainLabels = \"D:/DataScience/train_labels.csv\"\r\norig_testData = \"D:/DataScience/test_values.csv\"\r\n\r\n#output files\r\npostProcess_trainData = \"D:/DataScience/pp_train_values.csv\"\r\npostProcess_trainLabels = \"D:/DataScience/pp_train_labels.csv\"\r\npostProcess_testData = \"D:/DataScience/pp_test_values.csv\"\r\n\r\n#estimator hyperparameters\r\netrClassifyParams = {'n_estimators':100,\r\n             'max_features':100,\r\n             'max_depth':25,\r\n             'min_samples_split':3,\r\n             'random_state':736283,\r\n             'n_jobs':-1}\r\n\r\n#interpolated columns\r\nimpute_cols = [['school__faculty_salary',{'n_estimators':600, 'random_state':736283, 'max_depth':None, 'max_features':None, 'min_samples_split':2, 'n_jobs':-1}], \r\n    ['student__demographics_age_entry',{'n_estimators':600, 'random_state':736283, 'max_depth':None, 'max_features':175, 'min_samples_split':2, 'n_jobs':-1}]]\r\n\r\nIDcolumnName = 'row_id'\r\n\r\n#*********************** HELPER FUNCTIONS ********************************\r\ndef descretizeByValue(X, y, descretizeByCol, newColName,bins,binValues):\r\n    combined = pd.concat([X,y[descretizeByCol]], axis=1,join='inner')\r\n    combined[newColName] = pd.qcut(combined[descretizeByCol],bins,binValues)\r\n    combined = combined.drop(labels=[descretizeByCol],axis=1)\r\n    \r\n    return combined\r\n\r\ndef synthesize(X, colA, colB, colC, newColName, category):\r\n    \r\n    X[newColName] = abs(\r\n            ((-X[colB] + sqrt(abs(X[colB]**2 - (4 * X[colA] * X[colC])))) / (2 * X[colA])) - \r\n            ((-X[colB] - sqrt(abs(X[colB]**2 + (4 * X[colA] * X[colC])))) / (2 * X[colA]))\r\n        )\r\n    \r\n    print(\"column [%s] created for %s.\" % (newColName, category))\r\n    return X\r\n\r\ndef createIncomeBins(train, test, binColName):\r\n    \r\n    predictors = [x for x in train.columns if x not in [IDcolumnName,binColName]]\r\n    \r\n    model = ExtraTreesClassifier(**etrClassifyParams)\r\n    model.fit(train[predictors], train[binColName])\r\n    \r\n    te_predict = model.predict(test[predictors])\r\n    test[binColName] = te_predict\r\n    \r\n    return test\r\n\r\ndef interpolateFeatureValues(train, test, imputedColumn, etrParameters):\r\n    \r\n    #define the training predictors\r\n    predictors = [x for x in train.columns if x not in [IDcolumnName, imputedColumn]]\r\n    \r\n    #partition data by training and prediction sets\r\n    X_tr = train[(train[imputedColumn] > 0)].copy()    #populated 'imputed label' data from training file\r\n    X_te = test[(test[imputedColumn] > 0)].copy()      #populated 'imputed lable' data from test file\r\n    X_fit = pd.concat([X_tr, X_te], axis=0)            #combine training and test values\r\n    y_fit = X_fit[imputedColumn]                       #create 1d array of predictors for fit\r\n    \r\n    X_trp = train[(train[imputedColumn] == 0)].copy()    #data with missing 'imputed label' information from training file\r\n    X_tep = test[(test[imputedColumn] == 0)].copy()      #data with missing 'imputed label' information from test file\r\n    \r\n    #define/train the model\r\n    model = ExtraTreesRegressor(**etrParameters)\r\n    model.fit(X_fit[predictors], y_fit)\r\n    \r\n    #tr_predict = np.exp(model.predict(X_trp[predictors]))\r\n    tr_predict = model.predict(X_trp[predictors])\r\n    X_trp[imputedColumn] = tr_predict\r\n    \r\n    #te_predict = np.exp(model.predict(X_tep[predictors]))\r\n    te_predict = model.predict(X_tep[predictors])\r\n    X_tep[imputedColumn] = te_predict\r\n    \r\n    #update the training and testing base data with the predicted values\r\n    train.update(X_trp[[imputedColumn]], join='left')\r\n    test.update(X_tep[[imputedColumn]], join='left')\r\n    \r\n    return train, test\r\n\r\n\r\ndef cleanValues(X):\r\n    # This function will clean the inbound college / graduate income prediction data.\r\n    #meanImputer = Imputer(missing_values=0, strategy='mean',axis=0)\r\n    #meanNaNImputer = Imputer(missing_values=np.NaN, strategy='mean',axis=0)\r\n    \r\n    #Map code values and re-type character data\r\n    X.report_year = X.report_year.map({'year_a':1,'year_f':2, 'year_w':3, 'year_z':4})\r\n    X.report_year = pd.to_numeric(X.report_year, errors='coerce')\r\n    \r\n    X.school__degrees_awarded_highest = X.school__degrees_awarded_highest.map({'Non-degree-granting':0,'Certificate degree':1, 'Associate degree':2, \"Bachelor's degree\":3, 'Graduate degree':4})\r\n    X.school__degrees_awarded_highest = pd.to_numeric(X.school__degrees_awarded_highest, errors='coerce')\r\n    \r\n    X.school__degrees_awarded_predominant = X.school__degrees_awarded_predominant.map({'Not classified':0,'Predominantly certificate-degree granting':1,\"Predominantly associate's-degree granting\":2,\"Predominantly bachelor's-degree granting\":3,'Entirely graduate-degree granting':4})\r\n    X.school__degrees_awarded_predominant = pd.to_numeric(X.school__degrees_awarded_predominant, errors='coerce')\r\n    \r\n    X.school__institutional_characteristics_level = X.school__institutional_characteristics_level.map({'2-year':2,'4-year':4,'Less-than-2-year':0.5})\r\n    X.school__institutional_characteristics_level = pd.to_numeric(X.school__institutional_characteristics_level, errors='coerce')\r\n    \r\n    X.school__main_campus = X.school__main_campus.map({'Main campus':1,'Not main campus':0})\r\n    X.school__main_campus = pd.to_numeric(X.school__main_campus, errors='coerce')\r\n    \r\n    X.school__online_only = X.school__online_only.map({'Not distance-education only':2, 'Distance-education only':1, np.NaN:0})\r\n    X.school__online_only = pd.to_numeric(X.school__online_only, errors='coerce')\r\n\r\n    #ordered state mapping from highest income to lowest income (nhl estimated based on region id)\r\n    \r\n    stateMap = {'twr':58,'iqy':57,'xws':56,'jgn':55,'kus':54,'rxy':53,'shi':52,'cmz':51,'oub':50,\r\n        'oli':49,'wjh':48,'rya':47,'axc':46,'qim':45,'lff':44,'dfy':43,'dkf':42,'bkc':41,\r\n        'wto':40,'znt':39,'zdl':38,'vvi':37,'luw':36,'cfi':35,'ccg':34,'oon':33,'oly':32,\r\n        'tbs':31,'ezv':30,'dmg':29,'xve':28,'jfm':27,'eyi':26,'kdg':25,'hbt':24,'bww':23,\r\n        'hww':22,'fxt':21,'idw':20,'kll':19,'ipu':18,'por':17,'uah':16,'slp':15,'rse':14,\r\n        'rgs':13,'hqy':12,'hks':11,'jsu':10,'gkt':9,'pxv':8,'uuo':7,'nhl':6,'hmr':5,\r\n        'slo':4,'ahh':3,'pdh':2,'fga':1}\r\n    \r\n    X.school__state = X.school__state.map(stateMap)\r\n    X.school__state = pd.to_numeric(X.school__state, errors='coerce')\r\n    \r\n    X = pd.get_dummies(X,columns=['school__ownership','school__region_id'])\r\n    \r\n    #drop noisy features\r\n    X = X.drop(labels=['admissions__act_scores_75th_percentile_cumulative',\r\n        'admissions__act_scores_midpoint_cumulative',\r\n        'admissions__act_scores_25th_percentile_english',\r\n        'admissions__act_scores_25th_percentile_math',\r\n        'admissions__act_scores_25th_percentile_writing',\r\n        'admissions__act_scores_75th_percentile_english',\r\n        'admissions__act_scores_75th_percentile_math',\r\n        'admissions__act_scores_75th_percentile_writing',\r\n        'admissions__sat_scores_25th_percentile_math',\r\n        'admissions__sat_scores_25th_percentile_critical_reading',\r\n        'admissions__sat_scores_75th_percentile_math',\r\n        'admissions__sat_scores_75th_percentile_critical_reading',\r\n        'student__demographics_first_generation'], axis=1)\r\n    \r\n    #derive male share demographics after mean averaging the few missing female demographics\r\n    X.student__demographics_female_share = X.student__demographics_female_share.fillna(0.5)\r\n    X['student__demographics_male_share'] = (1.0 - X.student__demographics_female_share)\r\n\r\n    #fill all null's with 0 now\r\n    X = X.fillna(0)\r\n    \r\n    return X\r\n\r\ndef removeDuplicates(X,y, labelColumn, features):\r\n    #This function will find duplicate features, average the label values into a single row (training only), then delete all duplicates\r\n    #second half of function removes outliers based on analysis of faculty salary and income\r\n    \r\n    data = X[X.duplicated(keep=False)]\r\n    data = pd.concat([data, y[labelColumn]], axis=1, join='inner')\r\n    \r\n    avg = pd.DataFrame(data.groupby(features, as_index=False).mean())\r\n    avg = data.merge(avg, on=features, how='left')\r\n    avg.index = data.index\r\n\r\n    data.income = avg.income_y\r\n    combined = pd.concat([X, y[labelColumn]], axis=1, join='inner')\r\n    combined.update(data, join='inner')    \r\n    combined = combined.drop_duplicates(keep=False)\r\n    combined.income = round(combined.income,5)\r\n    combined.drop_duplicates(subset=features, keep=False, inplace=True)\r\n    \r\n    #outliers are identified and deleted here from the training set\r\n    outliers = []\r\n    outliers.extend(combined[(combined.school__faculty_salary > 15000) & (combined.income < 30)].index)\r\n    outliers.extend(combined[(combined.school__faculty_salary > 20000)].index)\r\n    outliers.extend(combined[(combined.income > 120)].index)\r\n    combined.drop(outliers,inplace=True)\r\n    \r\n    #correct skew / kurtosis of label and important correlates by converting to normal distributions\r\n    #combined[labelColumn] = np.log(combined[labelColumn])\r\n    \r\n    #product output\r\n    labels = pd.DataFrame(combined[labelColumn])\r\n    data = combined.drop(labels=[labelColumn], axis=1)\r\n    \r\n    return data, labels\r\n\r\ndef smoothLabel(y,labelColumn):\r\n    #this function is used to apply natural logarithm for reduction in skew of dependent variable\r\n\r\n    cols = y.columns\r\n    labels = pd.DataFrame(np.log(y[labelColumn].copy()), columns=cols)\r\n\r\n    return labels\r\n\r\n\r\n#*************************** MAIN ROUTINE ************************************\r\ndef preprocess_data():\r\n    #open all files\r\n    X = pd.read_csv(orig_trainData, index_col=0)\r\n    y = pd.read_csv(orig_trainLabels, index_col=0)\r\n    T = pd.read_csv(orig_testData, index_col=0)\r\n    \r\n    #initial encoding, pruning, and NaN filling for train (X) and test (T) values\r\n    print(\"\\nCleaning / encoding raw data...\")\r\n    X = cleanValues(X)\r\n    T = cleanValues(T)\r\n    \r\n    #impute important missing values\r\n    for col, params in impute_cols:\r\n        print(\"Predicting new [%s] values...\" % col)\r\n        X, T = interpolateFeatureValues(X, T, col, params)\r\n    \r\n    \r\n    #NOTE: commands removed below resulted in nominal improvement in private score\r\n    \r\n    #synthesize distance of quadratic roots for 3 top features\r\n    print(\"Synthesizing new columns.\")\r\n    X = synthesize(X, 'school__degrees_awarded_predominant_recoded', 'school__institutional_characteristics_level', 'school__faculty_salary', 'synthesized', 'training data')\r\n    T = synthesize(T, 'school__degrees_awarded_predominant_recoded', 'school__institutional_characteristics_level', 'school__faculty_salary', 'synthesized', 'testing data')\r\n    #X = synthesize(X, 'school__degrees_awarded_highest', 'school__degrees_awarded_predominant', 'school__institutional_characteristics_level', 'synthesized', 'training data')\r\n    #T = synthesize(T, 'school__degrees_awarded_highest', 'school__degrees_awarded_predominant', 'school__institutional_characteristics_level', 'synthesized', 'testing data')\r\n    \r\n    \r\n    print(\"Creating three income bins for classification on training data...\")\r\n    X = descretizeByValue(X, y, 'income', 'income_bin', 3, [1,2,3])\r\n    \r\n    print(\"Predicting income bins via classification on test data...\")\r\n    T = createIncomeBins(X, T, 'income_bin')\r\n    \r\n    #print(\"log smoothing dependent variable.\")\r\n    #y = smoothLabel(y, 'income')\r\n    \r\n    #print(\"removing duplicates and outliers.\")\r\n    #X, y = removeDuplicates(X, y, 'income', [x for x in X.columns if x not in [IDcolumnName]])\r\n    \r\n    #write the post-processed files out\r\n    X.to_csv(postProcess_trainData)\r\n    y.to_csv(postProcess_trainLabels)\r\n    T.to_csv(postProcess_testData)\r\n    \r\n    print(\"post-processing file creation complete.\")\r\n", "tion = readme.read()\n\nwith open('LICENSE.txt') as license_file:\n    license = license_file.read()\n\nsetup(name='join',\n      packages=['join'],\n      version='0.1.1',\n      description='SQL-style joins for iterables.',\n      long_description=long_description,\n      license=license,\n      author='Stuart Axelbrooke',\n      author_email='stuart@axelbrooke.com',\n      url='https://github.com/StuartAxelOwen/join',\n      download_url='https://github.com/StuartAxelOwen/join/archive/0.1.zip',\n      keywords=['join', 'joins', 'merge', 'merges', 'list join', 'iterable join'],\n      )\n", " Python installation\n\"\"\"\n\n# If you change the version here, change it in setup.py\n# and docs/conf.py as well.\n__version__ = \"1.9.1\"  # following best practices\nvirtualenv_version = __version__  # legacy, again\n\nimport base64\nimport sys\nimport os\nimport codecs\nimport optparse\nimport re\nimport shutil\nimport logging\nimport tempfile\nimport zlib\nimport errno\nimport glob\nimport distutils.sysconfig\nfrom distutils.util import strtobool\nimport struct\nimport subprocess\n\nif sys.version_info < (2, 6):\n    print('ERROR: %s' % sys.exc_info()[1])\n    print('ERROR: this script requires Python 2.6 or greater.')\n    sys.exit(101)\n\ntry:\n    set\nexcept NameError:\n    from sets import Set as set\ntry:\n    basestring\nexcept NameError:\n    basestring = str\n\ntry:\n    import ConfigParser\nexcept ImportError:\n    import configparser as ConfigParser\n\njoin = os.path.join\npy_version = 'python%s.%s' % (sys.version_info[0], sys.version_info[1])\n\nis_jython = sys.platform.startswith('java')\nis_pypy = hasattr(sys, 'pypy_version_info')\nis_win = (sys.platform == 'win32')\nis_cygwin = (sys.platform == 'cygwin')\nis_darwin = (sys.platform == 'darwin')\nabiflags = getattr(sys, 'abiflags', '')\n\nuser_dir = os.path.expanduser('~')\nif is_win:\n    default_storage_dir = os.path.join(user_dir, 'virtualenv')\nelse:\n    default_storage_dir = os.path.join(user_dir, '.virtualenv')\ndefault_config_file = os.path.join(default_storage_dir, 'virtualenv.ini')\n\nif is_pypy:\n    expected_exe = 'pypy'\nelif is_jython:\n    expected_exe = 'jython'\nelse:\n    expected_exe = 'python'\n\n\nREQUIRED_MODULES = ['os', 'posix', 'posixpath', 'nt', 'ntpath', 'genericpath',\n                    'fnmatch', 'locale', 'encodings', 'codecs',\n                    'stat', 'UserDict', 'readline', 'copy_reg', 'types',\n                    're', 'sre', 'sre_parse', 'sre_constants', 'sre_compile',\n                    'zlib']\n\nREQUIRED_FILES = ['lib-dynload', 'config']\n\nmajver, minver = sys.version_info[:2]\nif majver == 2:\n    if minver >= 6:\n        REQUIRED_MODULES.extend(['warnings', 'linecache', '_abcoll', 'abc'])\n    if minver >= 7:\n        REQUIRED_MODULES.extend(['_weakrefset'])\n    if minver <= 3:\n        REQUIRED_MODULES.extend(['sets', '__future__'])\nelif majver == 3:\n    # Some extra modules are needed for Python 3, but different ones\n    # for different versions.\n    REQUIRED_MODULES.extend(['_abcoll', 'warnings', 'linecache', 'abc', 'io',\n                             '_weakrefset', 'copyreg', 'tempfile', 'random',\n                             '__future__', 'collections', 'keyword', 'tarfile',\n                             'shutil', 'struct', 'copy', 'tokenize', 'token',\n                             'functools', 'heapq', 'bisect', 'weakref',\n                             'reprlib'])\n    if minver >= 2:\n        REQUIRED_FILES[-1] = 'config-%s' % majver\n    if minver == 3:\n        import sysconfig\n        platdir = sysconfig.get_config_var('PLATDIR')\n        REQUIRED_FILES.append(platdir)\n        # The whole list of 3.3 modules is reproduced below - the current\n        # uncommented ones are required for 3.3 as of now, but more may be\n        # added as 3.3 development continues.\n        REQUIRED_MODULES.extend([\n            #\"aifc\",\n            #\"antigravity\",\n            #\"argparse\",\n            #\"ast\",\n            #\"asynchat\",\n            #\"asyncore\",\n            \"base64\",\n            #\"bdb\",\n            #\"binhex\",\n            #\"bisect\",\n            #\"calendar\",\n            #\"cgi\",\n            #\"cgitb\",\n            #\"chunk\",\n            #\"cmd\",\n            #\"codeop\",\n            #\"code\",\n            #\"colorsys\",\n            #\"_compat_pickle\",\n            #\"compileall\",\n            #\"concurrent\",\n            #\"configparser\",\n            #\"contextlib\",\n            #\"cProfile\",\n            #\"crypt\",\n            #\"csv\",\n            #\"ctypes\",\n            #\"curses\",\n            #\"datetime\",\n            #\"dbm\",\n            #\"decimal\",\n            #\"difflib\",\n            #\"dis\",\n            #\"doctest\",\n            #\"dummy_threading\",\n            \"_dummy_thread\",\n            #\"email\",\n            #\"filecmp\",\n            #\"fileinput\",\n            #\"formatter\",\n            #\"fractions\",\n            #\"ftplib\",\n            #\"functools\",\n            #\"getopt\",\n            #\"getpass\",\n            #\"gettext\",\n            #\"glob\",\n            #\"gzip\",\n            \"hashlib\",\n            #\"heapq\",\n            \"hmac\",\n            #\"html\",\n            #\"http\",\n            #\"idlelib\",\n            #\"imaplib\",\n            #\"imghdr\",\n            \"imp\",\n            \"importlib\",\n            #\"inspect\",\n            #\"json\",\n            #\"lib2to3\",\n            #\"logging\",\n            #\"macpath\",\n            #\"macurl2path\",\n            #\"mailbox\",\n            #\"mailcap\",\n            #\"_markupbase\",\n            #\"mimetypes\",\n            #\"modulefinder\",\n            #\"multiprocessing\",\n            #\"netrc\",\n            #\"nntplib\",\n            #\"nturl2path\",\n            #\"numbers\",\n            #\"opcode\",\n            #\"optparse\",\n            #\"os2emxpath\",\n            #\"pdb\",\n            #\"pickle\",\n            #\"pickletools\",\n            #\"pipes\",\n            #\"pkgutil\",\n            #\"platform\",\n            #\"plat-linux2\",\n            #\"plistlib\",\n            #\"poplib\",\n            #\"pprint\",\n            #\"profile\",\n            #\"pstats\",\n            #\"pty\",\n            #\"pyclbr\",\n            #\"py_compile\",\n            #\"pydoc_data\",\n            #\"pydoc\",\n            #\"_pyio\",\n            #\"queue\",\n            #\"quopri\",\n            #\"reprlib\",\n            \"rlcompleter\",\n            #\"runpy\",\n            #\"sched\",\n            #\"shelve\",\n            #\"shlex\",\n            #\"smtpd\",\n            #\"smtplib\",\n            #\"sndhdr\",\n            #\"socket\",\n            #\"socketserver\",\n            #\"sqlite3\",\n            #\"ssl\",\n            #\"stringprep\",\n            #\"string\",\n            #\"_strptime\",\n            #\"subprocess\",\n            #\"sunau\",\n            #\"symbol\",\n            #\"symtable\",\n            #\"sysconfig\",\n            #\"tabnanny\",\n            #\"telnetlib\",\n            #\"test\",\n            #\"textwrap\",\n            #\"this\",\n            #\"_threading_local\",\n            #\"threading\",\n            #\"timeit\",\n            #\"tkinter\",\n            #\"tokenize\",\n            #\"token\",\n            #\"traceback\",\n            #\"trace\",\n            #\"tty\",\n            #\"turtledemo\",\n            #\"turtle\",\n            #\"unittest\",\n            #\"urllib\",\n            #\"uuid\",\n            #\"uu\",\n            #\"wave\",\n            #\"weakref\",\n            #\"webbrowser\",\n            #\"wsgiref\",\n            #\"xdrlib\",\n            #\"xml\",\n            #\"xmlrpc\",\n            #\"zipfile\",\n        ])\n\nif is_pypy:\n    # these are needed to correctly display the exceptions that may happen\n    # during the bootstrap\n    REQUIRED_MODULES.extend(['traceback', 'linecache'])\n\nclass Logger(object):\n\n    \"\"\"\n    Logging object for use in command-line script.  Allows ranges of\n    levels, to avoid some redundancy of displayed information.\n    \"\"\"\n\n    DEBUG = logging.DEBUG\n    INFO = logging.INFO\n    NOTIFY = (logging.INFO+logging.WARN)/2\n    WARN = WARNING = logging.WARN\n    ERROR = logging.ERROR\n    FATAL = logging.FATAL\n\n    LEVELS = [DEBUG, INFO, NOTIFY, WARN, ERROR, FATAL]\n\n    def __init__(self, consumers):\n        self.consumers = consumers\n        self.indent = 0\n        self.in_progress = None\n        self.in_progress_hanging = False\n\n    def debug(self, msg, *args, **kw):\n        self.log(self.DEBUG, msg, *args, **kw)\n    def info(self, msg, *args, **kw):\n        self.log(self.INFO, msg, *args, **kw)\n    def notify(self, msg, *args, **kw):\n        self.log(self.NOTIFY, msg, *args, **kw)\n    def warn(self, msg, *args, **kw):\n        self.log(self.WARN, msg, *args, **kw)\n    def error(self, msg, *args, **kw):\n        self.log(self.ERROR, msg, *args, **kw)\n    def fatal(self, msg, *args, **kw):\n        self.log(self.FATAL, msg, *args, **kw)\n    def log(self, level, msg, *args, **kw):\n        if args:\n            if kw:\n                raise TypeError(\n                    \"You may give positional or keyword arguments, not both\")\n        args = args or kw\n        rendered = None\n        for consumer_level, consumer in self.consumers:\n            if self.level_matches(level, consumer_level):\n                if (self.in_progress_hanging\n                    and consumer in (sys.stdout, sys.stderr)):\n                    self.in_progress_hanging = False\n                    sys.stdout.write('\\n')\n                    sys.stdout.flush()\n                if rendered is None:\n                    if args:\n                        rendered = msg % args\n                    else:\n                        rendered = msg\n                    rendered = ' '*self.indent + rendered\n                if hasattr(consumer, 'write'):\n                    consumer.write(rendered+'\\n')\n                else:\n                    consumer(rendered)\n\n    def start_progress(self, msg):\n        assert not self.in_progress, (\n            \"Tried to start_progress(%r) while in_progress %r\"\n            % (msg, self.in_progress))\n        if self.level_matches(self.NOTIFY, self._stdout_level()):\n            sys.stdout.write(msg)\n            sys.stdout.flush()\n            self.in_progress_hanging = True\n        else:\n            self.in_progress_hanging = False\n        self.in_progress = msg\n\n    def end_progress(self, msg='done.'):\n        assert self.in_progress, (\n            \"Tried to end_progress without start_progress\")\n        if self.stdout_level_matches(self.NOTIFY):\n            if not self.in_progress_hanging:\n                # Some message has been printed out since start_progress\n                sys.stdout.write('...' + self.in_progress + msg + '\\n')\n                sys.stdout.flush()\n            else:\n                sys.stdout.write(msg + '\\n')\n                sys.stdout.flush()\n        self.in_progress = None\n        self.in_progress_hanging = False\n\n    def show_progress(self):\n        \"\"\"If we are in a progress scope, and no log messages have been\n        shown, write out another '.'\"\"\"\n        if self.in_progress_hanging:\n            sys.stdout.write('.')\n            sys.stdout.flush()\n\n    def stdout_level_matches(self, level):\n        \"\"\"Returns true if a message at this level will go to stdout\"\"\"\n        return self.level_matches(level, self._stdout_level())\n\n    def _stdout_level(self):\n        \"\"\"Returns the level that stdout runs at\"\"\"\n        for level, consumer in self.consumers:\n            if consumer is sys.stdout:\n                return level\n        return self.FATAL\n\n    def level_matches(self, level, consumer_level):\n        \"\"\"\n        >>> l = Logger([])\n        >>> l.level_matches(3, 4)\n        False\n        >>> l.level_matches(3, 2)\n        True\n        >>> l.level_matches(slice(None, 3), 3)\n        False\n        >>> l.level_matches(slice(None, 3), 2)\n        True\n        >>> l.level_matches(slice(1, 3), 1)\n        True\n        >>> l.level_matches(slice(2, 3), 1)\n        False\n        \"\"\"\n        if isinstance(level, slice):\n            start, stop = level.start, level.stop\n            if start is not None and start > consumer_level:\n                return False\n            if stop is not None and stop <= consumer_level:\n                return False\n            return True\n        else:\n            return level >= consumer_level\n\n    #@classmethod\n    def level_for_integer(cls, level):\n        levels = cls.LEVELS\n        if level < 0:\n            return levels[0]\n        if level >= len(levels):\n            return levels[-1]\n        return levels[level]\n\n    level_for_integer = classmethod(level_for_integer)\n\n# create a silent logger just to prevent this from being undefined\n# will be overridden with requested verbosity main() is called.\nlogger = Logger([(Logger.LEVELS[-1], sys.stdout)])\n\ndef mkdir(path):\n    if not os.path.exists(path):\n        logger.info('Creating %s', path)\n        os.makedirs(path)\n    else:\n        logger.info('Directory %s already exists', path)\n\ndef copyfileordir(src, dest):\n    if os.path.isdir(src):\n        shutil.copytree(src, dest, True)\n    else:\n        shutil.copy2(src, dest)\n\ndef copyfile(src, dest, symlink=True):\n    if not os.path.exists(src):\n        # Some bad symlink in the src\n        logger.warn('Cannot find file %s (bad symlink)', src)\n        return\n    if os.path.exists(dest):\n        logger.debug('File %s already exists', dest)\n        return\n    if not os.path.exists(os.path.dirname(dest)):\n        logger.info('Creating parent directories for %s' % os.path.dirname(dest))\n        os.makedirs(os.path.dirname(dest))\n    if not os.path.islink(src):\n        srcpath = os.path.abspath(src)\n    else:\n        srcpath = os.readlink(src)\n    if symlink and hasattr(os, 'symlink') and not is_win:\n        logger.info('Symlinking %s', dest)\n        try:\n            os.symlink(srcpath, dest)\n        except (OSError, NotImplementedError):\n            logger.info('Symlinking failed, copying to %s', dest)\n            copyfileordir(src, dest)\n    else:\n        logger.info('Copying to %s', dest)\n        copyfileordir(src, dest)\n\ndef writefile(dest, content, overwrite=True):\n    if not os.path.exists(dest):\n        logger.info('Writing %s', dest)\n        f = open(dest, 'wb')\n        f.write(content.encode('utf-8'))\n        f.close()\n        return\n    else:\n        f = open(dest, 'rb')\n        c = f.read()\n        f.close()\n        if c != content.encode(\"utf-8\"):\n            if not overwrite:\n                logger.notify('File %s exists with different content; not overwriting', dest)\n                return\n            logger.notify('Overwriting %s with new content', dest)\n            f = open(dest, 'wb')\n            f.write(content.encode('utf-8'))\n            f.close()\n        else:\n            logger.info('Content %s already in place', dest)\n\ndef rmtree(dir):\n    if os.path.exists(dir):\n        logger.notify('Deleting tree %s', dir)\n        shutil.rmtree(dir)\n    else:\n        logger.info('Do not need to delete %s; already gone', dir)\n\ndef make_exe(fn):\n    if hasattr(os, 'chmod'):\n        oldmode = os.stat(fn).st_mode & 0xFFF # 0o7777\n        newmode = (oldmode | 0x16D) & 0xFFF # 0o555, 0o7777\n        os.chmod(fn, newmode)\n        logger.info('Changed mode of %s to %s', fn, oct(newmode))\n\ndef _find_file(filename, dirs):\n    for dir in reversed(dirs):\n        files = glob.glob(os.path.join(dir, filename))\n        if files and os.path.isfile(files[0]):\n            return True, files[0]\n    return False, filename\n\ndef _install_req(py_executable, unzip=False, distribute=False,\n                 search_dirs=None, never_download=False):\n\n    if search_dirs is None:\n        search_dirs = file_search_dirs()\n\n    if not distribute:\n        egg_path = 'setuptools-*-py%s.egg' % sys.version[:3]\n        found, egg_path = _find_file(egg_path, search_dirs)\n        project_name = 'setuptools'\n        bootstrap_script = EZ_SETUP_PY\n        tgz_path = None\n    else:\n        # Look for a distribute egg (these are not distributed by default,\n        # but can be made available by the user)\n        egg_path = 'distribute-*-py%s.egg' % sys.version[:3]\n        found, egg_path = _find_file(egg_path, search_dirs)\n        project_name = 'distribute'\n        if found:\n            tgz_path = None\n            bootstrap_script = DISTRIBUTE_FROM_EGG_PY\n        else:\n            # Fall back to sdist\n            # NB: egg_path is not None iff tgz_path is None\n            # iff bootstrap_script is a generic setup script accepting\n            # the standard arguments.\n            egg_path = None\n            tgz_path = 'distribute-*.tar.gz'\n            found, tgz_path = _find_file(tgz_path, search_dirs)\n            bootstrap_script = DISTRIBUTE_SETUP_PY\n\n    if is_jython and os._name == 'nt':\n        # Jython's .bat sys.executable can't handle a command line\n        # argument with newlines\n        fd, ez_setup = tempfile.mkstemp('.py')\n        os.write(fd, bootstrap_script)\n        os.close(fd)\n        cmd = [py_executable, ez_setup]\n    else:\n        cmd = [py_executable, '-c', bootstrap_script]\n    if unzip and egg_path:\n        cmd.append('--always-unzip')\n    env = {}\n    remove_from_env = ['__PYVENV_LAUNCHER__']\n    if logger.stdout_level_matches(logger.DEBUG) and egg_path:\n        cmd.append('-v')\n\n    old_chdir = os.getcwd()\n    if egg_path is not None and os.path.exists(egg_path):\n        logger.info('Using existing %s egg: %s' % (project_name, egg_path))\n        cmd.append(egg_path)\n        if os.environ.get('PYTHONPATH'):\n            env['PYTHONPATH'] = egg_path + os.path.pathsep + os.environ['PYTHONPATH']\n        else:\n            env['PYTHONPATH'] = egg_path\n    elif tgz_path is not None and os.path.exists(tgz_path):\n        # Found a tgz source dist, let's chdir\n        logger.info('Using existing %s egg: %s' % (project_name, tgz_path))\n        os.chdir(os.path.dirname(tgz_path))\n        # in this case, we want to be sure that PYTHONPATH is unset (not\n        # just empty, really unset), else CPython tries to import the\n        # site.py that it's in virtualenv_support\n        remove_from_env.append('PYTHONPATH')\n    elif never_download:\n        logger.fatal(\"Can't find any local distributions of %s to install \"\n                     \"and --never-download is set.  Either re-run virtualenv \"\n                     \"without the --never-download option, or place a %s \"\n                     \"distribution (%s) in one of these \"\n                     \"locations: %r\" % (project_name, project_name,\n                                        egg_path or tgz_path,\n                                        search_dirs))\n        sys.exit(1)\n    elif egg_path:\n        logger.info('No %s egg found; downloading' % project_name)\n        cmd.extend(['--always-copy', '-U', project_name])\n    else:\n        logger.info('No %s tgz found; downloading' % project_name)\n    logger.start_progress('Installing %s...' % project_name)\n    logger.indent += 2\n    cwd = None\n    if project_name == 'distribute':\n        env['DONT_PATCH_SETUPTOOLS'] = 'true'\n\n    def _filter_ez_setup(line):\n        return filter_ez_setup(line, project_name)\n\n    if not os.access(os.getcwd(), os.W_OK):\n        cwd = tempfile.mkdtemp()\n        if tgz_path is not None and os.path.exists(tgz_path):\n            # the current working dir is hostile, let's copy the\n            # tarball to a temp dir\n            target = os.path.join(cwd, os.path.split(tgz_path)[-1])\n            shutil.copy(tgz_path, target)\n    try:\n        call_subprocess(cmd, show_stdout=False,\n                        filter_stdout=_filter_ez_setup,\n                        extra_env=env,\n                        remove_from_env=remove_from_env,\n                        cwd=cwd)\n    finally:\n        logger.indent -= 2\n        logger.end_progress()\n        if cwd is not None:\n            shutil.rmtree(cwd)\n        if os.getcwd() != old_chdir:\n            os.chdir(old_chdir)\n        if is_jython and os._name == 'nt':\n            os.remove(ez_setup)\n\ndef file_search_dirs():\n    here = os.path.dirname(os.path.abspath(__file__))\n    dirs = ['.', here,\n            join(here, 'virtualenv_support')]\n    if os.path.splitext(os.path.dirname(__file__))[0] != 'virtualenv':\n        # Probably some boot script; just in case virtualenv is installed...\n        try:\n            import virtualenv\n        except ImportError:\n            pass\n        else:\n            dirs.append(os.path.join(os.path.dirname(virtualenv.__file__), 'virtualenv_support'))\n    return [d for d in dirs if os.path.isdir(d)]\n\ndef install_setuptools(py_executable, unzip=False,\n                       search_dirs=None, never_download=False):\n    _install_req(py_executable, unzip,\n                 search_dirs=search_dirs, never_download=never_download)\n\ndef install_distribute(py_executable, unzip=False,\n                       search_dirs=None, never_download=False):\n    _install_req(py_executable, unzip, distribute=True,\n                 search_dirs=search_dirs, never_download=never_download)\n\n_pip_re = re.compile(r'^pip-.*(zip|tar.gz|tar.bz2|tgz|tbz)$', re.I)\ndef install_pip(py_executable, search_dirs=None, never_download=False):\n    if search_dirs is None:\n        search_dirs = file_search_dirs()\n\n    filenames = []\n    for dir in search_dirs:\n        filenames.extend([join(dir, fn) for fn in os.listdir(dir)\n                          if _pip_re.search(fn)])\n    filenames = [(os.path.basename(filename).lower(), i, filename) for i, filename in enumerate(filenames)]\n    filenames.sort()\n    filenames = [filename for basename, i, filename in filenames]\n    if not filenames:\n        filename = 'pip'\n    else:\n        filename = filenames[-1]\n    easy_install_script = 'easy_install'\n    if is_win:\n        easy_install_script = 'easy_install-script.py'\n    # There's two subtle issues here when invoking easy_install.\n    # 1. On unix-like systems the easy_install script can *only* be executed\n    #    directly if its full filesystem path is no longer than 78 characters.\n    # 2. A work around to [1] is to use the `python path/to/easy_install foo`\n    #    pattern, but that breaks if the path contains non-ASCII characters, as\n    #    you can't put the file encoding declaration before the shebang line.\n    # The solution is to use Python's -x flag to skip the first line of the\n    # script (and any ASCII decoding errors that may have occurred in that line)\n    cmd = [py_executable, '-x', join(os.path.dirname(py_executable), easy_install_script), filename]\n    # jython and pypy don't yet support -x\n    if is_jython or is_pypy:\n        cmd.remove('-x')\n    if filename == 'pip':\n        if never_download:\n            logger.fatal(\"Can't find any local distributions of pip to install \"\n                         \"and --never-download is set.  Either re-run virtualenv \"\n                         \"without the --never-download option, or place a pip \"\n                         \"source distribution (zip/tar.gz/tar.bz2) in one of these \"\n                         \"locations: %r\" % search_dirs)\n            sys.exit(1)\n        logger.info('Installing pip from network...')\n    else:\n        logger.info('Installing existing %s distribution: %s' % (\n                os.path.basename(filename), filename))\n    logger.start_progress('Installing pip...')\n    logger.indent += 2\n    def _filter_setup(line):\n        return filter_ez_setup(line, 'pip')\n    try:\n        call_subprocess(cmd, show_stdout=False,\n                        filter_stdout=_filter_setup)\n    finally:\n        logger.indent -= 2\n        logger.end_progress()\n\ndef filter_ez_setup(line, project_name='setuptools'):\n    if not line.strip():\n        return Logger.DEBUG\n    if project_name == 'distribute':\n        for prefix in ('Extracting', 'Now working', 'Installing', 'Before',\n                       'Scanning', 'Setuptools', 'Egg', 'Already',\n                       'running', 'writing', 'reading', 'installing',\n                       'creating', 'copying', 'byte-compiling', 'removing',\n                       'Processing'):\n            if line.startswith(prefix):\n                return Logger.DEBUG\n        return Logger.DEBUG\n    for prefix in ['Reading ', 'Best match', 'Processing setuptools',\n                   'Copying setuptools', 'Adding setuptools',\n                   'Installing ', 'Installed ']:\n        if line.startswith(prefix):\n            return Logger.DEBUG\n    return Logger.INFO\n\n\nclass UpdatingDefaultsHelpFormatter(optparse.IndentedHelpFormatter):\n    \"\"\"\n    Custom help formatter for use in ConfigOptionParser that updates\n    the defaults before expanding them, allowing them to show up correctly\n    in the help listing\n    \"\"\"\n    def expand_default(self, option):\n        if self.parser is not None:\n            self.parser.update_defaults(self.parser.defaults)\n        return optparse.IndentedHelpFormatter.expand_default(self, option)\n\n\nclass ConfigOptionParser(optparse.OptionParser):\n    \"\"\"\n    Custom option parser which updates its defaults by by checking the\n    configuration files and environmental variables\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        self.config = ConfigParser.RawConfigParser()\n        self.files = self.get_config_files()\n        self.config.read(self.files)\n        optparse.OptionParser.__init__(self, *args, **kwargs)\n\n    def get_config_files(self):\n        config_file = os.environ.get('VIRTUALENV_CONFIG_FILE', False)\n        if config_file and os.path.exists(config_file):\n            return [config_file]\n        return [default_config_file]\n\n    def update_defaults(self, defaults):\n        \"\"\"\n        Updates the given defaults with values from the config files and\n        the environ. Does a little special handling for certain types of\n        options (lists).\n        \"\"\"\n        # Then go and look for the other sources of configuration:\n        config = {}\n        # 1. config files\n        config.update(dict(self.get_config_section('virtualenv')))\n        # 2. environmental variables\n        config.update(dict(self.get_environ_vars()))\n        # Then set the options with those values\n        for key, val in config.items():\n            key = key.replace('_', '-')\n            if not key.startswith('--'):\n                key = '--%s' % key  # only prefer long opts\n            option = self.get_option(key)\n            if option is not None:\n                # ignore empty values\n                if not val:\n                    continue\n                # handle multiline configs\n                if option.action == 'append':\n                    val = val.split()\n                else:\n                    option.nargs = 1\n                if option.action == 'store_false':\n                    val = not strtobool(val)\n                elif option.action in ('store_true', 'count'):\n                    val = strtobool(val)\n                try:\n                    val = option.convert_value(key, val)\n                except optparse.OptionValueError:\n                    e = sys.exc_info()[1]\n                    print(\"An error occured during configuration: %s\" % e)\n                    sys.exit(3)\n                defaults[option.dest] = val\n        return defaults\n\n    def get_config_section(self, name):\n        \"\"\"\n        Get a section of a configuration\n        \"\"\"\n        if self.config.has_section(name):\n            return self.config.items(name)\n        return []\n\n    def get_environ_vars(self, prefix='VIRTUALENV_'):\n        \"\"\"\n        Returns a generator with all environmental vars with prefix VIRTUALENV\n        \"\"\"\n        for key, val in os.environ.items():\n            if key.startswith(prefix):\n                yield (key.replace(prefix, '').lower(), val)\n\n    def get_default_values(self):\n        \"\"\"\n        Overridding to make updating the defaults after instantiation of\n        the option parser possible, update_defaults() does the dirty work.\n        \"\"\"\n        if not self.process_default_values:\n            # Old, pre-Optik 1.5 behaviour.\n            return optparse.Values(self.defaults)\n\n        defaults = self.update_defaults(self.defaults.copy())  # ours\n        for option in self._get_all_options():\n            default = defaults.get(option.dest)\n            if isinstance(default, basestring):\n                opt_str = option.get_opt_string()\n                defaults[option.dest] = option.check_value(opt_str, default)\n        return optparse.Values(defaults)\n\n\ndef main():\n    parser = ConfigOptionParser(\n        version=virtualenv_version,\n        usage=\"%prog [OPTIONS] DEST_DIR\",\n        formatter=UpdatingDefaultsHelpFormatter())\n\n    parser.add_option(\n        '-v', '--verbose',\n        action='count',\n        dest='verbose',\n        default=0,\n        help=\"Increase verbosity\")\n\n    parser.add_option(\n        '-q', '--quiet',\n        action='count',\n        dest='quiet',\n        default=0,\n        help='Decrease verbosity')\n\n    parser.add_option(\n        '-p', '--python',\n        dest='python',\n        metavar='PYTHON_EXE',\n        help='The Python interpreter to use, e.g., --python=python2.5 will use the python2.5 '\n        'interpreter to create the new environment.  The default is the interpreter that '\n        'virtualenv was installed with (%s)' % sys.executable)\n\n    parser.add_option(\n        '--clear',\n        dest='clear',\n        action='store_true',\n        help=\"Clear out the non-root install and start from scratch\")\n\n    parser.set_defaults(system_site_packages=False)\n    parser.add_option(\n        '--no-site-packages',\n        dest='system_site_packages',\n        action='store_false',\n        help=\"Don't give access to the global site-packages dir to the \"\n             \"virtual environment (default)\")\n\n    parser.add_option(\n        '--system-site-packages',\n        dest='system_site_packages',\n        action='store_true',\n        help=\"Give access to the global site-packages dir to the \"\n             \"virtual environment\")\n\n    parser.add_option(\n        '--unzip-setuptools',\n        dest='unzip_setuptools',\n        action='store_true',\n        help=\"Unzip Setuptools or Distribute when installing it\")\n\n    parser.add_option(\n        '--relocatable',\n        dest='relocatable',\n        action='store_true',\n        help='Make an EXISTING virtualenv environment relocatable.  '\n        'This fixes up scripts and makes all .pth files relative')\n\n    parser.add_option(\n        '--distribute', '--use-distribute',  # the second option is for legacy reasons here. Hi Kenneth!\n        dest='use_distribute',\n        action='store_true',\n        help='Use Distribute instead of Setuptools. Set environ variable '\n        'VIRTUALENV_DISTRIBUTE to make it the default ')\n\n    parser.add_option(\n        '--no-setuptools',\n        dest='no_setuptools',\n        action='store_true',\n        help='Do not install distribute/setuptools (or pip) '\n        'in the new virtualenv.')\n\n    parser.add_option(\n        '--no-pip',\n        dest='no_pip',\n        action='store_true',\n        help='Do not install pip in the new virtualenv.')\n\n    parser.add_option(\n        '--setuptools',\n        dest='use_distribute',\n        action='store_false',\n        help='Use Setuptools instead of Distribute.  Set environ variable '\n        'VIRTUALENV_SETUPTOOLS to make it the default ')\n\n    # Set this to True to use distribute by default, even in Python 2.\n    parser.set_defaults(use_distribute=False)\n\n    default_search_dirs = file_search_dirs()\n    parser.add_option(\n        '--extra-search-dir',\n        dest=\"search_dirs\",\n        action=\"append\",\n        default=default_search_dirs,\n        help=\"Directory to look for setuptools/distribute/pip distributions in. \"\n        \"You can add any number of additional --extra-search-dir paths.\")\n\n    parser.add_option(\n        '--never-download',\n        dest=\"never_download\",\n        action=\"store_true\",\n        help=\"Never download anything from the network.  Instead, virtualenv will fail \"\n        \"if local distributions of setuptools/distribute/pip are not present.\")\n\n    parser.add_option(\n        '--prompt',\n        dest='prompt',\n        help='Provides an alternative prompt prefix for this environment')\n\n    if 'extend_parser' in globals():\n        extend_parser(parser)\n\n    options, args = parser.parse_args()\n\n    global logger\n\n    if 'adjust_options' in globals():\n        adjust_options(options, args)\n\n    verbosity = options.verbose - options.quiet\n    logger = Logger([(Logger.level_for_integer(2 - verbosity), sys.stdout)])\n\n    if options.python and not os.environ.get('VIRTUALENV_INTERPRETER_RUNNING'):\n        env = os.environ.copy()\n        interpreter = resolve_interpreter(options.python)\n        if interpreter == sys.executable:\n            logger.warn('Already using interpreter %s' % interpreter)\n        else:\n            logger.notify('Running virtualenv with interpreter %s' % interpreter)\n            env['VIRTUALENV_INTERPRETER_RUNNING'] = 'true'\n            file = __file__\n            if file.endswith('.pyc'):\n                file = file[:-1]\n            popen = subprocess.Popen([interpreter, file] + sys.argv[1:], env=env)\n            raise SystemExit(popen.wait())\n\n    # Force --distribute on Python 3, since setuptools is not available.\n    if majver > 2:\n        options.use_distribute = True\n\n    if os.environ.get('PYTHONDONTWRITEBYTECODE') and not options.use_distribute:\n        print(\n            \"The PYTHONDONTWRITEBYTECODE environment variable is \"\n            \"not compatible with setuptools. Either use --distribute \"\n            \"or unset PYTHONDONTWRITEBYTECODE.\")\n        sys.exit(2)\n    if not args:\n        print('You must provide a DEST_DIR')\n        parser.print_help()\n        sys.exit(2)\n    if len(args) > 1:\n        print('There must be only one argument: DEST_DIR (you gave %s)' % (\n            ' '.join(args)))\n        parser.print_help()\n        sys.exit(2)\n\n    home_dir = args[0]\n\n    if os.environ.get('WORKING_ENV'):\n        logger.fatal('ERROR: you cannot run virtualenv while in a workingenv')\n        logger.fatal('Please deactivate your workingenv, then re-run this script')\n        sys.exit(3)\n\n    if 'PYTHONHOME' in os.environ:\n        logger.warn('PYTHONHOME is set.  You *must* activate the virtualenv before using it')\n        del os.environ['PYTHONHOME']\n\n    if options.relocatable:\n        make_environment_relocatable(home_dir)\n        return\n\n    create_environment(home_dir,\n                       site_packages=options.system_site_packages,\n                       clear=options.clear,\n                       unzip_setuptools=options.unzip_setuptools,\n                       use_distribute=options.use_distribute,\n                       prompt=options.prompt,\n                       search_dirs=options.search_dirs,\n                       never_download=options.never_download,\n                       no_setuptools=options.no_setuptools,\n                       no_pip=options.no_pip)\n    if 'after_install' in globals():\n        after_install(options, home_dir)\n\ndef call_subprocess(cmd, show_stdout=True,\n                    filter_stdout=None, cwd=None,\n                    raise_on_returncode=True, extra_env=None,\n                    remove_from_env=None):\n    cmd_parts = []\n    for part in cmd:\n        if len(part) > 45:\n            part = part[:20]+\"...\"+part[-20:]\n        if ' ' in part or '\\n' in part or '\"' in part or \"'\" in part:\n            part = '\"%s\"' % part.replace('\"', '\\\\\"')\n        if hasattr(part, 'decode'):\n            try:\n                part = part.decode(sys.getdefaultencoding())\n            except UnicodeDecodeError:\n                part = part.decode(sys.getfilesystemencoding())\n        cmd_parts.append(part)\n    cmd_desc = ' '.join(cmd_parts)\n    if show_stdout:\n        stdout = None\n    else:\n        stdout = subprocess.PIPE\n    logger.debug(\"Running command %s\" % cmd_desc)\n    if extra_env or remove_from_env:\n        env = os.environ.copy()\n        if extra_env:\n            env.update(extra_env)\n        if remove_from_env:\n            for varname in remove_from_env:\n                env.pop(varname, None)\n    else:\n        env = None\n    try:\n        proc = subprocess.Popen(\n            cmd, stderr=subprocess.STDOUT, stdin=None, stdout=stdout,\n            cwd=cwd, env=env)\n    except Exception:\n        e = sys.exc_info()[1]\n        logger.fatal(\n            \"Error %s while executing command %s\" % (e, cmd_desc))\n        raise\n    all_output = []\n    if stdout is not None:\n        stdout = proc.stdout\n        encoding = sys.getdefaultencoding()\n        fs_encoding = sys.getfilesystemencoding()\n        while 1:\n            line = stdout.readline()\n            try:\n                line = line.decode(encoding)\n            except UnicodeDecodeError:\n                line = line.decode(fs_encoding)\n            if not line:\n                break\n            line = line.rstrip()\n            all_output.append(line)\n            if filter_stdout:\n                level = filter_stdout(line)\n                if isinstance(level, tuple):\n                    level, line = level\n                logger.log(level, line)\n                if not logger.stdout_level_matches(level):\n                    logger.show_progress()\n            else:\n                logger.info(line)\n    else:\n        proc.communicate()\n    proc.wait()\n    if proc.returncode:\n        if raise_on_returncode:\n            if all_output:\n                logger.notify('Complete output from command %s:' % cmd_desc)\n                logger.notify('\\n'.join(all_output) + '\\n----------------------------------------')\n            raise OSError(\n                \"Command %s failed with error code %s\"\n                % (cmd_desc, proc.returncode))\n        else:\n            logger.warn(\n                \"Command %s had error code %s\"\n                % (cmd_desc, proc.returncode))\n\n\ndef create_environment(home_dir, site_packages=False, clear=False,\n                       unzip_setuptools=False, use_distribute=False,\n                       prompt=None, search_dirs=None, never_download=False,\n                       no_setuptools=False, no_pip=False):\n    \"\"\"\n    Creates a new environment in ``home_dir``.\n\n    If ``site_packages`` is true, then the global ``site-packages/``\n    directory will be on the path.\n\n    If ``clear`` is true (default False) then the environment will\n    first be cleared.\n    \"\"\"\n    home_dir, lib_dir, inc_dir, bin_dir = path_locations(home_dir)\n\n    py_executable = os.path.abspath(install_python(\n        home_dir, lib_dir, inc_dir, bin_dir,\n        site_packages=site_packages, clear=clear))\n\n    install_distutils(home_dir)\n\n    if not no_setuptools:\n        if use_distribute:\n            install_distribute(py_executable, unzip=unzip_setuptools,\n                               search_dirs=search_dirs, never_download=never_download)\n        else:\n            install_setuptools(py_executable, unzip=unzip_setuptools,\n                               search_dirs=search_dirs, never_download=never_download)\n\n        if not no_pip:\n            install_pip(py_executable, search_dirs=search_dirs, never_download=never_download)\n\n    install_activate(home_dir, bin_dir, prompt)\n\ndef is_executable_file(fpath):\n    return os.path.isfile(fpath) and os.access(fpath, os.X_OK)\n\ndef path_locations(home_dir):\n    \"\"\"Return the path locations for the environment (where libraries are,\n    where scripts go, etc)\"\"\"\n    # XXX: We'd use distutils.sysconfig.get_python_inc/lib but its\n    # prefix arg is broken: http://bugs.python.org/issue3386\n    if is_win:\n        # Windows has lots of problems with executables with spaces in\n        # the name; this function will remove them (using the ~1\n        # format):\n        mkdir(home_dir)\n        if ' ' in home_dir:\n            import ctypes\n            GetShortPathName = ctypes.windll.kernel32.GetShortPathNameW\n            size = max(len(home_dir)+1, 256)\n            buf = ctypes.create_unicode_buffer(size)\n            try:\n                u = unicode\n            except NameError:\n                u = str\n            ret = GetShortPathName(u(home_dir), buf, size)\n            if not ret:\n                print('Error: the path \"%s\" has a space in it' % home_dir)\n                print('We could not determine the short pathname for it.')\n                print('Exiting.')\n                sys.exit(3)\n            home_dir = str(buf.value)\n        lib_dir = join(home_dir, 'Lib')\n        inc_dir = join(home_dir, 'Include')\n        bin_dir = join(home_dir, 'Scripts')\n    if is_jython:\n        lib_dir = join(home_dir, 'Lib')\n        inc_dir = join(home_dir, 'Include')\n        bin_dir = join(home_dir, 'bin')\n    elif is_pypy:\n        lib_dir = home_dir\n        inc_dir = join(home_dir, 'include')\n        bin_dir = join(home_dir, 'bin')\n    elif not is_win:\n        lib_dir = join(home_dir, 'lib', py_version)\n        multiarch_exec = '/usr/bin/multiarch-platform'\n        if is_executable_file(multiarch_exec):\n            # In Mageia (2) and Mandriva distros the include dir must be like:\n            # virtualenv/include/multiarch-x86_64-linux/python2.7\n            # instead of being virtualenv/include/python2.7\n            p = subprocess.Popen(multiarch_exec, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n            stdout, stderr = p.communicate()\n            # stdout.strip is needed to remove newline character\n            inc_dir = join(home_dir, 'include', stdout.strip(), py_version + abiflags)\n        else:\n            inc_dir = join(home_dir, 'include', py_version + abiflags)\n        bin_dir = join(home_dir, 'bin')\n    return home_dir, lib_dir, inc_dir, bin_dir\n\n\ndef change_prefix(filename, dst_prefix):\n    prefixes = [sys.prefix]\n\n    if is_darwin:\n        prefixes.extend((\n            os.path.join(\"/Library/Python\", sys.version[:3], \"site-packages\"),\n            os.path.join(sys.prefix, \"Extras\", \"lib\", \"python\"),\n            os.path.join(\"~\", \"Library\", \"Python\", sys.version[:3], \"site-packages\"),\n            # Python 2.6 no-frameworks\n            os.path.join(\"~\", \".local\", \"lib\",\"python\", sys.version[:3], \"site-packages\"),\n            # System Python 2.7 on OSX Mountain Lion\n            os.path.join(\"~\", \"Library\", \"Python\", sys.version[:3], \"lib\", \"python\", \"site-packages\")))\n\n    if hasattr(sys, 'real_prefix'):\n        prefixes.append(sys.real_prefix)\n    if hasattr(sys, 'base_prefix'):\n        prefixes.append(sys.base_prefix)\n    prefixes = list(map(os.path.expanduser, prefixes))\n    prefixes = list(map(os.path.abspath, prefixes))\n    # Check longer prefixes first so we don't split in the middle of a filename\n    prefixes = sorted(prefixes, key=len, reverse=True)\n    filename = os.path.abspath(filename)\n    for src_prefix in prefixes:\n        if filename.startswith(src_prefix):\n            _, relpath = filename.split(src_prefix, 1)\n            if src_prefix != os.sep: # sys.prefix == \"/\"\n                assert relpath[0] == os.sep\n                relpath = relpath[1:]\n            return join(dst_prefix, relpath)\n    assert False, \"Filename %s does not start with any of these prefixes: %s\" % \\\n        (filename, prefixes)\n\ndef copy_required_modules(dst_prefix):\n    import imp\n    # If we are running under -p, we need to remove the current\n    # directory from sys.path temporarily here, so that we\n    # definitely get the modules from the site directory of\n    # the interpreter we are running under, not the one\n    # virtualenv.py is installed under (which might lead to py2/py3\n    # incompatibility issues)\n    _prev_sys_path = sys.path\n    if os.environ.get('VIRTUALENV_INTERPRETER_RUNNING'):\n        sys.path = sys.path[1:]\n    try:\n        for modname in REQUIRED_MODULES:\n            if modname in sys.builtin_module_names:\n                logger.info(\"Ignoring built-in bootstrap module: %s\" % modname)\n                continue\n            try:\n                f, filename, _ = imp.find_module(modname)\n            except ImportError:\n                logger.info(\"Cannot import bootstrap module: %s\" % modname)\n            else:\n                if f is not None:\n                    f.close()\n                # special-case custom readline.so on OS X, but not for pypy:\n                if modname == 'readline' and sys.platform == 'darwin' and not (\n                        is_pypy or filename.endswith(join('lib-dynload', 'readline.so'))):\n                    dst_filename = join(dst_prefix, 'lib', 'python%s' % sys.version[:3], 'readline.so')\n                else:\n                    dst_filename = change_prefix(filename, dst_prefix)\n                copyfile(filename, dst_filename)\n                if filename.endswith('.pyc'):\n                    pyfile = filename[:-1]\n                    if os.path.exists(pyfile):\n                        copyfile(pyfile, dst_filename[:-1])\n    finally:\n        sys.path = _prev_sys_path\n\n\ndef subst_path(prefix_path, prefix, home_dir):\n    prefix_path = os.path.normpath(prefix_path)\n    prefix = os.path.normpath(prefix)\n    home_dir = os.path.normpath(home_dir)\n    if not prefix_path.startswith(prefix):\n        logger.warn('Path not in prefix %r %r', prefix_path, prefix)\n        return\n    return prefix_path.replace(prefix, home_dir, 1)\n\n\ndef install_python(home_dir, lib_dir, inc_dir, bin_dir, site_packages, clear):\n    \"\"\"Install just the base environment, no distutils patches etc\"\"\"\n    if sys.executable.startswith(bin_dir):\n        print('Please use the *system* python to run this script')\n        return\n\n    if clear:\n        rmtree(lib_dir)\n        ## FIXME: why not delete it?\n        ## Maybe it should delete everything with #!/path/to/venv/python in it\n        logger.notify('Not deleting %s', bin_dir)\n\n    if hasattr(sys, 'real_prefix'):\n        logger.notify('Using real prefix %r' % sys.real_prefix)\n        prefix = sys.real_prefix\n    elif hasattr(sys, 'base_prefix'):\n        logger.notify('Using base prefix %r' % sys.base_prefix)\n        prefix = sys.base_prefix\n    else:\n        prefix = sys.prefix\n    mkdir(lib_dir)\n    fix_lib64(lib_dir)\n    stdlib_dirs = [os.path.dirname(os.__file__)]\n    if is_win:\n        stdlib_dirs.append(join(os.path.dirname(stdlib_dirs[0]), 'DLLs'))\n    elif is_darwin:\n        stdlib_dirs.append(join(stdlib_dirs[0], 'site-packages'))\n    if hasattr(os, 'symlink'):\n        logger.info('Symlinking Python bootstrap modules')\n    else:\n        logger.info('Copying Python bootstrap modules')\n    logger.indent += 2\n    try:\n        # copy required files...\n        for stdlib_dir in stdlib_dirs:\n            if not os.path.isdir(stdlib_dir):\n                continue\n            for fn in os.listdir(stdlib_dir):\n                bn = os.path.splitext(fn)[0]\n                if fn != 'site-packages' and bn in REQUIRED_FILES:\n                    copyfile(join(stdlib_dir, fn), join(lib_dir, fn))\n        # ...and modules\n        copy_required_modules(home_dir)\n    finally:\n        logger.indent -= 2\n    mkdir(join(lib_dir, 'site-packages'))\n    import site\n    site_filename = site.__file__\n    if site_filename.endswith('.pyc'):\n        site_filename = site_filename[:-1]\n    elif site_filename.endswith('$py.class'):\n        site_filename = site_filename.replace('$py.class', '.py')\n    site_filename_dst = change_prefix(site_filename, home_dir)\n    site_dir = os.path.dirname(site_filename_dst)\n    writefile(site_filename_dst, SITE_PY)\n    writefile(join(site_dir, 'orig-prefix.txt'), prefix)\n    site_packages_filename = join(site_dir, 'no-global-site-packages.txt')\n    if not site_packages:\n        writefile(site_packages_filename, '')\n\n    if is_pypy or is_win:\n        stdinc_dir = join(prefix, 'include')\n    else:\n        stdinc_dir = join(prefix, 'include', py_version + abiflags)\n    if os.path.exists(stdinc_dir):\n        copyfile(stdinc_dir, inc_dir)\n    else:\n        logger.debug('No include dir %s' % stdinc_dir)\n\n    platinc_dir = distutils.sysconfig.get_python_inc(plat_specific=1)\n    if platinc_dir != stdinc_dir:\n        platinc_dest = distutils.sysconfig.get_python_inc(\n            plat_specific=1, prefix=home_dir)\n        if platinc_dir == platinc_dest:\n            # Do platinc_dest manually due to a CPython bug;\n            # not http://bugs.python.org/issue3386 but a close cousin\n            platinc_dest = subst_path(platinc_dir, prefix, home_dir)\n        if platinc_dest:\n            # PyPy's stdinc_dir and prefix are relative to the original binary\n            # (traversing virtualenvs), whereas the platinc_dir is relative to\n            # the inner virtualenv and ignores the prefix argument.\n            # This seems more evolved than designed.\n            copyfile(platinc_dir, platinc_dest)\n\n    # pypy never uses exec_prefix, just ignore it\n    if sys.exec_prefix != prefix and not is_pypy:\n        if is_win:\n            exec_dir = join(sys.exec_prefix, 'lib')\n        elif is_jython:\n            exec_dir = join(sys.exec_prefix, 'Lib')\n        else:\n            exec_dir = join(sys.exec_prefix, 'lib', py_version)\n        for fn in os.listdir(exec_dir):\n            copyfile(join(exec_dir, fn), join(lib_dir, fn))\n\n    if is_jython:\n        # Jython has either jython-dev.jar and javalib/ dir, or just\n        # jython.jar\n        for name in 'jython-dev.jar', 'javalib', 'jython.jar':\n            src = join(prefix, name)\n            if os.path.exists(src):\n                copyfile(src, join(home_dir, name))\n        # XXX: registry should always exist after Jython 2.5rc1\n        src = join(prefix, 'registry')\n        if os.path.exists(src):\n            copyfile(src, join(home_dir, 'registry'), symlink=False)\n        copyfile(join(prefix, 'cachedir'), join(home_dir, 'cachedir'),\n                 symlink=False)\n\n    mkdir(bin_dir)\n    py_executable = join(bin_dir, os.path.basename(sys.executable))\n    if 'Python.framework' in prefix:\n        # OS X framework builds cause validation to break\n        # https://github.com/pypa/virtualenv/issues/322\n        if os.environ.get('__PYVENV_LAUNCHER__'):\n          os.unsetenv('__PYVENV_LAUNCHER__')\n        if re.search(r'/Python(?:-32|-64)*$', py_executable):\n            # The name of the python executable is not quite what\n            # we want, rename it.\n            py_executable = os.path.join(\n                    os.path.dirname(py_executable), 'python')\n\n    logger.notify('New %s executable in %s', expected_exe, py_executable)\n    pcbuild_dir = os.path.dirname(sys.executable)\n    pyd_pth = os.path.join(lib_dir, 'site-packages', 'virtualenv_builddir_pyd.pth')\n    if is_win and os.path.exists(os.path.join(pcbuild_dir, 'build.bat')):\n        logger.notify('Detected python running from build directory %s', pcbuild_dir)\n        logger.notify('Writing .pth file linking to build directory for *.pyd files')\n        writefile(pyd_pth, pcbuild_dir)\n    else:\n        pcbuild_dir = None\n        if os.path.exists(pyd_pth):\n            logger.info('Deleting %s (not Windows env or not build directory python)' % pyd_pth)\n            os.unlink(pyd_pth)\n\n    if sys.executable != py_executable:\n        ## FIXME: could I just hard link?\n        executable = sys.executable\n        shutil.copyfile(executable, py_executable)\n        make_exe(py_executable)\n        if is_win or is_cygwin:\n            pythonw = os.path.join(os.path.dirname(sys.executable), 'pythonw.exe')\n            if os.path.exists(pythonw):\n                logger.info('Also created pythonw.exe')\n                shutil.copyfile(pythonw, os.path.join(os.path.dirname(py_executable), 'pythonw.exe'))\n            python_d = os.path.join(os.path.dirname(sys.executable), 'python_d.exe')\n            python_d_dest = os.path.join(os.path.dirname(py_executable), 'python_d.exe')\n            if os.path.exists(python_d):\n                logger.info('Also created python_d.exe')\n                shutil.copyfile(python_d, python_d_dest)\n            elif os.path.exists(python_d_dest):\n                logger.info('Removed python_d.exe as it is no longer at the source')\n                os.unlink(python_d_dest)\n            # we need to copy the DLL to enforce that windows will load the correct one.\n            # may not exist if we are cygwin.\n            py_executable_dll = 'python%s%s.dll' % (\n                sys.version_info[0], sys.version_info[1])\n            py_executable_dll_d = 'python%s%s_d.dll' % (\n                sys.version_info[0], sys.version_info[1])\n            pythondll = os.path.join(os.path.dirname(sys.executable), py_executable_dll)\n            pythondll_d = os.path.join(os.path.dirname(sys.executable), py_executable_dll_d)\n            pythondll_d_dest = os.path.join(os.path.dirname(py_executable), py_executable_dll_d)\n            if os.path.exists(pythondll):\n                logger.info('Also created %s' % py_executable_dll)\n                shutil.copyfile(pythondll, os.path.join(os.path.dirname(py_executable), py_executable_dll))\n            if os.path.exists(pythondll_d):\n                logger.info('Also created %s' % py_executable_dll_d)\n                shutil.copyfile(pythondll_d, pythondll_d_dest)\n            elif os.path.exists(pythondll_d_dest):\n                logger.info('Removed %s as the source does not exist' % pythondll_d_dest)\n                os.unlink(pythondll_d_dest)\n        if is_pypy:\n            # make a symlink python --> pypy-c\n            python_executable = os.path.join(os.path.dirname(py_executable), 'python')\n            if sys.platform in ('win32', 'cygwin'):\n                python_executable += '.exe'\n            logger.info('Also created executable %s' % python_executable)\n            copyfile(py_executable, python_executable)\n\n            if is_win:\n                for name in 'libexpat.dll', 'libpypy.dll', 'libpypy-c.dll', 'libeay32.dll', 'ssleay32.dll', 'sqlite.dll':\n                    src = join(prefix, name)\n                    if os.path.exists(src):\n                        copyfile(src, join(bin_dir, name))\n\n    if os.path.splitext(os.path.basename(py_executable))[0] != expected_exe:\n        secondary_exe = os.path.join(os.path.dirname(py_executable),\n                                     expected_exe)\n        py_executable_ext = os.path.splitext(py_executable)[1]\n        if py_executable_ext == '.exe':\n            # python2.4 gives an extension of '.4' :P\n            secondary_exe += py_executable_ext\n        if os.path.exists(secondary_exe):\n            logger.warn('Not overwriting existing %s script %s (you must use %s)'\n                        % (expected_exe, secondary_exe, py_executable))\n        else:\n            logger.notify('Also creating executable in %s' % secondary_exe)\n            shutil.copyfile(sys.executable, secondary_exe)\n            make_exe(secondary_exe)\n\n    if '.framework' in prefix:\n        if 'Python.framework' in prefix:\n            logger.debug('MacOSX Python framework detected')\n            # Make sure we use the the embedded interpreter inside\n            # the framework, even if sys.executable points to\n            # the stub executable in ${sys.prefix}/bin\n            # See http://groups.google.com/group/python-virtualenv/\n            #                              browse_thread/thread/17cab2f85da75951\n            original_python = os.path.join(\n                prefix, 'Resources/Python.app/Contents/MacOS/Python')\n        if 'EPD' in prefix:\n            logger.debug('EPD framework detected')\n            original_python = os.path.join(prefix, 'bin/python')\n        shutil.copy(original_python, py_executable)\n\n        # Copy the framework's dylib into the virtual\n        # environment\n        virtual_lib = os.path.join(home_dir, '.Python')\n\n        if os.path.exists(virtual_lib):\n            os.unlink(virtual_lib)\n        copyfile(\n            os.path.join(prefix, 'Python'),\n            virtual_lib)\n\n        # And then change the install_name of the copied python executable\n        try:\n            mach_o_change(py_executable,\n                          os.path.join(prefix, 'Python'),\n                          '@executable_path/../.Python')\n        except:\n            e = sys.exc_info()[1]\n            logger.warn(\"Could not call mach_o_change: %s. \"\n                        \"Trying to call install_name_tool instead.\" % e)\n            try:\n                call_subprocess(\n                    [\"install_name_tool\", \"-change\",\n                     os.path.join(prefix, 'Python'),\n                     '@executable_path/../.Python',\n                     py_executable])\n            except:\n                logger.fatal(\"Could not call install_name_tool -- you must \"\n                             \"have Apple's development tools installed\")\n                raise\n\n    if not is_win:\n        # Ensure that 'python', 'pythonX' and 'pythonX.Y' all exist\n        py_exe_version_major = 'python%s' % sys.version_info[0]\n        py_exe_version_major_minor = 'python%s.%s' % (\n            sys.version_info[0], sys.version_info[1])\n        py_exe_no_version = 'python'\n        required_symlinks = [ py_exe_no_version, py_exe_version_major,\n                         py_exe_version_major_minor ]\n\n        py_executable_base = os.path.basename(py_executable)\n\n        if py_executable_base in required_symlinks:\n            # Don't try to symlink to yourself.\n            required_symlinks.remove(py_executable_base)\n\n        for pth in required_symlinks:\n            full_pth = join(bin_dir, pth)\n            if os.path.exists(full_pth):\n                os.unlink(full_pth)\n            os.symlink(py_executable_base, full_pth)\n\n    if is_win and ' ' in py_executable:\n        # There's a bug with subprocess on Windows when using a first\n        # argument that has a space in it.  Instead we have to quote\n        # the value:\n        py_executable = '\"%s\"' % py_executable\n    # NOTE: keep this check as one line, cmd.exe doesn't cope with line breaks\n    cmd = [py_executable, '-c', 'import sys;out=sys.stdout;'\n        'getattr(out, \"buffer\", out).write(sys.prefix.encode(\"utf-8\"))']\n    logger.info('Testing executable with %s %s \"%s\"' % tuple(cmd))\n    try:\n        proc = subprocess.Popen(cmd,\n                            stdout=subprocess.PIPE)\n        proc_stdout, proc_stderr = proc.communicate()\n    except OSError:\n        e = sys.exc_info()[1]\n        if e.errno == errno.EACCES:\n            logger.fatal('ERROR: The executable %s could not be run: %s' % (py_executable, e))\n            sys.exit(100)\n        else:\n            raise e\n\n    proc_stdout = proc_stdout.strip().decode(\"utf-8\")\n    proc_stdout = os.path.normcase(os.path.abspath(proc_stdout))\n    norm_home_dir = os.path.normcase(os.path.abspath(home_dir))\n    if hasattr(norm_home_dir, 'decode'):\n        norm_home_dir = norm_home_dir.decode(sys.getfilesystemencoding())\n    if proc_stdout != norm_home_dir:\n        logger.fatal(\n            'ERROR: The executable %s is not functioning' % py_executable)\n        logger.fatal(\n            'ERROR: It thinks sys.prefix is %r (should be %r)'\n            % (proc_stdout, norm_home_dir))\n        logger.fatal(\n            'ERROR: virtualenv is not compatible with this system or executable')\n        if is_win:\n            logger.fatal(\n                'Note: some Windows users have reported this error when they '\n                'installed Python for \"Only this user\" or have multiple '\n                'versions of Python installed. Copying the appropriate '\n                'PythonXX.dll to the virtualenv Scripts/ directory may fix '\n                'this problem.')\n        sys.exit(100)\n    else:\n        logger.info('Got sys.prefix result: %r' % proc_stdout)\n\n    pydistutils = os.path.expanduser('~/.pydistutils.cfg')\n    if os.path.exists(pydistutils):\n        logger.notify('Please make sure you remove any previous custom paths from '\n                      'your %s file.' % pydistutils)\n    ## FIXME: really this should be calculated earlier\n\n    fix_local_scheme(home_dir)\n\n    if site_packages:\n        if os.path.exists(site_packages_filename):\n            logger.info('Deleting %s' % site_packages_filename)\n            os.unlink(site_packages_filename)\n\n    return py_executable\n\n\ndef install_activate(home_dir, bin_dir, prompt=None):\n    home_dir = os.path.abspath(home_dir)\n    if is_win or is_jython and os._name == 'nt':\n        files = {\n            'activate.bat': ACTIVATE_BAT,\n            'deactivate.bat': DEACTIVATE_BAT,\n            'activate.ps1': ACTIVATE_PS,\n        }\n\n        # MSYS needs paths of the form /c/path/to/file\n        drive, tail = os.path.splitdrive(home_dir.replace(os.sep, '/'))\n        home_dir_msys = (drive and \"/%s%s\" or \"%s%s\") % (drive[:1], tail)\n\n        # Run-time conditional enables (basic) Cygwin compatibility\n        home_dir_sh = (\"\"\"$(if [ \"$OSTYPE\" \"==\" \"cygwin\" ]; then cygpath -u '%s'; else echo '%s'; fi;)\"\"\" %\n                       (home_dir, home_dir_msys))\n        files['activate'] = ACTIVATE_SH.replace('__VIRTUAL_ENV__', home_dir_sh)\n\n    else:\n        files = {'activate': ACTIVATE_SH}\n\n        # suppling activate.fish in addition to, not instead of, the\n        # bash script support.\n        files['activate.fish'] = ACTIVATE_FISH\n\n        # same for csh/tcsh support...\n        files['activate.csh'] = ACTIVATE_CSH\n\n    files['activate_this.py'] = ACTIVATE_THIS\n    if hasattr(home_dir, 'decode'):\n        home_dir = home_dir.decode(sys.getfilesystemencoding())\n    vname = os.path.basename(home_dir)\n    for name, content in files.items():\n        content = content.replace('__VIRTUAL_PROMPT__', prompt or '')\n        content = content.replace('__VIRTUAL_WINPROMPT__', prompt or '(%s)' % vname)\n        content = content.replace('__VIRTUAL_ENV__', home_dir)\n        content = content.replace('__VIRTUAL_NAME__', vname)\n        content = content.replace('__BIN_NAME__', os.path.basename(bin_dir))\n        writefile(os.path.join(bin_dir, name), content)\n\ndef install_distutils(home_dir):\n    distutils_path = change_prefix(distutils.__path__[0], home_dir)\n    mkdir(distutils_path)\n    ## FIXME: maybe this prefix setting should only be put in place if\n    ## there's a local distutils.cfg with a prefix setting?\n    home_dir = os.path.abspath(home_dir)\n    ## FIXME: this is breaking things, removing for now:\n    #distutils_cfg = DISTUTILS_CFG + \"\\n[install]\\nprefix=%s\\n\" % home_dir\n    writefile(os.path.join(distutils_path, '__init__.py'), DISTUTILS_INIT)\n    writefile(os.path.join(distutils_path, 'distutils.cfg'), DISTUTILS_CFG, overwrite=False)\n\ndef fix_local_scheme(home_dir):\n    \"\"\"\n    Platforms that use the \"posix_local\" install scheme (like Ubuntu with\n    Python 2.7) need to be given an additional \"local\" location, sigh.\n    \"\"\"\n    try:\n        import sysconfig\n    except ImportError:\n        pass\n    else:\n        if sysconfig._get_default_scheme() == 'posix_local':\n            local_path = os.path.join(home_dir, 'local')\n            if not os.path.exists(local_path):\n                os.mkdir(local_path)\n                for subdir_name in os.listdir(home_dir):\n                    if subdir_name == 'local':\n                        continue\n                    os.symlink(os.path.abspath(os.path.join(home_dir, subdir_name)), \\\n                                                            os.path.join(local_path, subdir_name))\n\ndef fix_lib64(lib_dir):\n    \"\"\"\n    Some platforms (particularly Gentoo on x64) put things in lib64/pythonX.Y\n    instead of lib/pythonX.Y.  If this is such a platform we'll just create a\n    symlink so lib64 points to lib\n    \"\"\"\n    if [p for p in distutils.sysconfig.get_config_vars().values()\n        if isinstance(p, basestring) and 'lib64' in p]:\n        logger.debug('This system uses lib64; symlinking lib64 to lib')\n        assert os.path.basename(lib_dir) == 'python%s' % sys.version[:3], (\n            \"Unexpected python lib dir: %r\" % lib_dir)\n        lib_parent = os.path.dirname(lib_dir)\n        top_level = os.path.dirname(lib_parent)\n        lib_dir = os.path.join(top_level, 'lib')\n        lib64_link = os.path.join(top_level, 'lib64')\n        assert os.path.basename(lib_parent) == 'lib', (\n            \"Unexpected parent dir: %r\" % lib_parent)\n        if os.path.lexists(lib64_link):\n            return\n        os.symlink('lib', lib64_link)\n\ndef resolve_interpreter(exe):\n    \"\"\"\n    If the executable given isn't an absolute path, search $PATH for the interpreter\n    \"\"\"\n    if os.path.abspath(exe) != exe:\n        paths = os.environ.get('PATH', '').split(os.pathsep)\n        for path in paths:\n            if os.path.exists(os.path.join(path, exe)):\n                exe = os.path.join(path, exe)\n                break\n    if not os.path.exists(exe):\n        logger.fatal('The executable %s (from --python=%s) does not exist' % (exe, exe))\n        raise SystemExit(3)\n    if not is_executable(exe):\n        logger.fatal('The executable %s (from --python=%s) is not executable' % (exe, exe))\n        raise SystemExit(3)\n    return exe\n\ndef is_executable(exe):\n    \"\"\"Checks a file is executable\"\"\"\n    return os.access(exe, os.X_OK)\n\n############################################################\n## Relocating the environment:\n\ndef make_environment_relocatable(home_dir):\n    \"\"\"\n    Makes the already-existing environment use relative paths, and takes out\n    the #!-based environment selection in scripts.\n    \"\"\"\n    home_dir, lib_dir, inc_dir, bin_dir = path_locations(home_dir)\n    activate_this = os.path.join(bin_dir, 'activate_this.py')\n    if not os.path.exists(activate_this):\n        logger.fatal(\n            'The environment doesn\\'t have a file %s -- please re-run virtualenv '\n            'on this environment to update it' % activate_this)\n    fixup_scripts(home_dir)\n    fixup_pth_and_egg_link(home_dir)\n    ## FIXME: need to fix up distutils.cfg\n\nOK_ABS_SCRIPTS = ['python', 'python%s' % sys.version[:3],\n                  'activate', 'activate.bat', 'activate_this.py']\n\ndef fixup_scripts(home_dir):\n    # This is what we expect at the top of scripts:\n    shebang = '#!%s/bin/python' % os.path.normcase(os.path.abspath(home_dir))\n    # This is what we'll put:\n    new_shebang = '#!/usr/bin/env python%s' % sys.version[:3]\n    if is_win:\n        bin_suffix = 'Scripts'\n    else:\n        bin_suffix = 'bin'\n    bin_dir = os.path.join(home_dir, bin_suffix)\n    home_dir, lib_dir, inc_dir, bin_dir = path_locations(home_dir)\n    for filename in os.listdir(bin_dir):\n        filename = os.path.join(bin_dir, filename)\n        if not os.path.isfile(filename):\n            # ignore subdirs, e.g. .svn ones.\n            continue\n        f = open(filename, 'rb')\n        try:\n            try:\n                lines = f.read().decode('utf-8').splitlines()\n            except UnicodeDecodeError:\n                # This is probably a binary program instead\n                # of a script, so just ignore it.\n                continue\n        finally:\n            f.close()\n        if not lines:\n            logger.warn('Script %s is an empty file' % filename)\n            continue\n        if not lines[0].strip().startswith(shebang):\n            if os.path.basename(filename) in OK_ABS_SCRIPTS:\n                logger.debug('Cannot make script %s relative' % filename)\n            elif lines[0].strip() == new_shebang:\n                logger.info('Script %s has already been made relative' % filename)\n            else:\n                logger.warn('Script %s cannot be made relative (it\\'s not a normal script that starts with %s)'\n                            % (filename, shebang))\n            continue\n        logger.notify('Making script %s relative' % filename)\n        script = relative_script([new_shebang] + lines[1:])\n        f = open(filename, 'wb')\n        f.write('\\n'.join(script).encode('utf-8'))\n        f.close()\n\ndef relative_script(lines):\n    \"Return a script that'll work in a relocatable environment.\"\n    activate = \"import os; activate_this=os.path.join(os.path.dirname(os.path.realpath(__file__)), 'activate_this.py'); execfile(activate_this, dict(__file__=activate_this)); del os, activate_this\"\n    # Find the last future statement in the script. If we insert the activation\n    # line before a future statement, Python will raise a SyntaxError.\n    activate_at = None\n    for idx, line in reversed(list(enumerate(lines))):\n        if line.split()[:3] == ['from', '__future__', 'import']:\n            activate_at = idx + 1\n            break\n    if activate_at is None:\n        # Activate after the shebang.\n        activate_at = 1\n    return lines[:activate_at] + ['', activate, ''] + lines[activate_at:]\n\ndef fixup_pth_and_egg_link(home_dir, sys_path=None):\n    \"\"\"Makes .pth and .egg-link files use relative paths\"\"\"\n    home_dir = os.path.normcase(os.path.abspath(home_dir))\n    if sys_path is None:\n        sys_path = sys.path\n    for path in sys_path:\n        if not path:\n            path = '.'\n        if not os.path.isdir(path):\n            continue\n        path = os.path.normcase(os.path.abspath(path))\n        if not path.startswith(home_dir):\n            logger.debug('Skipping system (non-environment) directory %s' % path)\n            continue\n        for filename in os.listdir(path):\n            filename = os.path.join(path, filename)\n            if filename.endswith('.pth'):\n                if not os.access(filename, os.W_OK):\n                    logger.warn('Cannot write .pth file %s, skipping' % filename)\n                else:\n                    fixup_pth_file(filename)\n            if filename.endswith('.egg-link'):\n                if not os.access(filename, os.W_OK):\n                    logger.warn('Cannot write .egg-link file %s, skipping' % filename)\n                else:\n                    fixup_egg_link(filename)\n\ndef fixup_pth_file(filename):\n    lines = []\n    prev_lines = []\n    f = open(filename)\n    prev_lines = f.readlines()\n    f.close()\n    for line in prev_lines:\n        line = line.strip()\n        if (not line or line.startswith('#') or line.startswith('import ')\n            or os.path.abspath(line) != line):\n            lines.append(line)\n        else:\n            new_value = make_relative_path(filename, line)\n            if line != new_value:\n                logger.debug('Rewriting path %s as %s (in %s)' % (line, new_value, filename))\n            lines.append(new_value)\n    if lines == prev_lines:\n        logger.info('No changes to .pth file %s' % filename)\n        return\n    logger.notify('Making paths in .pth file %s relative' % filename)\n    f = open(filename, 'w')\n    f.write('\\n'.join(lines) + '\\n')\n    f.close()\n\ndef fixup_egg_link(filename):\n    f = open(filename)\n    link = f.readline().strip()\n    f.close()\n    if os.path.abspath(link) != link:\n        logger.debug('Link in %s already relative' % filename)\n        return\n    new_link = make_relative_path(filename, link)\n    logger.notify('Rewriting link %s in %s as %s' % (link, filename, new_link))\n    f = open(filename, 'w')\n    f.write(new_link)\n    f.close()\n\ndef make_relative_path(source, dest, dest_is_directory=True):\n    \"\"\"\n    Make a filename relative, where the filename is dest, and it is\n    being referred to from the filename source.\n\n        >>> make_relative_path('/usr/share/something/a-file.pth',\n        ...                    '/usr/share/another-place/src/Directory')\n        '../another-place/src/Directory'\n        >>> make_relative_path('/usr/share/something/a-file.pth',\n        ...                    '/home/user/src/Directory')\n        '../../../home/user/src/Directory'\n        >>> make_relative_path('/usr/share/a-file.pth', '/usr/share/')\n        './'\n    \"\"\"\n    source = os.path.dirname(source)\n    if not dest_is_directory:\n        dest_filename = os.path.basename(dest)\n        dest = os.path.dirname(dest)\n    dest = os.path.normpath(os.path.abspath(dest))\n    source = os.path.normpath(os.path.abspath(source))\n    dest_parts = dest.strip(os.path.sep).split(os.path.sep)\n    source_parts = source.strip(os.path.sep).split(os.path.sep)\n    while dest_parts and source_parts and dest_parts[0] == source_parts[0]:\n        dest_parts.pop(0)\n        source_parts.pop(0)\n    full_parts = ['..']*len(source_parts) + dest_parts\n    if not dest_is_directory:\n        full_parts.append(dest_filename)\n    if not full_parts:\n        # Special case for the current directory (otherwise it'd be '')\n        return './'\n    return os.path.sep.join(full_parts)\n\n\n\n############################################################\n## Bootstrap script creation:\n\ndef create_bootstrap_script(extra_text, python_version=''):\n    \"\"\"\n    Creates a bootstrap script, which is like this script but with\n    extend_parser, adjust_options, and after_install hooks.\n\n    This returns a string that (written to disk of course) can be used\n    as a bootstrap script with your own customizations.  The script\n    will be the standard virtualenv.py script, with your extra text\n    added (your extra text should be Python code).\n\n    If you include these functions, they will be called:\n\n    ``extend_parser(optparse_parser)``:\n        You can add or remove options from the parser here.\n\n    ``adjust_options(options, args)``:\n        You can change options here, or change the args (if you accept\n        different kinds of arguments, be sure you modify ``args`` so it is\n        only ``[DEST_DIR]``).\n\n    ``after_install(options, home_dir)``:\n\n        After everything is installed, this function is called.  This\n        is probably the function you are most likely to use.  An\n        example would be::\n\n            def after_install(options, home_dir):\n                subprocess.call([join(home_dir, 'bin', 'easy_install'),\n                                 'MyPackage'])\n                subprocess.call([join(home_dir, 'bin', 'my-package-script'),\n                                 'setup', home_dir])\n\n        This example immediately installs a package, and runs a setup\n        script from that package.\n\n    If you provide something like ``python_version='2.5'`` then the\n    script will start with ``#!/usr/bin/env python2.5`` instead of\n    ``#!/usr/bin/env python``.  You can use this when the script must\n    be run with a particular Python version.\n    \"\"\"\n    filename = __file__\n    if filename.endswith('.pyc'):\n        filename = filename[:-1]\n    f = codecs.open(filename, 'r', encoding='utf-8')\n    content = f.read()\n    f.close()\n    py_exe = 'python%s' % python_version\n    content = (('#!/usr/bin/env %s\\n' % py_exe)\n               + '## WARNING: This file is generated\\n'\n               + content)\n    return content.replace('##EXT' 'END##', extra_text)\n\n\n\ndefault_target_dir = 'venv'\n\npip_install_packages = filter(len, open('requirements.txt').readlines())\n\nimport os\nimport subprocess\nimport sys\n\ndef adjust_options(options, args):\n  if len(args)==0:\n    os.chdir(os.path.dirname(__file__))\n    args.append(default_target_dir)\n\ndef after_install(options, home_dir):\n  from os.path import join\n  pip = join(home_dir, 'bin/pip')\n  if not os.path.exists(pip):\n    # on windows\n    pip = join(home_dir, 'Scripts/pip.exe')\n  if not os.path.exists(pip):\n    print \"error\", pip, \"is missing\"\n  if sys.version_info < (2, 7):\n    subprocess.call([pip, 'install', 'importlib'])\n  for prog in pip_install_packages:\n    subprocess.call([pip, 'install', prog])\n\n\n\ndef convert(s):\n    b = base64.b64decode(s.encode('ascii'))\n    return zlib.decompress(b).decode('utf-8')\n\n##file site.py\nSITE_PY = convert(\"\"\"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\nUM3ckC/Q8aP8VLy+mjYY5MxOtAdgjULwf2RtvCc=\n\"\"\")\n\n##file ez_setup.py\nEZ_SETUP_PY = convert(\"\"\"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\nqL7+C1AkCcw=\n\"\"\")\n\n##file distribute_from_egg.py\nDISTRIBUTE_FROM_EGG_PY = convert(\"\"\"\neJw9j8tqAzEMRfcG/4MgmxQyptkGusonZBmGoGTUGYFfWPKE6dfXTkM3gqt7rh47OKP3NMF3SQFW\nLlrRU1zhybpAxoKBlIqcrNnBdRjQP3GTocYfzmNrrCPQPN9iwzpxSQfQhWBi0cL3qtRtYIG/4Mv0\nKApY5hooqrOGQ05FQTaxptF9Fnx16Rq0XofjaE1XGXVxHIWK7j8P8EY/rHndLqQ1a0pe3COFgHFy\nhLLdWkDbi/DeEpCjNb3u/zccT2Ob8gtnwVyI\n\"\"\")\n\n##file distribute_setup.py\nDISTRIBUTE_SETUP_PY = convert(\"\"\"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\n\"\"\")\n\n##file activate.sh\nACTIVATE_SH = convert(\"\"\"\neJytVVFvokAQfudXTLEPtTlLeo9tvMSmJpq02hSvl7u2wRUG2QR2DSxSe7n/frOACEVNLlceRHa+\nnfl25pvZDswCnoDPQ4QoTRQsENIEPci4CsBMZBq7CAsuLOYqvmYKTTj3YxnBgiXBudGBjUzBZUJI\nBXEqgCvweIyuCjeG4eF2F5x14bcB9KQiQQWrjSddI1/oQIx6SYYeoFjzWIoIhYI1izlbhJjkKO7D\nM/QEmKfO9O7WeRo/zr4P7pyHwWxkwitcgwpQ5Ej96OX+PmiFwLeVjFUOrNYKaq1Nud3nR2n8nI2m\nk9H0friPTGVsUdptaxGrTEfpNVFEskxpXtUkkCkl1UNF9cgLBkx48J4EXyALuBtAwNYIjF5kcmUU\nabMKmMq1ULoiRbgsDEkTSsKSGFCJ6Z8vY/2xYiSacmtyAfCDdCNTVZoVF8vSTQOoEwSnOrngBkws\nMYGMBMg8/bMBLSYKS7pYEXP0PqT+ZmBT0Xuy+Pplj5yn4aM9nk72JD8/Wi+Gr98sD9eWSMOwkapD\nBbUv91XSvmyVkICt2tmXR4tWmrcUCsjWOpw87YidEC8i0gdTSOFhouJUNxR+4NYBG0MftoCTD9F7\n2rTtxG3oPwY1b2HncYwhrlmj6Wq924xtGDWqfdNxap+OYxplEurnMVo9RWks+rH8qKEtx7kZT5zJ\n4H7oOFclrN6uFe+d+nW2aIUsSgs/42EIPuOhXq+jEo3S6tX6w2ilNkDnIpHCWdEQhFgwj9pkk7FN\nl/y5eQvRSIQ5+TrL05lewxWpt/Lbhes5cJF3mLET1MGhcKCF+40tNWnUulxrpojwDo2sObdje3Bz\nN3QeHqf3D7OjEXMVV8LN3ZlvuzoWHqiUcNKHtwNd0IbvPGKYYM31nPKCgkUILw3KL+Y8l7aO1ArS\nAd37nIU0fCj5NE5gQCuC5sOSu+UdI2NeXg/lFkQIlFpdWVaWZRfvqGiirC9o6liJ9FXGYrSY9mI1\nD/Ncozgn13vJvsznr7DnkJWXsyMH7e42ljdJ+aqNDF1bFnKWFLdj31xtaJYK6EXFgqmV/ymD/ROG\n+n8O9H8f5vsGOWXsL1+1k3g=\n\"\"\")\n\n##file activate.fish\nACTIVATE_FISH = convert(\"\"\"\neJyVVWFv2jAQ/c6vuBoqQVWC9nVSNVGVCaS2VC2rNLWVZZILWAs2s52wVvvxsyEJDrjbmgpK7PP5\n3bt3d22YLbmGlGcIq1wbmCPkGhPYcLMEEsGciwGLDS+YwSjlekngLFVyBe73GXSXxqw/DwbuTS8x\nyyKpFr1WG15lDjETQhpQuQBuIOEKY5O9tlppLqxHKSDByjVAPwEy+mXtCq5MzjIUBTCRgEKTKwFG\ngpBqxTLYXgN2myspVigMaYF92tZSowGZJf4mFExxNs9Qb614CgZtmH0BpEOn11f0cXI/+za8pnfD\n2ZjA1sg9zlV/8QvcMhxbNu0QwgYokn/d+n02nt6Opzcjcnx1vXcIoN74O4ymWQXmHURfJw9jenc/\nvbmb0enj6P5+cuVhqlKm3S0u2XRtRbA2QQAhV7VhBF0rsgUX9Ur1rBUXJgVSy8O751k8mzY5OrKH\nRW3eaQhYGTr8hrXO59ALhxQ83mCsDLAid3T72CCSdJhaFE+fXgicXAARUiR2WeVO37gH3oYHzFKo\n9k7CaPZ1UeNwH1tWuXA4uFKYYcEa8vaKqXl7q1UpygMPhFLvlVKyNzsSM3S2km7UBOl4xweUXk5u\n6e3wZmQ9leY1XE/Ili670tr9g/5POBBpGIJXCCF79L1siarl/dbESa8mD8PL61GpzqpzuMS7tqeB\n1YkALrRBloBMbR9yLcVx7frQAgUqR7NZIuzkEu110gbNit1enNs82Rx5utq7Z3prU78HFRgulqNC\nOTwbqJa9vkJFclQgZSjbKeBgSsUtCtt9D8OwAbIVJuewQdfvQRaoFE9wd1TmCuRG7OgJ1bVXGHc7\nz5WDL/WW36v2oi37CyVBak61+yPBA9C1qqGxzKQqZ0oPuocU9hpud0PIp8sDHkXR1HKkNlzjuUWA\na0enFUyzOWZA4yXGP+ZMI3Tdt2OuqU/SO4q64526cPE0A7ZyW2PMbWZiZ5HamIZ2RcCKLXhcDl2b\nvXL+eccQoRzem80mekPDEiyiWK4GWqZmwxQOmPM0eIfgp1P9cqrBsewR2p/DPMtt+pfcYM+Ls2uh\nhALufTAdmGl8B1H3VPd2af8fQAc4PgqjlIBL9cGQqNpXaAwe3LrtVn8AkZTUxg==\n\"\"\")\n\n##file activate.csh\nACTIVATE_CSH = convert(\"\"\"\neJx9VG1P2zAQ/u5fcYQKNgTNPtN1WxlIQ4KCUEGaxuQ6yYVYSuzKdhqVX7+zk3bpy5YPUXL3PPfc\nne98DLNCWshliVDV1kGCUFvMoJGugMjq2qQIiVSxSJ1cCofD1BYRnOVGV0CfZ0N2DD91DalQSjsw\ntQLpIJMGU1euvPe7QeJlkKzgWixlhnAt4aoUVsLnLBiy5NtbJWQ5THX1ZciYKKWwkOFaE04dUm6D\nr/zh7pq/3D7Nnid3/HEy+wFHY/gEJydg0aFaQrBFgz1c5DG1IhTs+UZgsBC2GMFBlaeH+8dZXwcW\nVPvCjXdlAvCfQsE7al0+07XjZvrSCUevR5dnkVeKlFYZmUztG4BdzL2u9KyLVabTU0bdfg7a0hgs\ncSmUg6UwUiQl2iHrcbcVGNvPCiLOe7+cRwG13z9qRGgx2z6DHjfm/Op2yqeT+xvOLzs0PTKHDz2V\ntkckFHoQfQRXoGJAj9el0FyJCmEMhzgMS4sB7KPOE2ExoLcSieYwDvR+cP8cg11gKkVJc2wRcm1g\nQhYFlXiTaTfO2ki0fQoiFM4tLuO4aZrhOzqR4dIPcWx17hphMBY+Srwh7RTyN83XOWkcSPh1Pg/k\nTXX/jbJTbMtUmcxZ+/bbqOsy82suFQg/BhdSOTRhMNBHlUarCpU7JzBhmkKmRejKOQzayQe6MWoa\nn1wqWmuh6LZAaHxcdeqIlVLhIBJdO9/kbl0It2oEXQj+eGjJOuvOIR/YGRqvFhttUB2XTvLXYN2H\n37CBdbW2W7j2r2+VsCn0doVWcFG1/4y1VwBjfwAyoZhD\n\"\"\")\n\n##file activate.bat\nACTIVATE_BAT = convert(\"\"\"\neJx9UdEKgjAUfW6wfxjiIH+hEDKUFHSKLCMI7kNOEkIf9P9pTJ3OLJ/03HPPPed4Es9XS9qqwqgT\nPbGKKOdXL4aAFS7A4gvAwgijuiKlqOpGlATS2NeMLE+TjJM9RkQ+SmqAXLrBo1LLIeLdiWlD6jZt\nr7VNubWkndkXaxg5GO3UaOOKS6drO3luDDiO5my3iA0YAKGzPRV1ack8cOdhysI0CYzIPzjSiH5X\n0QcvC8Lfaj0emsVKYF2rhL5L3fCkVjV76kShi59NHwDniAHzkgDgqBcwOgTMx+gDQQqXCw==\n\"\"\")\n\n##file deactivate.bat\nDEACTIVATE_BAT = convert(\"\"\"\neJxzSE3OyFfIT0vj4ipOLVEI8wwKCXX0iXf1C7Pl4spMU0hJTcvMS01RiPf3cYmHyQYE+fsGhCho\ncCkAAUibEkTEVhWLMlUlLk6QGixStlyaeCyJDPHw9/Pw93VFsQguim4ZXAJoIUw5DhX47XUM8UCx\nEchHtwsohN1bILUgw61c/Vy4AJYPYm4=\n\"\"\")\n\n##file activate.ps1\nACTIVATE_PS = convert(\"\"\"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\nQastYw==\n\"\"\")\n\n##file distutils-init.py\nDISTUTILS_INIT = convert(\"\"\"\neJytV1uL4zYUfvevOE0ottuMW9q3gVDa3aUMXXbLMlDKMBiNrSTqOJKRlMxkf33PkXyRbGe7Dw2E\nUXTu37lpxLFV2oIyifAncxmOL0xLIfcG+gv80x9VW6maw7o/CANSWWBwFtqeWMPlGY6qPjV8A0bB\nC4eKSTgZ5LRgFeyErMEeOBhbN+Ipgeizhjtnhkn7DdyjuNLPoCS0l/ayQTG0djwZC08cLXozeMss\naG5EzQ0IScpnWtHSTXuxByV/QCmxE7y+eS0uxWeoheaVVfqSJHiU7Mhhi6gULbOHorshkrEnKxpT\n0n3A8Y8SMpuwZx6aoix3ouFlmW8gHRSkeSJ2g7hU+kiHLDaQw3bmRDaTGfTnty7gPm0FHbIBg9U9\noh1kZzAFLaue2R6htPCtAda2nGlDSUJ4PZBgCJBGVcwKTAMz/vJiLD+Oin5Z5QlvDPdulC6EsiyE\nNFzb7McNTKJzbJqzphx92VKRFY1idenzmq3K0emRcbWBD0ryqc4NZGmKOOOX9Pz5x+/l27tP797c\nf/z0d+4NruGNai8uAM0bfsYaw8itFk8ny41jsfpyO+BWlpqfhcG4yxLdi/0tQqoT4a8Vby382mt8\np7XSo7aWGdPBc+b6utaBmCQ7rQKQoWtAuthQCiold2KfJIPTT8xwg9blPumc+YDZC/wYGdAyHpJk\nvUbHbHWAp5No6pK/WhhLEWrFjUwtPEv1Agf8YmnsuXUQYkeZoHm8ogP16gt2uHoxcEMdf2C6pmbw\nhUMsWGhanboh4IzzmsIpWs134jVPqD/c74bZHdY69UKKSn/+KfVhxLgUlToemayLMYQOqfEC61bh\ncbhwaqoGUzIyZRFHPmau5juaWqwRn3mpWmoEA5nhzS5gog/5jbcFQqOZvmBasZtwYlG93k5GEiyw\nbuHhMWLjDarEGpMGB2LFs5nIJkhp/nUmZneFaRth++lieJtHepIvKgx6PJqIlD9X2j6pG1i9x3pZ\n5bHuCPFiirGHeO7McvoXkz786GaKVzC9DSpnOxJdc4xm6NSVq7lNEnKdVlnpu9BNYoKX2Iq3wvgh\ngGEUM66kK6j4NiyoneuPLSwaCWDxczgaolEWpiMyDVDb7dNuLAbriL8ig8mmeju31oNvQdpnvEPC\n1vAXbWacGRVrGt/uXN/gU0CDDwgooKRrHfTBb1/s9lYZ8ZqOBU0yLvpuP6+K9hLFsvIjeNhBi0KL\nMlOuWRn3FRwx5oHXjl0YImUx0+gLzjGchrgzca026ETmYJzPD+IpuKzNi8AFn048Thd63OdD86M6\n84zE8yQm0VqXdbbgvub2pKVnS76icBGdeTHHXTKspUmr4NYo/furFLKiMdQzFjHJNcdAnMhltBJK\n0/IKX3DVFqvPJ2dLE7bDBkH0l/PJ29074+F0CsGYOxsb7U3myTUncYfXqnLLfa6sJybX4g+hmcjO\nkMRBfA1JellfRRKJcyRpxdS4rIl6FdmQCWjo/o9Qz7yKffoP4JHjOvABcRn4CZIT2RH4jnxmfpVG\nqgLaAvQBNfuO6X0/Ux02nb4FKx3vgP+XnkX0QW9pLy/NsXgdN24dD3LxO2Nwil7Zlc1dqtP3d7/h\nkzp1/+7hGBuY4pk0XD/0Ao/oTe/XGrfyM773aB7iUhgkpy+dwAMalxMP0DrBcsVw/6p25+/hobP9\nGBknrWExDhLJ1bwt1NcCNblaFbMKCyvmX0PeRaQ=\n\"\"\")\n\n##file distutils.cfg\nDISTUTILS_CFG = convert(\"\"\"\neJxNj00KwkAMhfc9xYNuxe4Ft57AjYiUtDO1wXSmNJnK3N5pdSEEAu8nH6lxHVlRhtDHMPATA4uH\nxJ4EFmGbvfJiicSHFRzUSISMY6hq3GLCRLnIvSTnEefN0FIjw5tF0Hkk9Q5dRunBsVoyFi24aaLg\n9FDOlL0FPGluf4QjcInLlxd6f6rqkgPu/5nHLg0cXCscXoozRrP51DRT3j9QNl99AP53T2Q=\n\"\"\")\n\n##file activate_this.py\nACTIVATE_THIS = convert(\"\"\"\neJyNU01v2zAMvetXEB4K21jmDOstQA4dMGCHbeihlyEIDMWmG62yJEiKE//7kXKdpN2KzYBt8euR\nfKSyLPs8wiEo8wh4wqZTGou4V6Hm0wJa1cSiTkJdr8+GsoTRHuCotBayiWqQEYGtMCgfD1KjGYBe\n5a3p0cRKiAe2NtLADikftnDco0ko/SFEVgEZ8aRC5GLux7i3BpSJ6J1H+i7A2CjiHq9z7JRZuuQq\nsiwTIvpxJYCeuWaBpwZdhB+yxy/eWz+ZvVSU8C4E9FFZkyxFsvCT/ZzL8gcz9aXVE14Yyp2M+2W0\ny7n5mp0qN+avKXvbsyyzUqjeWR8hjGE+2iCE1W1tQ82hsCZN9UzlJr+/e/iab8WfqsmPI6pWeUPd\nFrMsd4H/55poeO9n54COhUs+sZNEzNtg/wanpjpuqHJaxs76HtZryI/K3H7KJ/KDIhqcbJ7kI4ar\nXL+sMgXnX0D+Te2Iy5xdP8yueSlQB/x/ED2BTAtyE3K4SYUN6AMNfbO63f4lBW3bUJPbTL+mjSxS\nPyRfJkZRgj+VbFv+EzHFi5pKwUEepa4JslMnwkowSRCXI+m5XvEOvtuBrxHdhLalG0JofYBok6qj\nYdN2dEngUlbC4PG60M1WEN0piu7Nq7on0mgyyUw3iV1etLo6r/81biWdQ9MWHFaePWZYaq+nmp+t\ns3az+sj7eA0jfgPfeoN1\n\"\"\")\n\nMH_MAGIC = 0xfeedface\nMH_CIGAM = 0xcefaedfe\nMH_MAGIC_64 = 0xfeedfacf\nMH_CIGAM_64 = 0xcffaedfe\nFAT_MAGIC = 0xcafebabe\nBIG_ENDIAN = '>'\nLITTLE_ENDIAN = '<'\nLC_LOAD_DYLIB = 0xc\nmaxint = majver == 3 and getattr(sys, 'maxsize') or getattr(sys, 'maxint')\n\n\nclass fileview(object):\n    \"\"\"\n    A proxy for file-like objects that exposes a given view of a file.\n    Modified from macholib.\n    \"\"\"\n\n    def __init__(self, fileobj, start=0, size=maxint):\n        if isinstance(fileobj, fileview):\n            self._fileobj = fileobj._fileobj\n        else:\n            self._fileobj = fileobj\n        self._start = start\n        self._end = start + size\n        self._pos = 0\n\n    def __repr__(self):\n        return '<fileview [%d, %d] %r>' % (\n            self._start, self._end, self._fileobj)\n\n    def tell(self):\n        return self._pos\n\n    def _checkwindow(self, seekto, op):\n        if not (self._start <= seekto <= self._end):\n            raise IOError(\"%s to offset %d is outside window [%d, %d]\" % (\n                op, seekto, self._start, self._end))\n\n    def seek(self, offset, whence=0):\n        seekto = offset\n        if whence == os.SEEK_SET:\n            seekto += self._start\n        elif whence == os.SEEK_CUR:\n            seekto += self._start + self._pos\n        elif whence == os.SEEK_END:\n            seekto += self._end\n        else:\n            raise IOError(\"Invalid whence argument to seek: %r\" % (whence,))\n        self._checkwindow(seekto, 'seek')\n        self._fileobj.seek(seekto)\n        self._pos = seekto - self._start\n\n    def write(self, bytes):\n        here = self._start + self._pos\n        self._checkwindow(here, 'write')\n        self._checkwindow(here + len(bytes), 'write')\n        self._fileobj.seek(here, os.SEEK_SET)\n        self._fileobj.write(bytes)\n        self._pos += len(bytes)\n\n    def read(self, size=maxint):\n        assert size >= 0\n        here = self._start + self._pos\n        self._checkwindow(here, 'read')\n        size = min(size, self._end - here)\n        self._fileobj.seek(here, os.SEEK_SET)\n        bytes = self._fileobj.read(size)\n        self._pos += len(bytes)\n        return bytes\n\n\ndef read_data(file, endian, num=1):\n    \"\"\"\n    Read a given number of 32-bits unsigned integers from the given file\n    with the given endianness.\n    \"\"\"\n    res = struct.unpack(endian + 'L' * num, file.read(num * 4))\n    if len(res) == 1:\n        return res[0]\n    return res\n\n\ndef mach_o_change(path, what, value):\n    \"\"\"\n    Replace a given name (what) in any LC_LOAD_DYLIB command found in\n    the given binary with a new name (value), provided it's shorter.\n    \"\"\"\n\n    def do_macho(file, bits, endian):\n        # Read Mach-O header (the magic number is assumed read by the caller)\n        cputype, cpusubtype, filetype, ncmds, sizeofcmds, flags = read_data(file, endian, 6)\n        # 64-bits header has one more field.\n        if bits == 64:\n            read_data(file, endian)\n        # The header is followed by ncmds commands\n        for n in range(ncmds):\n            where = file.tell()\n            # Read command header\n            cmd, cmdsize = read_data(file, endian, 2)\n            if cmd == LC_LOAD_DYLIB:\n                # The first data field in LC_LOAD_DYLIB commands is the\n                # offset of the name, starting from the beginning of the\n                # command.\n                name_offset = read_data(file, endian)\n                file.seek(where + name_offset, os.SEEK_SET)\n                # Read the NUL terminated string\n                load = file.read(cmdsize - name_offset).decode()\n                load = load[:load.index('\\0')]\n                # If the string is what is being replaced, overwrite it.\n                if load == what:\n                    file.seek(where + name_offset, os.SEEK_SET)\n                    file.write(value.encode() + '\\0'.encode())\n            # Seek to the next command\n            file.seek(where + cmdsize, os.SEEK_SET)\n\n    def do_file(file, offset=0, size=maxint):\n        file = fileview(file, offset, size)\n        # Read magic number\n        magic = read_data(file, BIG_ENDIAN)\n        if magic == FAT_MAGIC:\n            # Fat binaries contain nfat_arch Mach-O binaries\n            nfat_arch = read_data(file, BIG_ENDIAN)\n            for n in range(nfat_arch):\n                # Read arch header\n                cputype, cpusubtype, offset, size, align = read_data(file, BIG_ENDIAN, 5)\n                do_file(file, offset, size)\n        elif magic == MH_MAGIC:\n            do_macho(file, 32, BIG_ENDIAN)\n        elif magic == MH_CIGAM:\n            do_macho(file, 32, LITTLE_ENDIAN)\n        elif magic == MH_MAGIC_64:\n            do_macho(file, 64, BIG_ENDIAN)\n        elif magic == MH_CIGAM_64:\n            do_macho(file, 64, LITTLE_ENDIAN)\n\n    assert(len(what) >= len(value))\n    do_file(open(path, 'r+b'))\n\n\nif __name__ == '__main__':\n    main()\n\n## TODO:\n## Copy python.exe.manifest\n## Monkeypatch distutils.sysconfig\n", "name = name\n\n    def getName(self):\n        return self.name\n\n    def greet(self):\n        print('Hello, world! I\\'m {0}'.format(self.name))\n\nfoo = Person()\nbar = Person()\n\nfoo.setName('Luke')\nbar.setName('Alan')\n\nfoo.greet()  # Hello, world! I'm Luke\nbar.greet()  # Hello, world! I'm Alan\n\n\n# self\u53c2\u6570\u5e76\u4e0d\u53d6\u51b3\u4e8e\u8c03\u7528\u65b9\u6cd5\u7684\u65b9\u5f0f\uff0c\u76ee\u524d\u4f7f\u7528\u7684\u662f\u5b9e\u4f8b\u8c03\u7528\u65b9\u6cd5\n# \u53ef\u4ee5\u968f\u610f\u4f7f\u7528\u5f15\u7528\u540c\u4e00\u4e2a\u65b9\u6cd5\u7684\u5176\u4ed6\u53d8\u91cf\nclass Bird:\n    song = 'Squaawk!'\n\n    def sing(self):\n        print(self.song)\n\nbird = Bird()\nbird.sing()     # Squaawk!\nbirdsong = bird.sing\nbirdsong()      # Squaawk!\n\n\n# Python \u5e76\u4e0d\u76f4\u63a5\u652f\u6301\u79c1\u6709\u65b9\u5f0f\uff0c\u800c\u8981\u9760\u7a0b\u5e8f\u5458\u81ea\u5df1\u628a\u63e1\u5728\u5916\u90e8\u8fdb\u884c\u7279\u6027\u4fee\u6539\u7684\u65f6\u673a\n# \u6bd5\u7adf\u5728\u4f7f\u7528\u5bf9\u8c61\u524d\u5e94\u8be5\u77e5\u9053\u5982\u4f55\u4f7f\u7528\n\n# \u8fbe\u5230\u79c1\u6709\u7279\u6027\u7684\u5c0f\u6280\u5de7\uff1a\n# \u4e3a\u4e86\u8ba9\u65b9\u6cd5\u6216\u8005\u7279\u6027\u53d8\u4e3a\u79c1\u6709\uff08\u4ece\u5916\u90e8\u65e0\u6cd5\u8bbf\u95ee\uff09\uff0c\u53ea\u8981\u5728\u5b83\u7684\u540d\u5b57\u524d\u9762\u52a0\u4e0a\u53cc\u4e0b\u5212\u7ebf\u5373\u53ef\nclass Secretive:\n\n    def __inaccessible(self):\n        print('Bet you can\\'t see me...')\n\n    def accessible(self):\n        print('The secret message is: ')\n        self.__inaccessible()\n\ns = Secretive()\n# s.__inaccesible()     # \u62a5\u9519\uff0c\u65e0\u6cd5\u8bbf\u95ee\ns.accessible()\n\n\n# \u7c7b\u7684\u547d\u540d\u7a7a\u95f4\nclass MemberCounter:\n    members = 0    # \u5728\u7c7b\u4f5c\u7528\u57df\u5185\u5b9a\u4e49\u4e86\u4e00\u4e2a\u53ef\u4f9b\u6240\u6709\u6210\u5458\uff08\u5b9e\u4f8b\uff09\u8bbf\u95ee\u7684\u53d8\u91cf\n\n    def init(self):\n        MemberCounter.members += 1\n\nm1 = MemberCounter()\nm1.init()\n\nprint(MemberCounter.members)    # 1\nprint(m1.members)   # 1\n\nm2 = MemberCounter()\nm2.init()\n\nprint(MemberCounter.members)    # 2\nprint(m2.members)   # 2\n\n\n# \u7528\u5b50\u7c7b\u6269\u5c55\u8d85\u7c7b\u7684\u5b9a\u4e49\n# \u5c06\u5176\u4ed6\u7c7b\u540d\u5199\u5728class\u8bed\u53e5\u540e\u7684\u5706\u62ec\u53f7\u5185\u53ef\u4ee5\u6307\u5b9a\u8d85\u7c7b\nclass Filter:\n\n    def init(self):\n        self.blocked = []\n\n    def filter(self, sequence):\n        return [x for x in sequence if x not in self.blocked]\n\n\nclass SPAMFilter(Filter):\n    def init(self):     # \u91cd\u5199\u7236\u7c7b\u7684init\u65b9\u6cd5\n        self.blocked = ['SPAM']\n\n# Filter \u662f\u7528\u4e8e\u8fc7\u6ee4\u5e8f\u5217\u7684\u901a\u7528\u7c7b\uff0c\u4e8b\u5b9e\u4e0a\u5b83\u4e0d\u80fd\u8fc7\u6ee4\u4efb\u4f55\u4e1c\u897f\nf = Filter()\nf.init()\nprint(f.filter([1, 2, 3]))  # [1, 2, 3]\n\n# Filter\u7c7b\u7684\u7528\u5904\u5728\u4e8e\u5b83\u53ef\u4ee5\u7528\u4f5c\u5176\u4ed6\u7c7b\u7684\u57fa\u7c7b\uff08\u8d85\u7c7b\uff09\n# \u6bd4\u5982SPAMFilter\u7c7b\uff0c\u53ef\u4ee5\u5c06\u5e8f\u5217\u4e2d\u7684\u300cSPAM\u300d\u8fc7\u6ee4\u51fa\u53bb\ns = SPAMFilter()\ns.init()\nprint(s.filter(['SPAM', 'SPAM', 'egg', 'SPAM', 'SPAM', 'bacon']))   # ['egg', 'bacon']\n\n\n# \u67e5\u770b\u4e00\u4e2a\u7c7b\u662f\u5426\u662f\u53e6\u4e00\u4e2a\u7684\u5b50\u7c7b\uff0c\u53ef\u4ee5\u4f7f\u7528\u5185\u5efa\u7684issubclass\u51fd\u6570\nprint(issubclass(SPAMFilter, Filter))   # True\nprint(issubclass(Filter, SPAMFilter))   # False\n\n# \u68c0\u67e5\u4e00\u4e2a\u5bf9\u8c61\u662f\u5426\u662f\u4e00\u4e2a\u7c7b\u7684\u5b9e\u4f8b\uff0c\u53ef\u4ee5\u4f7f\u7528isinstance\u65b9\u6cd5\nprint(isinstance(s, SPAMFilter))    # True\nprint(isinstance(s, Filter))        # True\nprint(isinstance(s, str))           # False\n\n# \u67e5\u770b\u4e00\u4e2a\u5bf9\u8c61\u5c5e\u4e8e\u54ea\u4e2a\u7c7b\uff0c\u53ef\u4ee5\u4f7f\u7528__class__\u7279\u6027\nprint(s.__class__)  # <class '__main__.SPAMFilter'>\n\n\n# 7.2.7 \u591a\u4e2a\u8d85\u7c7b\u3001\u591a\u91cd\u7ee7\u627f\n# \u5b50\u7c7b\u81ea\u5df1\u53ef\u4ee5\u4e0d\u505a\u4efb\u4f55\u4e8b\uff0c\u5b83\u4ece\u81ea\u5df1\u7684\u8d85\u7c7b\u7ee7\u627f\u6240\u6709\u7684\u884c\u4e3a\nprint('------- \u591a\u91cd\u7ee7\u627f -------')\n\n\nclass Calculator:\n\n    def calculate(self, expression):\n        self.value = eval(expression)\n\n\nclass Talker:\n\n    def talk(self):\n        print('Hi, my value is {0}'.format(self.value))\n\n\nclass TalkingCalculator(Calculator, Talker):\n    pass\n\ntc = TalkingCalculator()\ntc.calculate('1 + 2 + 3 * 4')\ntc.talk()   # Hi, my value is 15\n\n\n# 7.2.8 \u63a5\u53e3\u548c\u5185\u7701\n# \u63a5\u53e3\u7684\u6982\u5ff5\u4e0e\u591a\u6001\u6709\u5173\uff0c\u5728\u5904\u7406\u591a\u6001\u5bf9\u8c61\u65f6\uff0c\u53ea\u8981\u5173\u5fc3\u5b83\u7684\u63a5\u53e3\uff08\u6216\u79f0\uff1a\u534f\u8bae\uff09\u5373\u53ef\n# \u4e5f\u5c31\u662f\u516c\u5f00\u7684\u65b9\u6cd5\u548c\u7279\u6027\n# \u5728python\u4e2d\uff0c\u4e0d\u7528\u663e\u5f0f\u5730\u6307\u5b9a\u5bf9\u8c61\u5fc5\u987b\u5305\u542b\u54ea\u4e9b\u65b9\u6cd5\u624d\u80fd\u4f5c\u4e3a\u53c2\u6570\u63a5\u6536\nprint(hasattr(tc, 'talk'))   # True\nprint(hasattr(tc, 'fnord'))  # False\n\n# \u76f8\u5bf9\u5e94\u7684\uff0c\u53ef\u4ee5\u7528setattr\u6765\u8bbe\u7f6e\u5bf9\u8c61\u7684\u7279\u6027\nprint(tc.__dict__)   # {'value': 15}\nsetattr(tc, 'name', 'Mr. Gumby')\nprint(tc.__dict__)   # {'value': 15, 'name': 'Mr. Gumby'}", "\nfrom chainer import gradient_check\nfrom chainer import testing\nfrom chainer.testing import attr\n\n\nclass TestReshape(unittest.TestCase):\n    out_shape = (2, 2, 6)\n\n    def setUp(self):\n        self.x = numpy.random.uniform(-1, 1, (4, 3, 2)).astype(numpy.float32)\n        self.gy = numpy.random.uniform(-1, 1, (2, 2, 6)).astype(numpy.float32)\n\n    def check_forward(self, x_data):\n        shape = self.out_shape\n        x = chainer.Variable(x_data)\n        y = functions.reshape(x, shape)\n        self.assertEqual(y.data.dtype, numpy.float32)\n        self.assertTrue((self.x.reshape(shape) == cuda.to_cpu(y.data)).all())\n\n    def test_forward_cpu(self):\n        self.check_forward(self.x)\n\n    @attr.gpu\n    def test_forward_gpu(self):\n        self.check_forward(cuda.to_gpu(self.x))\n\n    def check_backward(self, x_data, y_grad):\n        gradient_check.check_backward(\n            functions.Reshape(self.gy.shape), x_data, y_grad)\n\n\nclass TestReshapeUnknownDimension(TestReshape):\n    out_shape = (2, -1, 6)\n\n\ntesting.run_module(__name__, __file__)\n", "ding} \".sub\" TWOPASS\nglobal tmpdir\nimport os\n#os.system('mkdir -p ' + tmpdir)\nastrom = 'solve-field'\nimport traceback, tempfile\n\ndef check_scamp(cluster):\n    ''' look for catalog mismatches '''\n    from glob import glob    \n    import os, re, sys, commands, MySQLdb\n                                                                                                 \n    db2 = MySQLdb.connect(db='subaru', user='weaklensing', passwd='darkmatter', host='ki-rh8')\n    cdb = db2.cursor()\n                                                                                                 \n    #info = 'Zw7160 W-S-I+' \n    filters = ['W-S-I+','W-C-IC','W-J-V','u','W-C-RC','g','W-S-Z+','W-J-B'] #re.split('\\s+',info)[1:]       \n    print filters\n                                                                                                 \n    upperlimit = 20000\n    for filter in filters:\n        imlist = glob(os.environ['subdir'] + '/' + cluster + '/' + filter + '/SCIENCE/*.fits' )\n\n        #cat = glob(os.environ['subdir'] + '/' + cluster + '/' + filter + '/SCIENCE/cat/*.fits' )\n\n        catlist = glob(os.environ['subdir'] + '/' + cluster + '/' + filter + '/SCIENCE/cat_scamp/*.cat' )\n\n        for f in catlist:\n            file = f.replace('cat_scamp','cat_scampIC').replace('.cat','*.cat')\n            if not glob(file):\n                print file, 'missing'\n                os.system('dfits ' + file.replace('/cat_scampIC','').replace('.cat','.fits') + ' | fitsort BADCCD')\n\n        catIClist = glob(os.environ['subdir'] + '/' + cluster + '/' + filter + '/SCIENCE/cat_scampIC/*.cat' )\n\n        print len(imlist), len(catlist), len(catIClist), filter\n        #raw_input()\n\n\n\ndef prep_scamp(info=None):\n    ''' throw out entire exposures were chip is above 24000 '''\n    from glob import glob    \n    import os, re, sys, commands, MySQLdb\n\n    db2 = MySQLdb.connect(db='subaru', user='weaklensing', passwd='darkmatter', host='ki-rh8')\n    cdb = db2.cursor()\n\n    #info = 'Zw7160 W-S-I+' \n    filters = re.split('\\s+',info)[1:]       \n    cluster = re.split('\\s+',info)[0]              \n    print filters\n\n    upperlimit = 20000\n    for filter in filters:\n        imlist = glob(os.environ['subdir'] + '/' + cluster + '/' + filter + '/SCIENCE/*I.fits' )\n        print imlist\n        ims = {} \n        for imchip in imlist:\n            im = re.split('_',imchip)[0]\n            if not im in ims: ims[im] = [imchip]\n            else: ims[im].append(imchip)\n            \n        print ims.keys()       \n                                                                                                                \n        for im in ims.keys(): \n            bad = 0\n            max = 0\n            min = 100000\n        \n            level = {}\n     \n            for imchip in ims[im]:\n                print imchip\n                c = commands.getoutput('imstats ' + imchip )\n                r1 = re.split('\\n',c)\n                print r1\n                #res = re.split(c[-1]\n                r2 = re.split('\\t',r1[-1])\n                print r2 \n                mode = float(r2[1])\n                level[imchip] = mode\n                print mode\n                if mode > max: max = mode\n                if mode < min: min = mode\n                if mode > 23500:\n                    bad += 1\n                if mode < 1: \n                    raise Exception \n                if mode < 15000 and bad==0: break\n            if bad > 0:\n                ''' set all of the chips to badccd '''                \n                for imchip in ims[im]: \n                    os.system('sethead ' + imchip + ' BADCCD=1')\n                    command = 'delete from badccd_db where CHIP=\"' + imchip + '\"'\n                    print command\n                    cdb.execute(command)\n\n                    command = 'insert into badccd_db (CHIP,CLUSTER,FILTER,MODE) VALUES (\"' + imchip + '\",\"' + cluster + '\",\"' + filter+'\",\"' + str(level[imchip]) + '\")'\n                    print command\n                    cdb.execute(command)\n            elif max > 20000:\n                upperlimit = 25000\n\n            if max - min > 5000:\n                print max-min, 'maxmin'\n                raise Exception\n\n    command = 'update clusters_db set badccd=\"yes\" where objname=\"' + cluster + '\"'\n    cdb.execute(command)\n    command = 'update clusters_db set upperlimit=\"' + str(upperlimit) + '\" where objname=\"' + cluster + '\"'\n    cdb.execute(command)\n    \n    return upperlimit\n\n\ndef run_scamp(OBJNAME=None):\n    import MySQLdb, sys, os, re, time, utilities, pyfits, string                                                                                                                          \n    from copy import copy\n\n    loop = True\n    while loop:        \n        \n        db2 = MySQLdb.connect(db='subaru', user='weaklensing', passwd='darkmatter', host='ki-rh8')\n        c = db2.cursor()\n\n\n        db_keys_t = describe_db(c,['clusters_db'])\n\n        if OBJNAME is None:\n            command='SELECT * from clusters_db c join try_db t on c.objname=t.objname where (checkedoff is null or checkedoff!=\"yes\") and scampstatus is null group by c.objname order by rand()'  \n        else:\n            command='SELECT * from clusters_db c join try_db t on c.objname=t.objname where c.OBJNAME=\"' + OBJNAME + '\"' \n            loop = False\n\n        print command                                                  \n        c.execute(command)                                             \n        results=c.fetchall()                                           \n        random_dict = {}\n        line = results[0]\n        dtop2 = {}  \n        for i in range(len(db_keys_t)):\n            dtop2[db_keys_t[i]] = str(line[i])\n        command='SELECT * from try_db where (correction_applied is null or correction_applied!=\"finished\") and objname=\"' + dtop2['objname'] + '\"'  \n        print command                                                  \n        c.execute(command)                                             \n        results=c.fetchall()   \n        print results\n        from datetime import datetime\n        if len(results) > 0:\n            commandst = 'update clusters_db set scampstatus=\"not ready\" where objname=\"' + dtop2['objname'] + '\"'\n            c.execute(commandst)                                             \n        else:\n            os.system('mkdir -p ' + os.environ['sne'] + '/scamp/')\n            logfile = os.environ['sne'] + '/scamp/' + dtop2['objname']\n\n            commandst = 'update clusters_db set scampstatus=\"started ' + str(datetime.now()) + '\" where objname=\"' + dtop2['objname'] + '\"'\n            c.execute(commandst)                                             \n                                                                                                                                            \n            commandst = 'update clusters_db set logfile=\"' + logfile + '\" where objname=\"' + dtop2['objname'] + '\"'\n            c.execute(commandst)                                             \n\n            try:\n                if dtop2['badccd'] != 'yes':                                                                                      \n                    upperlimit = prep_scamp(dtop2['info'])\n                else: upperlimit = dtop2['upperlimit']\n\n                #upperlimit = prep_scamp(dtop2['info'])\n                                                                                                                                  \n                print upperlimit\n                                                                                                                                  \n                if __name__ == '__main__': \n                    command = 'do_Subaru_register_4batch_level.sh ' + str(upperlimit) + ' ' + dtop2['info'] + ' >& ' + logfile\n                else:\n                    command = 'do_Subaru_register_4batch_level.sh ' + str(upperlimit) + ' ' + dtop2['info'] #+ ' >& ' + logfile\n                print command, dtop2['info']\n\n                raw_input()\n\n            \n            \n\n            \n            \n\n\n                os.chdir(os.environ['bonn'])                                                                             \n                os.putenv('INSTRUMENT','SUBARU')\n                os.system('setenv INSTRUMENT SUBARU') \n                print os.environ['INSTRUMENT']\n                print command\n                a = os.system(command)\n                if float(a) != 0: \n                    commandst = 'update clusters_db set scampstatus=\"failed ' + str(datetime.now()) + '\" where objname=\"' + dtop2['objname'] + '\"'\n                else:\n                    commandst = 'update clusters_db set scampstatus=\"finished\" where objname=\"' + dtop2['objname'] + '\"'\n                c.execute(commandst)                                             \n            except:\n                import traceback\n                print traceback.print_exc(file=sys.stdout)\n                commandst = 'update clusters_db set scampstatus=\"failed ' + str(datetime.now()) + '\" where objname=\"' + dtop2['objname'] + '\"'\n                c.execute(commandst)                                             \n        \ndef run_correction(OBJNAME=None,FILTER=None,PPRUN=None):\n    import MySQLdb, sys, os, re, time, utilities, pyfits, string                                                                                                                          \n    from copy import copy\n\n    loop = True\n    while loop:        \n        \n        db2 = MySQLdb.connect(db='subaru', user='weaklensing', passwd='darkmatter', host='ki-rh8')\n        c = db2.cursor()\n\n\n        db_keys_t = describe_db(c,['try_db'])\n        command=\"SELECT * from try_db where todo='good' and var_correction > 0.08 order by rand()\"           \n        command='SELECT * from try_db i where i.todo=\"good\" and i.correction_applied!=\"yes\" and (i.objname like \"MACS0018%\" or i.objname like \"MACS0025%\" or i.objname like \"MACS0257%\" or i.objname like \"MACS0454%\" or i.objname like \"MACS0647%\" or i.objname like \"MACS0717%\" or i.objname like \"MACS0744%\" or i.objname like \"MACS0911%\" or i.objname like \"MACS1149%\" or i.objname like \"MACS1423%\" or i.objname like \"MACS2129%\" or i.objname like \"MACS2214%\" or i.objname like \"MACS2243%\" or i.objname like \"A2219\" or i.objname like \"A2390\") order by rand()'           \n        command='SELECT * from try_db i where i.correction_applied is null and not (i.objname like \"MACS0018%\" or i.objname like \"MACS0025%\" or i.objname like \"MACS0257%\" or i.objname like \"MACS0454%\" or i.objname like \"MACS0647%\" or i.objname like \"MACS0717%\" or i.objname like \"MACS0744%\" or i.objname like \"MACS0911%\" or i.objname like \"MACS1149%\" or i.objname like \"MACS1423%\" or i.objname like \"MACS2129%\" or i.objname like \"MACS2214%\" or i.objname like \"MACS2243%\" or i.objname like \"A2219\" or i.objname like \"A2390\") order by rand() limit 1'           \n\n        command='SELECT * from clusters_db c join try_db t on c.objname=t.objname where (checkedoff is null or checkedoff!=\"yes\") and correction_applied is null group by c.objname order by rand()'  \n\n        command='SELECT * from try_db where correction_applied=\"redo\" group by objname order by rand()'  \n\n\n        command='SELECT * from try_db where correction_applied is null and (OBJNAME=\"MACS0850+36\") order by rand()'  \n\n        #command='SELECT * from try_db i where i.correction_applied is null and (i.objname like \"MACS0018%\" or i.objname like \"MACS0025%\" or i.objname like \"MACS0257%\" or i.objname like \"MACS0454%\") order by rand() limit 1 '           \n\n        if OBJNAME is not None:\n            command='SELECT * from try_db i where OBJNAME=\"' + OBJNAME + '\" and PPRUN=\"' + PPRUN + '\" limit 1'           \n            loop = False\n        #command='SELECT * from try_db i where (i.objname like \"MACS0018%\") and i.pprun like \"%2009%\" order by rand()'           \n        #command='SELECT * from try_db i where (i.sdssstatus like \"%finished\" or i.Nonestatus like \"%finished\") and (i.objname like \"A2219%\") order by rand()'           \n        print command                                                  \n        c.execute(command)                                             \n        results=c.fetchall()                                           \n        random_dict = {}\n        line = results[0]\n        if 1:\n            dtop2 = {}                                                                                                                                                          \n            for i in range(len(db_keys_t)):\n                dtop2[db_keys_t[i]] = str(line[i])\n\n            path='/nfs/slac/g/ki/ki05/anja/SUBARU/%(OBJNAME)s/' % {'OBJNAME':dtop2['OBJNAME']}  \n            illum_dir = path + 'PHOTOMETRY/ILLUMINATION/' + dtop2['FILTER'] + '/' + dtop2['PPRUN'] + '/'\n            os.system('mkdir -p ' + illum_dir)\n            #logfile  = open(illum_dir + 'logfile','w')\n            print illum_dir + 'logfile'\n\n            OBJNAME_use, FILTER_use, PPRUN_use = dtop2['OBJNAME'], dtop2['FILTER'], dtop2['PPRUN']\n    \n            sample = 'notselected' \n\n            ''' if no bootstrap use good fit '''\n            if dtop2['todo'] == 'good' and (string.find(dtop2['sdssstatus'],'finished') != -1 or string.find(dtop2['Nonestatus'],'finished')):\n                if string.find(dtop2['sdssstatus'],'finished') != -1: \n                    sample = 'sdss'\n                if string.find(dtop2['Nonestatus'],'finished') != -1:\n                    sample = 'None'\n            elif dtop2['todo'] == 'bootstrap' and str(dtop2['todo']) == 'True'  : \n                sample = 'bootstrap'\n            \n            if sample == 'notselected':\n                OBJNAME_use, FILTER_use, PPRUN_use, sample = find_nearby(dtop2['OBJNAME'],dtop2['FILTER'],dtop2['PPRUN'])\n                print 'find' \n\n            print  'parameters:', sample, dtop2['sdssstatus'], dtop2['Nonestatus'], dtop2['bootstrapstatus'], dtop2['todo'],sample, OBJNAME_use, FILTER_use, PPRUN_use, dtop2['OBJNAME'], dtop2['FILTER'], dtop2['PPRUN']\n            if sample!='notselected' and sample!=None:\n\n                import sys               \n                #stderr_orig = sys.stderr\n                #stdout_orig = sys.stdout\n                #sys.stdout = logfile \n                #sys.stderr = logfile \n\n                print dtop2['OBJNAME'],dtop2['FILTER'],dtop2['PPRUN'],sample,'all',OBJNAME_use,FILTER_use,PPRUN_use\n                raw_input()\n\n                construct_correction(dtop2['OBJNAME'],dtop2['FILTER'],dtop2['PPRUN'],sample,'all',OBJNAME_use,FILTER_use,PPRUN_use)\n\n                #sys.stderr = stderr_orig  \n                #sys.stdout = stdout_orig\n                #logfile.close()\n\n            else:                \n                save_fit({'PPRUN':dtop2['PPRUN'],'OBJNAME':dtop2['OBJNAME'],'FILTER':dtop2['FILTER'],'sample':'record','sample_size':'record','correction_applied':'no match'},db='try_db')\n\n\n\n    #if 0: #help_list[y]['primary']==None or help_list[y]['secondary']==None:\n    \ndef find_nearby(OBJNAME,FILTER,PPRUN):\n    ''' figure out the right (closest) correction to apply '''\n    import MySQLdb, sys, os, re, time, utilities, pyfits                                                                                                                          \n    from copy import copy\n    db2 = MySQLdb.connect(db='subaru', user='weaklensing', passwd='darkmatter', host='ki-rh8')\n    c = db2.cursor()\n\n    db_keys_t = describe_db(c,['illumination_db'])\n    command=\"SELECT * from illumination_db where PPRUN='\" + PPRUN + \"' and OBJNAME='\" + OBJNAME + \"'\" # and sample_size='all'\" # and sample='sdss'\"\n    print command                                                  \n    c.execute(command)                                             \n    results=c.fetchall()                                           \n    print len(results)\n    for line in results:\n        dtop = {}                                                                                                                                                          \n        for i in range(len(db_keys_t)):\n            dtop[db_keys_t[i]] = str(line[i])\n\n    db_keys = describe_db(c,['fit_db','try_db'])        \n    print OBJNAME\n    ''' select runs with little cloud cover '''\n    #command=\"SELECT * from fit_db f left join try_db t on (t.pprun=f.pprun and t.OBJNAME=f.OBJNAME) where t.zpstd<0.01 and (t.mean - 1.5*t.std) > 1.005 and t.var_correction < 0.08 and f.sample_size='all' and f.sample='sdss' and f.CONFIG='\" + dtop['CONFIG'] + \"' and f.FILTER='\" + dtop['FILTER'] + \"'\"           \n\n    ''' pick runs with good statistics and no zp variations '''\n    if dtop['CONFIG'] == '10_3':\n    \n        from config_bonn import wavelength_groups, wavelength_order\n        ''' '''\n\n        for i in range(len(wavelength_groups)):\n            for filt in wavelength_groups[i]:\n                if filt == dtop['filter']: \n                    FILTER_NUM_ZERO = i\n                    break\n\n        command=\"SELECT * from fit_db f left join try_db t on (t.pprun=f.pprun and t.OBJNAME=f.OBJNAME) where f.CONFIG='\" + dtop['CONFIG'] + \"'\"           \n        print command                                                                                                                                                                                                                                                                                                    \n        c.execute(command)                                             \n        results=c.fetchall()                                           \n        use = []\n        print len(results), ' # of results '\n        for line in results:\n            dp = {}                                                                                                                                                          \n            for i in range(len(db_keys)):\n                dp[db_keys[i]] = str(line[i])\n\n\n            for i in range(len(wavelength_groups)):\n                for filt in wavelength_groups[i]:\n                    if filt == dp['FILTER']: \n                        FILTER_NUM = i\n                        break\n\n            use.append([abs(FILTER_NUM - FILTER_NUM_ZERO),dp])\n\n        use.sort()\n       \n        use = [x[1] for x in use]                                                                                                                                                                                                                                                                                                                  \n    else:\n        ''' use B filter if U '''\n        if dtop['filter'] == 'W-J-U': filter = 'W-J-B'\n        else: filter = dtop['filter']\n\n        ''' use 10_2 if 10_1 and W-J-B '''        \n        if dtop['CONFIG'] == '10_1' and filter == 'W-J-B':\n            dtop['CONFIG'] = '10_2'\n\n\n        db_keys = describe_db(c,['try_db'])        \n\n        command=\"SELECT * from try_db t where (t.todo='good' or (t.todo='bootstrap' and t.bootstrap_good='True')) and t.CONFIG='\" + dtop['CONFIG'] + \"' and t.FILTER='\" + filter + \"' order by todo desc\"           \n        print command                                                                                                                                                                                                                                                                                                    \n        c.execute(command)                                             \n        results=c.fetchall()                                           \n        use = []\n        print len(results), ' # of results '\n        for line in results:\n            dp = {}                                                                                                                                                          \n            for i in range(len(db_keys)):\n                dp[db_keys[i]] = str(line[i])\n            use.append(dp)\n                                                                                                                                                                                                                                                                                                                         \n        def use_comp(x,y):\n            date = [float(q) for q in re.split('-',re.split('_',PPRUN)[0])]\n            date_x = [float(q) for q in re.split('-',re.split('_',x['PPRUN'])[0])]\n            date_y = [float(q) for q in re.split('-',re.split('_',y['PPRUN'])[0])]\n                                                                                                                                                                                                                                                                                                                         \n            #print date, date_x, date_y, \n                                                                                                                                                                                                                                                                                                                        \n            diff = lambda a,b: abs((a[0]-b[0])*365 + (a[1]-b[1])*30 + a[2]-b[2])\n            diff_x = diff(date_x,date) \n            diff_y = diff(date_y,date) \n            \n            if diff_x < diff_y:\n                return -1    \n            elif diff_x == diff_y:\n                return 0\n            else:\n                return 1\n\n        use.sort(use_comp)                \n\n    #for k in use:\n    #        print k['objname'],k['pprun'], PPRUN\n    if len(use) > 0:\n        print use[0]['OBJNAME'], use[0]['PPRUN'], PPRUN                    \n\n        sample = 'not set'\n\n        ''' make sure that the illumination correction is in place '''            \n                                                                           \n        import string\n\n        if float(use[0]['bootstrap_zpstd']) < 0.01 and string.find(use[0]['sdssstatus'],'finished') != -1: \n            sample = 'bootstrap'\n        if float(use[0]['None_zpstd']) < 0.01 and string.find(use[0]['Nonestatus'],'finished') != -1:\n            sample = 'None'\n        if float(use[0]['sdss_zpstd']) < 0.01 and string.find(use[0]['sdssstatus'],'finished') != -1: \n            sample = 'sdss'\n\n        #print use[0]['sdssstatus'], use[0]['Nonestatus'], use[0]['bootstrapstatus']\n        #raw_input() \n                                                                           \n        #print use[0:2]\n        if sample != 'not set':\n            return (use[0]['OBJNAME'],use[0]['FILTER'],use[0]['PPRUN'],sample)\n        else: return(None,None,None,None)    \n    else: return(None,None,None,None)    \n\n\ndef test():\n    import MySQLdb, sys, os, re, time, utilities, pyfits                                                                                                                          \n    from copy import copy\n    db2 = MySQLdb.connect(db='subaru', user='weaklensing', passwd='darkmatter', host='ki-rh8')\n    c = db2.cursor()\n    db_keys_t = describe_db(c,['try_db'])\n    command=\"SELECT * from try_db where todo='good' and var_correction > 0.08 order by rand()\"           \n\n    command='SELECT * from try_db i where i.todo is null and (i.sdssstatus like \"%finished\" or i.Nonestatus like \"%finished\") and (i.objname like \"MACS0018%\" or i.objname like \"MACS0025%\" or i.objname like \"MACS0257%\" or i.objname like \"MACS0454%\" or i.objname like \"MACS0647%\" or i.objname like \"MACS0717%\" or i.objname like \"MACS0744%\" or i.objname like \"MACS0911%\" or i.objname like \"MACS1149%\" or i.objname like \"MACS1423%\" or i.objname like \"MACS2129%\" or i.objname like \"MACS2214%\" or i.objname like \"MACS2243%\" or i.objname like \"A2219\" or i.objname like \"A2390\") order by rand()'           \n\n    command='SELECT * from try_db i where i.todo is null order by rand()' # and (i.objname like \"A68%\" and i.pprun like \"2007-07-18_W-J-B\")' # or i.objname like \"MACS0025%\" or i.objname like \"MACS0257%\" or i.objname like \"MACS0454%\" or i.objname like \"MACS0647%\" or i.objname like \"MACS0717%\" or i.objname like \"MACS0744%\" or i.objname like \"MACS0911%\" or i.objname like \"MACS1149%\" or i.objname like \"MACS1423%\" or i.objname like \"MACS2129%\" or i.objname like \"MACS2214%\" or i.objname like \"MACS2243%\" or i.objname like \"A2219\" or i.objname like \"A2390\") order by rand()'           \n\n    #command='SELECT * from try_db i where i.todo is null order by rand()' #and objname=\"MACS0018+16\" and pprun=\"2003-09-25_W-J-V\"' # i.todo=\"bootstrap\" and (i.sdssstatus like \"%finished\" or i.Nonestatus like \"%finished\") and bootstrapstatus=\"fitfinished\" and objname=\"MACS1824+43\" and PPRUN=\"2000-08-06_W-C-IC\"'\n    #command='SELECT * from try_db i where i.todo is null and (i.sdssstatus like \"%finished\" or i.Nonestatus like \"%finished\")  order by rand()'           \n#    command='SELECT * from try_db i where (i.sdssstatus like \"%finished\" or i.Nonestatus like \"%finished\") and (i.objname like \"MACS0717%\")  order by rand()'           \n\n    command='select * from try_db where objname=\"MACS2140-23\" and pprun=\"2007-07-18_W-C-IC\"'\n    print command                                                  \n    c.execute(command)                                             \n    results=c.fetchall()                                           \n    random_dict = {}\n    for line in results:\n        if 1:\n            dtop = {}                                                                                                                                                          \n            for i in range(len(db_keys_t)):\n                dtop[db_keys_t[i]] = str(line[i])\n\n            ''' if not fit has been attempted, set todo '''\n        \n            print dtop['sdssstatus'], dtop['Nonestatus'], dtop['PPRUN'], dtop['OBJNAME'] \n            if dtop['sdssstatus'] == None and dtop['Nonestatus'] == None:\n                save_fit({'PPRUN':dtop['PPRUN'],'OBJNAME':dtop['OBJNAME'],'FILTER':dtop['FILTER'],'sample':'record','sample_size':'record','todo': 'no fit'},db='try_db')\n\n            elif dtop['sdssstatus']=='failed' or dtop['Nonestatus']=='failed':\n                save_fit({'PPRUN':dtop['PPRUN'],'OBJNAME':dtop['OBJNAME'],'FILTER':dtop['FILTER'],'sample':'record','sample_size':'record','todo': 'failed fit'},db='try_db')\n        \n            else:                \n\n                import MySQLdb, sys, os, re, time, utilities, pyfits                                                                                                                                                                                                                                                                    \n                from copy import copy\n                db2 = MySQLdb.connect(db='subaru', user='weaklensing', passwd='darkmatter', host='ki-rh8')\n                c = db2.cursor()\n                db_keys = describe_db(c,['illumination_db','try_db'])                                                                                                                                      \n                #command=\"SELECT * from illumination_db where  OBJNAME='\" + dtop['OBJNAME'] + \"' and PPRUN='\" + dtop['PPRUN'] + \"' and filter like '\" + dtop['filter'] + \"' and pasted_cat is not NULL\"    \n                print dtop['OBJNAME']\n                #command=\"SELECT * from illumination_db i left join try_db f on (i.pprun=f.pprun and i.OBJNAME=f.OBJNAME) where i.OBJNAME='\" + dtop['OBJNAME'] + \"' and i.pasted_cat is not NULL and f.std is not null and f.mean is not null and f.rots is not null and f.var_correction is not null group by f.pprun\"           \n                                                                                                                                                                                                                                                                                                                                        \n                command=\"SELECT * from illumination_db i left join try_db f on (i.pprun=f.pprun and i.OBJNAME=f.OBJNAME) where i.OBJNAME='\" + dtop['OBJNAME'] + \"' and i.pasted_cat is not NULL and (f.sdssstatus is not null or f.Nonestatus is not null) and ((f.sdssstatus!='failed' or f.sdssstatus is null) and (f.Nonestatus is null or f.Nonestatus!='failed')) group by f.pprun\"           \n\n                #and (f.bootstrapstatus like '%finished' or f.sdssstatus like '%finished' or f.Nonestatus like '%finished')\n                print command                                                  \n                c.execute(command)                                             \n                results=c.fetchall()                                           \n                sort_results(results,db_keys)\n\n\ndef sort_results(results2,db_keys):\n    import config_bonn\n    reload(config_bonn)\n\n    from config_bonn import wavelength_groups, wavelength_order\n    rotation_runs = {} \n    for line in results2: \n        dict = {}  \n        for i in range(len(db_keys)):\n            dict[db_keys[i]] = str(line[i])\n        GID = float(dict['GABODSID'])\n        CONFIG_IM = find_config(GID)\n\n        FILTER_NUM =  None \n\n        for i in range(len(wavelength_groups)):\n            for filt in wavelength_groups[i]:\n                if filt == dict['filter']: \n                    FILTER_NUM = i\n                    dict['FILTER_NUM'] = FILTER_NUM\n\n        if FILTER_NUM is None: \n            print dict['filter']\n            raise NoFilterMatch\n\n        import copy            \n        if True: #float(dict['EXPTIME']) > 10.0:\n            if not dict['PPRUN'] in rotation_runs:                                                                                                                               \n                rotation_runs[dict['PPRUN']] = copy.copy(dict)\n                    #{'ROTATION':{dict['ROTATION']:'yes'},'FILTER':dict['filter'],'CONFIG_IM':CONFIG_IM,'EXPTIME':dict['EXPTIME'],'file':dict['file'],'linearfit':dict['linearfit'],'OBJNAME':dict['OBJNAME'],'catalog':dict['catalog'],'FILTER_NUM':FILTER_NUM,}\n            #rotation_runs[dict['PPRUN']]['ROTATION'][dict['ROTATION']] = 'yes'\n    #print rotation_runs\n\n    help_list = {} \n    good_list = {}\n    for y in rotation_runs.keys():\n      \n        import MySQLdb, sys, os, re, time, utilities, pyfits                                                                                                                           \n        from copy import copy\n        db2 = MySQLdb.connect(db='subaru', user='weaklensing', passwd='darkmatter', host='ki-rh8')\n        c = db2.cursor()\n\n        ''' figure out what the sample is '''\n        command=\"SELECT todo from try_db where OBJNAME='\" + rotation_runs[y]['OBJNAME'] + \"' and PPRUN='\" + rotation_runs[y]['PPRUN'] + \"'\"           \n        print command                                                  \n        c.execute(command)                                             \n        results=c.fetchall()                                           \n        todo = results[0][0]   \n                                                                                                                                                                                             \n        command=\"SELECT sample from fit_db where OBJNAME='\" + rotation_runs[y]['OBJNAME'] + \"' and PPRUN='\" + rotation_runs[y]['PPRUN'] + \"' and sample_size='all'  group by sample\"           \n        print command                                                  \n        c.execute(command)                                             \n        results=c.fetchall()                                           \n        samples = {}                                                                                                                                                                        \n        for line in results:\n            samples[line[0]] = 'yes'\n                                                                                                                                                                                             \n        if samples.has_key('sdss'):\n            s = 'sdss'\n        else: s = 'None'\n \n        db_keys_t = describe_db(c,['fit_db'])\n        command=\"SELECT * from fit_db where PPRUN='\" + rotation_runs[y]['PPRUN'] + \"' and OBJNAME='\" + rotation_runs[y]['OBJNAME'] + \"' and sample_size='all'\" # and sample='sdss'\"\n        print command                                                  \n        c.execute(command)                                             \n        results=c.fetchall()                                           \n        for line in results:\n            dtop = {}                                                                                                                                                          \n            for i in range(len(db_keys_t)):\n                dtop[db_keys_t[i]] = str(line[i])\n\n        print results \n        print dtop\n\n\n        #if (rotation_runs[y]['sdss$good'] == 'y' or rotation_runs[y]['None$good'] =='y') and rotation_runs[y]['CONFIG_IM'] != '8' and  rotation_runs[y]['CONFIG_IM'] != '9' and  rotation_runs[y]['CONFIG_IM'] != '10_3' and len(rotation_runs[y]['ROTATION'].keys()) > 1:\n        #print y, rotation_runs[y]['EXPTIME'], rotation_runs[y]['file'],  (float(rotation_runs[y]['mean']) - 1*float(rotation_runs[y]['std']) > 1.005) , (rotation_runs[y]['rots'] > 1 or dtop['sample']=='sdss'),  float(rotation_runs[y]['EXPTIME']) > 10\n\n\n        #print float(rotation_runs[y]['mean']) , float(rotation_runs[y]['std']) , rotation_runs[y][s + '_var_correction'] , rotation_runs[y]['rots'] , dtop['sample']=='sdss' , float(rotation_runs[y]['EXPTIME']) \n        #print (float(rotation_runs[y]['mean']) - 1*float(rotation_runs[y]['std']) > 1.005) , float(rotation_runs[y]['var_correction']) < 0.08 , (rotation_runs[y]['rots'] > 1 or dtop['sample']=='sdss') , float(rotation_runs[y]['EXPTIME']) > 10\n\n\n        sl = s + '_'\n        \n        \n        \n        \n        \n\n\n\n        print y, rotation_runs[y]['OBJNAME'], s + 'status',rotation_runs[y][s+'status']\n        import string\n        if string.find(rotation_runs[y][s + 'status'],'finished')!= -1:\n\n            ''' if not stats, then calculate them!!! '''\n            if str(rotation_runs[y]['stats']) == 'None' or (str(rotation_runs[y][sl + 'var_correction']) == 'None' or str(rotation_runs[y][sl + 'mean']) == 'None' or str(rotation_runs[y][sl+'std'])): calc_good(rotation_runs[y]['OBJNAME'],rotation_runs[y]['FILTER'],rotation_runs[y]['PPRUN'])\n            print rotation_runs[y]['CONFIG'] , str(8.0)\n            if rotation_runs[y]['sdss_imp_all'] != 'None':     \n                if rotation_runs[y]['CONFIG'] == str(8.0):\n                    good = float(rotation_runs[y][sl + 'var_correction']) < 0.02 and (float(rotation_runs[y][sl+ 'mean']) - 1.*float(rotation_runs[y][sl + 'std']) > 1.00) and ( (float(rotation_runs[y]['sdss_imp_all'])>1.00 and float(rotation_runs[y]['match_stars'])>400) or float(rotation_runs[y]['match_stars'])<400)\n                else:\n                    good = float(rotation_runs[y][sl + 'var_correction']) < 0.01 and (float(rotation_runs[y][sl+ 'mean']) - 1.5*float(rotation_runs[y][sl + 'std']) > 1.00) and ( (float(rotation_runs[y]['sdss_imp_all'])>1.00 and float(rotation_runs[y]['match_stars'])>400) or float(rotation_runs[y]['match_stars'])<400)\n            else:\n                if rotation_runs[y]['CONFIG'] == str(8.0):\n                    good = float(rotation_runs[y][sl + 'var_correction']) < 0.02 and (float(rotation_runs[y][sl + 'mean']) - 1.*float(rotation_runs[y][sl + 'std']) > 1.00) \n                else:\n                    good = float(rotation_runs[y][sl + 'var_correction']) < 0.01 and (float(rotation_runs[y][sl + 'mean']) - 1.5*float(rotation_runs[y][sl + 'std']) > 1.00) \n        else: good = False\n\n        print (rotation_runs[y][sl + 'var_correction']) , (rotation_runs[y][sl + 'mean']) , (rotation_runs[y][sl + 'std']) , rotation_runs[y]['sdss_imp_all'] \n        print string.find(rotation_runs[y][s + 'status'],'finished')     \n        print s + 'status'          \n        print good, y\n\n        print sl                                                                                                                                                                                                                         \n        if str(rotation_runs[y][sl + 'var_correction']) == 'None': sl = ''\n        print sl\n        print 'stats', rotation_runs[y][sl + 'var_correction'], rotation_runs[y][sl + 'mean'], rotation_runs[y][sl + 'std'], rotation_runs[y]['sdss_imp_all'],rotation_runs[y]['match_stars'],rotation_runs[y]['match_stars']\n        print rotation_runs[y]['OBJNAME'],rotation_runs[y]['FILTER'],rotation_runs[y]['PPRUN']\n\n\n\n\n\n\n        ''' look to see if bootstrap was tried and failed or if the regular fit failed because of too few objects, set boostrap_good to use for applying the correction '''\n        import string\n        if str(rotation_runs[y]['bootstrapstatus']) == 'failed' or (str(rotation_runs[y]['bootstrapstatus']) == 'None' and   (string.find(rotation_runs[y]['exception'],'end')!=-1 or string.find(rotation_runs[y]['exception'],'few')!=-1)): bootstrap_good = 'failed' \n        elif str(rotation_runs[y]['bootstrap_var_correction']) != 'None' and str(rotation_runs[y]['bootstrap_mean']) != 'None' and str(rotation_runs[y]['bootstrap_std']) != 'None':\n            bootstrap_good = float(rotation_runs[y]['bootstrap_var_correction']) < 0.01 and (float(rotation_runs[y]['bootstrap_mean']) - 1.5*float(rotation_runs[y]['bootstrap_std']) > 1.00) \n        else: bootstrap_good = 'no input measurements'\n        save_fit({'PPRUN':rotation_runs[y]['PPRUN'],'OBJNAME':rotation_runs[y]['OBJNAME'],'FILTER':rotation_runs[y]['FILTER'],'sample':'record','sample_size':'record','bootstrap_good': bootstrap_good},db='try_db')\n\n        print y, bootstrap_good, rotation_runs[y]['bootstrapstatus']\n\n        if good:\n            print 'GOOD'\n            good_list[y] = {}\n            good_list[y]['FILTER_NUM'] = rotation_runs[y]['FILTER_NUM']\n            good_list[y]['FILTER'] = rotation_runs[y]['FILTER']\n            good_list[y]['OBJNAME'] = rotation_runs[y]['OBJNAME']\n            good_list[y]['catalog'] = dtop['catalog'] \n            good_list[y]['EXPTIME'] = rotation_runs[y]['EXPTIME']\n            good_list[y]['PPRUN'] = rotation_runs[y]['PPRUN']\n            good_list[y]['file'] = rotation_runs[y]['file']\n            good_list[y]['todo'] = rotation_runs[y]['todo']\n            good_list[y]['status'] = 'good'\n            good_list[y]['primary'] = None \n            good_list[y]['secondary'] = None \n            good_list[y]['bootstrap_good'] = bootstrap_good\n            save_fit({'PPRUN':rotation_runs[y]['PPRUN'],'OBJNAME':rotation_runs[y]['OBJNAME'],'FILTER':rotation_runs[y]['FILTER'],'sample':'record','sample_size':'record','todo': 'good'},db='try_db')\n            print good_list\n        else:\n            print 'HELP'\n            help_list[y] = {}\n            help_list[y]['FILTER_NUM'] = rotation_runs[y]['FILTER_NUM']\n            help_list[y]['FILTER'] = rotation_runs[y]['FILTER']\n            help_list[y]['OBJNAME'] = rotation_runs[y]['OBJNAME']\n            #help_list[y]['file'] = rotation_runs[y]['file']\n            help_list[y]['EXPTIME'] = rotation_runs[y]['EXPTIME']\n            help_list[y]['PPRUN'] = rotation_runs[y]['PPRUN']\n            help_list[y]['todo'] = rotation_runs[y]['todo']\n            help_list[y]['catalog'] = dtop['catalog'] \n            help_list[y]['status'] = 'help'\n            help_list[y]['primary'] = None \n            help_list[y]['secondary'] = None \n            help_list[y]['bootstrap_good'] = bootstrap_good\n\n    orphan_list = {}\n    matched_list = {}\n    for y in help_list.keys():\n        ''' use rules to assign comparison cats'''\n        ''' first determine the closest filter ''' \n        primaries = []\n        for x in good_list.keys():\n            if good_list[x]['catalog'] != 'None':\n                ''' if same filter, use that '''\n                if good_list[x]['FILTER'] == help_list[y]['FILTER']:\n                    primaries.append([-1,x,good_list[x]['FILTER'],good_list[x]['catalog']])\n                else:\n                    primaries.append([abs(help_list[y]['FILTER_NUM'] - good_list[x]['FILTER_NUM']),x,good_list[x]['FILTER'],good_list[x]['catalog']])\n        primaries.sort()\n        if len(primaries) > 0:\n            if primaries[0][0] < 3:\n                primary = primaries[0][1]                          \n                primary_filter = primaries[0][2]\n                help_list[y]['primary'] = primary\n                help_list[y]['primary_catalog'] = primaries[0][3] \n            else: \n                primary = None\n                primary_filter = None\n                help_list[y]['primary'] = None\n                help_list[y]['primary_catalog'] = None\n        else: \n            primary = None\n            primary_filter = None\n            help_list[y]['primary'] = None\n            help_list[y]['primary_catalog'] = None\n            \n        print 'primary', primaries, primary, y\n\n        secondaries = []\n        for x in good_list.keys():\n            if good_list[x]['FILTER'] != primary_filter and x != primary and help_list[y]['FILTER_NUM'] != good_list[x]['FILTER_NUM']:\n                secondaries.append([abs(help_list[y]['FILTER_NUM'] - good_list[x]['FILTER_NUM']),x,good_list[x]['FILTER'],good_list[x]['catalog']])\n        #''' if no calibrated secondary, use the same catalog '''\n        #if len(secondaries) == 0:\n        #    for x in help_list.keys():\n        #        secondaries.append([abs(help_list[y]['FILTER_NUM'] - help_list[x]['FILTER_NUM']),x])\n        #''' guaranteed to be a secondary '''\n        if len(secondaries)>0:\n            secondaries.sort()                     \n            secondary = secondaries[0][1]\n            help_list[y]['secondary'] = secondary \n            help_list[y]['secondary_catalog'] = secondaries[0][3] \n        else: \n            secondary = None\n            help_list[y]['secondary'] = None\n            help_list[y]['secondary_catalog'] = None\n                                                                                                              \n        print 'secondary', secondaries, secondary, y                                                               \n        print help_list\n        \n\n        if help_list[y]['primary']!=None and help_list[y]['secondary']!=None:\n            save_fit({'PPRUN':help_list[y]['PPRUN'],'OBJNAME':help_list[y]['OBJNAME'],'FILTER':help_list[y]['FILTER'],'sample':'record','sample_size':'record','todo': 'bootstrap', 'primary_catalog':help_list[y]['primary_catalog'],'primary_filt':str(help_list[y]['primary']), 'secondary_catalog':help_list[y]['secondary_catalog'], 'secondary_filt':str(help_list[y]['secondary'])},db='try_db')\n        else:\n            print 'primaries', primaries \n            print '\\n','\\n'\n            print 'secondaries', secondaries\n\n            print help_list[y]['primary'], help_list[y]['secondary'] \n            print help_list[y]['PPRUN']\n            print 'good_list',good_list.keys()\n            print 'help_list',help_list.keys()\n            save_fit({'PPRUN':help_list[y]['PPRUN'],'OBJNAME':help_list[y]['OBJNAME'],'FILTER':help_list[y]['FILTER'],'sample':'record','sample_size':'record','todo': 'orphaned', 'primary_catalog':help_list[y]['primary_catalog'],'primary_filt':str(help_list[y]['primary']), 'secondary_catalog':help_list[y]['secondary_catalog'], 'secondary_filt':str(help_list[y]['secondary'])},db='try_db')\n\n\n\n        \n            #save_fit({'PPRUN':help_list[y]['PPRUN'],'OBJNAME':help_list[y]['OBJNAME'],'FILTER':help_list[y]['FILTER'],'sample':'record','sample_size':'record','todo': 'bootstrap', 'primary_catalog':help_list[y]['primary_catalog'],'primary_filt':str(help_list[y]['primary']), 'secondary_catalog':help_list[y]['secondary_catalog'], 'secondary_filt':str(help_list[y]['secondary'])},db='try_db')\n\n    print good_list\n    print help_list\n\n    for g in good_list.keys(): print g, good_list[g], '\\n'\n\n    for h in help_list.keys(): print h, help_list[h], '\\n'\n    \n    print 'good'                                                                                        \n    for key in sorted(good_list.keys()): print key, good_list[key]['EXPTIME'], #good_list[key]['file']\n    print 'help'                                                                                      \n    for key in sorted(help_list.keys()): print key, help_list[key]['EXPTIME'],#help_list[key]['file']\n    print 'matched'\n    for key in sorted(matched_list.keys()): print key, matched_list[key]['EXPTIME'],#matched_list[key]['file']\n    print 'orphaned'\n    for key in sorted(orphan_list.keys()): print key, orphan_list[key]['EXPTIME'],#orphan_list[key]['file']\n    import copy \n    all_list = copy.copy(good_list)\n    all_list.update(help_list)\n    print all_list\n    return all_list\n\ndef plot_values():\n    import MySQLdb, sys, os, re, time, utilities, pyfits                                       \n    from copy import copy\n    db2 = MySQLdb.connect(db='subaru', user='weaklensing', passwd='darkmatter', host='ki-rh8')\n    c = db2.cursor()\n    \n    db_keys_t = describe_db(c,['try_db'])\n    command=\"SELECT * from try_db where var_correction is not null and mean is not null and std is not null\"           \n    print command                                                  \n    c.execute(command)                                             \n    results=c.fetchall()                                           \n    random_dict = {}\n    x = []\n    y = []\n    for line in results:\n        dtop = {}                                                                                                                                                          \n        for i in range(len(db_keys_t)):\n            dtop[db_keys_t[i]] = str(line[i])\n        if float(dtop['mean']) != 0:\n            x.append(float(dtop['mean']) - 1.5*float(dtop['std']))        \n            y.append(float(dtop['var_correction']))\n    \n\n    import numpy, math, pyfits, os                                                                              \n    import copy\n    from ppgplot   import *\n\n    pgbeg(\"/XTERM\",1,1)\n\n    #pgbeg(\"sigmavar.ps/cps\",1,1)\n                                                                                                                                             \n    pgiden()\n    pgpanl(1,1) \n    from Numeric import *\n    plotx = copy.copy(x)\n    ploty = copy.copy(y)\n    x.sort()    \n    y.sort()\n    mediany = y[int(len(y)/2.)]\n    lowx=-2 #x[2]\n    highx=2 #x[-2]\n    lowy=mediany + 1.5\n    highy=mediany -1.5\n    pgswin(0.9,1.3,0.0,0.05)\n    plotx = array(plotx)\n    ploty = array(ploty)\n    #pylab.scatter(z,x)\n    #print plotx, ploty\n    pgpt(plotx,ploty,2)\n\n    pglab('1.5 Sigma Lower Limit of Chi-Squared Improvement','Variance of Correction')\n    #pylab.savefig('sigmavar.ps')\n    \n    pgbox()\n    pgend()\n\n\n    db_keys_t = describe_db(c,['try_db'])\n    command=\"SELECT * from try_db where var_correction is not null and mean is not null and sdss_imp is not null\" # and rots=1\"           \n    print command                                                  \n    c.execute(command)                                             \n    results=c.fetchall()                                           \n    random_dict = {}\n    x = []\n    y = []\n    for line in results:\n        dtop = {}                                                                                                                                                          \n        for i in range(len(db_keys_t)):\n            dtop[db_keys_t[i]] = str(line[i])\n        if float(dtop['mean']) != 0:\n            x.append(float(dtop['mean']) - 1.5*float(dtop['std']))        \n            y.append(float(dtop['sdss_imp']))\n    \n                                                                                                                                                                            \n    import numpy, math, pyfits, os                                                                              \n    import copy\n    from ppgplot   import *\n    raw_input()\n                                                                                                                                                                            \n    pgbeg(\"/XTERM\",1,1)\n                                                                                                                                             \n    #pgbeg(\"sigmasdss.ps/cps\",1,1)\n\n    pgiden()\n    pgpanl(1,1) \n    from Numeric import *\n    plotx = copy.copy(x)\n    ploty = copy.copy(y)\n    x.sort()    \n    y.sort()\n    mediany = y[int(len(y)/2.)]\n    lowx=-2 #x[2]\n    highx=2 #x[-2]\n    lowy=mediany + 1.5\n    highy=mediany -1.5\n    pgswin(0.9,1.3,0.7,1.3)\n    plotx = array(plotx)\n    ploty = array(ploty)\n    #pylab.scatter(z,x)\n    #print plotx, ploty\n    pgpt(plotx,ploty,2)\n\n    pglab('1.5 Sigma Lower Limit of Chi-Squared Improvement','SDSS Chi-Squared Improvement')\n    #pylab.savefig('sigmavar.ps')\n    \n    pgbox()\n    pgend()\n\ndef calc_good(OBJNAME=None,FILTER=None,PPRUN=None):    \n    import MySQLdb, sys, os, re, time, utilities, pyfits                                       \n    from copy import copy\n    db2 = MySQLdb.connect(db='subaru', user='weaklensing', passwd='darkmatter', host='ki-rh8')\n    c = db2.cursor()\n    \n    db_keys_t = describe_db(c,['try_db'])\n    command='SELECT * from try_db i where i.bootstrapstatus=\"fitfinished\" and bootstrap_mean is null and i.stats is null ' #and (i.sdssstatus like \"%finished\" or i.Nonestatus like \"%finished\") and (i.objname like \"MACS0018%\" or i.objname like \"MACS0025%\" or i.objname like \"MACS0257%\" or i.objname like \"MACS0454%\" or i.objname like \"MACS0647%\" or i.objname like \"MACS0717%\" or i.objname like \"MACS0744%\" or i.objname like \"MACS0911%\" or i.objname like \"MACS1149%\" or i.objname like \"MACS1423%\" or i.objname like \"MACS2129%\" or i.objname like \"MACS2214%\" or i.objname like \"MACS2243%\" or i.objname like \"A2219\" or i.objname like \"A2390\") order by rand()'           \n\n    #command='SELECT * from try_db i where i.stats is null order by rand()'           \n\n    #command='SELECT * from try_db i where (i.sdssstatus like \"%finished\" or i.Nonestatus like \"%finished\") and stats is null order by rand()' # and (i.objname like \"MACS0744%\")' # and i.pprun=\"2001-12-11_W-C-IC\" order by rand()'           \n    if OBJNAME is not None:\n        command='SELECT * from try_db i where PPRUN=\"' + PPRUN + '\" and OBJNAME=\"' + OBJNAME + '\"' \n    print command\n    # and (i.objname like \"MACS0744%\")' # and i.pprun=\"2001-12-11_W-C-IC\" order by rand()'           \n    print command                                                  \n    c.execute(command)                                             \n    results=c.fetchall()                                           \n    random_dict = {}\n    for line in results:\n        dtop = {}                                                                                                                                                          \n        for i in range(len(db_keys_t)):\n            dtop[db_keys_t[i]] = str(line[i])\n\n        save_fit({'PPRUN':dtop['PPRUN'],'OBJNAME':dtop['OBJNAME'],'FILTER':dtop['FILTER'],'stats':'start', 'sample':'record','sample_size':'record',},db='try_db')\n\n        command=\"SELECT todo from try_db where OBJNAME='\" + dtop['OBJNAME'] + \"' and PPRUN='\" + dtop['PPRUN'] + \"'\"           \n        print command                                                  \n        c.execute(command)                                             \n        results=c.fetchall()                                           \n        todo = results[0][0]   \n \n        command=\"SELECT sdssstatus, Nonestatus, bootstrapstatus from try_db where OBJNAME='\" + dtop['OBJNAME'] + \"' and PPRUN='\" + dtop['PPRUN'] + \"'\" # and sample_size='all'  group by sample\"           \n        print command                                                  \n        c.execute(command)                                             \n        results=c.fetchall()                                           \n        samples = {}                                                                                                                                                                        \n        s = []\n        import string as st\n        sdssstatus = results[0][0]\n        if st.find(str(sdssstatus),'finished')!=-1: s.append('sdss')\n        Nonestatus = results[0][1]\n        if st.find(str(Nonestatus),'finished')!=-1: s.append('None')\n        bootstrapstatus = results[0][2]\n        if st.find(str(bootstrapstatus),'finished')!=-1: s.append('bootstrap')\n\n        if len(s) == 0:\n            print 'no fits'\n            save_fit({'PPRUN':dtop['PPRUN'],'OBJNAME':dtop['OBJNAME'],'FILTER':dtop['FILTER'],'stats':'no fits', 'sample':'record','sample_size':'record',},db='try_db')\n        print len(s)\n\n        for sample in s:\n            if 1: #try:\n                if True:                                                                                                                                                                         \n                    db_keys_f = describe_db(c,['fit_db'])                                                                                                                          \n                    command=\"SELECT * from fit_db where OBJNAME='\" + dtop['OBJNAME'] + \"' and PPRUN='\" + dtop['PPRUN'] + \"' and sample_size='all' and sample='\" + sample + \"' limit 1\"           \n                    print command                                                  \n                    c.execute(command)                                             \n                    results=c.fetchall()                                           \n                    random_dict = {}\n                    epsilons = []\n                                                                                                                                                                                                                                                                        \n                    for line in results:\n                        drand = {}  \n                        for i in range(len(db_keys_f)):\n                            drand[db_keys_f[i]] = str(line[i])\n                        print sample\n                        zp_images = re.split(',',drand['zp_images'])\n                        import scipy\n                        zpstd = scipy.std([float(zp) for zp in zp_images])\n                        save_fit({'PPRUN':dtop['PPRUN'],'OBJNAME':dtop['OBJNAME'],'FILTER':dtop['FILTER'],sample + '_zpstd':zpstd, 'zp_images': drand['zp_images'], 'sample':'record','sample_size':'record',},db='try_db')\n                                                                                                                                                                                                                                                                        \n                if True:\n                    db_keys_f = describe_db(c,['fit_db'])                                                                                                                          \n                    command=\"SELECT * from fit_db where OBJNAME='\" + dtop['OBJNAME'] + \"' and PPRUN='\" + dtop['PPRUN'] + \"' and sample_size like 'rand%'  and sample='\" + sample + \"' limit 1\"           \n                    print command                                                  \n                    c.execute(command)                                             \n                    results=c.fetchall()                                           \n                    random_dict = {}\n                    epsilons = []\n                    for line in results:\n                        drand = {}  \n                        for i in range(len(db_keys_f)):\n                            drand[db_keys_f[i]] = str(line[i])\n                        rots = []                                                                                                                                                    \n                        for rot in ['0','1','2','3','40']:\n                            print rot, drand[rot + '$1x1y'] \n                            if drand[rot + '$2x2y'] != 'None': \n                                rots.append(rot)\n                        save_fit({'PPRUN':dtop['PPRUN'],'OBJNAME':dtop['OBJNAME'],'FILTER':dtop['FILTER'],'rots': int(len(rots)), 'sample':'record','sample_size':'record',},db='try_db')\n                    \n                \n                if  True:                                                                                                                                                                   \n                    ''' retrieve all random fits '''                                                                                                                                   \n                    db_keys_f = describe_db(c,['fit_db'])\n                    command=\"SELECT * from fit_db where OBJNAME='\" + dtop['OBJNAME'] + \"' and PPRUN='\" + dtop['PPRUN'] + \"' and sample_size like 'rand%' and positioncolumns is not null  and sample='\" + sample + \"'\" # and CHIPS is not null\"           \n                    print command                                                  \n                    c.execute(command)                                             \n                    results=c.fetchall()                                           \n                    random_dict = {}\n                    epsilons = []\n                    for line in results:\n                        drand = {}  \n                        for i in range(len(db_keys_f)):\n                            drand[db_keys_f[i]] = str(line[i])\n                        import string\n                        name = drand['sample_size'].replace('corr','').replace('un','')\n                        if not name in random_dict: random_dict[name] = {}\n                                                                                                                                                                                      \n                        if string.find(drand['sample_size'],'corr') != -1 and string.find(drand['sample_size'],'uncorr') == -1:\n                            random_dict[name]['corr'] = drand['rejectedreducedchi']\n                        if string.find(drand['sample_size'],'uncorr') != -1:\n                            random_dict[name]['uncorr'] = drand['rejectedreducedchi']\n                        if string.find(drand['sample_size'],'orr') == -1:\n                            print dtop['OBJNAME'],dtop['FILTER'],dtop['PPRUN'],drand['sample'],drand['sample_size']\n                            epsilon, diff_bool = test_correction(dtop['OBJNAME'],dtop['FILTER'],dtop['PPRUN'],drand['sample'],drand['sample_size'])\n                            epsilons.append(epsilon)\n                                                                                                                                                                                                                                                                        \n                    import scipy, numpy\n                    print epsilons\n                    if len(epsilons) > 0:\n                        surfs = numpy.array(epsilons)                                                                                                                                                                       \n                        stds = numpy.std(epsilons,axis=0)               \n                        var_correction = numpy.median(stds.flatten().compress(diff_bool.flatten()))\n                        print var_correction\n                        \n                        chi_diffs = []\n                        print random_dict\n                        for key in random_dict.keys():    \n                            print random_dict[key].has_key('corr') and random_dict[key].has_key('uncorr')\n                            if random_dict[key].has_key('corr') and random_dict[key].has_key('uncorr'):\n                                if random_dict[key]['corr'] != 'None' and random_dict[key]['uncorr'] != 'None' and float(random_dict[key]['corr'])!=0:\n                                    print dtop['OBJNAME'], dtop['PPRUN']                                                              \n                                    random_dict[key]['chi_diff'] = float(random_dict[key]['uncorr'])/float(random_dict[key]['corr']) \n                                    #print float(random_dict[key]['uncorr']),float(random_dict[key]['corr']) \n                                    #print random_dict[key]['chi_diff']\n                                    chi_diffs.append(random_dict[key]['chi_diff'])\n                                                                                                                                                                                          \n                        print chi_diffs \n                        import scipy\n                        mean = scipy.mean(chi_diffs)\n                        print 'mean', mean\n                        std = scipy.std(chi_diffs)\n                        print 'std', std\n                        save_fit({'PPRUN':dtop['PPRUN'],'OBJNAME':dtop['OBJNAME'],'FILTER':dtop['FILTER'],sample + '_mean':mean, sample + '_std':std, sample + '_var_correction': var_correction, 'sample':'record','sample_size':'record',},db='try_db')\n                                                                                                                                                                                                                                                                        \n                if True:\n                    db_keys_f = describe_db(c,['fit_db'])\n                    command=\"SELECT * from fit_db where OBJNAME='\" + dtop['OBJNAME'] + \"' and PPRUN='\" + dtop['PPRUN'] + \"' and sample_size like 'all'     and sample='sdss'     \"           \n                    print command                                                  \n                    c.execute(command)                                             \n                    results=c.fetchall()                                           \n                    random_dict = {}\n                    o = {}\n                    print len(results)\n                    if len(results) > 0:\n                        for line in results:                                                                                                                                                                                            \n                            drand = {}  \n                            for i in range(len(db_keys_f)):\n                                drand[db_keys_f[i]] = str(line[i])\n                                                                                                                                                                                                                                                                        \n                        save_fit({'PPRUN':dtop['PPRUN'],'OBJNAME':dtop['OBJNAME'],'FILTER':dtop['FILTER'],sample + '_match_stars':drand['match_stars'],'sample':'record','sample_size':'record',},db='try_db')\n                                                                                                                                                                                                                                                                        \n                                                                                                                                                                                                                                                                        \n                        import string\n                        name = drand['sample_size'].replace('corr','').replace('un','')\n                        if not name in random_dict: random_dict[name] = {}\n                                                                                                                                                                                                                          \n                        for rot in ['0','1','2','3']:                                                                                                                                   \n                            if not o.has_key(rot):\n                                o[rot] = {}\n                            if drand['sdssredchinocorr$' + rot] != 'None':\n                                o[rot]['corr'] = drand['sdssredchicorr$' + rot]\n                                o[rot]['uncorr'] = drand['sdssredchinocorr$' + rot]\n                        num = 0\n                        factor = 0                                                                                                                                                                                                  \n                        for rot in ['0','1','2','3']:                                                                                                                                   \n                            if 'corr' in o[rot] and 'uncorr' in o[rot]:  \n                                if o[rot].has_key('uncorr') and o[rot].has_key('corr'):\n                                    if float(o[rot]['corr']) != 0:\n                                        factor += float(o[rot]['uncorr'])/float(o[rot]['corr'])\n                                        num += 1.\n                        if num!=0:\n                            save_fit({'PPRUN':dtop['PPRUN'],'OBJNAME':dtop['OBJNAME'],'FILTER':dtop['FILTER'],'match_stars':drand['match_stars'],'sdss_imp_all':factor/num, 'sample':'record','sample_size':'record',},db='try_db')\n                                                                                                                                                                                                                                                                        \n                                                                                                                                                                                                                                                                        \n                if True:\n                    ''' retrieve all random fits '''                                                                                                                                                                      \n                    db_keys_f = describe_db(c,['fit_db'])\n                    command=\"SELECT * from fit_db where OBJNAME='\" + dtop['OBJNAME'] + \"' and PPRUN='\" + dtop['PPRUN'] + \"' and sample_size like 'allsdss%corr'          \"           \n                    print command                                                  \n                    c.execute(command)                                             \n                    results=c.fetchall()                                           \n                    random_dict = {}\n                    o = {}\n                    if len(results) > 0:\n                        for line in results:                                                                                                                                                                                            \n                            drand = {}  \n                            for i in range(len(db_keys_f)):\n                                drand[db_keys_f[i]] = str(line[i])\n                            import string\n                            name = drand['sample_size'].replace('corr','').replace('un','')\n                            if not name in random_dict: random_dict[name] = {}\n                                                                                                                                                                                                                              \n                            for rot in ['0','1','2','3']:                                                                                                                                   \n                                if not o.has_key(rot):\n                                    o[rot] = {}\n                                if drand['sdssredchinocorr$' + rot] != 'None':\n                                                                                                                                                                                                                                                                        \n                                    print drand['sample_size'], string.find(drand['sample_size'],'uncorr') != -1\n                                    if string.find(drand['sample_size'],'corr') != -1 and string.find(drand['sample_size'],'uncorr') == -1:  \n                                        #if drand['sdssredchinocorr$' + rot]\n                                        o[rot]['corr'] = drand['sdssredchinocorr$' + rot]\n                                    if string.find(drand['sample_size'],'uncorr') != -1:\n                                        o[rot]['uncorr'] = drand['sdssredchinocorr$' + rot]\n                        num = 0\n                        factor = 0 \n                        for rot in ['0','1','2','3']:                                                                                                                                   \n                            if 'corr' in o[rot] and 'uncorr' in o[rot]:  \n                                if o[rot].has_key('uncorr') and o[rot].has_key('corr'):\n                                    if float(o[rot]['corr']) != 0:\n                                        factor += float(o[rot]['uncorr'])/float(o[rot]['corr'])\n                                        num += 1.\n                                                                                                                                                                                                                                                                        \n                                                                                                                                                                                                                                                                        \n                        if num!=0:\n                            save_fit({'PPRUN':dtop['PPRUN'],'OBJNAME':dtop['OBJNAME'],'FILTER':dtop['FILTER'],'sdss_imp':factor/num, 'sample':'record','sample_size':'record',},db='try_db')\n                                                                                                                                                                                                                                                                        \n                                                                                                                                                                                                                                                                        \n                                                                                                                                                                                                                                                                        \n                ''' calculate the mean and std of the reduced chi sq improvement '''\n                \n                \n                \n                ''' calculate the variance in the best fit '''\n                \n                ''' retrieve all sdss tests '''\n                \n                ''' decide if good '''\n                                                                                                                                                                                                                                                                        \n                save_fit({'PPRUN':dtop['PPRUN'],'OBJNAME':dtop['OBJNAME'],'FILTER':dtop['FILTER'],'stats':'yes', 'sample':'record','sample_size':'record',},db='try_db')\n    \n            #except:        \n            #    save_fit({'PPRUN':dtop['PPRUN'],'OBJNAME':dtop['OBJNAME'],'FILTER':dtop['FILTER'],'stats':'failed', 'sample':'record','sample_size':'record',},db='try_db')\n        #except: print 'failed'\n\ndef test_correction(OBJNAME,FILTER,PPRUN,sample,sample_size):\n\n    sample = str(sample)\n    sample_size = str(sample_size)\n\n    import scipy, re, string, os\n\n    ''' create chebychev polynomials '''\n    cheby_x = [{'n':'0x','f':lambda x,y:1.},{'n':'1x','f':lambda x,y:x},{'n':'2x','f':lambda x,y:2*x**2-1},{'n':'3x','f':lambda x,y:4*x**3.-3*x}] \n    cheby_y = [{'n':'0y','f':lambda x,y:1.},{'n':'1y','f':lambda x,y:y},{'n':'2y','f':lambda x,y:2*y**2-1},{'n':'3y','f':lambda x,y:4*y**3.-3*y}]\n    cheby_terms = []\n    cheby_terms_no_linear = []\n    for tx in cheby_x:\n        for ty in cheby_y:\n            if not ((tx['n'] == '0x' and ty['n'] == '0y')):\n                cheby_terms.append({'n':tx['n'] + ty['n'],'fx':tx['f'],'fy':ty['f']})\n            if not ((tx['n'] == '0x' and ty['n'] == '0y') or (tx['n'] == '0x' and ty['n'] == '1y') or (tx['n'] == '1x' and ty['n'] == '0y')) :\n                cheby_terms_no_linear.append({'n':tx['n'] + ty['n'],'fx':tx['f'],'fy':ty['f']})\n\n    import re, time                                                                                                                \n    dt = get_a_file(OBJNAME,FILTER,PPRUN)                \n    d = get_fits(OBJNAME,FILTER,PPRUN, sample, sample_size)                \n    print d.keys()\n\n    column_prefix = '' \n    position_columns_names = re.split('\\,',d['positioncolumns']) \n    print position_columns_names, 'position_columns_names'\n    fitvars = {}\n    cheby_terms_dict = {}\n    print column_prefix, position_columns_names\n    ROTS_dict = {} \n    for ele in position_columns_names:                      \n        print ele\n        if type(ele) != type({}):\n            ele = {'name':ele}\n        res = re.split('\\$',ele['name'])\n        if len(res) > 1:\n            ROTS_dict[res[0]] = ''\n            print res\n        if string.find(ele['name'],'zp_image') == -1:\n            print sample, sample_size, ele['name']\n            fitvars[ele['name']] = float(d[ele['name']]) \n            for term in cheby_terms:\n                if len(res) > 1:\n                    if term['n'] == res[1]:                 \n                        cheby_terms_dict[term['n']] = term \n\n    ROTS = ROTS_dict.keys()\n    print ROTS\n                                                                                     \n    zp_images = re.split(',',d['zp_images'])\n    zp_images_names = re.split(',',d['zp_images_names'])\n                                                                                     \n    for i in range(len(zp_images)):\n        fitvars[zp_images_names[i]] = float(zp_images[i])\n                                                                                     \n    cheby_terms_use =  [cheby_terms_dict[k] for k in cheby_terms_dict.keys()]\n\n    print cheby_terms_use, fitvars\n    print dt \n    print dt['CHIPS']\n\n    CHIPS = [int(x) for x in re.split(',',dt['CHIPS'])]\n    print CHIPS \n    print dt.keys()\n    LENGTH1, LENGTH2 = dt['LENGTH1'], dt['LENGTH2']\n\n    per_chip = True\n\n    coord_conv_x = lambda x:(2.*x-0-LENGTH1)/(LENGTH1-0) \n    coord_conv_y = lambda x:(2.*x-0-LENGTH2)/(LENGTH2-0) \n\n    bin = 100\n    import numpy, pyfits\n    x,y = numpy.meshgrid(numpy.arange(0,LENGTH1,bin),numpy.arange(0,LENGTH2,bin))\n\n    x_conv = coord_conv_x(x)\n    y_conv = coord_conv_y(y)\n    \n    epsilon = 0\n    index = 0\n    ROT=ROTS[0]\n    for term in cheby_terms_use:\n        index += 1\n        #print index, ROT, term, fitvars[str(ROT)+'$'+term['n']]\n        epsilon += fitvars[str(ROT)+'$'+term['n']]*term['fx'](x_conv,y_conv)*term['fy'](x_conv,y_conv)\n\n    diff = ((x-LENGTH1/2.)**2.+(y-LENGTH2/2.)**2.) - (LENGTH1/2.)**2.\n    diff_bool = diff[diff<0]\n    diff[diff>0] = 0\n    diff[diff<0] = 1\n    import copy\n    diff2 = copy.copy(diff) \n    diff2[diff2==0] = -999\n    diff2[diff2==1] = 0\n    hdu = pyfits.PrimaryHDU(diff)\n    im = '/usr/work/pkelly/diff.fits'                                                                                                             \n    #os.system('rm ' + im)\n    #hdu.writeto(im)\n    print im, 'finished'\n\n    epsilon = epsilon * diff + diff2\n\n    \n    flat = epsilon.flatten().compress(epsilon.flatten()[epsilon.flatten()!=0])\n     \n    \n    print numpy.median(flat), len(epsilon.flatten()), len(flat)\n\n    epsilon = epsilon - numpy.median(flat)\n\n    if False:\n        print 'writing'                                                                                                                                \n        hdu = pyfits.PrimaryHDU(epsilon)\n        #os.system('rm ' + tmpdir + 'correction' + ROT + filter + sample_size + '.fits')\n        #hdu.writeto(tmpdir + '/correction' + ROT + filter + sample_size + '.fits')\n        \n        im = '/usr/work/pkelly/test.fits'                                                                                                             \n        os.system('rm ' + im)\n        hdu.writeto(im)\n        print im, 'finished'\n        raw_input()\n\n    return epsilon, diff_bool\n\n    \n    \n    \n\n\ndef describe_db_long(c,db=['illumination_db']):\n    if type(db) != type([]):\n        db = [db]\n    keys = []\n    for d in db:\n        command = \"DESCRIBE \" + d \n        #print command\n        c.execute(command)\n        results = c.fetchall()\n        for line in results:\n            keys.append([line[0],line[1]])\n    return keys    \n\ndef fix_table3():\n    import MySQLdb, sys, os, re, time, utilities, pyfits                                       \n    from copy import copy\n    db2 = MySQLdb.connect(db='subaru', user='weaklensing', passwd='darkmatter', host='ki-rh8')\n    c = db2.cursor()\n    db_keys = describe_db_long(c,['try_db','illumination_db'])\n    print db_keys\n    for key in db_keys:\n        command = 'alter table fit_db change ' + key[0] + ' ' + key[0].replace('None','None') + ' ' + key[1] \n        print command\n        c.execute(command)\n\ndef find_bad_rot():\n    import MySQLdb, sys, os, re, time, utilities, pyfits\n    from copy import copy\n    db2 = MySQLdb.connect(db='subaru', user='weaklensing', passwd='darkmatter', host='ki-rh8')\n    c = db2.cursor()\n    db_keys_f = describe_db(c,['try_db'])\n    command = 'select * from try_db i where i.todo=\"good\" and i.rotprob is null' #i.objname is not null and i.objname!=\"ki06\" group by i.objname, i.pprun'\n    print command          \n    c.execute(command)\n    results=c.fetchall()\n    for line in results:\n        global tmpdir\n        dtop = {}  \n        for i in range(len(db_keys_f)):\n            dtop[db_keys_f[i]] = str(line[i])\n        FILTER = dtop['FILTER']\n        PPRUN = dtop['PPRUN']\n        OBJNAME = dtop['OBJNAME']\n        db = 'try_db'\n        db_keys_t = describe_db(c,['fit_db'])        \n        c.execute(\"SELECT  * from fit_db where sample_size='all' and  pprun= '\" + PPRUN + \"' and objname= '\" + OBJNAME + \"' limit 1\")\n        results = c.fetchall() \n        dtop_t = {}\n        for i in range(len(db_keys_t)):\n            dtop_t[db_keys_t[i]] = str(results[0][i])\n\n\n        print dtop_t['zp_images_names']\n        res = re.split('\\,',dtop_t['zp_images_names'])\n        print res\n\n        \n        rots = {} \n        for rot in [0,1,2,3]:\n            print dtop_t[str(rot)+'$1x1y']\n            if str(dtop_t[str(rot)+'$1x1y']) != 'None':\n                rots[rot] =  0 \n        print rots\n\n        for i in res:    \n            print i\n            im = i.replace('zp_image_','')\n            print im\n            db_keys_i = describe_db(c,['illumination_db'])                                                                                                      \n            command = 'select * from illumination_db i where i.supa=\"' + im + '\"' #i.objname is not null and i.objname!=\"ki06\" group by i.objname, i.pprun'\n            print command          \n            c.execute(command)\n            results=c.fetchall()\n            for line in results:\n                global tmpdir\n                dtop2 = {}  \n                for i in range(len(db_keys_i)):\n                    dtop2[db_keys_i[i]] = str(line[i])\n                print dtop2['ROTATION'], dtop_t['1$1x1y']\n                if str(dtop2['ROTATION']) != 'None':\n                    rots[int(float(dtop2['ROTATION']))] += 1 \n        print rots\n        ok = True\n        for key in rots.keys():\n            if rots[key] < 2:   \n                print key\n                command = 'update try_db set rotprob=\"yes\" where OBJNAME=\"' + dtop['OBJNAME'] + '\" and PPRUN = \"' + dtop['PPRUN'] + '\"'               \n                print command\n                c.execute(command)\n                ok = False\n        if ok: \n            command = 'update try_db set rotprob=\"no\" where OBJNAME=\"' + dtop['OBJNAME'] + '\" and PPRUN = \"' + dtop['PPRUN'] + '\"'               \n            print command\n            c.execute(command)\n\n\n\n\n\n\ndef update_fit_db():\n    import MySQLdb, sys, os, re, time, utilities, pyfits\n    from copy import copy\n    db2 = MySQLdb.connect(db='subaru', user='weaklensing', passwd='darkmatter', host='ki-rh8')\n    c = db2.cursor()\n    db_keys_f = describe_db(c,['illumination_db'])\n\n    command = 'select * from illumination_db i where i.objname is not null and i.objname!=\"ki06\" group by i.objname, i.pprun'\n    print command          \n    c.execute(command)\n    results=c.fetchall()\n    for line in results:\n        global tmpdir\n        dtop = {}  \n        for i in range(len(db_keys_f)):\n            dtop[db_keys_f[i]] = str(line[i])\n        FILTER = dtop['filter']\n        PPRUN = dtop['PPRUN']\n        OBJNAME = dtop['OBJNAME']\n        db = 'try_db'\n        c.execute(\"SELECT pprun from try_db where  pprun= '\" + PPRUN + \"' and objname= '\" + OBJNAME + \"'\")\n        results = c.fetchall()                                                                                                   \n        print results\n        print OBJNAME, FILTER, PPRUN\n        if len(results) > 0:\n            print 'already added'\n        else:\n            command = \"INSERT INTO \" + db + \" (OBJNAME,FILTER,PPRUN,sample,sample_size) VALUES ('\" + dtop['OBJNAME'] + \"','\" + dtop['filter'] + \"','\" + dtop['PPRUN'] + \"','record','record')\" \n            print command          \n            c.execute(command)\n            raw_input()\n\n\n\n\n\n\ndef fix_table_all():\n    import MySQLdb, sys, os, re, time, utilities, pyfits\n    from copy import copy\n    db2 = MySQLdb.connect(db='subaru', user='weaklensing', passwd='darkmatter', host='ki-rh8')\n    c = db2.cursor()\n    db_keys_f = describe_db(c,['try_db','illumination_db'])\n\n    command = 'select * from illumination_db i left join try_db t on (t.pprun=i.pprun and t.objname=i.objname) where t.pprun is null and t.objname is null group by i.objname, i.pprun'\n    print command          \n    c.execute(command)\n    results=c.fetchall()\n    for line in results:\n        global tmpdir\n        dtop = {}  \n        for i in range(len(db_keys_f)):\n            dtop[db_keys_f[i]] = str(line[i])\n        FILTER = dtop['FILTER']\n        PPRUN = dtop['PPRUN']\n        OBJNAME = dtop['OBJNAME']\n        db = 'try_db'\n        command = \"INSERT INTO \" + db + \" (OBJNAME,FILTER,PPRUN,sample,sample_size) VALUES ('\" + dtop['OBJNAME'] + \"','\" + dtop['FILTER'] + \"','\" + dtop['PPRUN'] + \"','record','record')\"\n        print command          \n        raw_input()\n        c.execute(command)\n\n\ndef fix_table():\n    import MySQLdb, sys, os, re, time, utilities, pyfits\n    from copy import copy\n    db2 = MySQLdb.connect(db='subaru', user='weaklensing', passwd='darkmatter', host='ki-rh8')\n    c = db2.cursor()\n    db_keys_f = describe_db(c,['try_db'])\n\n    command = 'select * from try_db group by objname, pprun'\n    print command          \n    c.execute(command)\n    results=c.fetchall()\n    for line in results:\n        global tmpdir\n        dtop = {}  \n        for i in range(len(db_keys_f)):\n            dtop[db_keys_f[i]] = str(line[i])\n        FILTER = dtop['FILTER']\n        PPRUN = dtop['PPRUN']\n        OBJNAME = dtop['OBJNAME']\n        command = 'select * from try_db where OBJNAME=\"' + dtop['OBJNAME'] + '\" and PPRUN = \"' + dtop['PPRUN'] + '\"' \n        print command          \n        c.execute(command)\n        results=c.fetchall()\n        ids = []\n        for line in results:\n            global tmpdir\n            dtop = {}  \n            for i in range(len(db_keys_f)):\n                dtop[db_keys_f[i]] = str(line[i])\n            ids.append(dtop['id'])\n        print ids\n        ids.sort()\n        print ids[-1]\n        if len(ids) > 1:\n            for i in ids[:-1]:\n                command = 'delete from try_db where id =' +  i                \n                print command\n                c.execute(command)\n\n\ndef fix_try2():\n    import MySQLdb, sys, os, re, time, utilities, pyfits                                       \n    from copy import copy\n    db2 = MySQLdb.connect(db='subaru', user='weaklensing', passwd='darkmatter', host='ki-rh8')\n    c = db2.cursor()\n    db_keys_f = describe_db(c,['try_db'])\n\n    command = 'select * from try_db' # group by objname, pprun'\n    print command          \n    c.execute(command)\n    results=c.fetchall()\n    for line in results:\n        try:\n            global tmpdir                                                                                                                                   \n            dtop = {}  \n            for i in range(len(db_keys_f)):\n                dtop[db_keys_f[i]] = str(line[i])\n            FILTER = re.split('_',dtop['PPRUN'])[1]\n            PPRUN = dtop['PPRUN']\n            OBJNAME = dtop['OBJNAME']\n            command = 'update try_db set FILTER=\"' + FILTER + '\" where OBJNAME=\"' + dtop['OBJNAME'] + '\" and PPRUN = \"' + dtop['PPRUN'] + '\"'              \n            print command\n            c.execute(command)\n        except: print 'failed'\n\ndef update_9():\n    import MySQLdb, sys, os, re, time, utilities, pyfits                                       \n    from copy import copy\n    db2 = MySQLdb.connect(db='subaru', user='weaklensing', passwd='darkmatter', host='ki-rh8')\n    c = db2.cursor()\n    db_keys_f = describe_db(c,['illumination_db','try_db'])\n\n    command = 'select * from illumination_db i left join try_db j on i.pprun=j.pprun and i.objname=j.objname  where config like \"%9%\" or config like \"%8%\" group by j.objname, j.pprun ' \n\n    print command          \n    c.execute(command)\n    results=c.fetchall()\n    for line in results:\n        global tmpdir                                                                                                                                   \n        dtop = {}  \n        for i in range(len(db_keys_f)):\n            dtop[db_keys_f[i]] = str(line[i])\n        print dtop['PPRUN']\n\n        if str(dtop['PPRUN']) != 'None':\n            FILTER = re.split('_',dtop['PPRUN'])[1]                                                                                                                     \n            PPRUN = dtop['PPRUN']\n            OBJNAME = dtop['OBJNAME']\n            command = 'update try_db set bootstrapstatus=\"NULL\" where OBJNAME=\"' + dtop['OBJNAME'] + '\" and PPRUN = \"' + dtop['PPRUN'] + '\"'              \n            print command\n            c.execute(command)\n\n            #command = 'update try_db set Nonestatus=\"NULL\" where OBJNAME=\"' + dtop['OBJNAME'] + '\" and PPRUN = \"' + dtop['PPRUN'] + '\"'              \n            #print command\n            #c.execute(command)\n\n\n\n''' add CONFIG to try_db '''\ndef add_config():\n    import MySQLdb, sys, os, re, time, utilities, pyfits                                       \n    from copy import copy\n    db2 = MySQLdb.connect(db='subaru', user='weaklensing', passwd='darkmatter', host='ki-rh8')\n    c = db2.cursor()\n    db_keys_f = describe_db(c,['try_db'])\n\n    command = 'select * from try_db where CONFIG=8'\n    print command          \n    c.execute(command)\n    results=c.fetchall()\n    for line in results:\n        global tmpdir\n        dtop = {}  \n        for i in range(len(db_keys_f)):\n            dtop[db_keys_f[i]] = str(line[i])\n        FILTER = dtop['FILTER']\n        PPRUN = dtop['PPRUN']\n        OBJNAME = dtop['OBJNAME']\n        command = 'select * from illumination_db where OBJNAME=\"' + dtop['OBJNAME'] + '\" and PPRUN = \"' + dtop['PPRUN'] + '\" and CONFIG is not null' \n        print command          \n        c.execute(command)\n        results=c.fetchall()\n        if len(results)>0:\n            db_keys = describe_db(c,['illumination_db'])                                                                                                                          \n            dtop2 = {}  \n            line = results[0]\n            for i in range(len(db_keys)):\n                dtop2[db_keys[i]] = str(line[i])\n            command = 'update fit_db set CONFIG=\"' + dtop2['CONFIG'] + '\" where OBJNAME=\"' + dtop['OBJNAME'] + '\" and PPRUN = \"' + dtop['PPRUN'] + '\"'              \n            print command\n            c.execute(command)\n            \n\n\n\ndef fix_try():\n    import MySQLdb, sys, os, re, time, utilities, pyfits                                       \n    from copy import copy\n    db2 = MySQLdb.connect(db='subaru', user='weaklensing', passwd='darkmatter', host='ki-rh8')\n    c = db2.cursor()\n    db_keys_f = describe_db(c,['try_db'])\n\n    command = 'select * from try_db group by objname, pprun'\n    print command          \n    c.execute(command)\n    results=c.fetchall()\n    for line in results:\n        global tmpdir\n        dtop = {}  \n        for i in range(len(db_keys_f)):\n            dtop[db_keys_f[i]] = str(line[i])\n        FILTER = dtop['FILTER']\n        PPRUN = dtop['PPRUN']\n        OBJNAME = dtop['OBJNAME']\n        command = 'select * from illumination_db where OBJNAME=\"' + dtop['OBJNAME'] + '\" and PPRUN = \"' + dtop['PPRUN'] + '\"' \n        print command          \n        c.execute(command)\n        results=c.fetchall()\n        if len(results)==0:\n            command = 'delete from try_db where OBJNAME=\"' + dtop['OBJNAME'] + '\" and PPRUN = \"' + dtop['PPRUN'] + '\"'              \n            print command\n            c.execute(command)\n\n\n\ndef check_format():\n    import MySQLdb, sys, os, re, time, utilities, pyfits                                       \n    from copy import copy\n    db2 = MySQLdb.connect(db='subaru', user='weaklensing', passwd='darkmatter', host='ki-rh8')\n    c = db2.cursor()\n    db_keys_f = describe_db(c,['try_db'])\n\n    command = 'select * from try_db and (bootstrapstatus=\"failed\" or sdssstatus=\"failed\") group by objname, pprun'\n    print command          \n    c.execute(command)\n    results=c.fetchall()\n    for line in results:\n        global tmpdir\n        dtop = {}  \n        for i in range(len(db_keys_f)):\n            dtop[db_keys_f[i]] = str(line[i])\n        FILTER = dtop['FILTER']\n        PPRUN = dtop['PPRUN']\n        OBJNAME = dtop['OBJNAME']\n        command = 'select * from fit_db where OBJNAME=\"' + dtop['OBJNAME'] + '\" and PPRUN = \"' + dtop['PPRUN'] + '\" and sample_size=\"all\" and (sample=\"None\" or sample=\"sdss\" )' \n        db_keys = describe_db(c,['fit_db'])\n        print db_keys\n        print command          \n        c.execute(command)\n        results=c.fetchall()\n        if len(results) > 0:\n            for line in results:                    \n                dtop = {}  \n                for i in range(len(db_keys)):\n                    dtop[db_keys[i]] = str(line[i])\n            catalog = dtop['catalog']\n            import astropy, astropy.io.fits as pyfits, glob\n            if len(glob.glob(catalog)) > 0:\n                p = pyfits.open(catalog)        \n                format = p[1].columns[0].format\n                print format\n                if format == 'D': print 'good'\n                else: \n                    print 'bad'\n                    command = 'update try_db set format=\"bad\" where OBJNAME=\"' + OBJNAME + '\" and PPRUN =\"'+ PPRUN +'\"'   \n                    print command\n                    c.execute(command)\n                    command = 'update try_db set Nonestatus=\"NULL\" where OBJNAME=\"' + OBJNAME + '\" and PPRUN =\"'+ PPRUN +'\"'   \n                    print command\n                    c.execute(command)\n                    command = 'update try_db set sdssstatus=\"NULL\" where OBJNAME=\"' + OBJNAME + '\" and PPRUN =\"'+ PPRUN +'\"'   \n                    print command\n                    c.execute(command)\n\n\n\n\n\ndef delete_nodata_table():\n    import MySQLdb, sys, os, re, time, utilities, pyfits                                       \n    from copy import copy\n    db2 = MySQLdb.connect(db='subaru', user='weaklensing', passwd='darkmatter', host='ki-rh8')\n    c = db2.cursor()\n    db_keys_f = describe_db(c,['illumination_db'])\n\n    command = 'select * from illumination_db group by objname, pprun'\n    print command          \n    c.execute(command)\n    results=c.fetchall()\n    for line in results:\n        global tmpdir\n        dtop = {}  \n        for i in range(len(db_keys_f)):\n            dtop[db_keys_f[i]] = str(line[i])\n        PPRUN = dtop['PPRUN']\n        OBJNAME = dtop['OBJNAME']\n        import glob\n        files = glob.glob(dtop['file'])\n        if len(files) == 0:\n            command = 'delete from fit_db where PPRUN =\"'+dtop['PPRUN']+'\" and OBJNAME=\"'+dtop['OBJNAME']+'\"'               \n            print command\n            c.execute(command)\n            command = 'delete from try_db where PPRUN =\"'+dtop['PPRUN']+'\" and OBJNAME=\"'+dtop['OBJNAME']+'\"'               \n            print command\n            c.execute(command)\n            command = 'delete from illumination_db where PPRUN =\"'+dtop['PPRUN']+'\" and OBJNAME=\"'+dtop['OBJNAME']+'\"'               \n            print command\n            c.execute(command)\n            raw_input()\n\n\n\ndef fix_objname():\n    import MySQLdb, sys, os, re, time, utilities, pyfits                                       \n    from copy import copy\n    db2 = MySQLdb.connect(db='subaru', user='weaklensing', passwd='darkmatter', host='ki-rh8')\n    c = db2.cursor()\n    db_keys_f = describe_db(c,['illumination_db'])\n\n    command = 'select * from illumination_db where objname=\"ki06\" or objname=\"error:\"'\n    print command          \n    c.execute(command)\n    results=c.fetchall()\n    for line in results:\n        global tmpdir\n        dtop = {}  \n        for i in range(len(db_keys_f)):\n            dtop[db_keys_f[i]] = str(line[i])\n        PPRUN = dtop['PPRUN']\n        OBJNAME = dtop['OBJNAME']\n        import glob\n        files = glob.glob(dtop['file'])\n        import re \n        res = re.split('\\/',re.split('\\/SUBARU\\/',dtop['file'].replace('\\/\\/','\\/'))[1])\n        import string\n        if string.find('-',res[0]) == -1 and string.find('_',res[0]) == -1:                                                                    \n            res = res[1:]\n        objname = res                                                                                                                            \n        print dtop['file'], objname , dtop['PPRUN'], dtop['OBJNAME'], dtop['OBJECT']\n        if len(files) == 0:\n            command = 'update illumination_db set OBJNAME=\"' + objname+ '\" where PPRUN =\"'+dtop['PPRUN']+'\" and OBJNAME=\"'+dtop['OBJNAME']+'\"'   \n            print command\n            #c.execute(command)\n            raw_input()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef variance(data,err):\n\n    d = 0\n    w = 0\n\n    for i in range(len(data)):\n        w += 1/err[i]**2.\n        d += data[i]/err[i]**2.\n\n    mean = d/w\n\n    w = 0\n    d = 0\n    for i in range(len(data)):\n        w += 1/err[i]**2.\n        d += 1/err[i]**2.*(data[i] - mean)**2.\n\n    weight_variance = d/w    \n    import scipy\n    variance = scipy.var(data)\n\n    n = 0\n    d = 0\n    for i in range(len(data)):\n        n += 1.\n        d += ((data[i] - mean)/err[i])**2.\n\n    ''' this is not quite right '''\n\n    redchi = d/n\n\n    return variance, weight_variance, redchi\n\n\ndef random_cmp(x,y):\n    import random\n    a = random.random()\n    b = random.random()\n    if a > b: return 1\n    else: return -1\n\ndef bright_cmp(x,y):\n    import random\n    if x['mag'] > y['mag']: return 1\n    else: return -1\n\ndef starStats(supas):\n    dict = {} \n    dict_ims = {}\n    dict['rot'] = 0\n    dict['match'] = 0\n    dict['match_exists'] = 0\n    for s in supas:\n        if s['match']: dict['match'] += 1\n        if s['match_exists']: dict['match_exists'] += 1\n        s = s['supa files']\n        for ele in s:\n            if not dict.has_key(ele['name']):\n                dict[ele['name']] = 0 \n                dict_ims[ele['name']] = 0 \n            rots = {}\n            dict[ele['name']] += 1\n            dict_ims[ele['name']] += 1\n            rots[ele['rotation']] = 'yes' \n        if len(rots.keys()) > 1:\n            dict['rot'] += 1\n           \n    dict['ims'] = dict_ims \n    for key in dict.keys():\n        print key, dict[key]\n\n    return dict\n\n\ndef length_swarp(SUPA,FLAT_TYPE,CHIPS):\n    import os, re, utilities, bashreader, sys, string\n    from copy import copy\n    from glob import glob\n    dict = get_files(SUPA,FLAT_TYPE)\n    search_params = initialize(dict['filter'],dict['OBJNAME'])\n    search_params.update(dict)\n    path='/nfs/slac/g/ki/ki05/anja/SUBARU/%(OBJNAME)s/' % {'OBJNAME':search_params['OBJNAME']}\n\n    all_chip_dict = {}\n    NUMScommas = reduce(lambda x,y: str(x) + ',' + str(y),CHIPS.keys())\n    all_chip_dict['CHIPS'] = NUMScommas\n\n    print sorted(CHIPS.keys())\n    NUMS = []\n    start = 1\n    crpix1s = []\n    crpix2s = []\n    for CHIP in CHIPS.keys():\n        NUMS.append(CHIP)        \n        \n\n        if len(CHIPS[CHIP]) == 0:\n            print CHIP\n        if len(CHIPS[CHIP]) > 0:\n\n            crpix = CHIPS[CHIP] \n            import re                                                                                                                               \n            p = re.compile('\\_\\d+O')\n            file = p.sub('_' + str(CHIP) + 'O',search_params['file'])\n            print file, CHIP\n            \n            naxis = utilities.get_header_kw(file,['NAXIS1','NAXIS2'])\n            print naxis, CHIP\n            \n            for kw in ['NAXIS1','NAXIS2']:\n                crpix[kw] = float(naxis[kw])\n                print naxis[kw]\n            print file\n            \n            if start == 1:\n                crpixzero = copy(crpix)\n                crpixhigh = copy(crpix)\n                start = 0\n            from copy import copy \n            print  float(crpix['CRPIX1'])  < float(crpixzero['CRPIX1']), float(crpix['CRPIX2'])  < float(crpixzero['CRPIX2'])\n            if float(crpix['CRPIX1']) + 0   >= float(crpixzero['CRPIX1']):\n                crpixzero['CRPIX1'] = copy(crpix['CRPIX1'])\n            if float(crpix['CRPIX2'])  + 0 >= float(crpixzero['CRPIX2']):\n                crpixzero['CRPIX2'] = copy(crpix['CRPIX2'])\n                                                                                                                              \n            if float(crpix['CRPIX1']) - 0  <= float(crpixhigh['CRPIX1']):\n                crpixhigh['CRPIX1'] = copy(crpix['CRPIX1'])\n            if float(crpix['CRPIX2']) - 0  <= float(crpixhigh['CRPIX2']):\n                crpixhigh['CRPIX2'] = copy(crpix['CRPIX2'])\n            \n            crpix1s.append(copy(crpix['CRPIX1']))\n            crpix2s.append(copy(crpix['CRPIX2']))\n                                                                                                                                                   \n            print crpix['CRPIX1'], crpix['CRPIX2'], crpixzero['CRPIX1'], crpixzero['CRPIX2'], crpixhigh['CRPIX1'], crpixhigh['CRPIX2']#, crpixhigh\n            print crpix.keys()\n            for kw in ['CRPIX1','CRPIX2','CRVAL1','CRVAL2','NAXIS1','NAXIS2']:\n                all_chip_dict[kw+ '_' + str(CHIP)] = crpix[kw]\n\n\n    #plot_chips(crpix1s,crpix2s)\n    for i in range(len(crpix1s)): \n        print crpix1s[i],crpix2s[i], NUMS[i] \n    crpix1s.sort()\n    crpix2s.sort()\n\n    print len(crpix1s), crpix1s, crpix2s, crpix1s[-1] - crpix1s[0] + crpix['NAXIS1'], crpix2s[-1] - crpix2s[0] + crpix['NAXIS2']\n\n    print all_chip_dict                                                                                                                                                                                    \n    \n    LENGTH1 =  abs(float(crpixhigh['CRPIX1']) - float(crpixzero['CRPIX1'])) + float(crpix['NAXIS1'])\n    LENGTH2 =  abs(float(crpixhigh['CRPIX2']) - float(crpixzero['CRPIX2'])) + float(crpix['NAXIS2']) \n    \n    print LENGTH1, LENGTH2, crpixzero['CRPIX1'], crpixzero['CRPIX2'], crpixhigh['CRPIX1'], crpixhigh['CRPIX2']#, crpixhigh   \n    all_chip_dict.update({'crfixednew':'third','LENGTH1':LENGTH1,'LENGTH2':LENGTH2,'CRPIX1ZERO':crpixzero['CRPIX1'],'CRPIX2ZERO':crpixzero['CRPIX2'],'CRVAL1':crpix['CRVAL1'],'CRVAL2':crpix['CRVAL2']})     \n    save_exposure(all_chip_dict,SUPA,FLAT_TYPE)                                                                                                                                                           \n    return all_chip_dict\n\ndef fix_radec(SUPA,FLAT_TYPE):\n    #cats = [{'im_type': 'DOMEFLAT', 'cat': '' + search_params['TEMPDIR'] + '/SUPA0005188_3OCFS.DOMEFLAT.fixwcs.rawconv'}, {'im_type': 'SKYFLAT', 'cat': '' + search_params['TEMPDIR'] + '/SUPA0005188_3OCFS.SKYFLAT.fixwcs.rawconv'}, {'im_type': 'OCIMAGE', 'cat': '' + search_params['TEMPDIR'] + '/SUPA0005188_3OCFS.OCIMAGE.fixwcs.rawconv'}] \n    #outfile = '' + search_params['TEMPDIR'] + 'stub'\n    #cats = [{'im_type': 'MAIN', 'cat': '' + search_params['TEMPDIR'] + '/SUPA0005188_3OCFS..fixwcs.rawconv'}, {'im_type': 'D', 'cat': '' + search_params['TEMPDIR'] + '/SUPA0005188_3OCFS.D.fixwcs.rawconv'}]\n\n    import astropy, astropy.io.fits as pyfits, sys, os, re, string, copy\n    from config_bonn import cluster, tag, arc, filters\n    ppid = str(os.getppid())\n\n    #chips = length(SUPA,FLAT_TYPE)\n\n    #import time\n    #time.sleep(2)\n\n    dict = get_files(SUPA,FLAT_TYPE)\n    search_params = initialize(dict['filter'],dict['OBJNAME'])\n    search_params.update(dict)\n    path='/nfs/slac/g/ki/ki05/anja/SUBARU/%(OBJNAME)s/' % {'OBJNAME':search_params['OBJNAME']}\n\n    from copy import copy\n    chips = {}\n    NUMS = []\n    at_least_one = False \n    print dict['file']\n    for image in dict['files']:\n        params = copy(search_params)     \n        ROOT = re.split('\\.',re.split('\\/',image)[-1])[0]\n        params['ROOT'] = ROOT\n        BASE = re.split('O',ROOT)[0]\n        params['BASE'] = BASE \n        NUM = re.split('O',re.split('\\_',ROOT)[1])[0]\n        params['NUM'] = NUM\n        print NUM, BASE, ROOT, image\n        params['GAIN'] = 2.50 ## WARNING!!!!!!\n        print ROOT\n        finalflagim = \"%(TEMPDIR)sflag_%(ROOT)s.fits\" % params     \n        res = re.split('SCIENCE',image)\n        res = re.split('/',res[0])\n        if res[-1]=='':res = res[:-1]\n        params['path'] = reduce(lambda x,y:x+'/'+y,res[:-1])\n        params['fil_directory'] = res[-1]\n        print params['fil_directory']\n        res = re.split('_',res[-1])\n    \n        ''' if three second exposure, use the headers in the directory '''\n        if string.find(dict['fil_directory'],'CALIB') != -1:\n            params['directory'] = params['fil_directory'] \n        else: \n            params['directory'] = res[0]\n\n\n        print params['directory']\n        print BASE\n        SDSS = \"/%(path)s/%(directory)s/SCIENCE/headers_scamp_SDSS-R6/%(BASE)s.head\" % params   # it's not a ZERO!!!\n        TWOMASS = \"/%(path)s/%(directory)s/SCIENCE/headers_scamp_2MASS/%(BASE)s.head\" % params\n        NOMAD = \"/%(path)s/%(directory)s/SCIENCE/headers_scamp_NOMAD*/%(BASE)s.head\" % params\n\n        SDSS = SDSS.replace('I_','_').replace('I.','.')\n                                                                                                     \n        from glob import glob \n        print SDSS\n        print SDSS, TWOMASS, NOMAD\n        print glob(SDSS), glob(TWOMASS), glob(NOMAD)\n        head = None\n        heads = []\n        if len(glob(TWOMASS)) > 0:\n            heads.append(glob(TWOMASS)[0])\n        if len(glob(TWOMASS.replace('.head','O*.head'))) > 0:\n            heads.append(glob(TWOMASS.replace('.head','O*.head'))[0])\n        if len(glob(NOMAD)) > 0:\n            heads.append(glob(NOMAD)[0])\n        if len(glob(NOMAD.replace('.head','O*.head'))) > 0:\n            heads.append(glob(NOMAD.replace('.head','O*.head'))[0])\n\n        print heads\n\n\n        ''' pick out latest SCAMP solution not SDSS '''\n        if len(heads) > 0:\n            a = [[os.stat(f).st_mtime,f] for f in heads ]\n            a.sort()\n            print a \n            head = a[-1][1]\n            print head \n       \n        ''' if SDSS exists, use that '''\n        if len(glob(SDSS)) > 0:\n            head = glob(SDSS)[0]\n        if len(glob(SDSS.replace('.head','O*.head'))) > 0:\n            head = glob(SDSS.replace('.head','O*.head'))[0]\n\n        print head, SDSS, glob(SDSS)\n\n\n        #else:\n        #    raise Exception\n\n\n\n        print head, SDSS\n          \n        w = {}\n\n        if head is not None:\n            keys = []\n            hf = open(head,'r').readlines()                                                                    \n            print head\n            for line in hf:\n                at_least_one = True\n                import re\n                if string.find(line,'=') != -1:\n                    res = re.split('=',line)\n                    name = res[0].replace(' ','')\n                    res = re.split('/',res[1])\n                    value = res[0].replace(' ','')\n                    print name, value\n                    if string.find(name,'CD')!=-1 or string.find(name,'PV')!=-1 or string.find(name,'CR')!=-1 or string.find(name,'NAXIS') != -1:\n                        w[name] = float(value)\n                        print line, w[name]\n                        keys.append(name)\n        from copy import copy\n        chips[NUM] = copy(w)\n        print w \n        NUMS.append(NUM)\n\n    if at_least_one:\n\n        chip_dict = length_swarp(SUPA,FLAT_TYPE,chips)                                                                                                                                          \n        vecs = {}        \n        for key in keys:\n            vecs[key] = []\n        vecs['good_scamp'] = []\n        \n        try:\n            hdu= pyfits.open(search_params['pasted_cat']) \n            print search_params['pasted_cat']\n            table = hdu['OBJECTS'].data \n        except:\n            import calc_tmpsave\n            calc_tmpsave.sextract(search_params['SUPA'],search_params['FLAT_TYPE'])\n\n            hdu= pyfits.open(search_params['pasted_cat'])                 \n            print search_params['pasted_cat']\n            table = hdu['OBJECTS'].data \n\n        print type(table)\n\n        if str(type(table)) == \"<type 'NoneType'>\":\n\n            save_exposure({'fixradecCR':-2},SUPA,FLAT_TYPE)\n            return -2 \n\n        else:\n\n        \n            CHIP = table.field('CHIP')                                                                                                                                                                     \n            print keys                                                                                                                                                                                    \n            print chips.keys()\n            for k in chips.keys():\n                print chips[k].has_key('CRVAL1'), k\n            print keys\n            for i in range(len(CHIP)):\n                NUM = str(int(CHIP[i]))\n                good = False\n                for key in keys:\n                    if chips[NUM].has_key(key):\n                        good = True\n                        vecs[key].append(float(chips[NUM][key]))\n                    else:\n                        vecs[key].append(-1.)\n                if good:\n                    vecs['good_scamp'].append(1)\n                else:\n                    vecs['good_scamp'].append(0)\n                                                                                                                                                                                                           \n            print vecs['good_scamp']\n                                                                                                                                                                                                           \n                                                                                                                                                                                                           \n            print vecs.keys()\n            import scipy\n            for key in vecs.keys():\n                vecs[key] = scipy.array(vecs[key])\n                print vecs[key][0:20], key\n                                                                                                                                                                                        \n            ra_cat = table.field('ALPHA_J2000')\n            dec_cat = table.field('DELTA_J2000')\n            \n            x0 = (table.field('Xpos') - vecs['CRPIX1'])\n            y0 = (table.field('Ypos') - vecs['CRPIX2'])\n                                                                                                                                                                                                           \n                                                                                                                                                                                                           \n            x0_ABS = (table.field('Xpos') + chip_dict['CRPIX1ZERO'] - vecs['CRPIX1'])\n            y0_ABS = (table.field('Ypos') + chip_dict['CRPIX2ZERO'] - vecs['CRPIX2'])\n                                                                                                                                                                                                           \n                                                                                                                                                                                        \n            x = x0*vecs['CD1_1'] + y0*vecs['CD1_2']\n            y = x0*vecs['CD2_1'] + y0*vecs['CD2_2']\n                                                                                                                                                                                        \n            r = (x**2. + y**2.)**0.5\n                                                                                                                                                                                        \n            xi_terms = {'PV1_0':scipy.ones(len(x)),'PV1_1':x,'PV1_2':y,'PV1_3':r,'PV1_4':x**2.,'PV1_5':x*y,'PV1_6':y**2.,'PV1_7':x**3.,'PV1_8':x**2.*y,'PV1_9':x*y**2.,'PV1_10':y**3.}\n                                                                                                                                                                                        \n            pv1_keys = filter(lambda x: string.find(x,'PV1') != -1, vecs.keys())\n            print 'pv1_keys', pv1_keys\n            xi = reduce(lambda x,y: x + y, [xi_terms[k]*vecs[k] for k in pv1_keys])\n                                                                                                                                                                                        \n            eta_terms = {'PV2_0':scipy.ones(len(x)),'PV2_1':y,'PV2_2':x,'PV2_3':r,'PV2_4':y**2.,'PV2_5':y*x,'PV2_6':x**2.,'PV2_7':y**3.,'PV2_8':y**2.*x,'PV2_9':y*x**2.,'PV2_10':x**3.}\n                                                                                                                                                                                        \n            pv2_keys = filter(lambda x: string.find(x,'PV2') != -1, vecs.keys())\n            print 'pv2_keys', pv2_keys\n            eta = reduce(lambda x,y: x + y, [eta_terms[k]*vecs[k] for k in pv2_keys])\n                                                                                                                                                                                        \n            print xi[0:10],eta[0:10], len(eta)\n            print vecs.keys(), vecs['CD1_1'][0],vecs['CD1_2'][0],vecs['CD2_2'][0],vecs['CD2_1'][0]\n            import math\n                                                                                                                                                                                        \n            ra_out = []\n            dec_out = []\n            os.system('mkdir -p ' + tmpdir)\n            cat = open(tmpdir + '/' + BASE + 'cat','w')\n            for i in range(len(xi)):\n                XI = xi[i] / 180.0   * math.pi                                                     \n                ETA = eta[i] / 180.0 * math.pi\n                CRVAL1 = vecs['CRVAL1'][i]/180.0* math.pi\n                CRVAL2 = vecs['CRVAL2'][i]/180.0 * math.pi\n                p = math.sqrt(XI**2. + ETA**2.) \n                c = math.atan(p)\n                                                                             \n                a = CRVAL1 + math.atan((XI*math.sin(c))/(p*math.cos(CRVAL2)*math.cos(c) - ETA*math.sin(CRVAL2)*math.sin(c)))\n                d = math.asin(math.cos(c)*math.sin(CRVAL2) + ETA*math.sin(c)*math.cos(CRVAL2)/p)\n                                                                                                                                                                                        \n                ra = a*180.0/math.pi\n                dec = d*180.0/math.pi\n                if i % 100== 0:\n                    print 'ra_cat','dec_cat',ra,ra_cat[i], dec, dec_cat[i]    \n                    print (ra-ra_cat[i])*3600.,(dec-dec_cat[i])*3600.\n                ''' if no solution, give a -999 value '''\n                if vecs['good_scamp'][i] != 1: \n                    import random\n                    ra = -999  - 200*random.random()\n                    dec = -999  - 200*random.random()          \n                ra_out.append(ra)\n                dec_out.append(dec)\n                cat.write(str(ra) + ' ' + str(dec) + '\\n')\n                #cat.write(str(ra[i]) + ' ' + str(dec[i]) + '\\n')\n            cat.close()\n            import random\n            index = int(random.random()*4)\n            colour = ['red','blue','green','yellow'][index]\n            rad = [1,2,3,4][index]\n            #os.system(' mkreg.pl -xcol 0 -ycol 1 -c -rad ' + str(rad) + ' -wcs -colour ' + colour + ' ' + BASE + 'cat')\n                                                                                                                                                                                        \n            hdu[2].data.field('Xpos_ABS')[:] = scipy.array(x0_ABS)\n            hdu[2].data.field('Ypos_ABS')[:] = scipy.array(y0_ABS)\n            hdu[2].data.field('ALPHA_J2000')[:] = scipy.array(ra_out)\n            hdu[2].data.field('DELTA_J2000')[:] = scipy.array(dec_out)\n            table = hdu[2].data \n                                                                                                                                                                                        \n            print 'BREAK'\n            print ra_out[0:10], table.field('ALPHA_J2000')[0:10]\n            print 'BREAK'\n            print dec_out[0:10], table.field('DELTA_J2000')[0:10]\n            print SUPA, search_params['pasted_cat']\n                                                                                                                                                                                        \n            os.system('rm ' + search_params['pasted_cat'])\n            hdu.writeto(search_params['pasted_cat'])\n                                                                                                                                                                                                           \n            save_exposure({'fixradecCR':1},SUPA,FLAT_TYPE)\n            return 1 \n    \n    else: \n        save_exposure({'fixradecCR':-1},SUPA,FLAT_TYPE)\n        return -1 \n\ndef mk_tab(list):\n    import astropy, astropy.io.fits as pyfits\n    from pyfits import Column        \n    import numarray \n    cols = []\n    for ele in list:\n        array = ele[0]\n        name = ele[1]\n        vec = numarray.array(array)                    \n        cols.append(Column(name=name,format='1E',array=array))\n    coldefs = pyfits.ColDefs(cols)\n    hdu = pyfits.BinTableHDU.from_columns(coldefs)\n    return hdu\n\ndef merge(t1,t2):\n    import astropy, astropy.io.fits as pyfits\n    t = t1.columns + t2[1].columns\n    hdu = pyfits.BinTableHDU.from_columns(t)\n    return hdu\n\ndef cutout(infile,mag,color='red'):\n    import os, utilities\n    ppid = str(os.getppid())\n\n    print ppid + 'a'\n\n    #pylab.show()                 \n\n    outfile = raw_input('name of output file?')\n\n    color = raw_input('color of regions?')\n\n    limits = ['lower_mag','upper_mag','lower_diff','upper_diff']\n    lim_dict = {}\n    for lim in limits:\n        print lim + '?'\n        b = raw_input()\n        lim_dict[lim] = b\n\n    utilities.run('ldacfilter -i ' + infile + ' -t PSSC\\\n                    -c \"(((SEx_' + mag + '>' + str(lim_dict['lower_mag']) + ') AND (SEx_' + mag + '<' + str(lim_dict['upper_mag']) + ')) AND (magdiff>' + str(lim_dict['lower_diff']) + ')) AND (magdiff<' + str(lim_dict['upper_diff']) + ');\"\\\n                    -o cutout1.' + ppid,['cutout1.' + ppid])\n    utilities.run('ldactoasc -b -q -i cutout1.' + ppid + '  -t PSSC\\\n            -k Ra Dec > ' + tmpdir + outfile,[outfile])\n    utilities.run('mkreg.pl -c -rad 8 -xcol 0 -ycol 1 -wcs -colour ' + color + ' ' + tmpdir +  +  outfile)\n\ndef get_median(cat,key):\n    import astropy, astropy.io.fits as pyfits, sys, os, re, string, copy\n\n    p = pyfits.open(cat)\n    magdiff = p[1].data.field(key)\n    magdiff.sort()\n\n    return magdiff[int(len(magdiff)/2)] \n\ndef coordinate_limits(cat):\n    import astropy, astropy.io.fits as pyfits, sys, os, re, string, copy\n\n    p = pyfits.open(cat)\n\n    good_entries = p[2].data\n\n    if 1:\n        mask = abs(good_entries.field('ALPHA_J2000')) > 0.0001  \n        good_entries = good_entries[mask] \n        mask = abs(good_entries.field('ALPHA_J2000')) <  400 \n        good_entries = good_entries[mask]\n        mask = abs(good_entries.field('DELTA_J2000')) > 0.0001\n        good_entries = good_entries[mask]\n        mask = abs(good_entries.field('DELTA_J2000')) < 300 \n        good_entries = good_entries[mask]\n        mask = 100000 > abs(good_entries.field('Xpos')) \n        good_entries = good_entries[mask]\n        mask = abs(good_entries.field('Xpos')) > 0.00001 \n        good_entries = good_entries[mask]\n        mask = 100000 > abs(good_entries.field('Ypos')) \n        good_entries = good_entries[mask]\n        mask = abs(good_entries.field('Ypos')) > 0.00001 \n        good_entries = good_entries[mask]\n    \n    ra = good_entries.field('ALPHA_J2000')\n    ra.sort()\n    dec = good_entries.field('DELTA_J2000')\n    dec.sort()\n\n    print cat, 'cat'\n\n    print ra[:100]\n    print dec[:100]\n\n    print ra[-100:]\n    print dec[-100:]\n    return ra[0],ra[-1],dec[0],dec[-1]\n\ndef combine_cats(cats,outfile,search_params):\n    #cats = [{'im_type': 'DOMEFLAT', 'cat': '' + search_params['TEMPDIR'] + '/SUPA0005188_3OCFS.DOMEFLAT.fixwcs.rawconv'}, {'im_type': 'SKYFLAT', 'cat': '' + search_params['TEMPDIR'] + '/SUPA0005188_3OCFS.SKYFLAT.fixwcs.rawconv'}, {'im_type': 'OCIMAGE', 'cat': '' + search_params['TEMPDIR'] + '/SUPA0005188_3OCFS.OCIMAGE.fixwcs.rawconv'}] \n    #outfile = '' + search_params['TEMPDIR'] + 'stub'\n    #cats = [{'im_type': 'MAIN', 'cat': '' + search_params['TEMPDIR'] + '/SUPA0005188_3OCFS..fixwcs.rawconv'}, {'im_type': 'D', 'cat': '' + search_params['TEMPDIR'] + '/SUPA0005188_3OCFS.D.fixwcs.rawconv'}]\n\n    import astropy, astropy.io.fits as pyfits, sys, os, re, string, copy\n    from config_bonn import cluster, tag, arc, filters\n    ppid = str(os.getppid())\n\n\n    tables = {} \n    colset = 0\n    cols = []\n    for catalog in cats: \n        file = catalog['cat'] \n        os.system('mkdir ' + search_params['TEMPDIR'] )\n        aper = tempfile.NamedTemporaryFile(dir=search_params['TEMPDIR']).name\n        os.system('ldactoasc -i ' + catalog['cat'] + ' -b -s -k MAG_APER MAGERR_APER -t OBJECTS > ' + aper)\n        cat1 = tempfile.NamedTemporaryFile(dir=search_params['TEMPDIR']).name\n        os.system('asctoldac -i ' + aper + ' -o ' + cat1 + ' -t OBJECTS -c ' + os.environ['bonn'] + '/photconf/MAG_APER.conf')\n        allconv = tempfile.NamedTemporaryFile(dir=search_params['TEMPDIR']).name\n        os.system('ldacjoinkey -i ' + catalog['cat'] + ' -p ' + cat1 + ' -o ' + allconv + '  -k MAG_APER1 MAG_APER2 MAGERR_APER1 MAGERR_APER2')\n        tables[catalog['im_type']] = pyfits.open(allconv)\n        #if filter == filters[0]:\n        #    tables['notag'] = pyfits.open('' + search_params['TEMPDIR'] + 'all.conv' )\n    \n    for catalog in cats:\n        for i in range(len(tables[catalog['im_type']][1].columns)): \n            print catalog['im_type'], catalog['cat']\n            if catalog['im_type'] != '':\n                tables[catalog['im_type']][1].columns[i].name = tables[catalog['im_type']][1].columns[i].name + catalog['im_type'] \n            else:\n                tables[catalog['im_type']][1].columns[i].name = tables[catalog['im_type']][1].columns[i].name\n            cols.append(tables[catalog['im_type']][1].columns[i])\n    \n    print cols\n    print len(cols)\n    hdu = pyfits.PrimaryHDU()\n    hduIMHEAD = pyfits.BinTableHDU.from_columns(tables[catalog['im_type']][2].columns)\n    hduOBJECTS = pyfits.BinTableHDU.from_columns(cols) \n    hdulist = pyfits.HDUList([hdu])\n    hdulist.append(hduIMHEAD)\n    hdulist.append(hduOBJECTS)\n    hdulist[1].header['EXTNAME']='FIELDS'\n    hdulist[2].header['EXTNAME']='OBJECTS'\n    print file\n    os.system('rm ' + outfile)\n    import re\n    res = re.split('/',outfile)\n    os.system('mkdir -p ' + reduce(lambda x,y: x + '/' + y,res[:-1]))\n    hdulist.writeto(outfile)\n    print outfile , '$#######$'\n    #print 'done'\n\ndef paste_cats(cats,outfile): #cats,outfile,search_params):\n      \n  \n    import astropy, astropy.io.fits as pyfits, sys, os, re, string, copy        \n    from config_bonn import cluster, tag, arc, filters\n    ppid = str(os.getppid())\n    tables = {} \n    colset = 0\n    cols = []\n   \n    table = pyfits.open(cats[0])\n\n    data = [] \n    nrows = 0\n\n    good_cats = []\n    ''' get rid of empty tables '''\n    for catalog in cats:\n        cattab = pyfits.open(catalog)\n        if not str(type(cattab[2].data)) == \"<type 'NoneType'>\":\n            good_cats.append(catalog)\n    cats = good_cats\n\n    for catalog in cats:\n        cattab = pyfits.open(catalog)\n        nrows += cattab[2].data.shape[0]\n\n    hduOBJECTS = pyfits.BinTableHDU.from_columns(table[2].columns, nrows=nrows) \n   \n    rowstart = 0\n    rowend = 0\n    for catalog in cats:\n        cattab = pyfits.open(catalog)\n        rowend += cattab[2].data.shape[0]\n        for i in range(len(cattab[2].columns)): \n            hduOBJECTS.data.field(i)[rowstart:rowend]=cattab[2].data.field(i)\n        rowstart = rowend\n\n\n    # update SeqNr\n    print rowend,len(        hduOBJECTS.data.field('SeqNr')), len(range(1,rowend+1))\n    hduOBJECTS.data.field('SeqNr')[0:rowend]=range(1,rowend+1)\n\n    #hdu[0].header['EXTNAME']='FIELDS'\n\n\n    hduIMHEAD = pyfits.BinTableHDU.from_columns(table[1])\n\n    print cols\n    print len(cols)\n    hdu = pyfits.PrimaryHDU()\n    hdulist = pyfits.HDUList([hdu])\n    hdulist.append(hduIMHEAD)\n    hdulist.append(hduOBJECTS)\n    hdulist[1].header['EXTNAME']='FIELDS'\n    hdulist[2].header['EXTNAME']='OBJECTS'\n    print file\n\n    os.system('rm ' + outfile)\n    hdulist.writeto(outfile)\n    print outfile , '$#######$'\n    #print 'done'\n\ndef imstats(SUPA,FLAT_TYPE):\n    import os, re, utilities, bashreader, sys, string\n    from copy import copy\n    from glob import glob\n    dict = get_files(SUPA,FLAT_TYPE)\n    search_params = initialize(dict['filter'],dict['OBJNAME'])\n    search_params.update(dict)\n    path='/nfs/slac/g/ki/ki05/anja/SUBARU/%(OBJNAME)s/' % {'OBJNAME':search_params['OBJNAME']}\n    print dict['files']\n    import commands\n    tmp_dicts = [] \n    for file in dict['files']:\n        op = commands.getoutput('imstats ' + dict['files'][0]) \n        print op\n        res = re.split('\\n',op)\n        for line in res:\n            if string.find(line,'filename') != -1:\n                line = line.replace('$ imstats: ','')\n                res2 = re.split('\\t',line)\n                                                               \n        res3 = re.split('\\s+',res[-1]) \n\n        tmp_dict = {}\n        for i in range(len(res3)):\n            tmp_dict[res2[i]] = res3[i] \n        tmp_dicts.append(tmp_dict)\n    print tmp_dicts\n\n    median_average = 0\n    sigma_average = 0\n    for d in tmp_dicts:\n        print d.keys()\n        sigma_average += float(d['sigma'])\n        median_average += float(d['median'])\n\n    dict['sigma_average'] = sigma_average / len(tmp_dicts)\n    dict['median_average'] = median_average / len(tmp_dicts)\n\n    print dict['sigma_average'], dict['median_average']\n\n    save_exposure(dict,SUPA,FLAT_TYPE)\n\ndef save_fit_WHATISTHIS(fits,im_type,type,SUPA,FLAT_TYPE):\n    import MySQLdb, sys, os, re, time \n    from copy import copy\n    db2 = MySQLdb.connect(db='subaru', user='weaklensing', passwd='darkmatter', host='ki-rh8')\n    c = db2.cursor()\n\n\n    for fit in fits:\n        #which_solution += 1\n        user_name = os.environ['USER']\n        time_now = time.asctime() \n        user = user_name #+ str(time.time())\n        \n        dict = {} \n        #copy array but exclude lists                                                   \n        for ele in fit['class'].fitvars.keys():\n            if ele != 'condition' and ele != 'model_name' and ele != 'fixed_name': \n                dict[ele + '_' + type + '_' + im_type] = fit['class'].fitvars[ele]\n    save_exposure(dict,SUPA,FLAT_TYPE)\n    db2.close()\n\n\ndef select_analyze():\n    import MySQLdb, sys, os, re, time, string \n    from copy import copy\n    db2,c = connect_except()\n    command = \"DESCRIBE try_db\"\n    print command\n    c.execute(command)\n    results = c.fetchall()\n    keys = []\n    for line in results:\n        keys.append(line[0])\n    command = \"SELECT * from illumination_db where zp_err_galaxy_D is null and PPRUN='2002-06-04_W-J-V'\" # where OBJNAME='HDFN' and filter='W-J-V' and ROTATION=0\"\n    command = \"SELECT * from fit_db where color1_star > 0.2 and OBJNAME!='HDFN' limit 2\" # where matched_cat_star is null\" # where OBJNAME='HDFN' and filter='W-J-V' and ROTATION=0\"\n\n    first = True \n    while len(results) > 0 or first:\n        first = False\n        command= \"SELECT * from illumination_db where (OBJNAME like 'A%' or OBJNAME like 'MACS%') and (pasted_cat is null or pasted_cat like '%None%') and CORRECTED='True' \" # and PPRUN='2003-04-04_W-C-IC'\"                                                                                     \n        command= \"SELECT * from try_db where sdssstatus='fitfinished' and OBJNAME like 'MACS2129%' ORDER BY RAND()\" # and PPRUN='2003-04-04_W-C-IC'\"  \n\n        #command= \"SELECT * from illumination_db where SUPA='SUPA0011100'\" # and PPRUN='2003-04-04_W-C-IC'\"  \n        #command= \"SELECT * from illumination_db where (OBJNAME like 'A%' or OBJNAME like 'MACS%') and SUPA='SUPA0021827'\" # and PPRUN='2003-04-04_W-C-IC'\"  \n        #command = \"select * from illumination_db where SUPA='SUPA0028506'\"\n        #command = \"select * from illumination_db where (OBJECT like '%0018short%') and (FILTER='W-J-B' or FILTER='W-S-Z+')\" # or OBJECT like '%0018short%')\" # and pasted_cat is null\" # and color1_star_ is null\"\n        print command\n        c.execute(command)\n        results = c.fetchall()\n        print len(results)\n        #print results\n        dicts = [] \n        dict = {} \n        for i in range(len(results[0])):  \n            dict[keys[i]] = results[0][i]\n\n        if 1:\n            construct_correction(dict['OBJNAME'],dict['FILTER'],dict['PPRUN'],'sdss','all')\n\n        if 0:            \n            #try: \n            #    fix_radec(dict['SUPA'],dict['FLAT_TYPE']) \n            #except: \n            #    print 'failed'\n            trial = True \n            ppid = str(os.getppid())\n            try:\n                construct_correction(dict['OBJNAME'],dict['FILTER'],dict['PPRUN'])\n                print 'finished'\n            except:\n                ppid_loc = str(os.getppid())\n                print traceback.print_exc(file=sys.stdout)\n                if ppid_loc != ppid: sys.exit(0) \n                print 'exiting here'\n                #if trial: raise Exception \n\n            print dict['OBJNAME'], dict['PPRUN']\n        if 0:\n            #print dict['SUPA'], dict['file'], dict['OBJNAME'], dict['pasted_cat'], dict['matched_cat_star']\n            d_update = get_files(dict['SUPA'],dict['FLAT_TYPE'])\n            go = 0\n            if d_update.has_key('TRIED'):\n                if d_update['TRIED'] != 'YES':\n                    go = 1\n            else: go = 1\n\n            if string.find(str(dict['TIME']),'N') == -1:\n                #print dict['TIME']\n                if time.time() - float(dict['TIME']) > 600:\n                    go = 1\n                else: go = 0\n            else: go = 1\n            if 0: # go:\n                #print str(time.time())\n                save_exposure({'ACTIVE':'YES','TIME':str(time.time())},dict['SUPA'],dict['FLAT_TYPE'])\n                os.system('rm -R ' + tmpdir)\n                analyze(dict['SUPA'],dict['FLAT_TYPE'],dict)\n                save_exposure({'ACTIVE':'FINISHED'},dict['SUPA'],dict['FLAT_TYPE'])\n\ndef analyze(SUPA,FLAT_TYPE,params={}):\n    #try:\n    import sys, os, string\n    #os.system('rm -rf ' + search_params['TEMPDIR'] + '*')\n    trial = True \n    ppid = str(os.getppid())\n    try:\n        construct_correction(dict['OBJNAME'],dict['FILTER'],dict['PPRUN'])\n        #imstats(SUPA,FLAT_TYPE) \n        #if string.find(str(params['CRPIX1ZERO']),'None') != -1:\n        #    length(SUPA,FLAT_TYPE)\n        #if string.find(str(params['fwhm']),'None') != -1 or str(params['fwhm'])=='0.3':\n        #    find_seeing(SUPA,FLAT_TYPE)      \n        #sextract(SUPA,FLAT_TYPE)\n        print 'finished'\n        #match_simple(SUPA,FLAT_TYPE)\n        #phot(SUPA,FLAT_TYPE)\n        #get_sdss_obj(SUPA,FLAT_TYPE)\n        #apply_photometric_calibration(SUPA,FLAT_TYPE)\n        print 'finished'\n    except:\n        ppid_loc = str(os.getppid())\n        print traceback.print_exc(file=sys.stdout)\n        if ppid_loc != ppid: sys.exit(0) \n        if trial: raise Exception \n\n    #except KeyboardInterrupt:\n    #    raise\n    #except: \n    #    ppid_loc = str(os.getppid())\n    #    print sys.exc_info()\n    #    print 'something else failed',ppid, ppid_loc \n    #    if ppid_loc != ppid: sys.exit(0) \n#   #     os.system('rm -rf /tmp/' + ppid)\n##\n#    os.system('rm -rf /tmp/' + ppid)\n#\n\ndef get_files(SUPA,FLAT_TYPE=None):    \n    import MySQLdb, sys, os, re                                                                     \n\n    db2,c = connect_except()\n\n    command = \"DESCRIBE illumination_db\"\n    #print command\n    c.execute(command)\n    results = c.fetchall()\n    keys = []\n    for line in results:\n        keys.append(line[0])\n\n    command = \"SELECT * from illumination_db where SUPA='\" + SUPA + \"'\" # AND FLAT_TYPE='\" + FLAT_TYPE + \"'\"\n    #print command\n    c.execute(command)\n    results = c.fetchall()\n    dict = {} \n    for i in range(len(results[0])):\n        dict[keys[i]] = results[0][i]\n    #print dict \n\n    file_pat = dict['file'] \n    import re, glob\n    res = re.split('_\\d+O',file_pat)\n    pattern = res[0] + '_*O' + res[1]\n\n    files = glob.glob(pattern)\n    dict['files'] = files\n\n    db2.close()\n    return dict\n\n\ndef get_a_file(OBJNAME,FILTER,PPRUN):    \n    ''' get a single file w/ OBJNAME FILTER PPRUN'''\n\n    import MySQLdb, sys, os, re                                                                     \n\n    db2,c = connect_except()\n\n    command = \"DESCRIBE illumination_db\"\n    #print command\n    c.execute(command)\n    results = c.fetchall()\n    keys = []\n    for line in results:\n        keys.append(line[0])\n\n    command=\"SELECT * from illumination_db where FILTER='\" + FILTER + \"' and OBJNAME='\" + OBJNAME + \"' and PPRUN='\" + PPRUN + \"' and CHIPS is not null limit 1\"\n    print command\n    c.execute(command)\n    results = c.fetchall()\n    dict = {} \n    for i in range(len(results[0])):\n        dict[keys[i]] = results[0][i]\n    #print dict \n\n    file_pat = dict['file'] \n    import re, glob\n    res = re.split('_\\d+O',file_pat)\n    pattern = res[0] + '_*O' + res[1]\n\n    files = glob.glob(pattern)\n    dict['files'] = files\n\n    db2.close()\n    return dict\n\ndef get_fits(OBJNAME,FILTER,PPRUN,sample, sample_size):    \n    import MySQLdb, sys, os, re                                                                     \n    db2,c = connect_except()\n\n    command=\"SELECT * from fit_db where FILTER='\" + FILTER + \"' and OBJNAME='\" + OBJNAME + \"' and PPRUN='\" + PPRUN + \"' and sample='\" + str(sample) + \"' and sample_size='\" + str(sample_size) + \"'\"\n    print command\n    c.execute(command)\n    results=c.fetchall()\n    db_keys = describe_db(c,'fit_db')\n    dtop = {}   \n    for line in results: \n        for i in range(len(db_keys)):\n            dtop[db_keys[i]] = str(line[i])\n\n    db2.close()\n    return dtop\n\ndef connect_except():\n    import MySQLdb, sys, os, re                                                                             \n    notConnect = True\n    tried = 0\n    while notConnect:\n        tried += 1                                                                                                     \n        try:\n            db2 = MySQLdb.connect(db='subaru', user='weaklensing', passwd='darkmatter', host='ki-rh8')  \n            c = db2.cursor()\n            notConnect = False\n        except:\n            print traceback.print_exc(file=sys.stdout)\n            import random, time\n            randwait = int(random.random()*30)\n            if randwait < 10: randwait=10\n            print 'rand wait!', randwait\n            time.sleep(randwait)\n            if tried > 15: \n                print 'too many failures' \n                sys.exit(0)\n    #print 'done'\n    return db2,c\n\ndef save_exposure(dict,SUPA=None,FLAT_TYPE=None):\n    if SUPA != None and FLAT_TYPE != None:\n        dict['SUPA'] = SUPA\n        dict['FLAT_TYPE'] = FLAT_TYPE\n\n    db2,c = connect_except()\n    \n    #command = \"CREATE TABLE IF NOT EXISTS illumination_db ( id MEDIUMINT NOT NULL AUTO_INCREMENT, PRIMARY KEY (id))\"\n    #print command\n    #c.execute(\"DROP TABLE IF EXISTS illumination_db\")\n    #c.execute(command)\n\n    from copy import copy\n    floatvars = {}  \n    stringvars = {}\n    #copy array but exclude lists                                                   \n    import string\n    letters = string.ascii_lowercase + string.ascii_uppercase.replace('E','') + '_' + '-' + ','\n    for ele in dict.keys():\n        type = 'float'\n        for l in letters:\n            if string.find(str(dict[ele]),l) != -1: \n                type = 'string'\n        if type == 'float':  \n            floatvars[ele] = str(float(dict[ele])) \n        elif type == 'string':\n            stringvars[ele] = dict[ele] \n                                                                                                                                                                                                           \n    # make database if it doesn't exist\n    print 'floatvars', floatvars\n    print 'stringvars', stringvars\n    \n    for column in stringvars: \n        try:\n            command = 'ALTER TABLE illumination_db ADD ' + column + ' varchar(240)'\n            c.execute(command)  \n        except: nope = 1 \n    \n    for column in floatvars: \n        try:\n            command = 'ALTER TABLE illumination_db ADD ' + column + ' float(30)'\n            c.execute(command)  \n        except: nope = 1 \n\n    # insert new observation \n\n    SUPA = dict['SUPA'] \n    flat = dict['FLAT_TYPE']\n    c.execute(\"SELECT SUPA from illumination_db where SUPA = '\" + SUPA + \"' and flat_type = '\" + flat + \"'\")\n    results = c.fetchall() \n    print results\n    if len(results) > 0:\n        print 'already added'\n    else:\n        command = \"INSERT INTO illumination_db (SUPA,FLAT_TYPE) VALUES ('\" + dict['SUPA'] + \"','\" + dict['FLAT_TYPE'] + \"')\"\n        print command\n        c.execute(command) \n\n    import commands\n\n     \n    vals = ''\n    for key in stringvars.keys():\n        print key, stringvars[key]\n        vals += ' ' + key + \"='\" + str(stringvars[key]) + \"',\"\n\n    for key in floatvars.keys():\n        print key, floatvars[key]\n        vals += ' ' + key + \"='\" + floatvars[key] + \"',\"\n    vals = vals[:-1]\n\n    command = \"UPDATE illumination_db set \" + vals + \" WHERE SUPA='\" + dict['SUPA'] + \"' AND FLAT_TYPE='\" + dict['FLAT_TYPE'] + \"'\" \n    print command\n    c.execute(command)\n        \n\n    print vals\n        \n\n    #names = reduce(lambda x,y: x + ',' + y, [x for x in floatvars.keys()])\n    #values = reduce(lambda x,y: str(x) + ',' + str(y), [floatvars[x] for x in floatvars.keys()])\n    #names += ',' + reduce(lambda x,y: x + ',' + y, [x for x in stringvars.keys()])\n    #values += ',' + reduce(lambda x,y: x + ',' + y, [\"'\" + str(stringvars[x]) + \"'\" for x in stringvars.keys()])\n\n        \n    #command = \"INSERT INTO illumination_db (\" + names + \") VALUES (\" + values + \")\"\n    #print command\n    #os.system(command)\n\n    db2.close()\n\n\ndef initialize(filter,OBJNAME):\n    import os, re, bashreader, sys, string, utilities\n    from glob import glob\n    from copy import copy\n\n    dict = bashreader.parseFile(os.environ['bonn'] + 'progs.ini')\n    for key in dict.keys():\n        os.environ[key] = str(dict[key])\n    import os\n    ppid = str(os.getppid())\n    PHOTCONF = os.environ['bonn'] + '/photconf/'\n    #TEMPDIR = '/usr/work/pkelly/' + ppid + '/'\n    TEMPDIR = tmpdir \n    os.system('mkdir ' + TEMPDIR)\n    path='/nfs/slac/g/ki/ki05/anja/SUBARU/%(OBJNAME)s/' % {'OBJNAME':OBJNAME}\n    search_params = {'path':path, 'OBJNAME':OBJNAME, 'filter':filter, 'PHOTCONF':PHOTCONF, 'DATACONF':os.environ['DATACONF'], 'TEMPDIR':TEMPDIR} \n\n    return search_params\n\n\ndef update_dict(SUPA,FLAT_TYPE):    \n    import utilities\n    dict = get_files(SUPA,FLAT_TYPE)\n    kws = utilities.get_header_kw(dict['file'],['ROTATION','OBJECT','GABODSID','CONFIG','EXPTIME','AIRMASS','INSTRUM','PPRUN','BADCCD']) # return KEY/NA if not SUBARU \n    save_exposure(kws,SUPA,FLAT_TYPE)\n    \n\ndef gather_exposures(OBJNAME,filters=None):\n\n    Corrected = True\n    if Corrected: pattern = 'I.fits'\n    else: pattern = ''\n\n    if not filters:\n        filters =  ['B','W-J-B','W-J-V','W-C-RC','W-C-IC','I','W-S-Z+']        \n    for filter_name in filters:\n        search_params = initialize(filter_name,OBJNAME) \n        import os, re, bashreader, sys, string, utilities\n        from glob import glob\n        from copy import copy\n       \n        searchstr = \"/%(path)s/%(filter)s/SCIENCE/*.fits\" % search_params\n        print searchstr\n        files = glob(searchstr)\n\n        ''' filter_name out corrected or not corrected files '''\n        print files\n        if Corrected: \n            files = filter(lambda x:string.find(x,'I.fits')!=-1,files) \n        elif not Corrected:\n            files = filter(lambda x:string.find(x,'I.fits')==-1,files) \n        print files\n\n        files.sort()\n        #print files\n        exposures =  {} \n        # first 30 files\n        #print files[0:30]\n        \n        import MySQLdb, sys, os, re                                                                     \n        db2 = MySQLdb.connect(db='subaru', user='weaklensing', passwd='darkmatter', host='ki-rh8')\n        c = db2.cursor()\n        \n        for file in files:\n            if string.find(file,'wcs') == -1 and string.find(file,'.sub.fits') == -1:\n                res = re.split('_',re.split('/',file)[-1])                                        \n                exp_name = res[0]\n                if not exposures.has_key(exp_name): exposures[exp_name] = {'images':[],'keywords':{}}\n                exposures[exp_name]['images'].append(file) # exp_name is the root of the image name\n                if len(exposures[exp_name]['keywords'].keys()) == 0: #not exposures[exp_name]['keywords'].has_key('ROTATION'): #if exposure does not have keywords yet, then get them\n                    exposures[exp_name]['keywords']['filter'] = filter_name\n                    exposures[exp_name]['keywords']['file'] = file \n                    res2 = re.split('/',file)   \n                    for r in res2:\n                        if string.find(r,filter_name) != -1:\n                            print r\n                            exposures[exp_name]['keywords']['date'] = r.replace(filter_name + '_','')\n                            exposures[exp_name]['keywords']['fil_directory'] = r \n                            search_params['fil_directory'] = r\n                    kws = utilities.get_header_kw(file,['CRVAL1','CRVAL2','ROTATION','OBJECT','GABODSID','CONFIG','EXPTIME','AIRMASS','INSTRUM','PPRUN','BADCCD']) # return KEY/NA if not SUBARU \n                                                                                                                                                                                      \n                    ''' figure out a way to break into SKYFLAT, DOMEFLAT '''\n                                                                                                                                                                                      \n                    ppid = str(os.getppid())\n                    command = 'dfits ' + file + ' > ' + search_params['TEMPDIR'] + '/header'\n                    utilities.run(command)\n                    file = open('' + search_params['TEMPDIR'] + 'header','r').read()\n                    import string                    \n                    if string.find(file,'SKYFLAT') != -1: exposures[exp_name]['keywords']['FLAT_TYPE'] = 'SKYFLAT' \n                    elif string.find(file,'DOMEFLAT') != -1: exposures[exp_name]['keywords']['FLAT_TYPE'] = 'DOMEFLAT' \n                    #print file, exposures[exp_name]['keywords']['FLAT_TYPE'] \n                                                                                                                                                                                      \n                    file = open('' + search_params['TEMPDIR'] + 'header','r').readlines()\n                    import string                    \n                    for line in file:\n                        print line\n                        if string.find(line,'Flat frame:') != -1 and string.find(line,'illum') != -1:\n                            import re                   \n                            res = re.split('SET',line)\n                            if len(res) > 1:\n                                res = re.split('_',res[1])                                                                                                                                 \n                                set = res[0]\n                                exposures[exp_name]['keywords']['FLAT_SET'] = set\n                                                                                                                                                                                          \n                                res = re.split('illum',line)\n                                res = re.split('\\.',res[1])\n                                smooth = res[0]\n                                exposures[exp_name]['keywords']['SMOOTH'] = smooth \n                            break\n                                                                                                                                                                                      \n                    for kw in kws.keys(): \n                        exposures[exp_name]['keywords'][kw] = kws[kw]\n                    if Corrected: \n                        exposures[exp_name]['keywords']['SUPA'] = exp_name+'I'\n                    if not Corrected:\n                        exposures[exp_name]['keywords']['SUPA'] = exp_name\n                    exposures[exp_name]['keywords']['OBJNAME'] = OBJNAME \n                    exposures[exp_name]['keywords']['CORRECTED'] = str(Corrected) \n                    print exposures[exp_name]['keywords']\n                    save_exposure(exposures[exp_name]['keywords'])\n\n    return exposures\n\n\n\ndef find_seeing(SUPA,FLAT_TYPE):     \n    import os, re, utilities, sys\n    from copy import copy\n    dict = get_files(SUPA,FLAT_TYPE)\n    print dict['file']\n    search_params = initialize(dict['filter'],dict['OBJNAME'])\n    search_params.update(dict)\n\n    print dict['files']\n\n    #params PIXSCALE GAIN\n\n    ''' quick run through for seeing '''\n    children = []\n    for image in search_params['files']:                                                                                 \n        child = os.fork()\n        if child:\n            children.append(child)\n        else:\n            params = copy(search_params)     \n            \n            ROOT = re.split('\\.',re.split('\\/',image)[-1])[0]\n            params['ROOT'] = ROOT\n            params['ROOT_WEIGHT'] = ROOT.replace('I','')\n            NUM = re.split('O',re.split('\\_',ROOT)[1])[0]\n            params['NUM'] = NUM\n            print ROOT\n                                                                                                                     \n            weightim = \"/%(path)s/%(fil_directory)s/WEIGHTS/%(ROOT)s.weight.fits\" % params\n            #flagim = \"/%(path)s/%(fil_directory)s/WEIGHTS/globalflag_%(NUM)s.fits\" % params\n            #finalflagim = TEMPDIR + \"flag_%(ROOT)s.fits\" % params \n            os.system('mkdir -p ' + params['TEMPDIR'])\n\n            params['finalflagim'] = weightim\n            #os.system('rm ' + finalflagim)\n            #command = \"ic -p 16 '1 %2 %1 0 == ?' \" + weightim + \" \" + flagim + \" > \" + finalflagim\n            #utilities.run(command)\n            \n            command = \"nice sex %(file)s -c %(PHOTCONF)s/singleastrom.conf.sex \\\n                        -FLAG_IMAGE ''\\\n                        -FLAG_TYPE MAX \\\n                        -CATALOG_NAME %(TEMPDIR)s/seeing_%(ROOT)s.cat \\\n                        -FILTER_NAME %(PHOTCONF)s/default.conv\\\n                        -CATALOG_TYPE 'ASCII' \\\n                        -DETECT_MINAREA 8 -DETECT_THRESH 8.\\\n                        -ANALYSIS_THRESH 8 \\\n                        -WEIGHT_IMAGE /%(path)s/%(fil_directory)s/WEIGHTS/%(ROOT_WEIGHT)s.weight.fits\\\n                        -WEIGHT_TYPE MAP_WEIGHT\\\n                        -PARAMETERS_NAME %(PHOTCONF)s/singleastrom.ascii.flag.sex\" %  params \n                                                                                                                     \n            print command\n            os.system(command)\n            sys.exit(0)\n    for child in children:  \n        os.waitpid(child,0)\n                                                                                                                          \n                                                                                                                          \n    command = 'cat ' + search_params['TEMPDIR'] + 'seeing_' +  SUPA.replace('I','*I') + '*cat > ' + search_params['TEMPDIR'] + 'paste_seeing_' + SUPA.replace('I','*I') + '.cat' \n    utilities.run(command)\n                                                                                                                          \n    file_seeing = search_params['TEMPDIR'] + '/paste_seeing_' + SUPA.replace('I','*I') + '.cat'\n    PIXSCALE = float(search_params['PIXSCALE'])\n    reload(utilities)\n    fwhm = utilities.calc_seeing(file_seeing,10,PIXSCALE)\n\n    save_exposure({'fwhm':fwhm},SUPA,FLAT_TYPE)\n\n    print file_seeing, SUPA, PIXSCALE\n\ndef length_DONTUSE(SUPA,FLAT_TYPE):\n    import os, re, utilities, bashreader, sys, string\n    from copy import copy\n    from glob import glob\n    dict = get_files(SUPA,FLAT_TYPE)\n    search_params = initialize(dict['filter'],dict['OBJNAME'])\n    search_params.update(dict)\n    path='/nfs/slac/g/ki/ki05/anja/SUBARU/%(OBJNAME)s/' % {'OBJNAME':search_params['OBJNAME']}\n\n    res = re.split('SCIENCE',search_params['files'][0])                         \n    res = re.split('/',res[0])\n    if res[-1]=='':res = res[:-1]\n    search_params['path'] = reduce(lambda x,y:x+'/'+y,res[:-1])\n    search_params['fil_directory'] = res[-1]\n    print search_params['path'], search_params['fil_directory'], 'list'\n    save_exposure({'path':search_params['path'],'fil_directory':search_params['fil_directory']},SUPA,FLAT_TYPE)\n\n    ''' get the CRPIX values '''\n    start = 1\n    #CRPIXZERO is at the chip at the bottom left and so has the greatest value!!!!\n    x = []\n    y = []\n    chips = {} \n    NUMS = []\n    all_chip_dict = {}\n    for image in search_params['files']:\n        print image                                                 \n        res = re.split('\\_\\d+',re.split('\\/',image)[-1])\n        #print res\n        imroot = \"/%(path)s/%(fil_directory)s/SCIENCE/\" % search_params\n        im = imroot + res[0] + '_1' + res[1] \n        #print im\n        crpix = utilities.get_header_kw(image,['CRPIX1','CRPIX2','NAXIS1','NAXIS2','CRVAL1','CRVAL2','IMAGEID'])\n        if start == 1:\n            crpixzero = copy(crpix)\n            crpixhigh = copy(crpix)\n            start = 0\n        from copy import copy \n        print  float(crpix['CRPIX1'])  < float(crpixzero['CRPIX1']), float(crpix['CRPIX2'])  < float(crpixzero['CRPIX2'])\n        if float(crpix['CRPIX1']) + 0   >= float(crpixzero['CRPIX1']):\n            crpixzero['CRPIX1'] = copy(crpix['CRPIX1'])\n        if float(crpix['CRPIX2'])  + 0 >= float(crpixzero['CRPIX2']):\n            crpixzero['CRPIX2'] = copy(crpix['CRPIX2'])\n                                                                                                                          \n        if float(crpix['CRPIX1']) - 0  <= float(crpixhigh['CRPIX1']):\n            crpixhigh['CRPIX1'] = copy(crpix['CRPIX1'])\n        if float(crpix['CRPIX2']) - 0  <= float(crpixhigh['CRPIX2']):\n            crpixhigh['CRPIX2'] = copy(crpix['CRPIX2'])\n\n        print crpix['CRPIX1'], crpix['CRPIX2'], crpixzero['CRPIX1'], crpixzero['CRPIX2'], crpixhigh['CRPIX1'], crpixhigh['CRPIX2']#, crpixhigh\n        x.append(float(crpix['CRPIX1']))\n        y.append(float(crpix['CRPIX2']))\n\n        chips[crpix['IMAGEID']] = crpix\n        NUMS.append(crpix['IMAGEID'])\n        for kw in ['CRPIX1','CRPIX2','NAXIS1','NAXIS2','CRVAL1','CRVAL2']:\n            all_chip_dict[kw+ '_' + str(crpix['IMAGEID'])] = crpix[kw]\n\n    NUMScommas = reduce(lambda x,y: str(x) + ',' + str(y),NUMS)\n    all_chip_dict['CHIPS'] = NUMScommas\n\n    print all_chip_dict \n\n    LENGTH1 =  abs(float(crpixhigh['CRPIX1']) - float(crpixzero['CRPIX1'])) + float(crpix['NAXIS1']) \n    LENGTH2 =  abs(float(crpixhigh['CRPIX2']) - float(crpixzero['CRPIX2'])) + float(crpix['NAXIS2']) \n\n    chips['CRPIX1ZERO'] = crpixzero['CRPIX1']\n    chips['CRPIX2ZERO'] = crpixzero['CRPIX2']\n\n    chips['NAXIS1'] = crpixzero['NAXIS1']\n    chips['NAXIS2'] = crpixzero['NAXIS2']\n\n    print LENGTH1, LENGTH2, crpixzero['CRPIX1'], crpixzero['CRPIX2'], crpixhigh['CRPIX1'], crpixhigh['CRPIX2']#, crpixhigh\n\n    all_chip_dict.update({'crfixed':'third','LENGTH1':LENGTH1,'LENGTH2':LENGTH2,'CRPIX1ZERO':crpixzero['CRPIX1'],'CRPIX2ZERO':crpixzero['CRPIX2'],'CRVAL1':crpix['CRVAL1'],'CRVAL2':crpix['CRVAL2']})\n    save_exposure(all_chip_dict,SUPA,FLAT_TYPE)\n\n    return chips\n    #return x,y\n\ndef apply_correction2(SUPA,FLAT_TYPE):\n\n    chips = length(SUPA,FLAT_TYPE)\n\n    for chip in [1]:\n        #retrieve coefficients        \n\n        d = get_fits(CLUSTER,FILTER,PPRUN)                \n        column_prefix = sample+'$'+sample_size+'$'\n        position_columns_names = re.split('\\,',d[column_prefix + 'positioncolumns']) \n        fitvars = {}\n        cheby_terms_dict = {}\n        for ele in position_columns:                      \n            res = re.split('$',ele['name'])\n            fitvars[ele['name']] = float(d[sample+'$'+sample_size+'$'+ele['name']])\n            for term in cheby_terms:\n                if term['n'] == ele['name'][2:]:\n                    cheby_terms_dict[term['n']] = term \n        cheby_terms_use =  [cheby_terms_dict[k] for k in cheby_terms_dict.keys()]\n                                                                                                                          \n        print cheby_terms_use, fitvars\n                                                                                                                          \n        ''' make images of illumination corrections '''                                                                  \n        for ROT in EXPS.keys():\n            size_x=LENGTH1\n            size_y=LENGTH2\n            bin=100\n            import numpy, math, pyfits, os                                                                              \n            x,y = numpy.meshgrid(numpy.arange(0,size_x,bin),numpy.arange(0,size_y,bin))\n            F=0.1\n            print 'calculating'\n            x = coord_conv_x(x)\n            y = coord_conv_y(y)\n            \n            epsilon = 0\n            index = 0\n            for term in cheby_terms_use:\n                index += 1\n                print index, ROT, term, fitvars[str(ROT)+'$'+term['n']]\n                epsilon += fitvars[str(ROT)+'$'+term['n']]*term['fx'](x,y)*term['fy'](x,y)\n                                                                                                                          \n                                                                                                                          \n            print 'writing'\n            hdu = pyfits.PrimaryHDU(epsilon)\n            #os.system('rm ' + tmpdir + 'correction' + ROT + filter + sample_size + '.fits')\n            #hdu.writeto(tmpdir + '/correction' + ROT + filter + sample_size + '.fits')\n                                                                                                                         \n            path='/nfs/slac/g/ki/ki05/anja/SUBARU/%(OBJNAME)s/' % {'OBJNAME':CLUSTER}\n            illum_dir = path + 'PHOTOMETRY/ILLUMINATION/' + FILTER + '/' + str(ROT)\n            os.system('mkdir -p ' + illum_dir)\n                                                                                                                         \n            im = illum_dir + '/correction' + sample + sample_size + '.fits'\n            save_fit({'PPRUN':PPRUN,'FILTER':FILTER,'CLUSTER':CLUSTER,sample+'$'+sample_size+'$'+str(ROT)+'$im':im})\n                                                                                                                         \n            os.system('rm ' + im)\n            hdu.writeto(im)\n\n\n\n\ndef sdss_coverage(SUPA,FLAT_TYPE):\n    import commands, string                                                                                    \n    dict = get_files(SUPA,FLAT_TYPE)\n    search_params = initialize(dict['filter'],dict['OBJNAME'])\n    search_params.update(dict)\n    if 0:\n        #print 'CRVAL1', search_params['CRVAL1'], search_params['CRVAL1'] == 'None'                                                                                             \n        #if str(search_params['CRVAL1']) == 'None':\n        #    print search_params['FLAT_TYPE'], 'FLAT_TYPE'\n        \n        if search_params['CRVAL1'] is None:\n            length(search_params['SUPA'],search_params['FLAT_TYPE'])\n                                                                                                                                                                               \n        dict = get_files(SUPA,FLAT_TYPE)\n        search_params.update(dict)\n        print search_params['CRVAL1']\n        crval1 = float(search_params['CRVAL1'])\n        crval2 = float(search_params['CRVAL2'])\n        query = 'select ra, dec from star where ra between ' + str(crval1-0.1) + ' and ' + str(crval1+0.1) + ' and dec between ' + str(crval2-0.1) + ' and ' + str(crval2+0.1)\n        print query\n\n\n        import sqlcl\n        lines = sqlcl.query(query).readlines()\n        print lines\n        if len(lines) > 1: sdss_coverage=True \n        else: sdss_coverage=False \n        save_exposure({'sdss_coverage':sdss_coverage},SUPA,FLAT_TYPE)\n\n    import MySQLdb, sys, os, re, time, utilities, pyfits\n    from copy import copy\n    db2 = MySQLdb.connect(db='subaru', user='weaklensing', passwd='darkmatter', host='ki-rh8')\n    c = db2.cursor()\n\n    command = \"select cov from sdss_db where OBJNAME='\" + dict['OBJNAME'] + \"'\"\n    c.execute(command)\n    results=c.fetchall()\n    print results\n\n    if len(results) == 0:\n        import calc_tmpsave\n        calc_tmpsave.get_sdss_cats(dict['OBJNAME'])\n        command = \"select cov from sdss_db where OBJNAME='\" + dict['OBJNAME'] + \"'\"\n        c.execute(command)\n        results=c.fetchall()\n        print results\n\n\n\n    sdss_coverage = results[0][0]\n\n    import string\n    if string.find(sdss_coverage,'True') != -1:\n        cov = True\n    else: cov=False\n\n    starcat ='/nfs/slac/g/ki/ki05/anja/SUBARU/%(OBJNAME)s/PHOTOMETRY/sdssstar.cat' % {'OBJNAME':search_params['OBJNAME']}\n    galaxycat ='/nfs/slac/g/ki/ki05/anja/SUBARU/%(OBJNAME)s/PHOTOMETRY/sdssgalaxy.cat' % {'OBJNAME':search_params['OBJNAME']}\n    return cov, galaxycat, starcat\n\ndef sextract(SUPA,FLAT_TYPE):\n    import os, re, utilities, bashreader, sys, string\n    from copy import copy\n    from glob import glob\n\n    trial = False \n    \n    dict = get_files(SUPA,FLAT_TYPE)\n    search_params = initialize(dict['filter'],dict['OBJNAME'])\n    search_params.update(dict)\n    path='/nfs/slac/g/ki/ki05/anja/SUBARU/%(OBJNAME)s/' % {'OBJNAME':search_params['OBJNAME']}\n    subpath='/nfs/slac/g/ki/ki05/anja/SUBARU/'\n\n    print search_params\n\n    print SUPA, FLAT_TYPE, search_params['files'] \n    kws = utilities.get_header_kw(search_params['files'][0],['PPRUN'])\n    print kws['PPRUN']\n    pprun = kws['PPRUN']\n\n    #fs = glob.glob(subpath+pprun+'/SCIENCE_DOMEFLAT*.tarz')\n    #if len(fs) > 0: \n    #    os.system('tar xzvf ' + fs[0])\n    #fs = glob.glob(subpath+pprun+'/SCIENCE_SKYFLAT*.tarz')\n    #if len(fs) > 0: \n    #    os.system('tar xzvf ' + fs[0])\n\n\n    search_params['files'].sort()\n\n    children = []\n    if 1:\n        for image in search_params['files']:\n            ROOT = re.split('\\.',re.split('\\/',image)[-1])[0]\n            BASE = re.split('O',ROOT)[0]\n            NUM = re.split('O',re.split('\\_',ROOT)[1])[0]\n            print image, search_params['CRVAL1ASTROMETRY_'+NUM]\n       \n        ''' copy over ASTROMETRY keywords to Corrected if they exist for the unCorrected frame '''\n        if search_params['CORRECTED']=='True': # and string.find(str(search_params['CRVAL1ASTROMETRY_2']),'None') != -1:             \n            ''' adding correct WCS info '''\n            dict_uncorrected = get_files(SUPA[:-1],FLAT_TYPE)                                                               \n            d = {}                                                                                                          \n            akeys = filter(lambda x:string.find(x,'ASTROMETRY')!=-1,dict_uncorrected.keys())                                \n            for key in akeys:                    \n                d[key] = dict_uncorrected[key]    \n                print key, d[key], SUPA[:-1]\n            save_exposure(d,SUPA,FLAT_TYPE)                                                                                 \n            os.system('mkdir -p ' + search_params['TEMPDIR'])                                                               \n            \n            dict = get_files(SUPA,FLAT_TYPE)\n            search_params = initialize(dict['filter'],dict['OBJNAME'])\n            search_params.update(dict)\n\n            for key in akeys:                                                                                               \n                print key, search_params[key]\n\n\n        for image in search_params['files']:\n            print image\n            child = False \n            if not trial:\n                child = os.fork()           \n                if child:\n                    children.append(child)\n\n            params = copy(search_params)                                \n            ROOT = re.split('\\.',re.split('\\/',image)[-1])[0]\n            params['ROOT'] = ROOT\n            params['ROOT_WEIGHT'] = ROOT.replace('I','')\n            BASE = re.split('O',ROOT)[0]\n            params['BASE'] = BASE \n            NUM = re.split('O',re.split('\\_',ROOT)[1])[0]\n            params['NUM'] = NUM\n            print NUM, BASE, ROOT\n\n            if not child:\n                if (search_params['CORRECTED']!='True' or (search_params['CORRECTED']=='True' and string.find(str(search_params['CRVAL1ASTROMETRY_' + NUM]),'None') == -1)):\n                    try:                                                                                                                                                                                                                                                                                                                                                                                                               \n                        params['GAIN'] = 2.50 ## WARNING!!!!!!\n                        print ROOT\n                        finalflagim = \"%(TEMPDIR)sflag_%(ROOT)s.fits\" % params     \n                        res = re.split('SCIENCE',image)\n                        res = re.split('/',res[0])\n                        if res[-1]=='':res = res[:-1]\n                        params['path'] = reduce(lambda x,y:x+'/'+y,res[:-1])\n                        params['fil_directory'] = res[-1]\n                        weightim = \"/%(path)s/%(fil_directory)s/WEIGHTS/%(ROOT)s.weight.fits\" % params\n                        #flagim = \"/%(path)s/%(fil_directory)s/WEIGHTS/globalflag_%(NUM)s.fits\" % params\n                        #finalflagim = TEMPDIR + \"flag_%(ROOT)s.fits\" % params \n                        params['finalflagim'] = weightim\n                        im = \"/%(path)s/%(fil_directory)s/SCIENCE/%(ROOT)s.fits\" % params\n                        crpix = utilities.get_header_kw(im,['CRPIX1','CRPIX2'])\n                        \n                        #if search_params['SDSS_coverage'] == 'yes': catalog = 'SDSS-R6'\n                        #else: catalog = '2MASS'\n                                                                                                                                                                                                                                                                                                                                                                                                                                       \n                        SDSS1 = \"/%(path)s/%(fil_directory)s/SCIENCE/headers_scamp_SDSS-R6/%(BASE)s.head\" % params\n                        SDSS2 = \"/%(path)s/%(fil_directory)s/SCIENCE/headers_scamp_SDSS-R6/%(BASE)sO*.head\" % params\n                        from glob import glob \n                        print glob(SDSS1), glob(SDSS2)\n                        head = None\n                        if len(glob(SDSS1)) > 0:\n                            head = glob(SDSS1)[0]\n                        elif len(glob(SDSS2)) > 0:\n                            head = glob(SDSS2)[0]\n                                                                                                                                                                                                                                                                                                                                                                                                                                       \n                        ''' see if image has been run through astrometry.net. if not, run it. '''\n                        if True: \n                                                                                                                                                                                                                                                                                                                                                                                                                                       \n                            if not search_params.has_key('ASTROMETRYNET_' + NUM):\n                                save_exposure({'ASTROMETRYNET_' + NUM:'None'},SUPA,FLAT_TYPE)\n                                dict = get_files(dict['SUPA'],dict['FLAT_TYPE'])\n                                search_params.update(dict)\n                                                                                                                                                                                                                                                                                                                                                                                                                                       \n                            if search_params['CORRECTED']!='True' and string.find(str(search_params['CRVAL1ASTROMETRY_' + NUM]),'None') != -1: #head is None:\n                                save_exposure({'ASTROMETRYNET_' + NUM:'yes'},SUPA,FLAT_TYPE)\n                                                                                                                                                                                                                                                                                                                                                                                                                                       \n                                imtmp = \"%(TEMPDIR)s/%(ROOT)s.tmp.fits\" % params  \n                                imfix = \"%(TEMPDIR)s/%(ROOT)s.fixwcs.fits\" % params\n                                imwcs = \"%(TEMPDIR)s/%(ROOT)s.wcsfile\" % params\n                                                                                                                                                                                                                                                                                                                                                                                                                                       \n                                command = \"cp \" + im + \" \" + imtmp\n                                print command\n                                utilities.run(command)\n                                os.system('rm ' + imfix)\n                                #command = '/nfs/slac/g/ki/ki04/pkelly/astrometry/bin//solve-field --cpulimit 60  --no-verify --no-plots --overwrite --scale-units arcsecperpix --scale-low ' + str(float(params['PIXSCALE'])-0.005) + ' --scale-high '  + str(float(params['PIXSCALE'])+0.005) + ' -N ' + imfix + ' ' + imtmp\n                                command = astrom + ' --temp-dir ' + tmpdir + ' --cpulimit 100  --no-verify --no-plots --overwrite --scale-units arcsecperpix --scale-low ' + str(float(params['PIXSCALE'])-0.005) + ' --scale-high '  + str(float(params['PIXSCALE'])+0.005) + ' -N ' + imfix + ' ' + imtmp\n                                print command\n                                os.system(command)\n                                os.system('rm ' + imtmp)\n                                from glob import glob\n                                                                                                                                                                                                                                                                                                                                                                                                                                       \n                                if len(glob(imfix)):\n                                    command = 'imhead < ' + imfix + ' > ' +  imwcs\n                                    print command\n                                    os.system(command)\n                                    hf = open(imwcs,'r').readlines() \n                                    hdict = {}\n                                    for line in hf:\n                                        import re\n                                        if string.find(line,'=') != -1:\n                                            res = re.split('=',line)\n                                            name = res[0].replace(' ','')\n                                            res = re.split('/',res[1])\n                                            value = res[0].replace(' ','')\n                                            print name, value\n                                            hdict[name] = value               \n                                                                                                                            \n                                    ''' now save the wcs '''\n                                    wcsdict = {}                                                                       \n                                    import commands\n                                    out = commands.getoutput('gethead ' + imfix + ' CRPIX1 CRPIX2')\n                                    import re\n                                    res = re.split('\\s+',out)\n                                    os.system('sethead ' + imfix + ' CRPIX1OLD=' + res[0])\n                                    os.system('sethead ' + imfix + ' CRPIX2OLD=' + res[1])\n                                    for name in ['CRVAL1','CRVAL2','CD1_1','CD1_2','CD2_1','CD2_2','CRPIX1','CRPIX2']:\n                                        print name + 'ASTROMETRY_' + NUM, hdict[name]\n                                        wcsdict[name + 'ASTROMETRY_' + NUM] = hdict[name]                                        \n                                                                                                                           \n                                    save_exposure(wcsdict,SUPA,FLAT_TYPE)\n                                    dict = get_files(dict['SUPA'],dict['FLAT_TYPE'])\n                                    search_params.update(dict)\n                                                                                                                                                                                                                                                                                                                                                                                                                                       \n                            hdict = {}\n                            if string.find(str(search_params['CD1_1ASTROMETRY_' + NUM]),'None') == -1: #head is None:\n                                for name in ['CRVAL1','CRVAL2','CD1_1','CD1_2','CD2_1','CD2_2','CRPIX1','CRPIX2']:\n                                    print name + 'ASTROMETRY', search_params[name+'ASTROMETRY_' + NUM]\n                                    hdict[name] = search_params[name+'ASTROMETRY_' + NUM]                                        \n                            elif head is not None:\n                                ''' if no solution from astrometry.net, use the Swarp solution '''\n                                hf = open(head,'r').readlines() \n                                for line in hf:\n                                    import re\n                                    if string.find(line,'=') != -1:\n                                        res = re.split('=',line)\n                                        name = res[0].replace(' ','')\n                                        res = re.split('/',res[1])\n                                        value = res[0].replace(' ','')\n                                        print name, value\n                                        hdict[name] = value\n                            else: sys.exit(0)\n                                                                                                                                                       \n                            imfix = \"%(TEMPDIR)s/%(ROOT)s.fixwcs.fits\" % params\n                            print imfix\n                            \n                            os.system('mkdir ' + search_params['TEMPDIR'])\n                            command = \"cp \" + im + \" \" + imfix\n                            print command\n                            print 'copying file', im\n                            utilities.run(command)\n                            print 'finished copying'\n                            \n                            \n                            import commands\n                            out = commands.getoutput('gethead ' + imfix + ' CRPIX1 CRPIX2')\n                            import re\n                            res = re.split('\\s+',out)\n                            os.system('sethead ' + imfix + ' CRPIX1OLD=' + res[0])\n                            os.system('sethead ' + imfix + ' CRPIX2OLD=' + res[1])\n                            for name in ['CRVAL1','CRVAL2','CD1_1','CD1_2','CD2_1','CD2_2','CRPIX1','CRPIX2']:\n                                command = 'sethead ' + imfix + ' ' + name + '=' + str(hdict[name])\n                                print command\n                                os.system(command)\n                        ''' now run sextractor '''\n                        if 1:\n                            main_file = '%(TEMPDIR)s/%(ROOT)s.fixwcs.fits' % params\n                            doubles_raw = [{'file_pattern':main_file,'im_type':''},]\n                                           #{'file_pattern':subpath+pprun+'/SCIENCE_DOMEFLAT*/'+BASE+'OC*.fits','im_type':'D'},\n                                           #{'file_pattern':subpath+pprun+'/SCIENCE_SKYFLAT*/'+BASE+'OC*.fits','im_type':'S'}]\n                                           #{'file_pattern':subpath+pprun+'/SCIENCE/OC_IMAGES/'+BASE+'OC*.fits','im_type':'OC'}\n                                           # ] \n                                                                                                                                      \n                            print doubles_raw\n                            doubles_output = []\n                            print doubles_raw\n                            for double in doubles_raw:\n                                file = glob(double['file_pattern'])\n                                if len(file) > 0:\n                                    params.update(double) \n                                    params['double_cat'] = '%(TEMPDIR)s/%(ROOT)s.%(im_type)s.fixwcs.cat' % params\n                                    params['file_double'] = file[0]\n                                    #print params\n                                    #for par in ['fwhm','GAIN']:\n                                    #    print par, type(params[par]), params[par]\n                                    command = \"nice sex %(TEMPDIR)s%(ROOT)s.fixwcs.fits,%(file_double)s -c %(PHOTCONF)s/phot.conf.sex \\\n                                    -PARAMETERS_NAME %(PHOTCONF)s/phot.param.sex \\\n                                    -CATALOG_NAME %(double_cat)s \\\n                                    -FILTER_NAME %(DATACONF)s/default.conv\\\n                                    -FILTER  Y \\\n                                    -FLAG_TYPE MAX\\\n                                    -FLAG_IMAGE ''\\\n                                    -SEEING_FWHM %(fwhm).3f \\\n                                    -DETECT_MINAREA 3 -DETECT_THRESH 3 -ANALYSIS_THRESH 3 \\\n                                    -MAG_ZEROPOINT 27.0 \\\n                                    -GAIN %(GAIN).3f \\\n                                    -WEIGHT_IMAGE /%(path)s/%(fil_directory)s/WEIGHTS/%(ROOT_WEIGHT)s.weight.fits\\\n                                    -WEIGHT_TYPE MAP_WEIGHT\" % params\n                                                                                                                                                                                                                                                                                                                                                                                                                                       \n                                    #-CHECKIMAGE_TYPE BACKGROUND,APERTURES,SEGMENTATION\\\n                                    #-CHECKIMAGE_NAME /%(path)s/%(fil_directory)s/PHOTOMETRY/coadd.background.fits,/%(path)s/%(fil_directory)s/PHOTOMETRY/coadd.apertures.fits,/%(path)s/%(fil_directory)s/PHOTOMETRY/coadd.segmentation.fits\\\n                                    catname = \"%(TEMPDIR)s/%(ROOT)s.cat\" % params\n                                    print command\n                                    \n                                    utilities.run(command,[catname])\n                                    command = 'ldacconv -b 1 -c R -i ' + params['double_cat']  + ' -o '  + params['double_cat'].replace('cat','rawconv')\n                                    print command\n                                    utilities.run(command)\n                                    #command = 'ldactoasc -b -q -i ' + params['double_cat'].replace('cat','rawconv') + '  -t OBJECTS\\\n                                    #        -k ALPHA_J2000 DELTA_J2000 > ' + params['double_cat'].replace('cat','pos')\n                                    #print command\n                                    #utilities.run(command)\n                                    #print 'mkreg.pl -c -rad 8 -xcol 0 -ycol 1 -wcs -colour green ' + params['double_cat'].replace('cat','pos')\n                                    #utilities.run(command)\n                                    #print params['double_cat'].replace('cat','pos')\n                                    # Xpos_ABS is difference of CRPIX and zero CRPIX\n                                    doubles_output.append({'cat':params['double_cat'].replace('cat','rawconv'),'im_type':double['im_type']})\n                            \n                            print doubles_output\n                            print '***********************************'\n                            \n                            outfile = params['TEMPDIR'] + params['ROOT'] + '.conv'\n                            combine_cats(doubles_output,outfile,search_params)\n                            \n                            #outfile_field = params['TEMPDIR'] + params['ROOT'] + '.field'\n                            #command = 'ldacdeltab -i ' + outfile + ' -t FIELDS -o ' + outfile_field\n                            #utilities.run(command)\n                            \n                            command = 'ldactoasc -b -q -i ' + outfile + '  -t OBJECTS\\\n                                            -k ALPHA_J2000 DELTA_J2000 > ' + outfile.replace('conv','pos')\n                            print command\n                            utilities.run(command)\n                            command = 'mkreg.pl -c -rad 8 -xcol 0 -ycol 1 -wcs -colour green ' + outfile.replace('conv','pos')\n                            print command\n                            utilities.run(command)\n                            print outfile\n                            command = 'ldaccalc -i ' + outfile + ' -o ' + params['TEMPDIR'] + params['ROOT'] + '.newpos -t OBJECTS -c \"(Xpos + ' +  str(float(search_params['CRPIX1ZERO']) - float(crpix['CRPIX1'])) + ');\" -k FLOAT -n Xpos_ABS \"\" -c \"(Ypos + ' + str(float(search_params['CRPIX2ZERO']) - float(crpix['CRPIX2'])) + ');\" -k FLOAT -n Ypos_ABS \"\" -c \"(Ypos*0 + ' + str(params['NUM']) + ');\" -k FLOAT -n CHIP \"\" ' \n                            print command\n                            utilities.run(command)\n                    except:\n                        print traceback.print_exc(file=sys.stdout)\n                        if not trial:\n                            sys.exit(0)\n                        if trial:\n                            raise Exception\n                if not trial: \n                    sys.exit(0)\n        print children\n        for child in children:  \n            print 'waiting for', child\n            os.waitpid(child,0)\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n        print 'finished waiting'\n\n    print path, SUPA, search_params['filter'], search_params['ROTATION']\n\n    pasted_cat = path + 'PHOTOMETRY/ILLUMINATION/' + 'pasted_' + SUPA + '_' + search_params['filter'] + '_' + str(search_params['ROTATION']) + '.cat'\n    print pasted_cat\n    os.system('mkdir -p ' + path + 'PHOTOMETRY/ILLUMINATION/')\n\n    from glob import glob        \n    outcat = search_params['TEMPDIR'] + 'tmppaste_' + SUPA + '.cat'\n    newposlist = glob(search_params['TEMPDIR'] + SUPA.replace('I','*I') + '*newpos')\n    print search_params['TEMPDIR'] + SUPA.replace('I','*I') + '*newpos'\n    if len(newposlist) > 1:\n        #command = 'ldacpaste -i ' + search_params['TEMPDIR'] + SUPA + '*newpos -o ' + pasted_cat \n        #print command\n        files = glob(search_params['TEMPDIR'] + SUPA.replace('I','*I') + '*newpos')\n        print files, search_params['TEMPDIR'] + SUPA.replace('I','*I') + '*newpos'\n        paste_cats(files,pasted_cat)\n    else:\n        command = 'cp ' + newposlist[0] + ' ' + pasted_cat \n        utilities.run(command)\n    save_exposure({'pasted_cat':pasted_cat},SUPA,FLAT_TYPE)\n\n    command = \"rm -rf \" + search_params['TEMPDIR']  \n    os.system(command)\n\n    #fs = glob.glob(subpath+pprun+'/SCIENCE_DOMEFLAT*.tarz'.replace('.tarz','')) \n    #if len(fs) > 0: \n    #    os.system('tar xzvf ' + fs[0])\n                                                            \n    #fs = glob.glob(subpath+pprun+'/SCIENCE_SKYFLAT*.tarz'.replace('.tarz',''))\n    #fs = glob.glob(subpath+pprun+'/SCIENCE_SKYFLAT*.tarz')\n    #if len(fs) > 0: \n    #    os.system('tar xzvf ' + fs[0])\n\n\n    #return exposures, LENGTH1, LENGTH2 \n\ndef get_sdss_obj_ext(SUPA, FLAT_TYPE):\n    dict = get_files(SUPA,FLAT_TYPE)\n    search_params = initialize(dict['filter'],dict['OBJNAME'])\n    search_params.update(dict)\n    \n    ROTATION = str(search_params['ROTATION']) #exposures[exposure]['keywords']['ROTATION']\n    \n    import os\n    starcat ='/nfs/slac/g/ki/ki05/anja/SUBARU/%(OBJNAME)s/PHOTOMETRY/sdssstar%(ROTATION)s.cat' % {'ROTATION':ROTATION,'OBJNAME':search_params['OBJNAME']}\n    galaxycat ='/nfs/slac/g/ki/ki05/anja/SUBARU/%(OBJNAME)s/PHOTOMETRY/sdssgalaxy%(ROTATION)s.cat' % {'ROTATION':ROTATION,'OBJNAME':search_params['OBJNAME']}\n    \n    path='/nfs/slac/g/ki/ki05/anja/SUBARU/%(OBJNAME)s/' % {'OBJNAME':search_params['OBJNAME']}\n    illum_path='/nfs/slac/g/ki/ki05/anja/SUBARU/ILLUMINATION/' % {'OBJNAME':search_params['OBJNAME']}\n    #os.system('mkdir -p ' + path + 'PHOTOMETRY/ILLUMINATION/') \n    os.system('mkdir -p ' + path + 'PHOTOMETRY/ILLUMINATION/STAR/') \n    os.system('mkdir -p ' + path + 'PHOTOMETRY/ILLUMINATION/GALAXY/') \n    from glob import glob\n    \n    print starcat\n    \n    for type,cat in [['star',starcat]]: #,['galaxy',galaxycat]]:\n        catalog = search_params['pasted_cat'] #exposures[exposure]['pasted_cat']\n        ramin,ramax, decmin, decmax = coordinate_limits(catalog)    \n        limits = {'ramin':ramin-0.2,'ramax':ramax+0.2,'decmin':decmin-0.2,'decmax':decmax+0.2}\n        print ramin,ramax, decmin, decmax\n        if len(glob(cat)) == 0:                                      \n            #os.system('rm ' + cat)\n            image = search_params['files'][0]\n            print image\n            import retrieve_test\n            retrieve_test.run(image,cat,type,limits)\n\n    save_exposure({'starcat':cat},SUPA,FLAT_TYPE)\n    return cat\n\n\ndef get_sdss_obj(SUPA, FLAT_TYPE):\n    dict = get_files(SUPA,FLAT_TYPE)\n    search_params = initialize(dict['filter'],dict['OBJNAME'])\n    search_params.update(dict)\n    \n    ROTATION = str(search_params['ROTATION']) #exposures[exposure]['keywords']['ROTATION']\n    \n    import os\n    starcat ='/nfs/slac/g/ki/ki05/anja/SUBARU/%(OBJNAME)s/PHOTOMETRY/sdssstar%(ROTATION)s.cat' % {'ROTATION':ROTATION,'OBJNAME':search_params['OBJNAME']}\n    galaxycat ='/nfs/slac/g/ki/ki05/anja/SUBARU/%(OBJNAME)s/PHOTOMETRY/sdssgalaxy%(ROTATION)s.cat' % {'ROTATION':ROTATION,'OBJNAME':search_params['OBJNAME']}\n    \n    path='/nfs/slac/g/ki/ki05/anja/SUBARU/%(OBJNAME)s/' % {'OBJNAME':search_params['OBJNAME']}\n    illum_path='/nfs/slac/g/ki/ki05/anja/SUBARU/ILLUMINATION/' % {'OBJNAME':search_params['OBJNAME']}\n    #os.system('mkdir -p ' + path + 'PHOTOMETRY/ILLUMINATION/') \n    os.system('mkdir -p ' + path + 'PHOTOMETRY/ILLUMINATION/STAR/') \n    os.system('mkdir -p ' + path + 'PHOTOMETRY/ILLUMINATION/GALAXY/') \n    from glob import glob\n    \n    print starcat\n    \n    for type,cat in [['star',starcat]]: #,['galaxy',galaxycat]]:\n        catalog = search_params['pasted_cat'] #exposures[exposure]['pasted_cat']\n        ramin,ramax, decmin, decmax = coordinate_limits(catalog)    \n        limits = {'ramin':ramin-0.2,'ramax':ramax+0.2,'decmin':decmin-0.2,'decmax':decmax+0.2}\n        print ramin,ramax, decmin, decmax\n        if len(glob(cat)) == 0:                                      \n            #os.system('rm ' + cat)\n            image = search_params['files'][0]\n            print image\n            import retrieve_test\n            retrieve_test.run(image,cat,type,limits)\n\n    save_exposure({'starcat':cat},SUPA,FLAT_TYPE)\n\ndef match_simple(SUPA,FLAT_TYPE):\n    dict = get_files(SUPA,FLAT_TYPE)\n    search_params = initialize(dict['filter'],dict['OBJNAME'])\n    search_params.update(dict)\n\n    print 'hey'\n    ROTATION = str(search_params['ROTATION']) #exposures[exposure]['keywords']['ROTATION']\n\n    import os\n    starcat ='/nfs/slac/g/ki/ki05/anja/SUBARU/%(OBJNAME)s/PHOTOMETRY/sdssstar%(ROTATION)s.cat' % {'ROTATION':ROTATION,'OBJNAME':search_params['OBJNAME']}\n    galaxycat ='/nfs/slac/g/ki/ki05/anja/SUBARU/%(OBJNAME)s/PHOTOMETRY/sdssgalaxy%(ROTATION)s.cat' % {'ROTATION':ROTATION,'OBJNAME':search_params['OBJNAME']}\n\n    path='/nfs/slac/g/ki/ki05/anja/SUBARU/%(OBJNAME)s/' % {'OBJNAME':search_params['OBJNAME']}\n    illum_path='/nfs/slac/g/ki/ki05/anja/SUBARU/ILLUMINATION/' % {'OBJNAME':search_params['OBJNAME']}\n    #os.system('mkdir -p ' + path + 'PHOTOMETRY/ILLUMINATION/') \n    os.system('mkdir -p ' + path + 'PHOTOMETRY/ILLUMINATION/STAR/') \n    os.system('mkdir -p ' + path + 'PHOTOMETRY/ILLUMINATION/GALAXY/') \n    from glob import glob\n\n    print starcat\n\n    for type,cat in [['star',starcat]]: #,['galaxy',galaxycat]]:\n        catalog = search_params['pasted_cat'] #exposures[exposure]['pasted_cat']\n        ramin,ramax, decmin, decmax = coordinate_limits(catalog)    \n        limits = {'ramin':ramin-0.2,'ramax':ramax+0.2,'decmin':decmin-0.2,'decmax':decmax+0.2}\n        print ramin,ramax, decmin, decmax\n        if len(glob(cat)) == 0:                                      \n            #os.system('rm ' + cat)\n            image = search_params['files'][0]\n            print image\n            import retrieve_test\n            retrieve_test.run(image,cat,type,limits)\n\n        filter = search_params['filter'] #exposures[exposure]['keywords']['filter']\n        #GABODSID = exposures[exposure]['keywords']['GABODSID']\n        OBJECT = search_params['OBJECT'] #exposures[exposure]['keywords']['OBJECT']\n        print catalog\n        outcat = path + 'PHOTOMETRY/ILLUMINATION/' + type + '/' + 'matched_' + SUPA + '_' + filter + '_' + ROTATION + '_' + type + '.cat'\n        outcat_dir = path + 'PHOTOMETRY/ILLUMINATION/' + type + '/' + ROTATION + '/' + OBJECT + '/'\n        os.system('mkdir -p ' + outcat_dir)\n        file = 'matched_' + SUPA + '.cat'               \n        linkdir = illum_path + '/' + filter + '/' + ROTATION + '/' + OBJECT + '/'              \n        #outcatlink = linkdir + 'matched_' + exposure + '_' + OBJNAME + '_' + GABODSID + '.cat' \n        outcatlink = linkdir + 'matched_' + SUPA + '_' + search_params['OBJNAME'] + '_' + type + '.cat' \n        os.system('mkdir -p ' + linkdir)\n        os.system('rm ' + outcat)\n        command = 'match_simple.sh ' + catalog + ' ' + cat + ' ' + outcat\n        print command\n        os.system(command)\n\n        os.system('rm ' + outcatlink)\n        command = 'ln -s ' + outcat + ' ' + outcatlink\n        print command\n        os.system(command)\n\n        save_exposure({'matched_cat_' + type:outcat},SUPA,FLAT_TYPE)\n\n        print type, 'TYPE!'\n        print outcat, type\n        #exposures[exposure]['matched_cat_' + type] = outcat\n\n    #return exposures\n\ndef phot(SUPA,FLAT_TYPE): \n    dict = get_files(SUPA,FLAT_TYPE)\n    print dict.keys()\n    search_params = initialize(dict['filter'],dict['OBJNAME'])\n    search_params.update(dict)\n\n    filter = dict['filter']\n\n    import utilities\n    info = {'B':{'filter':'g','color1':'gmr','color2':'umg','EXTCOEFF':-0.2104,'COLCOEFF':0.0},\\\n        'W-J-B':{'filter':'g','color1':'gmr','color2':'umg','EXTCOEFF':-0.2104,'COLCOEFF':0.0},\\\n        'W-J-V':{'filter':'g','color1':'gmr','color2':'rmi','EXTCOEFF':-0.1202,'COLCOEFF':0.0},\\\n        'W-C-RC':{'filter':'r','color1':'rmi','color2':'gmr','EXTCOEFF':-0.0925,'COLCOEFF':0.0},\\\n        'W-C-IC':{'filter':'i','color1':'imz','color2':'rmi','EXTCOEFF':-0.02728,'COLCOEFF':0.0},\\\n        'W-S-I+':{'filter':'i','color1':'imz','color2':'rmi','EXTCOEFF':-0.02728,'COLCOEFF':0.0},\\\n        'W-S-Z+':{'filter':'z','color1':'imz','color2':'rmi','EXTCOEFF':0.0,'COLCOEFF':0.0}}\n    \n    import mk_saturation_plot,os,re\n    os.environ['BONN_TARGET'] = search_params['OBJNAME']\n    os.environ['INSTRUMENT'] = 'SUBARU'\n\n    stars_0 = []\n    stars_90 = []\n\n    ROTATION = dict['ROTATION']\n    print ROTATION \n    import os\n    ppid = str(os.getppid())\n    from glob import glob\n    for im_type in ['']: #,'D','S']:\n        for type in ['star']: #,'galaxy']:\n            file = dict['matched_cat_' + type]\n            print file\n            print file\n            if type == 'galaxy':\n                mag='MAG_AUTO' + im_type      \n                magerr='MAGERR_AUTO' + im_type\n                class_star = \"<0.9\"\n            if type == 'star':\n                mag='MAG_APER2' + im_type      \n                magerr='MAGERR_APER2' + im_type\n                class_star = \">0.9\" \n            \n            print 'filter', filter\n            os.environ['BONN_FILTER'] = filter \n            filt = re.split('_',filter)[0]\n            d = info[filt]\n            print file\n            utilities.run('ldacfilter -i ' +  file + ' -o ' + search_params['TEMPDIR'] + 'good.stars' + ' -t PSSC\\\n                        -c \"(Flag!=-99);\"',['' + search_params['TEMPDIR'] + 'good.stars'])\n\n            utilities.run('ldacfilter -i ' + search_params['TEMPDIR'] + 'good.stars -o ' + search_params['TEMPDIR'] + 'good.colors -t PSSC\\\n                -c \"((((SEx_' + mag + '!=0 AND ' + d['color1'] + '<900) AND ' + d['color1'] + '!=0) AND ' + d['color1'] + '>-900) AND ' + d['color1'] + '!=0);\"',['' + search_params['TEMPDIR'] + 'good.colors'])\n            print '' + search_params['TEMPDIR'] + 'good.colors'\n            utilities.run('ldaccalc -i ' + search_params['TEMPDIR'] + 'good.colors -t PSSC -c \"(' + d['filter'] + 'mag - SEx_' + mag + ');\"  -k FLOAT -n magdiff \"\" -o ' + search_params['TEMPDIR'] + 'all.diffA.cat' ,[search_params['TEMPDIR'] + 'all.diffA.cat'] )\n\n            median = get_median('' + search_params['TEMPDIR'] + 'all.diffA.cat','magdiff')\n            utilities.run('ldacfilter -i ' + search_params['TEMPDIR'] + 'all.diffA.cat -o ' + search_params['TEMPDIR'] + 'all.diffB.cat -t PSSC\\\n                -c \"((magdiff > ' + str(median -1.25) + ') AND (magdiff < ' + str(median + 1.25) + '));\"',['' + search_params['TEMPDIR'] + 'good.colors'])\n            utilities.run('ldaccalc -i ' + search_params['TEMPDIR'] + 'all.diffB.cat -t PSSC -c \"(SEx_MaxVal + SEx_BackGr);\"  -k FLOAT -n MaxVal \"\" -o ' + search_params['TEMPDIR'] + 'all.diff.cat' ,['' + search_params['TEMPDIR'] + 'all.diff.cat'] )\n            command = 'ldactoasc -b -q -i ' + search_params['TEMPDIR'] + 'all.diff.cat -t PSSC -k SEx_' + mag + ' ' + d['filter'] + 'mag SEx_FLUX_RADIUS ' + im_type + ' SEx_CLASS_STAR' + im_type + ' ' + d['filter'] + 'err ' + d['color1'] + ' MaxVal > ' + search_params['TEMPDIR'] + 'mk_sat_all'\n            #print command\n            utilities.run(command,['' + search_params['TEMPDIR'] + 'mk_sat_all'] )\n            import commands\n            length = commands.getoutput('wc -l ' + search_params['TEMPDIR'] + 'mk_sat_all')\n            print 'TOTAL # of STARS:', length\n            cuts_to_make = ['MaxVal>27500.0','Clean!=1','SEx_IMAFLAGS_ISO'+im_type + '!=0','SEx_CLASS_STAR'+im_type+ class_star,'SEx_Flag'+im_type+'!=0',]\n            files = ['' + search_params['TEMPDIR'] + 'mk_sat_all']\n            titles = ['raw']\n            for cut in cuts_to_make:\n                #print 'making cut:', cut\n                cut_name = cut.replace('>','').replace('<','')\n                os.system('rm ' + cut_name)\n                command = 'ldacfilter -i ' + search_params['TEMPDIR'] + 'all.diff.cat -o ' + search_params['TEMPDIR'] + '' + cut_name + ' -t PSSC\\\n                       -c \"(' + cut + ');\"'\n                utilities.run(command,['' + search_params['TEMPDIR'] + '' + cut_name])\n                import glob\n                #print len(glob.glob('' + search_params['TEMPDIR'] + '' + cut_name)), glob.glob('' + search_params['TEMPDIR'] + '' + cut_name)\n                if len(glob.glob('' + search_params['TEMPDIR'] + '' + cut_name)) > 0:\n                    utilities.run('ldactoasc -b -q -i ' + search_params['TEMPDIR'] + '' + cut_name + '  -t PSSC\\\n                        -k SEx_' + mag + ' ' + d['filter'] + 'mag SEx_FLUX_RADIUS SEx_CLASS_STAR ' + d['filter'] + 'err ' + d['color1'] + ' > ' + search_params['TEMPDIR'] + '' + cut_name + '.cat',['' + search_params['TEMPDIR'] + '' + cut_name + '.cat'])\n\n                    length = commands.getoutput('wc -l ' + search_params['TEMPDIR'] + '' + cut_name + '.cat')\n                    print 'TOTAL # of STARS CUT:', length\n                    titles.append(cut_name)\n                    files.append('' + search_params['TEMPDIR'] + '' + cut_name + '.cat')\n                    #run('ldactoasc -b -q -i cutout1.' + ppid + '  -t PSSC\\\n                    #        -k Ra Dec > ' + search_params['TEMPDIR'] + '' + outfile,['' + search_params['TEMPDIR'] + '' + outfile])\n                    #run('mkreg.pl -c -rad 8 -xcol 0 -ycol 1 -wcs -colour ' + color + ' ' + search_params['TEMPDIR'] + '' + outfile)\n\n            utilities.run('ldacfilter -i ' + search_params['TEMPDIR'] + 'all.diff.cat -o ' + search_params['TEMPDIR'] + 'good.stars -t PSSC\\\n                    -c \"(MaxVal<27500 AND SEx_IMAFLAGS_ISO'+im_type+'=0);\"',['' + search_params['TEMPDIR'] + 'good.stars'])\n\n                  #-c \"((MaxVal<27500 AND SEx_CLASS_STAR'+im_type+class_star + ') AND SEx_IMAFLAGS_ISO'+im_type+'=0);\"',['' + search_params['TEMPDIR'] + 'good.stars'])\n\n                  #-c \"(MaxVal<27500 AND SEx_IMAFLAGS_ISO'+im_type+'=0);\"',['' + search_params['TEMPDIR'] + 'good.stars' + ppid])\n            \n            utilities.run('ldactoasc -b -q -i ' + search_params['TEMPDIR'] + 'good.stars  -t PSSC\\\n                    -k SEx_' + mag + ' ' + d['filter'] + 'mag SEx_FLUX_RADIUS' + im_type + ' SEx_CLASS_STAR'+im_type+' ' + d['filter'] + 'err ' + d['color1'] + ' > ' + search_params['TEMPDIR'] + 'mk_sat',['' + search_params['TEMPDIR'] + 'mk_sat'])\n                              \n            \n            if len(glob.glob('' + search_params['TEMPDIR'] + 'mk_sat')) > 0:\n                files.append('' + search_params['TEMPDIR'] + 'mk_sat')\n                titles.append('filtered')\n            print files, titles\n            mk_saturation_plot.mk_saturation_all(files,titles,filter)\n            #cutout('' + search_params['TEMPDIR'] + 'good.stars' + ppid,mag)\n          \n            print mag\n\n            val = raw_input(\"Look at the saturation plot?\")\n            if len(val)>0:\n                if val[0] == 'y' or val[0] == 'Y':\n                    mk_saturation_plot.mk_saturation(search_params['TEMPDIR'] + '/mk_sat',filter)\n            \n            val = raw_input(\"Make a box?\")\n            if len(val)>0:\n                if val[0] == 'y' or val[0] == 'Y':\n                    mk_saturation_plot.use_box(filter)\n                    lower_mag,upper_mag,lower_diff,upper_diff = re.split('\\s+',open('box' + filter,'r').readlines()[0])\n            \n                    utilities.run('ldacfilter -i ' + search_params['TEMPDIR'] + '/good.stars -t PSSC\\\n                                -c \"(((SEx_' + mag + '>' + lower_mag + ') AND (SEx_' + mag + '<' + upper_mag + ')) AND (magdiff>' + lower_diff + ')) AND (magdiff<' + upper_diff + ');\"\\\n                                -o ' + search_params['TEMPDIR'] + '/filt.mag.new.cat',[search_params['TEMPDIR'] + '/filt.mag.new.cat'])\n\n                    raw_input()\n                    os.system('mv ' + search_params['TEMPDIR'] + '/filt.mag.new.cat ' + search_params['TEMPDIR'] + '/good.stars')\n            #val = [] \n            #val = raw_input(\"Look at the saturation plot?\")\n            #if len(val)>0:\n            #    if val[0] == 'y' or val[0] == 'Y':\n            #        mk_saturation_plot.mk_saturation('' + search_params['TEMPDIR'] + 'mk_sat' + ppid,filter)\n                    # make stellar saturation plot                              \n            #lower_mag,upper_mag,lower_diff,upper_diff = re.split('\\s+',open('box' + filter,'r').readlines()[0])\n            lower_mag = str(10)\n            upper_mag = str(14.0)\n            lower_diff = str(5)\n            upper_diff = str(9)\n            if type == 'star': \n                lower_mag = str(13.2)\n             \n            utilities.run('ldactoasc -b -q -i ' + search_params['TEMPDIR'] + 'good.stars -t PSSC -k SEx_Xpos_ABS SEx_Ypos_ABS > ' + search_params['TEMPDIR'] + 'positions',[search_params['TEMPDIR'] + 'positions'] )\n            \n            utilities.run('ldacaddkey -i ' + search_params['TEMPDIR'] + 'good.stars -o ' + search_params['TEMPDIR'] + 'filt.airmass.cat -t PSSC -k AIRMASS 0.0 FLOAT \"\" ',[search_params['TEMPDIR'] + 'filt.airmass.cat']  )\n                                                                                                                                                                                                                                                                                                          \n            utilities.run('ldacfilter -i ' + search_params['TEMPDIR'] + 'filt.airmass.cat -o ' + search_params['TEMPDIR'] + 'filt.crit.cat -t PSSC\\\n              -c \"((magdiff>-900) AND magdiff<900) AND SEx_' + mag + '!=0) ;\"',['' + search_params['TEMPDIR'] + 'filt.crit.cat'])\n            utilities.run('ldacfilter -i ' + search_params['TEMPDIR'] + 'filt.crit.cat -o ' + search_params['TEMPDIR'] + 'all.colors.cat -t PSSC\\\n                    -c \"(((' + d['color1'] + '<900 AND ' + d['color2'] + '<900) AND ' + d['color1'] + '>-900) AND ' + d['color2'] + '>-900);\"',['' + search_params['TEMPDIR'] + 'all.colors.cat'])\n            \n            utilities.run('ldactoasc -b -q -i ' + search_params['TEMPDIR'] + 'all.colors.cat -t PSSC -k SEx_' + mag + ' ' + d['filter'] + 'mag ' + d['color1'] + ' ' + d['color2'] + ' AIRMASS SEx_' + magerr + ' ' + d['filter'] + 'err SEx_Xpos_ABS SEx_Ypos_ABS > ' + search_params['TEMPDIR'] + 'input.asc' ,['' + search_params['TEMPDIR'] + 'input.asc'] )\n            \n            import photo_abs_new                \n            \n            good = photo_abs_new.run_through('illumination',infile='' + search_params['TEMPDIR'] + 'input.asc',output='' + search_params['TEMPDIR'] + 'photo_res',extcoeff=d['color1'],sigmareject=6,step='STEP_1',bandcomp=d['filter'],color1which=d['color1'],color2which=d['color2'])\n            \n            import astropy, astropy.io.fits as pyfits\n            cols = [] \n            for key in ['corr_data','color1_good','color2_good','magErr_good','X_good','Y_good','airmass_good']: \n                cols.append(pyfits.Column(name=key, format='E',array=good[key]))\n            hdu = pyfits.PrimaryHDU()\n            hdulist = pyfits.HDUList([hdu])\n            print cols\n            tbhu = pyfits.BinTableHDU.from_columns(cols)\n            hdulist.append(tbhu)\n            hdulist[1].header['EXTNAME']='STDTAB'\n            \n            path='/nfs/slac/g/ki/ki05/anja/SUBARU/%(OBJNAME)s/' % {'OBJNAME':search_params['OBJNAME']}\n            outcat = path + 'PHOTOMETRY/ILLUMINATION/fit_' + im_type + '_' + search_params['SUPA'] + '_' +  type + '.cat'                \n            os.system('rm ' + outcat)\n            hdulist.writeto(outcat)\n            save_exposure({'fit_cat_' + im_type + '_' + type: outcat,'airmass_add':'yes'},SUPA,FLAT_TYPE)\n            save_fit(good['fits'],im_type,type,SUPA,FLAT_TYPE)\n\ndef nightrun():\n    import MySQLdb, sys, os, re, time, utilities, pyfits\n    from copy import copy\n    db2 = MySQLdb.connect(db='subaru', user='weaklensing', passwd='darkmatter', host='ki-rh8')\n    c = db2.cursor()\n\n    keystop = ['PPRUN']\n    list = reduce(lambda x,y: x + ',' + y, keystop)\n    command=\"SELECT \" + list + \" from illumination_db where zp_star_ is not null and PPRUN!='KEY_N/A' GROUP BY PPRUN\"\n    print command\n    c.execute(command)\n    results=c.fetchall()\n    db_keys = describe_db(c)\n    h = []\n    for line in results: \n        dtop = {}\n        for i in range(len(keystop)):\n            dtop[keystop[i]] = line[i]\n\n        directory = 'run_' + dtop['PPRUN'] \n\n        os.system('mkdir ' +  os.environ['sne'] + '/plots/' + directory )\n        os.system('rm ' + os.environ['sne'] + '/plots/' + directory + '/*')\n\n        keys = ['OBJNAME','ROTATION']\n        list = reduce(lambda x,y: x + ',' + y, keys)\n        command=\"SELECT \" + list + \" from illumination_db where zp_star_ is not null and PPRUN='\" + dtop['PPRUN'] + \"' GROUP BY OBJNAME,ROTATION\"\n        print command\n        c.execute(command)\n        results=c.fetchall()\n        db_keys = describe_db(c)\n        h = []\n        for line in results: \n            d = {}\n            for i in range(len(keys)):\n                d[keys[i]] = line[i]\n                                                                                                                                                                                                                                                             \n            if 1:\n                #print d\n                if 1:\n                    crit = reduce(lambda x,y: x + ' AND ' + y,[str(y) + \"='\" + str(d[y]) + \"'\" for y in keys]) \n                    file = directory + '/' + reduce(lambda x,y: x + 'AND' + y,[str(y)[0:4] + \"_\" + str(d[y])  for y in keys]) \n                    #print crit\n                \n                    command = \"SELECT * from illumination_db where zp_star_ is not null and \" + crit\n                    #print command\n                    c.execute(command)\n                    results = c.fetchall()\n                    #print results\n                    fit_files = [] \n                    for j in range(len(results)):\n                        dict = {} \n                        for i in range(len(results[j])):  \n                            dict[db_keys[i]] = results[j][i]\n                        #print dict['SUPA'], dict['OBJNAME'], dict['pasted_cat'], dict['matched_cat_star']\n                        fit_files.append(dict['fit_cat__star'])\n                                            \n                    #print fit_files\n                    dict = get_files(dict['SUPA'],dict['FLAT_TYPE'])\n                    #print dict.keys()\n                    search_params = initialize(dict['filter'],dict['OBJNAME'])\n                    search_params.update(dict)\n                                               \n                    from copy import copy\n                    import photo_abs_new\n                    reload(photo_abs_new)\n                    files = reduce(lambda x,y: x + ' ' + y,fit_files)\n                    #print files\n                    tempfile = '' + search_params['TEMPDIR'] + 'spit'\n                    command = 'ldacpaste -i ' + files + ' -t STDTAB -o ' + tempfile\n                    print command\n                    utilities.run(command)\n                    hdulist = pyfits.open(tempfile)\n                    args = {}\n                    for column in hdulist[\"STDTAB\"].columns:\n                        args[column.name] = hdulist[\"STDTAB\"].data.field(column.name)\n                    photo_abs_new.calcDataIllum(file,search_params['LENGTH1'], search_params['LENGTH2'], 1000, args['corr_data'], args['airmass_good'], args['color1_good'], args['color2_good'], args['magErr_good'], args['X_good'], args['Y_good'],rot=0)\n            #except: print 'failed'\n\ndef auto_print():\n    import MySQLdb, sys, os, re, time, utilities, pyfits\n    from copy import copy\n    db2 = MySQLdb.connect(db='subaru', user='weaklensing', passwd='darkmatter', host='ki-rh8')\n    c = db2.cursor()\n\n    keys = ['FILTER','ROTATION']\n    list = reduce(lambda x,y: x + ',' + y, keys)\n    command=\"SELECT \" + list + \" from illumination_db where zp_star_ is not null and PPRUN!='KEY_N/A' and good_stars_star_ > 400 GROUP BY \"+list\n    print command\n    c.execute(command)\n    results=c.fetchall()\n    db_keys = describe_db(c)\n    h = []\n    for line in results: \n        d = {}\n        for i in range(len(keys)):\n            d[keys[i]] = line[i]\n\n        if 1:\n            print d\n            if 1:\n                crit = reduce(lambda x,y: x + ' AND ' + y,[str(y) + \"='\" + str(d[y]) + \"'\" for y in keys]) \n                file = 'filt_' + reduce(lambda x,y: x + 'AND' + y,[str(y)[0:4] + \"_\" + str(d[y])  for y in keys]) \n                print crit\n            \n                command = \"SELECT * from illumination_db where zp_star_ is not null and \" + crit\n\n                print command\n                c.execute(command)\n                results = c.fetchall()\n                print results\n                fit_files = [] \n                for j in range(len(results)):\n                    dict = {} \n                    for i in range(len(results[j])):  \n                        dict[db_keys[i]] = results[j][i]\n                    print dict['SUPA'], dict['OBJNAME'], dict['pasted_cat'], dict['matched_cat_star']\n                    fit_files.append(dict['fit_cat__star'])\n                                        \n                print fit_files\n                dict = get_files(dict['SUPA'],dict['FLAT_TYPE'])\n                print dict.keys()\n                search_params = initialize(dict['filter'],dict['OBJNAME'])\n                search_params.update(dict)\n                                           \n                from copy import copy\n                import photo_abs_new\n                reload(photo_abs_new)\n                files = reduce(lambda x,y: x + ' ' + y,fit_files)\n                print files\n                tempfile = '' + search_params['TEMPDIR'] + 'spit'\n                command = 'ldacpaste -i ' + files + ' -t STDTAB -o ' + tempfile\n                print command\n                utilities.run(command)\n                hdulist = pyfits.open(tempfile)\n                args = {}\n                for column in hdulist[\"STDTAB\"].columns:\n                    args[column.name] = hdulist[\"STDTAB\"].data.field(column.name)\n                photo_abs_new.calcDataIllum(file,search_params['LENGTH1'], search_params['LENGTH2'], 1000, args['corr_data'], args['airmass_good'], args['color1_good'], args['color2_good'], args['magErr_good'], args['X_good'], args['Y_good'],rot=0)\n            #except: print 'failed'\n\ndef describe_db(c,db=['illumination_db']):\n    if type(db) != type([]):\n        db = [db]\n    keys = []\n    for d in db:\n        command = \"DESCRIBE \" + d \n        #print command\n        c.execute(command)\n        results = c.fetchall()\n        for line in results:\n            keys.append(line[0])\n    return keys    \n\n\ndef printer():\n    import MySQLdb, sys, os, re, time, utilities, pyfits\n    from copy import copy\n    db2 = MySQLdb.connect(db='subaru', user='weaklensing', passwd='darkmatter', host='ki-rh8')\n    c = db2.cursor()\n       \n    if 1: #for set in [{'OBJNAME':'HDFN', 'filters':['W-J-B','W-J-V','W-C-RC','W-C-IC','W-S-Z+']},{'OBJNAME':'MACS2243-09', 'filters':['W-J-V','W-C-RC','W-C-IC','W-S-Z+']},{'OBJNAME':'A2219', 'filters':['W-J-B','W-J-V','W-C-RC']}]:\n        #OBJNAME = set['OBJNAME']\n        if 1: #for filter in set['filters']:\n            if 1: #try:\n                print keys\n                OBJNAME = 'HDFN'                        \n                filter = 'W-C-ICSF'\n                ROTATION = 1\n                command = \"select * from illumination_db where OBJNAME='\" + OBJNAME + \"' and filter='\" + filter + \"' and fit_cat_galaxy is not null and crfixed='third' and good_stars_star is not null and good_stars_star>10 and ROTATION=\" + str(ROTATION)\n\n                command = \"select * from illumination_db where SUPA='SUPA0011022' and zp_err_galaxy_D is not null\"\n                #command = \"select * from illumination_db where OBJNAME='\" + OBJNAME + \"' and filter='\" + filter + \"' and fit_cat_galaxy is not null and crfixed='third' and ROTATION=\" + str(ROTATION) + ' and good_stars_star is not null and good_stars_star>10'\n\n                command = \"SELECT * from illumination_db where zp_star_ is not null and ROTATION='0'\" # where OBJNAME='HDFN' and filter='W-J-V' and ROTATION=0\"\n\n\n\n                print command\n                c.execute(command)\n                results = c.fetchall()\n                fit_files = [] \n                for j in range(len(results)):\n                    dict = {} \n                    for i in range(len(results[j])):  \n                        dict[keys[i]] = results[j][i]\n                    print dict['SUPA'], dict['OBJNAME'], dict['pasted_cat'], dict['matched_cat_star']\n                    fit_files.append(dict['fit_cat__star'])\n                                        \n                print fit_files\n                dict = get_files(dict['SUPA'],dict['FLAT_TYPE'])\n                print dict.keys()\n                search_params = initialize(dict['filter'],dict['OBJNAME'])\n                search_params.update(dict)\n                                           \n                from copy import copy\n                import photo_abs_new\n                reload(photo_abs_new)\n                files = reduce(lambda x,y: x + ' ' + y,fit_files)\n                print files\n                tempfile = '' + search_params['TEMPDIR'] + 'spit'\n                command = 'ldacpaste -i ' + files + ' -t STDTAB -o ' + tempfile\n                print command\n                utilities.run(command)\n                hdulist = pyfits.open(tempfile)\n                args = {}\n                for column in hdulist[\"STDTAB\"].columns:\n                    args[column.name] = hdulist[\"STDTAB\"].data.field(column.name)\n                file = OBJNAME + '_' + filter + '_' + str(ROTATION)\n                file = raw_input('filename?')\n                photo_abs_new.calcDataIllum(file,search_params['LENGTH1'], search_params['LENGTH2'], 1000, args['corr_data'], args['airmass_good'], args['color1_good'], args['color2_good'], args['magErr_good'], args['X_good'], args['Y_good'],rot=0)\n            #except: print 'failed'\n\n#filter = 'W-C-IC'\nimport pickle\n\n#filters = ['W-J-B','W-J-V','W-C-RC','W-C-IC','W-S-Z+']\n\n#for filter in filters:\n#    exposures_zero = {} \n#    exposures_one = {} \n#    print '$$$$$'\n#    print 'separating into different camera rotations'\n#    for exposure in exposures.keys(): \n#        print exposure,exposures[exposure]['keywords']['ROTATION']\n#        if int(exposures[exposure]['keywords']['ROTATION']) == 1:\n#            exposures_one[exposure] = exposures[exposure]\n#        if int(exposures[exposure]['keywords']['ROTATION']) == 0:\n#            exposures_zero[exposure] = exposures[exposure]\nif 0:\n    reopen = 0\n    save = 0\n    if reopen:\n        f = open('' + search_params['TEMPDIR'] + 'tmppickle' + OBJNAME + filter,'r')\n        m = pickle.Unpickler(f)\n        exposures, LENGTH1, LENGTH2 = m.load()\n    \n        print image.latest\n    \n    if 1: images = gather_exposures(filter,OBJNAME)\n    \n    print images\n    \n    ''' strip down exposure list '''\n    for key in exposures.keys():\n        print exposures[key]['images']\n    \n    for image in exposures:\n        if 1: image.find_seeing(exposures) # save seeing info?\n        if 1: image.sextract(exposures)\n        if 1: image.match_simple(exposures,OBJNAME)\n        if 1: image.phot(exposures,filter,type,LENGTH1,LENGTH2)\n    \n    if save:\n        f = open('' + search_params['TEMPDIR'] + 'tmppickle' + OBJNAME + filter,'w')\n        m = pickle.Pickler(f)\n        pickle.dump([exposures,LENGTH1,LENGTH2],m)\n        f.close()\n\ndef get_sdss(dict):\n    import MySQLdb, sys, os, re, time, utilities, pyfits\n    from copy import copy\n    import os\n    search_params = initialize(dict['filter'],dict['OBJNAME'])\n    search_params.update(dict)\n\n    starcat ='/nfs/slac/g/ki/ki05/anja/SUBARU/%(OBJNAME)s/PHOTOMETRY/sdssstar%(ROTATION)s.cat' % {'ROTATION':search_params['ROTATION'],'OBJNAME':search_params['OBJNAME']}\n    galaxycat ='/nfs/slac/g/ki/ki05/anja/SUBARU/%(OBJNAME)s/PHOTOMETRY/sdssgalaxy%(ROTATION)s.cat' % {'ROTATION':search_params['ROTATION'],'OBJNAME':search_params['OBJNAME']}\n    path='/nfs/slac/g/ki/ki05/anja/SUBARU/%(OBJNAME)s/' % {'OBJNAME':search_params['OBJNAME']}\n    illum_path='/nfs/slac/g/ki/ki05/anja/SUBARU/ILLUMINATION/' % {'OBJNAME':search_params['OBJNAME']}\n    #os.system('mkdir -p ' + path + 'PHOTOMETRY/ILLUMINATION/') \n    os.system('mkdir -p ' + path + 'PHOTOMETRY/ILLUMINATION/STAR/') \n    os.system('mkdir -p ' + path + 'PHOTOMETRY/ILLUMINATION/GALAXY/') \n    from glob import glob\n    print starcat\n    for type,cat in [['star',starcat]]: #,['galaxy',galaxycat]]:\n        catalog = search_params['pasted_cat'] #exposures[exposure]['pasted_cat']\n        ramin,ramax, decmin, decmax = coordinate_limits(catalog)    \n        limits = {'ramin':ramin-0.2,'ramax':ramax+0.2,'decmin':decmin-0.2,'decmax':decmax+0.2}\n        print ramin,ramax, decmin, decmax\n        if len(glob(cat)) == 0:                                      \n            #os.system('rm ' + cat)\n            image = search_params['files'][0]\n            print image\n            import retrieve_test\n            retrieve_test.run(image,cat,type,limits)\n    return starcat\n\ndef match_PPRUN(OBJNAME=None,FILTER=None,PPRUN=None):\n\n    associate = {'W-S-I+':['W-C-IC','W-C-RC'],'W-S-G+':['W-J-V','W-J-B'],'W-J-U':['W-J-B']}\n\n    if OBJNAME is None: \n        batchmode = True\n    else: batchmode = False\n    \n    trial = False \n    \n    import MySQLdb, sys, os, re, time, utilities, pyfits\n    from copy import copy\n    db2 = MySQLdb.connect(db='subaru', user='weaklensing', passwd='darkmatter', host='ki-rh8')\n    c = db2.cursor()\n    db_keys = describe_db(c,['illumination_db','fit_db'])\n\n    keystop = ['PPRUN','ROTATION','OBJNAME']\n    list = reduce(lambda x,y: x + ',' + y, keystop)\n\n    if OBJNAME is None:\n        command=\"SELECT * from illumination_db where zp_star_ is not null and PPRUN='2002-06-04_W-J-V' and OBJECT='MACSJ1423.8' GROUP BY OBJNAME,ROTATION\"       \n        #command=\"SELECT * from illumination_db where OBJNAME like '%2243%' and filter='W-J-V' GROUP BY OBJNAME,pprun,filter \"\n        #command=\"SELECT * from illumination_db where file not like '%CALIB%' and OBJECT like '%1423%' GROUP BY OBJNAME,pprun,filter\"\n        #command=\"SELECT * from illumination_db where file not like '%CALIB%' GROUP BY OBJNAME,pprun,filter\"\n        command=\"SELECT * from illumination_db where file not like '%CALIB%' GROUP BY OBJNAME,pprun,filter\"\n        command=\"SELECT * from illumination_db where file not like '%CALIB%' and SUPA not like '%I' and OBJNAME='MACS1423+24' and filter='W-J-V' GROUP BY OBJNAME,pprun,filter\"\n        #command=\"SELECT * from illumination_db where file not like '%CALIB%' and SUPA not like '%I' and OBJNAME like 'MACS1824%' and filter='W-C-IC' and PPRUN !='KEY_N/A' GROUP BY pprun,filter, OBJNAME\" # ORDER BY RAND()\" # and PPRUN='2006-12-21_W-J-B' GROUP BY OBJNAME,pprun,filter\"\n        command=\"SELECT * from illumination_db where  file not like '%CALIB%' and SUPA not like '%I' and PPRUN !='KEY_N/A' and fixradecCR=1 and PPRUN='2007-07-18_W-J-B' and OBJNAME='MACS2211-03' GROUP BY pprun,filter, OBJNAME ORDER BY RAND()\" # and PPRUN='2006-12-21_W-J-B' GROUP BY OBJNAME,pprun,filter\"\n        command=\"SELECT * from illumination_db where  file not like '%CALIB%' and SUPA not like '%I' and PPRUN !='KEY_N/A' and fixradecCR=1 and OBJNAME like 'MACS1423%'  GROUP BY pprun,filter,OBJNAME\" # ORDER BY RAND()\" # and PPRUN='2006-12-21_W-J-B' GROUP BY OBJNAME,pprun,filter\"\n        #command=\"SELECT * from illumination_db i left join fit_db f on (i.pprun=f.pprun and i.OBJNAME=f.OBJNAME) where i.file not like '%CALIB%' and i.SUPA not like '%I' and i.PPRUN !='KEY_N/A' and i.fixradecCR=1 and i.OBJNAME like 'MACS1423%'and f.linearfit=0 GROUP BY i.pprun,i.filter,i.OBJNAME\" # ORDER BY RAND()\" # and PPRUN='2006-12-21_W-J-B' GROUP BY OBJNAME,pprun,filter\"\n        command=\"SELECT * from illumination_db i left join fit_db f on (i.pprun=f.pprun and i.OBJNAME=f.OBJNAME) where i.file not like '%CALIB%' and i.SUPA not like '%I' and i.PPRUN !='KEY_N/A' and i.fixradecCR=1 and i.OBJNAME like 'Zw3146%' and i.filter='W-J-V' GROUP BY i.pprun,i.filter,i.OBJNAME\" # ORDER BY RAND()\" # and PPRUN='2006-12-21_W-J-B' GROUP BY OBJNAME,pprun,filter\"\n        command=\"SELECT * from illumination_db i left join fit_db f on (i.pprun=f.pprun and i.OBJNAME=f.OBJNAME) where i.file not like '%CALIB%' and i.SUPA not like '%I' and i.PPRUN !='KEY_N/A' and  i.OBJNAME like '%MACS0850%' GROUP BY i.OBJNAME limit 1\" # and PPRUN='2006-12-21_W-J-B' GROUP BY OBJNAME,pprun,filter\"\n        #command=\"SELECT * from illumination_db i where OBJNAME like '%MACS0850%' GROUP BY i.OBJNAME ORDER BY RAND()\" # and PPRUN='2006-12-21_W-J-B' GROUP BY OBJNAME,pprun,filter\"\n        #command=\"SELECT * from (illumination_db i left join fit_db f on (i.pprun=f.pprun and i.OBJNAME=f.OBJNAME)) left join sdss_db s on (s.OBJNAME = i.OBJNAME) where i.file not like '%CALIB%' and i.SUPA not like '%I' and i.PPRUN !='KEY_N/A' and  f.linearfit is null GROUP BY i.pprun,i.filter,i.OBJNAME ORDER BY RAND()\" # and PPRUN='2006-12-21_W-J-B' GROUP BY OBJNAME,pprun,filter\"\n        #command=\"SELECT * from illumination_db i left join fit_db f on (i.pprun=f.pprun and i.OBJNAME=f.OBJNAME) where i.file not like '%CALIB%' and i.SUPA not like '%I' and i.PPRUN !='KEY_N/A' and i.fixradecCR=1 and f.linearfit is null and i.PPRUN='2002-12-03_W-C-RC' GROUP BY i.pprun,i.filter,i.OBJNAME\" # and PPRUN='2006-12-21_W-J-B' GROUP BY OBJNAME,pprun,filter\"\n    else:\n        #command=\"SELECT * from illumination_db i left join fit_db f on (i.pprun=f.pprun and i.OBJNAME=f.OBJNAME) where i.file not like '%CALIB%' and i.SUPA not like '%I' and i.objname='\"+OBJNAME+\"' and i.pprun='\"+PPRUN+\"' and i.filter='\" + FILTER + \"' GROUP BY i.pprun,i.filter,i.OBJNAME ORDER BY RAND()\" # and PPRUN='2006-12-21_W-J-B' GROUP BY OBJNAME,pprun,filter\"\n        command=\"SELECT * from illumination_db where  file not like '%CALIB%' and  PPRUN !='KEY_N/A'  and OBJNAME like '\" + OBJNAME + \"' and FILTER like '\" + FILTER + \"' and PPRUN='\" + PPRUN + \"' GROUP BY pprun,filter,OBJNAME\" # ORDER BY RAND()\" # and PPRUN='2006-12-21_W-J-B' GROUP BY OBJNAME,pprun,filter\"\n\n    print command\n    c.execute(command)\n    results=c.fetchall()\n    for line in results: \n        'start next'\n        if 1: #try: \n            dtop = {}  \n            for i in range(len(db_keys)):\n                dtop[db_keys[i]] = str(line[i])\n            res = re.split('\\/',dtop['file'])\n\n            command=\"SELECT * from illumination_db i left join fit_db f on (i.pprun=f.pprun and i.OBJNAME=f.OBJNAME) where i.OBJNAME='\" + dtop['OBJNAME'] + \"' and i.pasted_cat is not NULL\"\n            print command\n            c.execute(command)\n            results2=c.fetchall()\n            rotation_runs = {} \n            for line in results2: \n                dict = {}  \n                for i in range(len(db_keys)):\n                    dict[db_keys[i]] = str(line[i])\n                GID = float(dict['GABODSID'])\n                config_list = [[575,691,'8'],[691,871,'9'],[817,1309,'10_1'],[1309,3470,'10_2'],[3470,4000,'10_3']]\n                CONFIG_IM = None\n                for config in config_list:\n                    if config[0] < GID < config[1]:\n                        CONFIG_IM = config[2]\n                        break\n                if float(dict['EXPTIME']) > 10.0:\n                    if not dict['PPRUN'] in rotation_runs:                                                                                                                               \n                        rotation_runs[dict['PPRUN']] = {'ROTATION':{dict['ROTATION']:'yes'},'FILTER':dict['filter'],'CONFIG_IM':CONFIG_IM,'EXPTIME':dict['EXPTIME'],'file':dict['file'],'linearfit':dict['linearfit'],'OBJNAME':dict['OBJNAME'],'catalog':dict['catalog']}\n                    rotation_runs[dict['PPRUN']]['ROTATION'][dict['ROTATION']] = 'yes'\n\n            print rotation_runs\n\n            help_list = {} \n            good_list = {}\n            for y in rotation_runs.keys():\n                print rotation_runs[y]['CONFIG_IM']\n                if rotation_runs[y]['CONFIG_IM'] != '8' and  rotation_runs[y]['CONFIG_IM'] != '9' and  rotation_runs[y]['CONFIG_IM'] != '10_3' and len(rotation_runs[y]['ROTATION'].keys()) > 1:\n                    good_list[y] = rotation_runs[y]\n                else:\n                    help_list[y] = rotation_runs[y]\n\n            orphan_list = {}\n            matched_list = {}\n\n            for y in help_list.keys():\n                matched = False\n                for x in good_list.keys():\n                    if help_list[y]['FILTER'] == good_list[x]['FILTER']:\n                        matched_list[y] = help_list[y]\n                        matched = True\n                        break \n                if matched == False:\n                    orphan_list[y] = help_list[y]\n\n            print good_list\n            print help_list\n            print 'good' \n            for key in sorted(good_list.keys()): print key, good_list[key]['EXPTIME'], good_list[key]['file']\n            print 'help' \n            for key in sorted(help_list.keys()): print key, help_list[key]['EXPTIME'],help_list[key]['file']\n            print 'matched'\n            for key in sorted(matched_list.keys()): print key, matched_list[key]['EXPTIME'],matched_list[key]['file']\n            print 'orphaned'\n            for key in sorted(orphan_list.keys()): print key, orphan_list[key]['EXPTIME'],orphan_list[key]['file']\n\n            ''' first run the good images '''\n\n            for run in good_list.keys():\n                if float(good_list[run]['linearfit']) != 1:\n                    print good_list[run]['linearfit']\n                    match_OBJNAME_specific(good_list[run]['OBJNAME'],goodlist[run]['FILTER'],goodlist[run]['PPRUN'])\n\n            ''' create a master catalog '''\n            input = [[good_list[x]['catalog'],good_list[x]['FILTER']] for x in good_list.keys()]    \n            print input\n\n            #mk_sdss_like_catalog(input)\n\n            match_many_multi_band(input)\n\n            ''' use the master catalog to fix remaining runs '''\n\n            ## need to figure out which band/color to use\n   \n\n\nclass TryDb(Exception):\n    def __init__(self,value):\n        self.value=value            \n    def __str__(self):        \n        return repr(self.value)      \n            \n\n\ndef match_OBJNAME(OBJNAME=None,FILTER=None,PPRUN=None):\n\n    if OBJNAME is None: \n        batchmode = True\n    else: batchmode = False\n\n    import os\n    ppid = str(os.getppid())\n  \n    trial = True \n    #trial = False\n    if __name__ == '__main__': \n        trial = False \n\n    start = 1\n    loop = False\n    go = True\n\n    while go: \n        import MySQLdb, sys, os, re, time, utilities, pyfits                                       \n        from copy import copy\n        db2 = MySQLdb.connect(db='subaru', user='weaklensing', passwd='darkmatter', host='ki-rh8')\n        c = db2.cursor()\n        db_keys_f = describe_db(c,['illumination_db'])\n                                                                                                   \n        keystop = ['PPRUN','ROTATION','OBJNAME']\n        list = reduce(lambda x,y: x + ',' + y, keystop)\n\n\n        if loop or OBJNAME is None: # or start == 0:\n            loop=True\n            command=\"SELECT * from illumination_db where zp_star_ is not null and PPRUN='2002-06-04_W-J-V' and OBJECT='MACSJ1423.8' GROUP BY OBJNAME,ROTATION\"       \n            #command=\"SELECT * from illumination_db where OBJNAME like '%2243%' and filter='W-J-V' GROUP BY OBJNAME,pprun,filter \"\n            #command=\"SELECT * from illumination_db where file not like '%CALIB%' and OBJECT like '%1423%' GROUP BY OBJNAME,pprun,filter\"\n            #command=\"SELECT * from illumination_db where file not like '%CALIB%' GROUP BY OBJNAME,pprun,filter\"\n            command=\"SELECT * from illumination_db where file not like '%CALIB%' GROUP BY OBJNAME,pprun,filter\"\n            command=\"SELECT * from illumination_db where file not like '%CALIB%' and SUPA not like '%I' and OBJNAME='MACS1423+24' and filter='W-J-V' GROUP BY OBJNAME,pprun,filter\"\n            #command=\"SELECT * from illumination_db where file not like '%CALIB%' and SUPA not like '%I' and OBJNAME like 'MACS1824%' and filter='W-C-IC' and PPRUN !='KEY_N/A' GROUP BY pprun,filter, OBJNAME\" # ORDER BY RAND()\" # and PPRUN='2006-12-21_W-J-B' GROUP BY OBJNAME,pprun,filter\"\n            command=\"SELECT * from illumination_db where  file not like '%CALIB%' and SUPA not like '%I' and PPRUN !='KEY_N/A' and fixradecCR=1 and PPRUN='2007-07-18_W-J-B' and OBJNAME='MACS2211-03' GROUP BY pprun,filter, OBJNAME ORDER BY RAND()\" # and PPRUN='2006-12-21_W-J-B' GROUP BY OBJNAME,pprun,filter\"\n            command=\"SELECT * from illumination_db where  file not like '%CALIB%' and SUPA not like '%I' and PPRUN !='KEY_N/A' and fixradecCR=1 and OBJNAME like 'MACS1423%'  GROUP BY pprun,filter,OBJNAME\" # ORDER BY RAND()\" # and PPRUN='2006-12-21_W-J-B' GROUP BY OBJNAME,pprun,filter\"\n            #command=\"SELECT * from illumination_db i left join fit_db f on (i.pprun=f.pprun and i.OBJNAME=f.OBJNAME) where i.file not like '%CALIB%' and i.SUPA not like '%I' and i.PPRUN !='KEY_N/A' and i.fixradecCR=1 and i.OBJNAME like 'MACS1423%'and f.linearfit=0 GROUP BY i.pprun,i.filter,i.OBJNAME\" # ORDER BY RAND()\" # and PPRUN='2006-12-21_W-J-B' GROUP BY OBJNAME,pprun,filter\"\n            command=\"SELECT * from illumination_db i left join fit_db f on (i.pprun=f.pprun and i.OBJNAME=f.OBJNAME) where i.file not like '%CALIB%' and i.SUPA not like '%I' and i.PPRUN !='KEY_N/A' and i.fixradecCR=1 and i.OBJNAME like 'Zw3146%' and i.filter='W-J-V' GROUP BY i.pprun,i.filter,i.OBJNAME\" # ORDER BY RAND()\" # and PPRUN='2006-12-21_W-J-B' GROUP BY OBJNAME,pprun,filter\"\n            command=\"SELECT * from illumination_db i left join fit_db f on (i.pprun=f.pprun and i.OBJNAME=f.OBJNAME) where i.file not like '%CALIB%' and i.SUPA not like '%I' and i.PPRUN !='KEY_N/A' and  (f.linearfit!=1 or f.linearfit is null) GROUP BY i.pprun,i.filter,i.OBJNAME ORDER BY RAND() \" # and PPRUN='2006-12-21_W-J-B' GROUP BY OBJNAME,pprun,filter\"\n\n            command=\"  select i.* from temp i left join fit_db f on (i.pprun=f.pprun and i.OBJNAME=f.OBJNAME) where i.PPRUN !='KEY_N/A' and i.file not like '%CALIB%' and i.pprun like '%' and (f.linearfit=1) and f.piggyback is null GROUP BY i.pprun,i.filter,i.OBJNAME ORDER BY RAND() \" # and PPRUN='2006-12-21_W-J-B' GROUP BY OBJNAME,pprun,filter\"\n\n            command=\" drop temporary table if exists temp ; create temporary table temp as select * from illumination_db group by objname, pprun; select i.* from temp i left join fit_db f on (i.pprun=f.pprun and i.OBJNAME=f.OBJNAME) where i.OBJNAME not like 'SXDS' and i.pasted_cat is not null GROUP BY i.pprun,i.filter,i.OBJNAME ORDER BY RAND() limit 1 \" # and PPRUN='2006-12-21_W-J-B' GROUP BY OBJNAME,pprun,filter\"\n\n            list = ['MACS0018%','MACS0025%','MACS0257%','MACS0454%','MACS0647%','MACS0717%','MACS0744%','MACS0911%','MACS1149%','MACS1423%','MACS2129%','MACS2214%','MACS2243%','A2219','A2390']\n            formatted = reduce(lambda x,y: x + ' or ' + y,['i.objname like \"' + x + '\"' for x in list])\n            print formatted\n            formatted = \"i.objname like '%'\"\n            print formatted\n\n            if False:\n                command=\" drop temporary table if exists temp  \"     \n                print command\n                c.execute(command)\n\n                command = \"create temporary table temp as select i.* from illumination_db  i left join try_db t on (i.pprun=t.pprun and i.OBJNAME=t.OBJNAME) where (t.sdssstatus!='finished' and t.Nonestatus!='finished') and (t.sdssstatus!='started' and t.Nonestatus!='started') and  (t.sdssstatus!='fitfinished' and t.Nonestatus!='fitfinished') and (\" + formatted + \") group by i.objname, i.pprun\"\n                #command = \" select * from try_db t where (t.sdssstatus='none' and t.Nonestatus='none') and (\" + formatted + \") group by t.objname, t.pprun  order by RAND()\"\n                print command\n                c.execute(command)\n                #command = \"create temporary table temp as select i.* from illumination_db  i left join try_db t on (i.pprun=t.pprun and i.OBJNAME=t.OBJNAME) where (t.sdssstatus='none' and t.Nonestatus='none') and (\" + formatted + \") group by i.objname, i.pprun  \"\n                print command\n                command = \"select i.* from temp i left join fit_db f on (i.pprun=f.pprun and i.OBJNAME=f.OBJNAME) where i.OBJNAME not like 'SXDS' and i.pasted_cat is not null and i.pprun is not null  GROUP BY i.pprun,i.filter,i.OBJNAME ORDER BY RAND() limit 1 \" # and PPRUN='2006-12-21_W-J-B' GROUP BY OBJNAME,pprun,filter\"\n                c.execute(command)\n\n            if True:\n                try_db_keys = describe_db(c,['try_db'])       \n                ill_db_keys = describe_db(c,['illumination_db']) \n                \n                command = \"select * from try_db t where (t.sdssstatus!='finished' and t.Nonestatus!='finished') and (t.sdssstatus!='started' and t.Nonestatus!='started') and  (t.sdssstatus!='fitfinished' and t.Nonestatus!='fitfinished') and (t.sdssstatus!='failed' and t.Nonestatus!='failed') group by t.objname, t.pprun order by rand()\"\n                \n                command = \"select * from try_db where sdssstatus is null and Nonestatus is null and bootstrapstatus is null and config=8 order by rand()\"\n\n                #command = \"select * from try_db where (Nonestatus is null and sdssstatus is null) and OBJNAME!='COSMOS' order by rand()\"\n\n                #command = \"select * from try_db where format='bad' and sdssstatus='NULL' and Nonestatus='NULL' order by rand()\"\n\n                #command = 'select * from try_db i where (i.sdssstatus is null and i.Nonestatus is null) and (i.bootstrapstatus is null) and (i.objname like \"MACS0018%\" or i.objname like \"MACS0025%\" or i.objname like \"MACS0257%\" or i.objname like \"MACS0454%\" or i.objname like \"MACS0647%\" or i.objname like \"MACS0717%\" or i.objname like \"MACS0744%\" or i.objname like \"MACS0911%\" or i.objname like \"MACS1149%\" or i.objname like \"MACS1423%\" or i.objname like \"MACS2129%\" or i.objname like \"MACS2214%\" or i.objname like \"MACS2243%\" or i.objname like \"A2219\" or i.objname like \"A2390\") order by rand()'\n\n                print command\n\n                c.execute(command)\n                results=c.fetchall()\n                ds = []\n               \n                if len(results) > 0: \n                    for line in results:                                                                                                                                                                                                                                                                       \n                        dtop = {}\n                        for i in range(len(try_db_keys)):\n                            dtop[try_db_keys[i]] = str(line[i])\n                        command = \"select i.* from illumination_db i where i.OBJNAME='\" + dtop['OBJNAME'] + \"' and i.PPRUN = '\" + dtop['PPRUN'] + \"' and i.pasted_cat is not null  GROUP BY i.pprun,i.filter,i.OBJNAME ORDER BY RAND() limit 1 \" # and PPRUN='2006-12-21_W-J-B' GROUP BY OBJNAME,pprun,filter\"\n                        c.execute(command)\n                        results_b=c.fetchall()\n                        print results_b\n                        if len(results_b) > 0:\n                            for i in range(len(ill_db_keys)):\n                                dtop[ill_db_keys[i]] = str(results_b[0][i])\n                            print dtop['PPRUN'], dtop['OBJNAME']\n                            break\n                    else:                     \n                        dtop= {}\n\n\n#            command = \"select i.* from temp i left join fit_db f on (i.pprun=f.pprun and i.OBJNAME=f.OBJNAME) where f.correction_applied is null and f.sample_size is null and i.PPRUN !='KEY_N/A' and i.file not like '%CALIB%' and i.pprun like '%' and i.OBJNAME not like 'SXDS' and i.pasted_cat is not null and i.pprun is not null and (i.OBJNAME like 'MACS0018%' or i.OBJNAME like 'MACS1423%' or i.OBJNAME like 'MACS2129%' or i.OBJNAME like 'MACS0454%' or i.OBJNAME like 'MACS0717%' or i.OBJNAME like 'MACS1149%') GROUP BY i.pprun,i.filter,i.OBJNAME ORDER BY RAND() limit 1 \" # and PPRUN='2006-12-21_W-J-B' GROUP BY OBJNAME,pprun,filter\"\n            #command=\"SELECT * from fit_db where (linearfit!=1 or linearfit is null) GROUP BY pprun,filter,OBJNAME ORDER BY RAND() limit 1 \" # and PPRUN='2006-12-21_W-J-B' GROUP BY OBJNAME,pprun,filter\"\n            #command=\"SELECT * from (illumination_db i left join fit_db f on (i.pprun=f.pprun and i.OBJNAME=f.OBJNAME)) left join sdss_db s on (s.OBJNAME = i.OBJNAME) where i.file not like '%CALIB%' and i.SUPA not like '%I' and i.PPRUN !='KEY_N/A' and  f.linearfit is null GROUP BY i.pprun,i.filter,i.OBJNAME ORDER BY RAND()\" # and PPRUN='2006-12-21_W-J-B' GROUP BY OBJNAME,pprun,filter\"\n            #command=\"SELECT * from illumination_db i left join fit_db f on (i.pprun=f.pprun and i.OBJNAME=f.OBJNAME) where i.file not like '%CALIB%' and i.SUPA not like '%I' and i.PPRUN !='KEY_N/A' and i.fixradecCR=1 and f.linearfit is null and i.PPRUN='2002-12-03_W-C-RC' GROUP BY i.pprun,i.filter,i.OBJNAME\" # and PPRUN='2006-12-21_W-J-B' GROUP BY OBJNAME,pprun,filter\"\n        else:\n            go = False\n            command=\" drop temporary table if exists temp  \"\n            c.execute(command)\n            command = \"create temporary table temp as select * from illumination_db group by objname, pprun \"\n            print command\n            c.execute(command)\n            command=\"SELECT * from temp i left join fit_db f on (i.pprun=f.pprun and i.OBJNAME=f.OBJNAME) where i.SUPA not like '%I' and i.objname='\"+OBJNAME+\"' and i.pprun='\"+PPRUN+\"' and i.filter='\" + FILTER + \"' GROUP BY i.pprun,i.filter,i.OBJNAME ORDER BY RAND() \" # and PPRUN='2006-12-21_W-J-B' GROUP BY OBJNAME,pprun,filter\"\n\n            #command=\"SELECT * from fit_db where (linearfit!=1 or linearfit is null) and objname='\"+OBJNAME+\"' and pprun='\"+PPRUN+\"' and filter='\" + FILTER + \"' GROUP BY pprun,filter,OBJNAME ORDER BY RAND() limit 1 \" # and PPRUN='2006-12-21_W-J-B' GROUP BY OBJNAME,pprun,filter\"\n\n            #command=\"SELECT * from fit_db where objname='\"+OBJNAME+\"' and pprun='\"+PPRUN+\"' and filter='\" + FILTER + \"' GROUP BY pprun,filter,OBJNAME ORDER BY RAND() limit 1 \" # and PPRUN='2006-12-21_W-J-B' GROUP BY OBJNAME,pprun,filter\"\n        #    command=\"SELECT * from illumination_db where  file not like '%CALIB%' and  PPRUN !='KEY_N/A'  and OBJNAME like '\" + OBJNAME + \"' and FILTER like '\" + FILTER + \"' and PPRUN='\" + PPRUN + \"' GROUP BY pprun,filter,OBJNAME\" # ORDER BY RAND()\" # and PPRUN='2006-12-21_W-J-B' GROUP BY OBJNAME,pprun,filter\"\n\n            start = 0                                                                                                                                                                             \n            print command\n            c.execute(command)\n            results=c.fetchall()\n            #for line in results[0]:\n            print len(results)\n            print results[0]\n            print 'len results', len(results)\n            ppid = str(os.getppid())\n                                                                                                                                                                                                  \n            print 'hey'\n            if len(results) == 0: \n            \n                print 'breaking!'\n                break\n                                                                                                                                                                                                  \n                                                                                                                                                                                                  \n            if len(results) > 0: \n                print 'start next'\n                line = results[0]\n                #print 'calc_test_save.linear_fit(' + OBJNAME + ',' + FILTER + ',' + PPRUN + ',' + cov + ',' + CONFIG + ',' + true_sdss + ',primary=' + primary + ',secondary=' + secondary + ')'\n                #if trial: raw_input()\n                global tmpdir\n                dtop = {}  \n                for i in range(len(db_keys_f)):\n                    dtop[db_keys_f[i]] = str(line[i])\n\n        if len(results) ==0: return\n        if len(results) > 0:            \n            #res = re.split('\\/',dtop['file'])\n            #for j in range(len(res)):\n            #    if res[j] == 'SUBARU':\n            #        break\n            FILTER = dtop['filter']\n            PPRUN = dtop['PPRUN']\n            OBJNAME = dtop['OBJNAME']\n\n            save_fit({'PPRUN':PPRUN,'OBJNAME':OBJNAME,'FILTER':FILTER,'sample':'record','sample_size':'record'},db='try_db')\n\n            path='/nfs/slac/g/ki/ki05/anja/SUBARU/%(OBJNAME)s/' % {'OBJNAME':OBJNAME}\n            illum_dir = path + 'PHOTOMETRY/ILLUMINATION/' + FILTER + '/' + PPRUN + '/'\n            os.system('mkdir -p ' + illum_dir)\n            logfile  = open(illum_dir + 'logfile','w')\n            print illum_dir + 'logfile'\n\n            import time                                                                                                                                                                                                                                         \n            print dtop['filter'], dtop['PPRUN'], dtop['OBJNAME']                                                                                                                                       \n                                                                                                                                                                                                       \n            keys = ['SUPA','OBJNAME','ROTATION','PPRUN','pasted_cat','filter','ROTATION','files']                                                                                                      \n            list = reduce(lambda x,y: x + ',' + y, keys)                                                                                                                                               \n\n            db_keys = describe_db(c,['try_db'])                                                                                                                                      \n            #command=\"SELECT * from illumination_db where  OBJNAME='\" + dtop['OBJNAME'] + \"' and PPRUN='\" + dtop['PPRUN'] + \"' and filter like '\" + dtop['filter'] + \"' and pasted_cat is not NULL\"    \n            command=\"SELECT * from try_db f  where f.OBJNAME='\" + dtop['OBJNAME'] + \"' and f.PPRUN='\" + dtop['PPRUN'] + \"' limit 1\" \n            print command                                                  \n            c.execute(command)                                             \n            results2=c.fetchall()                                           \n            #sort_results(results,db_keys)\n\n            for line in results2: \n                dict_temp = {}  \n                for i in range(len(db_keys)):\n                    dict_temp[db_keys[i]] = str(line[i])\n\n            primary = dict_temp['primary_filt']                           \n            primary_catalog = dict_temp['primary_catalog']\n            secondary = dict_temp['secondary_filt']                       \n            secondary_catalog = dict_temp['secondary_catalog']\n            match = dict_temp['todo']\n            print match\n            #match='sdss' \n\n            print '#########!!!!!!!!!!'\n            print primary, secondary                                      \n                                                                                                                                                                                                                                                 \n            ''' now run with PPRUN '''                                   \n            command=\"SELECT * from illumination_db i left join fit_db f on (i.pprun=f.pprun and i.OBJNAME=f.OBJNAME) where i.OBJNAME='\" + dtop['OBJNAME'] + \"' and i.pasted_cat is not NULL and i.PPRUN='\" + dtop['PPRUN'] + \"' and badccd!=1 group by i.supa\"    \n\n            db_keys = describe_db(c,['illumination_db'])                                                                                                                                      \n            print command                                               \n            c.execute(command)                                          \n            results=c.fetchall()                                        \n            print len(results)                                          \n            #raw_input()\n                                                                       \n            field = []                                                 \n            info = []                                                  \n            if len(results) > 0: #  and (len(results_try) == 0 or trial):                                                                                                                                                                              \n                ''' only redirect stdout if actually running on a pprun ''' \n                if not trial:\n                    import sys               \n                    stderr_orig = sys.stderr\n                    stdout_orig = sys.stdout\n                    sys.stdout = logfile \n                    sys.stderr = logfile \n                try: \n\n                    for line in results:         \n                        d = {}\n                        for i in range(len(db_keys)):\n                            d[db_keys[i]] = str(line[i])\n                        ana = '' #raw_input('analyze ' + d['SUPA'] + '?')\n                        if len(ana) > 0:\n                            if ana[0] == 'y':\n                                analyze(d['SUPA'],d['FLAT_TYPE'])\n                        ''' use SCAMP CRVAL, etc. ''' \n\n                        a=1 \n                   \n                        print d['CHIPS'], d['fixradecCR'] \n                        if str(d['CHIPS'])=='None' or str(d['fixradecCR']) != str(1.0): # or str(d['fixradecCR']) == '-1':\n                            a = fix_radec(d['SUPA'],d['FLAT_TYPE'])\n\n                        if a==1:\n                            key = str(int(float(d['ROTATION']))) + '$' + d['SUPA'] + '$' \n                            field.append({'key':key,'pasted_cat':d['pasted_cat'],'ROT':d['ROTATION'],'file':d['file']})\n                            info.append([d['ROTATION'],d['SUPA'],d['OBJNAME']])\n                            print d['file']\n                        if d['CRVAL1'] == 'None':\n                            length(d['SUPA'],d['FLAT_TYPE'])\n                        print d['SUPA']\n\n\n                    #print all_list[PPRUN]\n\n                    if match == 'bootstrap': #all_list[PPRUN]['status']=='help' and all_list[PPRUN]['primary'] is not None: \n                        print 'primary', primary, 'secondary', secondary \n                        ''' match images '''\n                        finalcat = match_many_multi_band([[dict_temp['primary_catalog'],'primary'],[dict_temp['secondary_catalog'],'secondary']])\n                        print finalcat\n                        #save_fit({'status':all_list[PPRUN]['status'],'PPRUN':PPRUN,'OBJNAME':OBJNAME,'FILTER':FILTER})\n                    else:                                                                                                                                                                                              \n                        ''' now check to see if there is SDSS '''\n                        sdss_cov,galaxycat,starcat = sdss_coverage(d['SUPA'],d['FLAT_TYPE']) \n                        ''' get SDSS matched stars, use photometric calibration to remove color term ''' \n                        if sdss_cov: \n                            match = 'sdss'\n                            print d['SUPA'], d['FLAT_TYPE'], d['OBJECT'], d['CRVAL1'], d['CRVAL2'] \n                            ''' retrieve SDSS catalog '''\n                            print d['pasted_cat']\n                            sdssmatch = get_cats_ready(d['SUPA'],d['FLAT_TYPE'],galaxycat,starcat)\n                            print 'calibration done'\n                        else: \n                            match=None\n\n\n\n                    #save_fit({'PPRUN':PPRUN,'OBJNAME':OBJNAME,'FILTER':FILTER,'logfile':illum_dir+'logfile','sample':'record','sample_size':'record','status':'started','time':str(time.localtime())},db='try_db')\n\n\n\n                    print match\n                    d = get_files(d['SUPA'],d['FLAT_TYPE'])\n                    print field\n                    input = [[x['pasted_cat'],x['key'],x['ROT']] for x in field]    \n\n                    input_files = [[x['pasted_cat']] for x in field]    \n                    print input_files\n\n                    import utilities \n      \n                    input_filt = [] \n                    print input\n                    for f in input: \n                        ''' may need fainter objects for bootstrap '''\n                        if match=='bootstrap':\n                            Ns = ['MAGERR_AUTO < 0.1)','Flag = 0)']                                                                       \n                        else:\n                            Ns = ['MAGERR_AUTO < 0.05)','Flag = 0)']                                                                       \n                        filt= '(' + reduce(lambda x,y: '(' + x + '  AND (' + y + ')',Ns)\n                        print filt, f\n                        filtered = f[0].replace('.cat','.filt.cat')\n                        print filtered\n                        command = 'ldacfilter -i ' + f[0] + ' -t OBJECTS -o ' + filtered + ' -c \"' + filt + ';\" '\n                        print command\n                        import utilities\n                        utilities.run(command,[filtered])\n                        input_filt.append([filtered,f[1],f[2]])\n\n                    if 0: #len(input) > 8: \n                        input_short = []\n                        i = 0\n                        while len(input_short) < 6 and len(input_short)<len(input):\n                            i += 1\n                            rot0 = filter(lambda x:float(x[1][0])==0,input)[0:i] \n                            rot1 = filter(lambda x:float(x[1][0])==1,input)[0:i]\n                            rot2 = filter(lambda x:float(x[1][0])==2,input)[0:i]\n                            rot3 = filter(lambda x:float(x[1][0])==2,input)[0:i]\n                            input_short = rot0 + rot1 + rot2 + rot3\n                        input = input_short\n                        print 'new', input\n                    print input\n                    input = input_filt\n                    print input_filt\n\n\n                    if match=='sdss':\n                        input.append([sdssmatch,'SDSS',None])\n                    elif match=='bootstrap':\n                        input.append([finalcat,'SDSS',None])\n\n                    if len(input) < 3: \n                        raise TryDb('too few images') \n\n\n                    print input\n\n                    match_many(input)                                                                   \n\n\n                    start_EXPS = getTableInfo()   \n                    print start_EXPS\n    \n                    dt = get_files(start_EXPS[start_EXPS.keys()[0]][0])\n                    import re\n                    CHIPS = [int(x) for x in re.split(',',dt['CHIPS'])]\n                    LENGTH1, LENGTH2 = dt['LENGTH1'], dt['LENGTH2']\n                    EXPS, star_good,supas, totalstars, mdn_background = selectGoodStars(start_EXPS,match,LENGTH1,LENGTH2)               \n                    info = starStats(supas)\n                    print info\n                    print match\n                    print mdn_background, len(supas)\n\n                    if len(supas) < 300 and mdn_background > 26000: \n                        raise TryDb('high background:'+ str(mdn_background)) \n\n\n\n                    print start_EXPS\n                    print start_EXPS.keys()\n\n                    print EXPS\n                    print EXPS.keys()\n\n                    start_ims = (reduce(lambda x,y: x + y, [len(start_EXPS[x]) for x in start_EXPS.keys()]))\n                    final_ims = (reduce(lambda x,y: x + y, [len(EXPS[x]) for x in EXPS.keys()]))\n\n                    print 'start_ims', start_ims\n                    print 'final_ims', final_ims\n                   \n                    if final_ims < 3: \n                        raise TryDb('start:'+str(start_ims)+',end:'+str(final_ims)) \n\n                    print info\n                    print 'match', match\n                    import os\n                    os.system('mkdir -p ' + tmpdir)\n                    uu = open(tmpdir + '/selectGoodStars','w')\n                    import pickle\n                    pickle.dump({'info':info,'EXPS':EXPS,'star_good':star_good,'supas':supas,'totalstars':totalstars},uu)\n                    uu.close()\n\n                    ''' if there are too few matches with SDSS stars, don't use them ''' \n                    if match == 'sdss' and info['match'] < 100:\n                        match = None\n\n                    save_fit({'PPRUN':PPRUN,'OBJNAME':OBJNAME,'FILTER':FILTER,'logfile':illum_dir+'logfile','sample':'record','sample_size':'record',str(match)+'status':'started','time':str(time.localtime())},db='try_db')\n                    print {'PPRUN':PPRUN,'OBJNAME':OBJNAME,'FILTER':FILTER,'logfile':illum_dir+'logfile','sample':'record','sample_size':'record',str(match)+'status':'started','time':str(time.localtime())}\n\n                    if match == 'bootstrap' and info['match'] < 200: \n                        print info['match']\n                        raise TryDb('too few objects/bootstrap:' + str(info['match'])) \n\n                    print match, info\n\n\n                    command=\"SELECT * from fit_db i  where i.OBJNAME='\" + dtop['OBJNAME'] + \"' and (i.sample_size='all' and i.sample='\" + str(match) + \"' and i.positioncolumns is not null) and i.PPRUN='\" + dtop['PPRUN'] + \"'\"        \n                    print command                                              \n                    c.execute(command)                                         \n                    results_try=c.fetchall()                                   \n                    print len(results_try)                                     \n\n                    print OBJNAME,FILTER,PPRUN,match\n\n                    if True: #len(results_try) == 0:\n                        print 'matched'\n                        CONFIG = find_config(d['GABODSID'])\n                        linear_fit(OBJNAME,FILTER,PPRUN,None,match,CONFIG,primary=primary,secondary=secondary)\n                        #save_fit({'PPRUN':PPRUN,'OBJNAME':OBJNAME,'FILTER':FILTER,'logfile':illum_dir+'logfile','sample':'record','sample_size':'record',match+'status':'fitfinished','time':str(time.localtime())},db='try_db')\n                        save_fit({'PPRUN':PPRUN,'OBJNAME':OBJNAME,'FILTER':FILTER,'logfile':illum_dir+'logfile','sample':'record','sample_size':'record',str(match)+'status':'fitfinished','time':str(time.localtime())},db='try_db')\n\n                    print 'done'\n\n                    #construct_correction(d['OBJNAME'],d['filter'],d['PPRUN'],match,'all')\n                    print 'done'\n\n                    #save_fit({'PPRUN':PPRUN,'OBJNAME':OBJNAME,'FILTER':FILTER,'logfile':illum_dir+'logfile','sample':'record','sample_size':'record',str(match)+'status':'finished','time':str(time.localtime())},db='try_db')\n\n\n                    print '\\n\\nDONE'                                                                                                                                                                              \n                    if batchmode:\n                        os.system('rm -rf ' + tmpdir)\n                except KeyboardInterrupt:\n                    raise \n                except TryDb,e:                \n                    print traceback.print_exc(file=sys.stdout)\n                    save_fit({'PPRUN':PPRUN,'OBJNAME':OBJNAME,'FILTER':FILTER,'logfile':illum_dir+'logfile','sample':'record','sample_size':'record',str(match)+'status':'failed','time':str(time.localtime()),'exception':e.value},db='try_db')\n\n                except: \n                    ppid_loc = str(os.getppid())\n                    print traceback.print_exc(file=sys.stdout)\n                    ''' if a child process fails, just exit '''\n                    if ppid_loc != ppid: sys.exit(0) \n                    print 'fail'\n                    print 'trial', trial\n                    \n                    save_fit({'PPRUN':PPRUN,'OBJNAME':OBJNAME,'FILTER':FILTER,'logfile':illum_dir+'logfile','sample':'record','sample_size':'record',str(match)+'status':'failed','time':str(time.localtime()),'exception':'no information'},db='try_db')\n                                                                                                                                                                                                                  \n                    if batchmode:\n                        os.system('rm -rf ' + tmpdir)\n                    if trial:\n                        print 'raising exception'\n                        raise Exception\n                \n                if not trial: \n                    sys.stderr = stderr_orig  \n                    sys.stdout = stdout_orig\n                    logfile.close()\n            \n\n\n\n\n\n\ndef find_config(GID):   \n    config_list = [[575,691,'8'],[691,871,'9'],[817,1309,'10_1'],[1309,3470,'10_2'],[3470,4000,'10_3']]\n    CONFIG_IM = None\n    for config in config_list:\n        if config[0] < GID < config[1]:\n            CONFIG_IM = config[2]\n            \n            break\n    return CONFIG_IM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef add_correction_new(cat_list,OBJNAME,FILTER,PPRUN):\n\n    import scipy, re, string, os\n\n    ''' create chebychev polynomials '''\n    cheby_x = [{'n':'0x','f':lambda x,y:1.},{'n':'1x','f':lambda x,y:x},{'n':'2x','f':lambda x,y:2*x**2-1},{'n':'3x','f':lambda x,y:4*x**3.-3*x}] \n    cheby_y = [{'n':'0y','f':lambda x,y:1.},{'n':'1y','f':lambda x,y:y},{'n':'2y','f':lambda x,y:2*y**2-1},{'n':'3y','f':lambda x,y:4*y**3.-3*y}]\n    cheby_terms = []\n    cheby_terms_no_linear = []\n    for tx in cheby_x:\n        for ty in cheby_y:\n            if not ((tx['n'] == '0x' and ty['n'] == '0y')): # or (tx['n'] == '0x' and ty['n'] == '1y') or (tx['n'] == '1x' and ty['n'] == '0y')) :\n                cheby_terms.append({'n':tx['n'] + ty['n'],'fx':tx['f'],'fy':ty['f']})\n            if not ((tx['n'] == '0x' and ty['n'] == '0y') or (tx['n'] == '0x' and ty['n'] == '1y') or (tx['n'] == '1x' and ty['n'] == '0y')) :\n                cheby_terms_no_linear.append({'n':tx['n'] + ty['n'],'fx':tx['f'],'fy':ty['f']})\n   \n    cov = 1\n\n    if cov:\n        samples = [['sdss',cheby_terms,True]] #,['None',cheby_terms_no_linear,False]] #[['None',cheby_terms_no_linear],['sdss',cheby_terms]]\n    else: \n        samples = [['None',cheby_terms_no_linear,False]]\n\n    sample = 'sdss'\n    sample_size = 'all'\n    import re, time                                                                                                                \n    dt = get_a_file(OBJNAME,FILTER,PPRUN)                \n    d = get_fits(OBJNAME,FILTER,PPRUN)                \n    print d.keys()\n    column_prefix = sample+'$'+sample_size+'$'\n    position_columns_names = re.split('\\,',d[column_prefix + 'positioncolumns']) \n    print position_columns_names, 'position_columns_names'\n    fitvars = {}\n    cheby_terms_dict = {}\n    print column_prefix, position_columns_names\n    for ele in position_columns_names:                      \n        print ele\n        if type(ele) != type({}):\n            ele = {'name':ele}\n        res = re.split('$',ele['name'])\n        if string.find(ele['name'],'zp_image') == -1:\n            fitvars[ele['name']] = float(d[sample+'$'+sample_size+'$'+ele['name']]) \n            for term in cheby_terms:\n                if term['n'] == ele['name'][2:]:\n                    cheby_terms_dict[term['n']] = term \n                                                                                     \n    zp_images = re.split(',',d[sample+'$'+sample_size+'$zp_images'])\n    zp_images_names = re.split(',',d[sample+'$'+sample_size+'$zp_images_names'])\n    \n    for i in range(len(zp_images)):\n        fitvars[zp_images_names[i]] = float(zp_images[i])\n    \n    cheby_terms_use =  [cheby_terms_dict[k] for k in cheby_terms_dict.keys()]\n\n    print cheby_terms_use, fitvars\n\n    CHIPS = [int(x) for x in re.split(',',dt['CHIPS'])]\n    LENGTH1, LENGTH2 = dt['LENGTH1'], dt['LENGTH2']\n\n    per_chip = True\n\n    coord_conv_x = lambda x:(2.*x-0-LENGTH1)/(LENGTH1-0) \n    coord_conv_y = lambda x:(2.*x-0-LENGTH2)/(LENGTH2-0) \n\n    ''' make images of illumination corrections '''                                                                  \n    cat_grads = []\n    for cat in cat_list:\n        \n        import astropy, astropy.io.fits as pyfits\n        p = pyfits.open(cat[0])\n        tab = p[\"OBJECTS\"].data\n        print str(type(tab))\n        if str(type(tab)) != \"<type 'NoneType'>\":\n            print tab.field('MAG_AUTO')[0:10]                          \n                                                                       \n            ROT = str(int(float(cat[2])))\n                                                                       \n            print cat\n            \n                                                    \n            x = coord_conv_x(scipy.array(tab.field('Xpos_ABS')[:]))\n            y = coord_conv_y(scipy.array(tab.field('Ypos_ABS')[:]))\n                                                                       \n            CHIPS = tab.field('CHIP')\n                                                                       \n            chip_zps = []\n            for i in range(len(CHIPS)):\n                chip_zps.append(float(fitvars['zp_' + str(int(CHIPS[i]))]))\n                                                                       \n            chip_zps = scipy.array(chip_zps)\n                                                                       \n            ''' save pattern w/ chip zps '''\n                                                                       \n            trial = False \n            children = []\n            \n            ''' correct w/ polynomial '''\n            epsilonC = 0\n            index = 0                                                  \n            for term in cheby_terms_use:\n                index += 1\n                print index, ROT, term, fitvars[str(ROT)+'$'+term['n']]\n                epsilonC += fitvars[str(ROT)+'$'+term['n']]*term['fx'](x,y)*term['fy'](x,y)\n            ''' add the zeropoint '''\n            epsilonC += chip_zps \n            ''' save pattern w/o chip zps '''\n\n            print LENGTH1, LENGTH2\n            print epsilonC[2000:2020]\n            print x[2000:2020]\n            print y[2000:2020]\n            print tab.field('Xpos_ABS')[2000:2020]\n            print tab.field('Ypos_ABS')[2000:2020]\n\n\n            tab.field('MAG_AUTO')[:] = tab.field('MAG_AUTO')[:] - epsilonC\n            print tab.field('MAG_AUTO')[0:20]\n            new_name = cat[0].replace('.cat','.gradient.cat')\n            os.system('rm ' + new_name)\n            p.writeto(new_name)\n            cat_grads.append([new_name,cat[1],ROT])\n    return cat_grads \n\n\n\n\n    \ndef add_gradient(cat_list):\n    import astropy, astropy.io.fits as pyfits, os\n    cat_grads = []\n    for cat in cat_list:\n        print cat\n        p = pyfits.open(cat[0])\n        tab = p[\"OBJECTS\"].data\n        print tab.field('MAG_AUTO')[0:10] \n        tab.field('MAG_AUTO')[:] = tab.field('MAG_AUTO') + 5./10000.*tab.field('Xpos_ABS')\n        new_name = cat[0].replace('.cat','.gradient.cat')\n        os.system('rm ' + new_name)\n        p.writeto(new_name)\n        cat_grads.append([new_name,cat[1]])\n    return cat_grads \n\ndef add_correction(cat_list):\n    import astropy, astropy.io.fits as pyfits, os\n    cat_grads = []\n    \n    EXPS = getTableInfo()\n\n    cheby_x = [{'n':'0x','f':lambda x,y:1.},{'n':'1x','f':lambda x,y:x},{'n':'2x','f':lambda x,y:2*x**2-1},{'n':'3x','f':lambda x,y:4*x**3.-3*x}] \n\n    cheby_y = [{'n':'0y','f':lambda x,y:1.},{'n':'1y','f':lambda x,y:y},{'n':'2y','f':lambda x,y:2*y**2-1},{'n':'3y','f':lambda x,y:4*y**3.-3*y}]\n\n    #func = lambda x,y: [cheby_x_dict[f[0:2]](x,y)*cheby_y_dict[f[2:]](x,y) for f in fitvars]\n\n    import scipy\n    x = scipy.array([-0.5,0,1])\n    y = scipy.array([-0.5,0,0.5])\n    \n    for cat in cat_list:\n        for ROT in EXPS.keys():\n            for SUPA in EXPS[ROT]:\n                import re\n                print SUPA, cat\n                res = re.split('$',cat[1])\n                file = res[1]\n                print file, cat \n                if file == SUPA: rotation = ROT\n\n        import pickle              \n        f=open(tmpdir + '/fitvars' + rotation,'r')\n        m=pickle.Unpickler(f)\n        fitvars=m.load()\n\n        cheby_terms = []\n        for tx in cheby_x:\n            for ty in cheby_y:\n                if fitvars.has_key(tx['n']+ty['n']): # not ((tx['n'] == '0x' and ty['n'] == '0y')): # or (tx['n'] == '0x' and ty['n'] == '1y') or (tx['n'] == '1x' and ty['n'] == '0y')) :\n                    cheby_terms.append({'n':tx['n'] + ty['n'],'fx':tx['f'],'fy':ty['f']})\n\n        print EXPS\n            \n        print cat\n        p = pyfits.open(cat[0])\n        tab = p[\"OBJECTS\"].data\n        print tab.field('MAG_AUTO')[0:10] \n\n        x = coord_conv_x(tab.field('Xpos_ABS'))\n        y = coord_conv_y(tab.field('Ypos_ABS'))\n\n        epsilon = 0                                                       \n        for term in cheby_terms:\n            epsilon += fitvars[term['n']]*term['fx'](x,y)*term['fy'](x,y)\n\n        print epsilon[0:20]\n        tab.field('MAG_AUTO')[:] = tab.field('MAG_AUTO')[:] - epsilon \n        print tab.field('MAG_AUTO')[0:20]\n        new_name = cat[0].replace('.cat','.gradient.cat')\n        os.system('rm ' + new_name)\n        p.writeto(new_name)\n        cat_grads.append([new_name,cat[1]])\n    return cat_grads \n\ndef make_ssc_config(list):\n\n    ofile = tmpdir + '/tmp.cat'\n    os.system('mkdir ' + tmpdir)\n    out = open(tmpdir + '/tmp.ssc','w')\n    import os, string, re\n\n    keys = []\n    i = -1 \n    for file_name,prefix in list:\n        i += 1                                                                                                                        \n        print file_name\n        os.system('ldacdesc -t OBJECTS -i ' + file_name + ' > ' + ofile)\n        file = open(ofile,'r').readlines()\n        for line in file:\n            if string.find(line,\"Key name\") != -1:\n                red = re.split('\\.+',line)            \n                key = red[1].replace(' ','').replace('\\n','')\n                out_key = prefix + key\n                out.write(\"COL_NAME = \" + out_key + '\\nCOL_INPUT = ' + key + '\\nCOL_MERGE = AVE_REG\\nCOL_CHAN = ' + str(i) + \"\\n#\\n\")\n                #print key\n            keys.append(key)\n\n    out.close()\n\ndef make_ssc_multi_color(list):\n\n    ofile = tmpdir + '/tmp.cat'\n    import os\n    os.system('mkdir -p ' + tmpdir)\n    out = open(tmpdir + '/tmp.ssc','w')\n    import os, string, re\n\n    #key_list = ['CHIP','Flag','MAG_AUTO','MAGERR_AUTO','MAG_APER2','MAGERR_APER2','Xpos_ABS','Ypos_ABS','CLASS_STAR','MaxVal','BackGr','stdMag_corr','stdMagErr_corr','stdMagColor_corr','stdMagClean_corr','stdMagStar_corr','Star_corr','ALPHA_J2000','DELTA_J2000']\n\n    Ns = []\n    keys = {} \n    i = -1 \n    for file_name,filter in list:\n        key_list = [['ALPHA_J2000','ALPHA_J2000'],['DELTA_J2000','DELTA_J2000'],['stdMag_corr',filter+'mag'],['stdMagErr_corr',filter+'err'],['stdMagClean_corr','Clean'],]\n        i += 1\n        Ns.append('N_0' + str(i) + ' = 1)')\n        print file_name\n        for key in key_list:\n            out_key = key[1] \n            in_key = key[0]\n            #if reduce(lambda x,y: x+ y, [string.find(out_key,k)!=-1 for k in key_list]):\n            if not key[1] in keys: \n                out.write(\"COL_NAME = \" + out_key + '\\nCOL_INPUT = ' + in_key + '\\nCOL_MERGE = AVE_REG\\nCOL_CHAN = ' + str(i) + \"\\n#\\n\")\n            print key\n            keys[key[1]] = True\n\n    out.close()\n    print out \n\n    return Ns\n\n\ndef make_ssc_config_few(list):\n\n    ofile = tmpdir + '/tmp.cat'\n    import os\n    os.system('mkdir -p ' + tmpdir)\n    out = open(tmpdir + '/tmp.ssc','w')\n    import os, string, re\n\n    key_list = ['CHIP','Flag','MAG_AUTO','MAGERR_AUTO','MAG_APER2','MAGERR_APER2','Xpos','Ypos','Xpos_ABS','Ypos_ABS','CLASS_STAR','MaxVal','BackGr','stdMag_corr','stdMagErr_corr','stdMagColor_corr','stdMagClean_corr','stdMagStar_corr','Star_corr','ALPHA_J2000','DELTA_J2000']\n    keys = []\n    i = -1 \n    for file_name,prefix,rot in list:\n        i += 1\n        print file_name\n        os.system('ldacdesc -t OBJECTS -i ' + file_name + ' > ' + ofile)\n        file = open(ofile,'r').readlines()\n        for line in file:\n            if string.find(line,\"Key name\") != -1 :\n                red = re.split('\\.+',line)\n                key = red[1].replace(' ','').replace('\\n','')\n                out_key = prefix + key\n                if reduce(lambda x,y: x+ y, [string.find(out_key,k)!=-1 for k in key_list]):\n                    out.write(\"COL_NAME = \" + out_key + '\\nCOL_INPUT = ' + key + '\\nCOL_MERGE = AVE_REG\\nCOL_CHAN = ' + str(i) + \"\\n#\\n\")\n                print key\n                keys.append(key)\n\n    out.close()\n\ndef make_ssc_config_colors(list):\n\n    ofile = tmpdir + '/tmp.cat'\n    out = open(tmpdir + '/tmp.ssc','w')\n    import os, string, re\n\n    keys = []\n    i = -1 \n    for file_name,prefix in list:\n        i += 1\n        print file_name\n        os.system('ldacdesc -t OBJECTS -i ' + file_name + ' > ' + ofile)\n        file = open(ofile,'r').readlines()\n        for line in file:\n            if string.find(line,\"Key name\") != -1:\n                red = re.split('\\.+',line)\n                key = red[1].replace(' ','').replace('\\n','')\n                out_key = key + '_' + prefix\n                out.write(\"COL_NAME = \" + out_key + '\\nCOL_INPUT = ' + key + '\\nCOL_MERGE = AVE_REG\\nCOL_CHAN = ' + str(i) + \"\\n#\\n\")\n                #print key\n            keys.append(key)\n\n    out.close()\n\ndef threesec():\n    list = [['/nfs/slac/g/ki/ki05/anja/SUBARU/MACS0417-11/PHOTOMETRY/ILLUMINATION/pasted_SUPA0105807_W-C-RC_2009-01-23_CALIB_0.0.cat','W-C-RC'],['/nfs/slac/g/ki/ki05/anja/SUBARU/MACS0417-11/PHOTOMETRY/ILLUMINATION/pasted_SUPA0105787_W-J-V_2009-01-23_CALIB_0.0.cat','W-J-V'],['/nfs/slac/g/ki/ki05/anja/SUBARU/MACS0417-11/PHOTOMETRY/ILLUMINATION/pasted_SUPA0050786_W-C-IC_2006-12-21_CALIB_0.0.cat','W-C-IC']]\n    match_many(list,True)\n\ndef match_many(list,color=False):\n\n    if color:\n        make_ssc_config_colors(list) \n        print color\n    else:\n        make_ssc_config_few(list) \n\n\n    import os \n    os.system('rm -rf ' + tmpdir + '/assoc/')\n    os.system('mkdir -p ' + tmpdir + '/assoc/')\n\n    import os\n    files = []\n    i=0\n    for file,prefix,rot in list:\n        print file\n        import re\n        res = re.split('\\/',file)\n        #os.system('ldactoasc -i ' + file + ' -t OBJECTS -k ALPHA_J2000 DELTA_J2000 > ' +  os.environ['bonn'] +  res[-1] )\n        #os.system('mkreg.pl -c -rad 3 -xcol 0 -ycol 1 -wcs ' +  os.environ['bonn'] + res[-1])\n\n        i += 1                                                                                                                  \n        colour = 'blue'\n        if i%2 ==0: colour = 'red'\n        if i%3 ==0: colour = 'green'\n        import re\n        res = re.split('\\/',file)\n        #os.system('ldactoasc -i ' + file + ' -t OBJECTS -k ALPHA_J2000 DELTA_J2000 > ' +  os.environ['bonn'] +  res[-1] )\n        #os.system('mkreg.pl -c -rad ' + str(i) + ' -xcol 0 -ycol 1 -colour ' + colour + ' -wcs ' +  os.environ['bonn'] + res[-1])\n        command = 'ldacaddkey -i %(inputcat)s -t OBJECTS -o %(outputcat)s -k A_WCS_assoc 0.0003 FLOAT \"\" \\\n                                        B_WCS_assoc 0.0003 FLOAT \"\" \\\n                                        Theta_assoc 0.0 FLOAT \"\" \\\n                                        Flag_assoc 0 SHORT \"\" ' % {'inputcat':file,'outputcat':file + '.assoc1'}\n        os.system(command)\n    \n        #command = 'ldacrenkey -i %(inputcat)s -o %(outputcat)s -k ALPHA_J2000 Ra DELTA_J2000 Dec' % {'inputcat':file + '.assoc1','outputcat':file+'.assoc2'} \n        #os.system(command)\n        files.append(file+'.assoc1')\n    import re\n    files_input = reduce(lambda x,y:x + ' ' + y,files)\n\n    files_output = reduce(lambda x,y:x + ' ' + y,[tmpdir + '/assoc/'+re.split('\\/',z)[-1] +'.assd' for z in files])\n\n    print files\n    print files_input, files_output\n    \n    command = 'associate -i %(inputcats)s -o %(outputcats)s -t OBJECTS -c %(bonn)s/photconf/fullphotom.alpha.associate' % {'inputcats':files_input,'outputcats':files_output, 'bonn':os.environ['bonn']}\n    print command\n    os.system(command)\n    print 'associated'\n\n    outputcat = tmpdir + '/final.cat'\n    command = 'make_ssc -i %(inputcats)s \\\n            -o %(outputcat)s\\\n            -t OBJECTS -c %(tmpdir)s/tmp.ssc ' % {'tmpdir': tmpdir, 'inputcats':files_output,'outputcat':outputcat}\n    os.system(command)\n\ndef match_many_multi_band(list,color=False):\n    Ns = make_ssc_multi_color(list)\n\n    import os \n    os.system('rm -rf ' + tmpdir + '/assoc/')\n    os.system('mkdir ' + tmpdir + '/assoc/')\n\n    import os\n    files = []\n    i = 0\n    for file,filter in list:\n        print file\n\n\n        i += 1\n        colour = 'blue'\n        if i%2 ==0: colour = 'red'\n        if i%3 ==0: colour = 'green'\n        import re\n        res = re.split('\\/',file)\n        #os.system('ldactoasc -i ' + file + ' -t OBJECTS -k ALPHA_J2000 DELTA_J2000 > ' +  os.environ['bonn'] +  res[-1] )\n        #os.system('mkreg.pl -c -rad ' + str(3+2*i) + ' -xcol 0 -ycol 1 -colour ' + colour + ' -wcs ' +  os.environ['bonn'] + res[-1])\n\n\n\n        command = 'ldacaddkey -i %(inputcat)s -t OBJECTS -o %(outputcat)s -k A_WCS_assoc 0.0003 FLOAT \"\" \\\n                                        B_WCS_assoc 0.0003 FLOAT \"\" \\\n                                        Theta_assoc 0.0 FLOAT \"\" \\\n                                        Flag_assoc 0 SHORT \"\" ' % {'inputcat':file,'outputcat':file + '.assoc1'}\n        os.system(command)\n    \n        #command = 'ldacrenkey -i %(inputcat)s -o %(outputcat)s -k ALPHA_J2000 Ra DELTA_J2000 Dec' % {'inputcat':file + '.assoc1','outputcat':file+'.assoc2'} \n        #os.system(command)\n        files.append(file+'.assoc1')\n    import re\n    files_input = reduce(lambda x,y:x + ' ' + y,files)\n\n    files_output = reduce(lambda x,y:x + ' ' + y,[tmpdir + '/assoc/'+re.split('\\/',z)[-1] +'.assd' for z in files])\n\n    print files\n    print files_input, files_output\n    \n    command = 'associate -i %(inputcats)s -o %(outputcats)s -t OBJECTS -c %(bonn)s/photconf/fullphotom.alpha.associate' % {'inputcats':files_input,'outputcats':files_output, 'bonn':os.environ['bonn']}\n    print command\n    os.system(command)\n    print 'associated'\n\n    outputcat = tmpdir + '/multiband.cat'\n    command = 'make_ssc -i %(inputcats)s \\\n            -o %(outputcat)s\\\n            -t OBJECTS -c %(tmpdir)s/tmp.ssc ' % {'tmpdir': tmpdir, 'inputcats':files_output,'outputcat':outputcat}\n    print command\n    os.system(command)\n    print outputcat, 'outputcat'\n\n    ''' now filter out the ones with incomplete colors '''\n\n    filt= '(' + reduce(lambda x,y: '(' + x + '  AND (' + y + ')',Ns)\n    print filt\n                                                                                                                 \n    intermediatecat = tmpdir + '/multiband_intermediate.cat'\n    command = 'ldacfilter -i ' + outputcat + ' -t PSSC -o ' + intermediatecat + ' -c \"' + filt + ';\" '\n    print command\n    import utilities\n    utilities.run(command,[intermediatecat])\n\n    finalcat = tmpdir + '/multiband_final.cat'\n    command = 'ldacrentab -i ' + intermediatecat + ' -t PSSC OBJECTS -o ' + finalcat \n    print command\n    import utilities\n    utilities.run(command,[finalcat])\n\n\n    print finalcat, 'finalcat'\n\n\n    ''' now make into SDSS format '''\n    tmp = {}    \n    import astropy, astropy.io.fits as pyfits, scipy\n    p = pyfits.open(finalcat)[1].data\n    cols = [] \n    print p.field('primarymag')[0:20]\n    print p.field('secondarymag')[0:20]\n                                                                                                          \n    print 'data start'\n    import Numeric \n    cols.append(pyfits.Column(name='stdMag_corr', format='D',array=p.field('primarymag')))\n    cols.append(pyfits.Column(name='stdMagErr_corr', format='D',array=p.field('primaryerr')))\n    cols.append(pyfits.Column(name='stdMagColor_corr', format='D',array=(p.field('primarymag')-p.field('secondarymag'))))\n    print p.field('primarymag')-p.field('secondarymag')\n    cols.append(pyfits.Column(name='stdMagClean_corr', format='D',array=p.field('Clean')))\n    cols.append(pyfits.Column(name='ALPHA_J2000', format='D',array=p.field('ALPHA_J2000')))\n    cols.append(pyfits.Column(name='DELTA_J2000', format='D',array=p.field('DELTA_J2000')))\n    cols.append(pyfits.Column(name='SeqNr', format='D',array=p.field('SeqNr')))\n    cols.append(pyfits.Column(name='Star_corr', format='D',array=scipy.ones(len(p.field('Clean')))))\n                                                                                                          \n    path = tmpdir \n    outcat = path + 'sdssfinalcat.cat'\n    print cols\n    hdu = pyfits.PrimaryHDU()\n    hdulist = pyfits.HDUList([hdu])\n    tbhu = pyfits.BinTableHDU.from_columns(cols)\n    hdulist.append(tbhu)\n    hdulist[1].header['EXTNAME']='OBJECTS'\n    os.system('rm ' + outcat)\n    hdulist.writeto( outcat )\n    print 'wrote out new cat'\n    print outcat\n\n    return outcat\n\n\ndef match_inside(SUPA1,SUPA2,FLAT_TYPE):\n\n    dict1 = get_files(SUPA1,FLAT_TYPE)\n    search_params1 = initialize(dict1['filter'],dict1['OBJNAME'])\n    search_params1.update(dict1)\n\n    dict2 = get_files(SUPA2,FLAT_TYPE)\n    search_params2 = initialize(dict2['filter'],dict2['OBJNAME'])\n    search_params2.update(dict2)\n\n    import os\n    path='/nfs/slac/g/ki/ki05/anja/SUBARU/%(OBJNAME)s/' % {'OBJNAME':search_params1['OBJNAME']}\n    illum_path='/nfs/slac/g/ki/ki05/anja/SUBARU/ILLUMINATION/' % {'OBJNAME':search_params1['OBJNAME']}\n    #os.system('mkdir -p ' + path + 'PHOTOMETRY/ILLUMINATION/') \n    os.system('mkdir -p ' + path + 'PHOTOMETRY/ILLUMINATION/SELF/') \n    from glob import glob\n\n\n    catalog1 = search_params1['pasted_cat']\n    catalog2 = search_params2['pasted_cat']\n\n\n    #os.system('ldacrentab -i ' + catalog2 + ' -t OBJECTS STDTAB -o ' + catalog2.replace('cat','std.cat'))\n\n\n    filter = search_params1['filter'] #exposures[exposure]['keywords']['filter']\n    OBJECT = search_params1['OBJECT'] #exposures[exposure]['keywords']['OBJECT']\n    outcat = path + 'PHOTOMETRY/ILLUMINATION/SELF/matched_' + SUPA1 + '_' + filter + '_' + '_self.cat'               \n    file = 'matched_' + SUPA1 + '.cat'               \n    os.system('rm ' + outcat)\n    command = 'match_simple_cats.sh ' + catalog1 + ' ' + catalog2 + ' ' + outcat\n    print command\n    os.system(command)\n\n    save_exposure({'matched_cat_self':outcat},SUPA1,FLAT_TYPE)\n\n    print outcat\n\ndef getTableInfo():\n    import astropy, astropy.io.fits as pyfits, sys, os, re, string, copy , string\n    \n    p = pyfits.open(tmpdir + '/final.cat')\n    tbdata = p[1].data\n    types = []\n   \n    ROTS = {}\n    KEYS = {} \n    for column in p[1].columns: \n        if string.find(column.name,'$') != -1:       \n            print column                                           \n            res = re.split('\\$',column.name)\n            ROT = res[0]\n            IMAGE = res[1]\n            KEY = res[2]\n            if not ROTS.has_key(ROT):\n                ROTS[ROT] = []\n            if not len(filter(lambda x:x==IMAGE,ROTS[ROT])): # and IMAGE!='SUPA0011082':\n                ROTS[ROT].append(IMAGE)\n    return ROTS\n\n\ndef diffCalcNew():\n    import astropy, astropy.io.fits as pyfits, sys, os, re, string, copy , string\n    \n    p = pyfits.open(tmpdir + '/final.cat')\n    tbdata = p[1].data\n    types = []\n   \n    ROTS = {}\n    KEYS = {} \n    for column in p[1].columns: \n        if string.find(column.name,'$') != -1:       \n            print column                                           \n            res = re.split('\\$',column.name)\n            ROT = res[0]\n            IMAGE = res[1]\n            KEY = res[2]\n                                                                  \n            if not ROTS.has_key(ROT):\n                ROTS[ROT] = []\n            if not len(filter(lambda x:x==IMAGE,ROTS[ROT])):\n                ROTS[ROT].append(IMAGE)\n\n        \n    print ROTS\n\n\n   \n    #good = 0\n    #for i in range(len(tbdata)):\n    #    array = []\n    #    for y in ROTS[ROT]:\n    #        array += [tbdata.field(ROT+'$'+y+'$CLASS_STAR')[i] for y in ROTS[ROT]]\n    #    array.sort()\n    #    if array[-1]>0.9 and array[-2]>0.9: \n    #        good += 1 \n    #print good, len(tbdata)\n\ndef starConstruction(EXPS):\n    ''' the top two most star-like objects have CLASS_STAR>0.9 and, for each rotation, their magnitudes differ by less than 0.01 '''\n    import astropy, astropy.io.fits as pyfits, sys, os, re, string, copy , string, scipy\n    \n    p = pyfits.open(tmpdir + '/final.cat')\n    table = p[1].data\n\n    from copy import copy \n    w = []\n    for ROT in EXPS.keys():\n        for y in EXPS[ROT]:\n            w.append(copy(table.field(ROT+'$'+y+'$MAG_AUTO')))\n    medians = []\n    stds = []\n    for i in range(len(w[0])):\n        non_zero = []\n        for j in range(len(w)): \n            if w[j][i] != 0:\n                non_zero.append(w[j][i])\n        if len(non_zero) != 0:\n            medians.append(float(scipy.median(non_zero)))\n            stds.append(float(scipy.std(non_zero)))\n        else: \n            medians.append(float(-99))\n            stds.append(99)\n            \n    print medians[0:99]\n    tnew = mk_tab([[medians,'median'],[stds,'std']])\n    tall = merge(tnew,p)\n    print 'done merging'\n\ndef selectGoodStars(EXPS,match,LENGTH1,LENGTH2):\n    ''' the top two most star-like objects have CLASS_STAR>0.9 and, for each rotation, their magnitudes differ by less than 0.01 '''\n    import astropy, astropy.io.fits as pyfits, sys, os, re, string, copy , string, scipy\n\n    ''' remove a rotation if it has no or one exposure '''   \n    EXPS_new = {} \n    for ROT in EXPS.keys():\n        if len(EXPS[ROT]) > 1:\n            EXPS_new[ROT] = EXPS[ROT]\n\n    print EXPS\n    print EXPS_new\n    EXPS = EXPS_new\n    #raw_input()\n\n\n    \n    p = pyfits.open(tmpdir + '/final.cat')\n    print tmpdir + '/final.cat'\n    #print p[1].columns\n    table = p[1].data\n    star_good = [] #= scipy.zeros(len(table)) \n    supas = []\n    from copy import copy \n\n    totalstars = 0\n\n    ''' if there is an image that does not match, throw it out '''\n    Finished = False \n    while not Finished:\n        temp = copy(table)\n        tlist = []\n        for ROT in EXPS.keys():\n            for y in EXPS[ROT]:    \n                tlist.append([y,ROT])\n        print EXPS\n        print tlist\n        \n        for y,ROT in tlist:\n            mask = temp.field(ROT+'$'+y+'$MAG_AUTO') != 0.0  \n            good_entries = temp[mask]\n            temp = good_entries\n            print len(good_entries.field(ROT+'$'+y+'$MAG_AUTO')), 'not 0.0'\n            mask = temp.field(ROT+'$'+y+'$MAG_AUTO') < 30  \n            good_entries = temp[mask]\n            temp = good_entries\n            print len(good_entries.field(ROT+'$'+y+'$MAG_AUTO')), 'less than 30'\n            mask = 0 < temp.field(ROT+'$'+y+'$MAG_AUTO') \n            good_entries = temp[mask]\n            temp = good_entries\n            print len(good_entries.field(ROT+'$'+y+'$MAG_AUTO')), 'greater than 0'\n            print ROT,y,  temp.field(ROT+'$'+y+'$MaxVal')[0:10],temp.field(ROT+'$'+y+'$BackGr')[0:10] \n            mask = (temp.field(ROT+'$'+y+'$MaxVal') + temp.field(ROT+'$'+y+'$BackGr')) < 26000\n            good_entries = temp[mask]\n            temp = good_entries\n            good_number = len(good_entries.field(ROT+'$'+y+'$MAG_AUTO'))\n            print ROT,y, good_number , EXPS\n            if good_number < 300:\n                print 'DROPPING!'\n                TEMP = {}\n                for ROTTEMP in EXPS.keys():\n                    TEMP[ROTTEMP] = []\n                    for z in EXPS[ROTTEMP]:    \n                        if y!=z:\n                            TEMP[ROTTEMP].append(z)\n                EXPS = TEMP\n                Finished = False\n                break\n        if good_number > 0:                        \n            Finished = True \n        print Finished\n\n    print len(temp), 'temp'\n    zps = {}\n\n    print EXPS.keys(), EXPS\n    for ROT in EXPS.keys():\n        for y in EXPS[ROT]:\n            s = good_entries.field(ROT+'$'+y+'$MAG_AUTO').sum()\n            print s\n            print s/len(good_entries)\n            zps[y] = s/len(good_entries)\n    print zps\n\n    from copy import copy    \n    tab = {}\n\n\n\n    for ROT in EXPS.keys():\n        for y in EXPS[ROT]:\n            keys = [ROT+'$'+y+'$CHIP',ROT+'$'+y+'$Xpos_ABS',ROT+'$'+y+'$Ypos_ABS',ROT+'$'+y+'$MAG_AUTO',ROT+'$'+y+'$MAGERR_AUTO',ROT+'$'+y+'$MaxVal',ROT+'$'+y+'$BackGr',ROT+'$'+y+'$CLASS_STAR',ROT+'$'+y+'$Flag',ROT+'$'+y+'$ALPHA_J2000',ROT+'$'+y+'$DELTA_J2000']                                                                                            \n            if match:\n                keys = [ROT+'$'+y+'$CHIP',ROT+'$'+y+'$Xpos_ABS',ROT+'$'+y+'$Ypos_ABS',ROT+'$'+y+'$MAG_AUTO',ROT+'$'+y+'$MAGERR_AUTO',ROT+'$'+y+'$MaxVal',ROT+'$'+y+'$BackGr',ROT+'$'+y+'$CLASS_STAR',ROT+'$'+y+'$Flag' ,'SDSSstdMag_corr','SDSSstdMagErr_corr','SDSSstdMagColor_corr','SDSSstdMagClean_corr','SDSSStar_corr',ROT+'$'+y+'$ALPHA_J2000',ROT+'$'+y+'$DELTA_J2000']\n                #print 'SDSS', table.field('SDSSstdMag_corr')[-1000:]\n            for key in keys: \n                tab[key] = copy(table.field(key))\n                print keys\n    print len(table)\n    backgrounds = []\n    for i in range(len(table)):\n        mags_ok = False \n        star_ok = False\n        class_star_array = []\n        include_star = []\n        in_box = []\n        name = []\n        mags_diff_array = []\n        mags_good_array = []\n        mags_array = []\n        from copy import copy\n        for ROT in EXPS.keys():\n            #for y in EXPS[ROT]:\n            #    if table.field(ROT+'$'+y+'$MAG_AUTO')[i] != 0.0:\n            mags_array += [tab[ROT+'$'+y+'$MAG_AUTO'][i] for y in EXPS[ROT]]\n            mags_diff_array += [zps[y] - tab[ROT+'$'+y+'$MAG_AUTO'][i] for y in EXPS[ROT]]\n            mags_good_array += [tab[ROT+'$'+y+'$MAG_AUTO'][i]!=0.0 for y in EXPS[ROT]]\n            #in_box += [1000 < tab[ROT+'$'+y+'$Xpos_ABS'][i] < 9000 and 1000 < tab[ROT+'$'+y+'$Ypos_ABS'][i] < 7000  for y in EXPS[ROT]]\n            if 0: #tab[ROT+'$'+y+'$MAG_AUTO'][i]!=0.0:  \n                print LENGTH1, LENGTH2, (tab[ROT+'$'+y+'$MaxVal'][i] + tab[ROT+'$'+y+'$BackGr'][i]) < 20000  ,  tab[ROT+'$'+y+'$Flag'][i]==0 , tab[ROT+'$'+y+'$MAG_AUTO'][i] < 30 , tab[ROT+'$'+y+'$MAG_AUTO'][i]!=0.0 , tab[ROT+'$'+y+'$MAGERR_AUTO'][i]<0.05 , ((tab[ROT+'$'+y+'$Xpos_ABS'][i]-LENGTH1/2.)**2.+(tab[ROT+'$'+y+'$Ypos_ABS'][i]-LENGTH2/2.)**2.) < (LENGTH1/2.)**2\n\n            if 0: # 0 < tab[ROT+'$'+EXPS[ROT][0]+'$Xpos_ABS'][i] < 200 or 0 < tab[ROT+'$'+EXPS[ROT][0]+'$Ypos_ABS'][i] < 200:\n                print LENGTH1, LENGTH2, [((tab[ROT+'$'+y+'$Xpos_ABS'][i]-LENGTH1/2.)**2.+(tab[ROT+'$'+y+'$Ypos_ABS'][i]-LENGTH2/2.)**2.) < ((LENGTH1/2.)**2 + (LENGTH2/2.)**2.) for y in EXPS[ROT][0:2]] \n                print [[tab[ROT+'$'+y+'$Xpos_ABS'][i]-LENGTH1/2., tab[ROT+'$'+y+'$Ypos_ABS'][i]-LENGTH2/2., tab[ROT+'$'+y+'$Xpos_ABS'][i],tab[ROT+'$'+y+'$Ypos_ABS'][i]] for y in EXPS[ROT][0:2]]\n\n                print [[tab[ROT+'$'+y+'$Xpos_ABS'][i], tab[ROT+'$'+y+'$Ypos_ABS'][i], tab[ROT+'$'+y+'$Xpos_ABS'][i],tab[ROT+'$'+y+'$Ypos_ABS'][i]] for y in EXPS[ROT][0:2]]\n\n            #include_star += [( tab[ROT+'$'+y+'$MAG_AUTO'][i]!=0.0  ) for y in EXPS[ROT]] # and \n\n            #print [[tab[ROT+'$'+y+'$MaxVal'][i] , tab[ROT+'$'+y+'$BackGr'][i]] for y in EXPS[ROT]]\n            #print [((tab[ROT+'$'+y+'$MaxVal'][i] + tab[ROT+'$'+y+'$BackGr'][i]) ,  tab[ROT+'$'+y+'$Flag'][i]==0 , tab[ROT+'$'+y+'$MAG_AUTO'][i]  , tab[ROT+'$'+y+'$MAG_AUTO'][i] , tab[ROT+'$'+y+'$MAGERR_AUTO'][i], ((tab[ROT+'$'+y+'$Xpos_ABS'][i]-LENGTH1/2.)**2.+(tab[ROT+'$'+y+'$Ypos_ABS'][i]-LENGTH2/2.)**2.) < (LENGTH1/2.)**2)  for y in EXPS[ROT]]\n            backgrounds += filter(lambda x: x!=0, [tab[ROT+'$'+y+'$MaxVal'][i] + tab[ROT+'$'+y+'$BackGr'][i] for y in EXPS[ROT]])\n            \n            include_star += [( 0 < (tab[ROT+'$'+y+'$MaxVal'][i] + tab[ROT+'$'+y+'$BackGr'][i]) < 25000  and  tab[ROT+'$'+y+'$Flag'][i]==0 and tab[ROT+'$'+y+'$MAG_AUTO'][i] < 30 and tab[ROT+'$'+y+'$MAG_AUTO'][i]!=0.0 and tab[ROT+'$'+y+'$MAGERR_AUTO'][i]<0.1 and ((tab[ROT+'$'+y+'$Xpos_ABS'][i]-LENGTH1/2.)**2.+(tab[ROT+'$'+y+'$Ypos_ABS'][i]-LENGTH2/2.)**2.) < (LENGTH1/2.)**2  )  for y in EXPS[ROT]] # and \n            # and (tab[ROT+'$'+y+'$CHIP'][i]!=2 and tab[ROT+'$'+y+'$CHIP'][i]!=7)\n\n            #include_star += [( 0 < (tab[ROT+'$'+y+'$MaxVal'][i] + tab[ROT+'$'+y+'$BackGr'][i]) < 25000  and  tab[ROT+'$'+y+'$Flag'][i]==0 and tab[ROT+'$'+y+'$MAG_AUTO'][i] < 30 and tab[ROT+'$'+y+'$MAG_AUTO'][i]!=0.0 and tab[ROT+'$'+y+'$MAGERR_AUTO'][i]<0.05)  for y in EXPS[ROT]] # and \n\n            #include_star += [((tab[ROT+'$'+y+'$MaxVal'][i] + tab[ROT+'$'+y+'$BackGr'][i]) < 25000  and  tab[ROT+'$'+y+'$Flag'][i]==0 and tab[ROT+'$'+y+'$MAG_AUTO'][i] < 30 and tab[ROT+'$'+y+'$MAG_AUTO'][i]!=0.0 and tab[ROT+'$'+y+'$MAGERR_AUTO'][i]<0.05)  for y in EXPS[ROT]] # and \n\n\n            #in_circ = lambda x,y,r: (x**2.+y**2.)<r**2.\n\n            #include_star += [((tab[ROT+'$'+y+'$MaxVal'][i] + tab[ROT+'$'+y+'$BackGr'][i]) < 25000  and  tab[ROT+'$'+y+'$Flag'][i]==0 and tab[ROT+'$'+y+'$MAG_AUTO'][i] < 30 and tab[ROT+'$'+y+'$MAG_AUTO'][i]!=0.0 and tab[ROT+'$'+y+'$MAGERR_AUTO'][i]<0.05 and in_circ(tab[ROT+'$'+y+'$Xpos_ABS'][i]-LENGTH1/2.,tab[ROT+'$'+y+'$Ypos_ABS'][i]-LENGTH2/2,LENGTH) for y in EXPS[ROT]]\n\n            #include_star += [((tab[ROT+'$'+y+'$MaxVal'][i] + tab[ROT+'$'+y+'$BackGr'][i]) < 25000  and  tab[ROT+'$'+y+'$Flag'][i]==0 and tab[ROT+'$'+y+'$MAG_AUTO'][i] < 30 and tab[ROT+'$'+y+'$MAG_AUTO'][i]!=0.0 and tab[ROT+'$'+y+'$MAGERR_AUTO'][i]<0.05) for y in EXPS[ROT]]\n            #for y in EXPS[ROT]:\n            #    print (tab[ROT+'$'+y+'$MaxVal'][i] + tab[ROT+'$'+y+'$BackGr'][i]) < 27500  , tab[ROT+'$'+y+'$Flag'][i]==0 , tab[ROT+'$'+y+'$MAG_AUTO'][i] < 40 , tab[ROT+'$'+y+'$MAG_AUTO'][i]!=0.0\n            name += [{'name':EXPS[ROT][z],'rotation':ROT} for z in range(len(EXPS[ROT]))]\n            class_star_array += [tab[ROT+'$'+y+'$CLASS_STAR'][i] for y in EXPS[ROT]]\n        class_star_array.sort()\n        #if len(mags_array) > 1: \n        #    if 1: #abs(mags_array[0] - mags_array[1]) < 0.5:\n        #        mags_ok = True \n        #    if 1: #abs(class_star_array[-1]) > 0.01: # MAIN PARAMETER!\n        #        star_ok = True \n\n        list = []\n        for k in range(len(mags_good_array)):\n            if mags_good_array[k]: \n                list.append(mags_diff_array[k])                     \n        if len(list) > 1:\n            median_mag_diff = scipy.median(list)                                                                                       \n            file_list=[]\n            for j in range(len(include_star)): \n                if include_star[j] and abs(mags_diff_array[j] - median_mag_diff) < 1.:  # MAIN PARAMETER!\n                    file_list.append(name[j])\n                    mag = mags_diff_array[j]\n            if match:\n                ''' if match object exists '''\n                if tab['SDSSstdMag_corr'][i] != 0.0: match_exists = 1\n                else: match_exists = 0\n                ''' if match object is good -- throw out galaxies for this '''\n                # \n                if float(tab['SDSSstdMagClean_corr'][i]) == 1 and abs(class_star_array[-1]) > 0.65 and 40. > tab['SDSSstdMag_corr'][i] > 0.0 and 5 > tab['SDSSstdMagColor_corr'][i] > -5: match_good = 1 \n                else: match_good = 0\n\n                #if match_good == 0 and match_exists:    \n                    #print float(tab['SDSSstdMagClean_corr'][i]) , abs(class_star_array[-1]) , tab['SDSSstdMag_corr'][i] , tab['SDSSstdMagColor_corr'][i]   \n\n\n            else: \n                match_good = 0 \n                match_exists = 0\n            #if match_good == 1:\n            #    print match_good\n            #else: print 'bad'\n\n            if len(file_list) > 1:\n                totalstars += len(file_list)\n                ''' if using chip dependent color terms, colors for each object are required '''\n                #if catalog='bootstrap':\n                #    if sdss==1:    \n                #        star_good.append(i)                                                                                                     \n                #        supas.append({'mag':mag,'table index':i,'supa files':file_list, 'match':match, 'match_exists':match_exists})                        \n                #else:\n                star_good.append(i)                                                                                                     \n                supas.append({'mag':mag,'table index':i,'supa files':file_list, 'match':match_good, 'match_exists':match_exists, 'std':scipy.std(list)})                        \n        if i%2000==0: print i\n\n    supas.sort(sort_supas)\n    import numpy\n    mdn_background = scipy.median(scipy.array(backgrounds))\n\n    EXPS_new = {}                        \n    for ROT in EXPS.keys():\n        if len(EXPS[ROT]) > 1:\n            EXPS_new[ROT] = EXPS[ROT]\n                                      \n    print EXPS\n    print EXPS_new\n    EXPS = EXPS_new\n    \n    print EXPS    \n\n    return EXPS, star_good, supas, totalstars, mdn_background\n\n\ndef sort_supas(x,y):\n    if x['mag'] > y['mag']:\n        return 1            \n    else: return -1\n\n\n\n\ndef diffCalc(SUPA1,FLAT_TYPE):\n    dict = get_files(SUPA1,FLAT_TYPE)\n    search_params = initialize(dict['filter'],dict['OBJNAME'])\n    search_params.update(dict)\n\n    import astropy, astropy.io.fits as pyfits, sys, os, re, string, copy \n    \n    print search_params['matched_cat_self']\n    p = pyfits.open(search_params['matched_cat_self'])\n    tbdata = p[1].data\n    mask = tbdata.field('SEx_MaxVal') + tbdata.field('SEx_BackGr') < 27500 \n    newtbdata = tbdata[mask]\n\n    print len(newtbdata)\n\n    mask = newtbdata.field('CLASS_STAR') > 0.95 \n    newtbdata = newtbdata[mask]\n\n    mask = abs(newtbdata.field('SEx_MAG_APER2') - newtbdata.field('MAG_APER2')) < 0.01 \n    new2tbdata = newtbdata[mask]\n\n    print len(new2tbdata)\n\n    data = new2tbdata.field('SEx_MAG_APER2') - new2tbdata.field('MAG_APER2')\n    magErr = new2tbdata.field('SEx_MAGERR_APER2')\n    X = new2tbdata.field('Xpos_ABS')\n    Y = new2tbdata.field('Ypos_ABS')\n\n    file = 'test'\n\n    calcDataIllum(file,search_params['LENGTH1'], search_params['LENGTH2'],data,magErr,X,Y) \n\n    data_save = []\n    magErr_save = []\n    X_save = []\n    Y_save = []\n    for i in range(len(data)):\n        data_save.append([new2tbdata.field('SEx_MAG_APER2')[i],new2tbdata.field('MAG_APER2')[i]])\n        magErr_save.append([new2tbdata.field('SEx_MAGERR_APER2')[i],new2tbdata.field('MAGERR_APER2')[i]])\n        X_save.append([new2tbdata.field('Xpos_ABS')[i],new2tbdata.field('SEx_Xpos_ABS')[i]])\n        Y_save.append([new2tbdata.field('Ypos_ABS')[i],new2tbdata.field('SEx_Ypos_ABS')[i]])\n\n    return data_save, magErr_save, X_save, Y_save\n\ndef linear_fit(OBJNAME,FILTER,PPRUN,run_these=None,match=None,CONFIG=None,primary=None,secondary=None):\n\n    print match, CONFIG\n    print OBJNAME,FILTER, PPRUN, tmpdir\n    maxSigIter=50\n    solutions = [] \n\n    redoselect = False \n\n    trial = True \n    if __name__ == '__main__': \n        trial = False \n\n\n    fit_db = {}\n\n    import pickle\n    ''' get data '''\n    \n    \n    \n    \n    \n    \n    \n\n    ''' create chebychev polynomials '''\n\n    if CONFIG == '10_3' or CONFIG == '8.0':\n        cheby_x = [{'n':'0x','f':lambda x,y:1.,'order':0},{'n':'1x','f':lambda x,y:x,'order':1},{'n':'2x','f':lambda x,y:2*x**2-1,'order':2}] #,{'n':'3x','f':lambda x,y:4*x**3.-3*x,'order':3},{'n':'4x','f':lambda x,y:8*x**4.-8*x**2.+1,'order':4}]#,{'n':'5x','f':lambda x,y:16*x**5.-20*x**3.+5*x,'order':5}]  \n        cheby_y = [{'n':'0y','f':lambda x,y:1.,'order':0},{'n':'1y','f':lambda x,y:y,'order':1},{'n':'2y','f':lambda x,y:2*y**2-1,'order':2}] #,{'n':'3y','f':lambda x,y:4*y**3.-3*y,'order':3},{'n':'4y','f':lambda x,y:8*y**4.-8*y**2.+1,'order':4}] #,{'n':'5y','f':lambda x,y:16*y**5.-20*y**3.+5*y,'order':5}]\n    else:\n        cheby_x = [{'n':'0x','f':lambda x,y:1.,'order':0},{'n':'1x','f':lambda x,y:x,'order':1},{'n':'2x','f':lambda x,y:2*x**2-1,'order':2},{'n':'3x','f':lambda x,y:4*x**3.-3*x,'order':3}] #,{'n':'4x','f':lambda x,y:8*x**4.-8*x**2.+1,'order':4},{'n':'5x','f':lambda x,y:16*x**5.-20*x**3.+5*x,'order':5}]  \n        cheby_y = [{'n':'0y','f':lambda x,y:1.,'order':0},{'n':'1y','f':lambda x,y:y,'order':1},{'n':'2y','f':lambda x,y:2*y**2-1,'order':2},{'n':'3y','f':lambda x,y:4*y**3.-3*y,'order':3}] #,{'n':'4y','f':lambda x,y:8*y**4.-8*y**2.+1,'order':4},{'n':'5y','f':lambda x,y:16*y**5.-20*y**3.+5*y,'order':5}]\n\n    cheby_terms = []\n    cheby_terms_no_linear = []\n    for tx in cheby_x:\n        for ty in cheby_y:\n            if 1: #tx['order'] + ty['order'] <=3:\n                if not ((tx['n'] == '0x' and ty['n'] == '0y')): # or (tx['n'] == '0x' and ty['n'] == '1y') or (tx['n'] == '1x' and ty['n'] == '0y')) : \n                    cheby_terms.append({'n':tx['n'] + ty['n'],'fx':tx['f'],'fy':ty['f']})\n                if not ((tx['n'] == '0x' and ty['n'] == '0y') or (tx['n'] == '0x' and ty['n'] == '1y') or (tx['n'] == '1x' and ty['n'] == '0y')) :\n                    cheby_terms_no_linear.append({'n':tx['n'] + ty['n'],'fx':tx['f'],'fy':ty['f']})\n\n\n    #ROTS, data, err, X, Y, maxVal, classStar = diffCalcNew()\n    #save = {'ROTS': ROTS, 'data':data,'err':err,'X':X,'Y':Y,'maxVal':maxVal,'classStar':classStar}\n    #uu = open(tmpdir + '/store','w')\n    #import pickle\n    #pickle.dump(save,uu)\n    #uu.close()\n   \n    ''' EXPS has all of the image information for different rotations '''\n\n    ''' make model '''\n    #fit = make_model(EXPS)\n    #position_fit = make_position_model(EXPS)\n    print fit\n\n    start_EXPS = getTableInfo()                                                                                                                                     \n    print start_EXPS\n    \n    for ROT in start_EXPS.keys():\n        print start_EXPS[ROT]\n        #save_fit({'PPRUN':PPRUN,'FILTER':FILTER,'OBJNAME':OBJNAME,str(ROT)+'images':len(EXPS[ROT]),str(ROT)+'supas':reduce(lambda x,y:x+','+y,EXPS[ROT])})\n    print start_EXPS\n\n    ''' see if in sdss, linear or not ''' \n    print start_EXPS\n    dt = get_files(start_EXPS[start_EXPS.keys()[0]][0])\n    import re\n    print dt['CHIPS']\n    CHIPS = [int(x) for x in re.split(',',dt['CHIPS'])]\n    LENGTH1, LENGTH2 = dt['LENGTH1'], dt['LENGTH2']\n    print LENGTH1, LENGTH2 \n\n    #cov, galaxycat, starcat = sdss_coverage(dt['SUPA'],dt['FLAT_TYPE']) \n\n    if redoselect:\n        EXPS, star_good,supas, totalstars, mdn_background = selectGoodStars(start_EXPS,match,LENGTH1,LENGTH2)               \n\n        \n        import os\n        os.system('mkdir -p ' + tmpdir)\n        uu = open(tmpdir + '/selectGoodStars','w')\n        import pickle\n        info = starStats(supas)\n        print info    \n        pickle.dump({'info':info,'EXPS':EXPS,'star_good':star_good,'supas':supas,'totalstars':totalstars},uu)\n        uu.close()\n\n\n    ''' if early chip configuration, use chip color terms ''' \n    if (CONFIG=='8' or CONFIG=='9'):\n        relative_colors = True\n    else: relative_colors = False                \n    print relative_colors\n\n    import pickle\n    f=open(tmpdir + '/selectGoodStars','r')\n    m=pickle.Unpickler(f)\n    d=m.load()\n\n    ''' read out of pickled dictionary '''\n    info = d['info']\n    EXPS = d['EXPS']\n    star_good = d['star_good']\n    supas = d['supas']\n    totalstars = d['totalstars']\n    print EXPS\n\n    print \"calc_test_save.linear_fit('\" + OBJNAME + \"','\" + FILTER + \"','\" + PPRUN + \"',\" + str(match) + \",'\" + CONFIG +  str(primary) + \"',secondary='\" + str(secondary) + \"',star_good='\" + str(len(star_good)) + \"')\"\n    print len(star_good)\n\n    #cheby_terms_use = cheby_terms_no_linear\n    fitvars_fiducial = False\n    \n    import scipy\n    import astropy, astropy.io.fits as pyfits\n    p = pyfits.open(tmpdir + '/final.cat')\n    table = p[1].data\n    \n    from copy import copy  \n    tab = {}\n    for ROT in EXPS.keys():\n        for y in EXPS[ROT]:\n            keys = [ROT+'$'+y+'$CHIP',ROT+'$'+y+'$Xpos',ROT+'$'+y+'$Ypos',ROT+'$'+y+'$CHIP',ROT+'$'+y+'$Xpos_ABS',ROT+'$'+y+'$Ypos_ABS',ROT+'$'+y+'$MAG_AUTO',ROT+'$'+y+'$MAGERR_AUTO',ROT+'$'+y+'$MaxVal',ROT+'$'+y+'$BackGr',ROT+'$'+y+'$CLASS_STAR',ROT+'$'+y+'$Flag',ROT+'$'+y+'$ALPHA_J2000',ROT+'$'+y+'$DELTA_J2000']                                                                                            \n            if match:\n                keys = [ROT+'$'+y+'$CHIP',ROT+'$'+y+'$Xpos',ROT+'$'+y+'$Ypos',ROT+'$'+y+'$CHIP',ROT+'$'+y+'$Xpos_ABS',ROT+'$'+y+'$Ypos_ABS',ROT+'$'+y+'$MAG_AUTO',ROT+'$'+y+'$MAGERR_AUTO',ROT+'$'+y+'$MaxVal',ROT+'$'+y+'$BackGr',ROT+'$'+y+'$CLASS_STAR',ROT+'$'+y+'$Flag' ,'SDSSstdMag_corr','SDSSstdMagErr_corr','SDSSstdMagColor_corr','SDSSstdMagClean_corr','SDSSStar_corr',ROT+'$'+y+'$ALPHA_J2000',ROT+'$'+y+'$DELTA_J2000']\n\n            for key in keys: \n                tab[key] = copy(table.field(key))\n\n    tab_copy = copy(tab)\n    \n    coord_conv_x = lambda x:(2.*x-(LENGTH1))/((LENGTH1))  \n    coord_conv_y = lambda x:(2.*x-(LENGTH2))/((LENGTH2)) \n\n    print LENGTH1, LENGTH2\n\n\n    supas_copy = copy(supas)\n\n\n    ''' find the color term '''\n    if  False:\n        data = []\n        magErr = []\n        color = []\n\n        for star in supas:                                                                                                                   \n            ''' each exp of each star '''\n            if star['match'] and (sample=='sdss' or sample=='bootstrap'):\n                for exp in star['supa files']:\n                    if 2 > tab['SDSSstdMagColor_corr'][star['table index']] > -2:\n                        rotation = exp['rotation']                                                                                                       \n                        sigma = tab['SDSSstdMagErr_corr'][star['table index']]\n                        data.append(tab[str(rotation)+'$'+exp['name']+'$MAG_AUTO'][star['table index']] - tab['SDSSstdMag_corr'][star['table index']])  \n                        magErr.append(tab['SDSSstdMagErr_corr'][star['table index']])\n                        color.append(tab['SDSSstdMagColor_corr'][star['table index']])\n\n        color.sort()\n\n    import copy\n\n    sample = str(match)\n    sample_copy = copy.copy(sample)\n    print sample, 'sample'\n\n    rands = ['all','rand1','rand2','rand3','rand4','rand5','rand6','rand7','rand8','rand9','rand10'] #,'rand11','rand12','rand13','rand14','rand15','rand16','rand17','rand18','rand19','rand20']\n\n    #rands = ['rand11','rand12','rand13','rand14','rand15','rand16','rand17','rand18','rand19','rand20']\n\n\n    import MySQLdb, sys, os, re, time, utilities, pyfits                                                                                                                           \n    from copy import copy\n    db2 = MySQLdb.connect(db='subaru', user='weaklensing', passwd='darkmatter', host='ki-rh8')\n    c = db2.cursor()\n\n    if run_these is None:\n        run_these = []                                                                                                                                                               \n        for k in rands:\n            db_keys_t = describe_db(c,['fit_db']) \n            command=\"SELECT * from fit_db where PPRUN='\" + PPRUN + \"' and OBJNAME='\" + OBJNAME + \"' and sample_size like '\" + k +  \"%' and sample='\" + sample + \"'\"\n            print command                                                  \n            c.execute(command)                                             \n            results=c.fetchall()                                           \n            if len(results) == 0:\n                run_these.append(k)\n        print run_these, sample\n\n    run_these = ['all']\n\n    #run_these = ['all','rand1','rand2','rand3','rand4','rand5','rand6','rand7','rand8','rand9','rand10']\n\n    for sample_size in run_these: #,'rand1','rand2','rand3']: #'rand2','rand3']: #,'rand1','rand2']: #,'rand2','rand3','rand4','all']: #,'rand3']:\n        save_fit({'PPRUN':PPRUN,'FILTER':FILTER,'OBJNAME':OBJNAME,'sample':sample,'sample_size':sample_size,'primary_filt':primary,'secondary_filt':secondary,'coverage':str(match),'relative_colors':relative_colors,'catalog':str(match),'CONFIG':CONFIG,'supas':len(supas),'match_stars':len(filter(lambda x:x['match'],supas))})\n        if sample_size == 'all':\n            if totalstars > 30000:                                                                    \n                print len(supas)                                                      \n                l = range(len(supas_copy))                     \n                ''' shorten star_good, supas '''\n                print totalstars, len(supas)\n                #supas = [supas_copy[i] for i in l[0:int(float(30000)/float(totalstars)*len(supas))]]\n                import copy \n                supas = []\n                ''' include bright stars and matched stars '''\n                for supa in supas_copy:\n                    if len(supas) < int(float(30000)/float(totalstars)*len(supas_copy))  or supa['match']:\n                        supas.append(supa)\n                #    print 'supa', supa['mag']\n                #supas = copy.copy(supas_copy[0:int(float(30000)/float(totalstars)*len(supas_copy))])\n            else: \n                import copy\n                supas = copy.copy(supas_copy)\n            print starStats(supas)\n            ''' if sdss comparison exists, run twice to see how statistics are improved ''' \n            if sample == 'sdss':\n                ''' first all info, then w/o sdss, then fit for zps w/ sdss but not position terms, then run fit for zps w/o position terms '''\n                runs = [[sample_size,True,supas,'sdss'],[sample_size + 'None',True,supas,'None'],[sample_size + 'sdsscorr',False,supas,'sdss'],[sample_size + 'sdssuncorr',False,supas,'sdss']]\n\n                runs = [[sample_size,True,supas,'sdss',True],[sample_size + 'None',True,supas,'None', False],[sample_size + 'sdsscorr',False,supas,'sdss',False],[sample_size + 'sdssuncorr',False,supas,'sdss',False]]\n\n                runs = [[sample_size,True,supas,'sdss',True],[sample_size + 'None',True,supas,'None', False],[sample_size + 'sdsscorr',False,supas,'sdss',False],[sample_size + 'sdssuncorr',False,supas,'sdss',False]]\n                #runs = [[sample_size,True,supas,'sdss',True]]\n            else:\n                runs = [[sample_size,True,supas,sample_copy,False]]\n\n        elif sample_size != 'all': \n            if totalstars > 60000:                                                                                   \n                print len(supas)                                                      \n                #l = range(len(supas_copy))                     \n                ''' shorten star_good, supas '''\n                print totalstars, len(supas)\n                #supas = [supas_copy[i] for i in l[0:int(float(30000)/float(totalstars)*len(supas))]]\n                import copy \n                supas_short = copy.copy(supas_copy[0:int(float(60000)/float(totalstars)*len(supas_copy))])\n            else: \n                import copy\n                supas_short = copy.copy(supas_copy)\n\n            ''' take a random sample of half ''' \n            ## changing the CLASS_STAR criterion upwards helps as does increasing the sigma on the SDSS stars\n            print len(supas_short)\n            l = range(len(supas_short))                     \n            print l[0:10]\n            l.sort(random_cmp)\n            print l[0:10]\n            ''' shorten star_good, supas '''\n            print len(supas_short), 'supas_short'\n            supas = [supas_short[i] for i in l[0:len(supas_short)/2]]\n            ''' make the complement '''\n            supas_complement = [supas_short[i] for i in l[len(supas_short)/2:]]\n            runs = [[sample_size,True,supas,sample_copy,True],[sample_size + 'corr',False,supas_complement,sample_copy,True],[sample_size + 'uncorr',False,supas_complement,sample_copy,True]]\n\n        print len(supas), 'supas', supas[0], totalstars\n        print sample_size, match, sample\n        print len(supas_copy), len(supas)\n        print supas[0:10]\n\n        for sample_size, calc_illum, supas,  sample , try_linear in runs:\n\n            #tab = copy.copy(tab_copy )\n\n            print info['match']\n            if try_linear:\n                if info['match'] > 600:\n                    samples = [['match','cheby_terms',True]]\n                    print 'all terms'\n                else:\n                    samples = [['match','cheby_terms_no_linear',True]]\n                    print 'no linear terms'\n            else: \n                samples = [['nomatch','cheby_terms_no_linear',False]]\n                                                                       \n\n\n            for hold_sample,which_terms,sample2 in samples:\n                \n                cheby_terms_use = locals()[which_terms] \n\n                print 'sample', sample\n\n                ''' if random, run first with one half, then the other half, applying the correction '''                                                                                                            \n                columns = []\n                column_dict = {}\n                \n                ''' position-dependent terms in design matrix '''\n                position_columns = []\n                index = -1\n                                                                                                                                                                                                                    \n                if calc_illum: \n                    for ROT in EXPS.keys():                                                                                     \n                        for term in cheby_terms_use:\n                            index += 1\n                            name = str(ROT) + '$' + term['n'] # + reduce(lambda x,y: x + 'T' + y,term)\n                            position_columns.append({'name':name,'fx':term['fx'],'fy':term['fy'],'rotation':ROT,'index':index})\n                    columns += position_columns\n                \n                ''' zero point terms in design matrix '''\n                per_chip = False # have a different zp for each chip on each exposures \n                same_chips =   True# have a different zp for each chip but constant across exposures\n                                                                                                                                                                                                                    \n                if not per_chip:\n                    zp_columns = []                                                             \n                    for ROT in EXPS.keys():\n                        for exp in EXPS[ROT]:\n                            index += 1\n                            zp_columns.append({'name':'zp_image_'+exp,'image':exp,'im_rotation':ROT,'index':index})\n                                                                                                                                                                                                                    \n                if per_chip:\n                    zp_columns = []                                                             \n                    for ROT in EXPS.keys():\n                        for exp in EXPS[ROT]:\n                            for chip in CHIPS:\n                                index += 1\n                                zp_columns.append({'name':'zp_image_'+exp + '_' + chip,'image':exp,'im_rotation':ROT, 'chip':chip,'index':index})\n                                                                                                                                                                                                                    \n                #if False: # CONFIG == '10_3':\n                #    first_empty = 0\n                #    for chip in CHIPS:                                                                                                                   \n                #        for sub_chip in [1,2,3,4]:\n                #            if first_empty != 0:\n                #                index += 1  \n                #                zp_columns.append({'name':'zp_'+str(chip)+'_'+str(sub_chip),'image':'chip_zp','chip':str(chip)+'_'+str(sub_chip),'index':index})\n                #            else: first_empty = 1\n                #else:                 \n                if calc_illum and not per_chip and same_chips:                                                                 \n                    for chip in CHIPS:\n                        index += 1\n                        zp_columns.append({'name':'zp_'+str(chip),'image':'chip_zp','chip':chip,'index':index})\n                                                                                                                                                                                                                    \n                if match:\n                    index += 1\n                    zp_columns.append({'name':'zp_SDSS','image':'match','index':index})\n                columns += zp_columns\n                                                                                                                                                                                                                    \n                                                                                                                                                                                                                    \n                import os \n                os.system('pwd')\n                import config_bonn\n                reload(config_bonn)\n                from config_bonn import chip_groups \n                color_columns = []\n                if match: \n                    if relative_colors:                                                                                      \n                        ''' add chip dependent color terms'''\n                        for group in chip_groups[str(CONFIG)].keys():\n                            ''' this is the relative color term, so leave out the first group '''\n                            if float(group) != 1:\n                                index += 1                                                                                                     \n                                color_columns.append({'name':'color_group_'+str(group),'image':'chip_color','chip_group':group,'index':index})\n                    ''' add a color term for the catalog '''\n                    index += 1\n                    color_columns+=[{'name':'SDSS_color','image':'match_color_term','index':index, 'chip_group':[]}]\n                                                                                                                                                                                                                    \n                                                                                                                                                                                                                    \n                else: color_columns = []\n                columns += color_columns\n                print color_columns, match, \n                \n                mag_columns = []\n                for star in supas:\n                    mag_columns.append({'name':'mag_' + str(star['table index'])})\n                columns += mag_columns\n                                                                                                                                                                                                                    \n                print len(columns)\n                \n                column_names = [x['name'] for x in columns] #reduce(lambda x,y: x+y,columns)] \n                print column_names[0:100]\n                \n                ''' total number of fit parameters summed over each rotation + total number of images of all rotations + total number of stars to fit '''\n                                                                                                                                                                                                                    \n                tot_exp = 0\n                for ROT in EXPS.keys():\n                    for ele in EXPS[ROT]:\n                        tot_exp += 1\n                                                                                                                                                                                                                    \n                x_length = len(position_columns) + len(zp_columns) + len(color_columns) + len(mag_columns) \n                print len(columns), x_length\n                x_length = len(columns)\n                y_length = reduce(lambda x,y: x + y,[len(star['supa files'])*2 for star in supas]) # double number of rows for SDSS\n                print x_length, y_length\n                print star['supa files']\n                print \n                Bstr = ''                   \n                row_num = -1\n                supa_num = -1\n                ''' each star '''\n                print 'creating matrix....'\n                sigmas = []\n                inst = []\n                data = {} \n                magErr = {} \n                whichimage = {}\n                X = {} \n                Y = {} \n                color = {}\n                chipnums = {}\n                Star = {}\n                catalog_values = {}\n                for ROT in EXPS.keys():\n                    data[ROT] = []\n                    magErr[ROT] = []\n                    X[ROT] = []\n                    Y[ROT] = []\n                    color[ROT] = []\n                    whichimage[ROT] = []\n                    chipnums[ROT] = []\n                    Star[ROT] = []\n                                                                                                                                                                                                                    \n                chip_dict = {}\n                                                                                                                                                                                                                    \n                x_positions = {}\n                y_positions = {}\n                                                                                                                                                                                                                    \n                for star in supas:   \n                    #print star['match']\n                    #if star['match']: raw_input()\n                    supa_num += 1\n                    ''' each exp of each star '''\n                    if 1:\n                        star_A = []\n                        star_B = []\n                        star_B_cat = []\n                        sigmas = []\n                        for exp in star['supa files']:                                                                                              \n                            row_num += 1           \n                            col_num = -1 \n                            rotation = exp['rotation'] \n                            x = tab[str(rotation) + '$' + exp['name'] + '$Xpos_ABS'][star['table index']]\n                            y = tab[str(rotation) + '$' + exp['name'] + '$Ypos_ABS'][star['table index']]\n                                                                                                                                                                                                                    \n                            x_rel = tab[str(rotation) + '$' + exp['name'] + '$Xpos'][star['table index']]\n                            y_rel = tab[str(rotation) + '$' + exp['name'] + '$Ypos'][star['table index']]\n                                                                                                                                                                                                                    \n                            if False: #CONFIG == '10_3':\n                                from config_bonn import chip_divide_10_3    \n                                chip_num = int(tab[str(rotation) + '$' + exp['name'] + '$CHIP'][star['table index']] )\n                                for div in chip_divide_10_3.keys():\n                                    if chip_divide_10_3[div][0] < x_rel <= chip_divide_10_3[div][1]:\n                                        sub_chip = div \n                                chip = str(chip_num) + '_' + str(sub_chip)\n                            else:\n                                chip = int(tab[str(rotation) + '$' + exp['name'] + '$CHIP'][star['table index']] )\n                                if not chip_dict.has_key(str(chip)):\n                                    chip_dict[str(chip)] = ''\n                                    print chip_dict.keys(), CHIPS\n                                                                                                                                                                                                                    \n                            #print CONFIG, CONFIG == '10_3'\n                            #print chip_div, x_rel, y_rel\n                                                                                                                                                                                                                    \n                            #if x < 2000 or y < 2000 or abs(LENGTH1 - x) < 2000 or abs(LENGTH2 - y) < 2000:\n                            #    sigma = 1.5 * tab[str(rotation) + '$' + exp['name'] + '$MAGERR_AUTO'][star['table index']] \n                            #else:\n                            sigma = tab[str(rotation) + '$' + exp['name'] + '$MAGERR_AUTO'][star['table index']] \n                                                                                                                                                                                                                    \n                            if sigma < 0.001: sigma = 0.001\n                            sigma = sigma # * 1000. \n                            #sigma = 1\n                                                                                                                                                                                                                    \n                            n = str(rotation) + '$' + exp['name'] + '$Xpos_ABS'\n                            x = tab[str(rotation) + '$' + exp['name'] + '$Xpos_ABS'][star['table index']]\n                            y = tab[str(rotation) + '$' + exp['name'] + '$Ypos_ABS'][star['table index']]\n                            x_positions[row_num] = x\n                            y_positions[row_num] = y\n                            x = coord_conv_x(x)\n                            y = coord_conv_y(y)\n                                                                                                                                                                                                                    \n                            if calc_illum:\n                                for c in position_columns:                                                            \n                                    col_num += 1\n                                    if c['rotation'] == rotation:\n                                        value = c['fx'](x,y)*c['fy'](x,y)/sigma\n                                        star_A.append([row_num,col_num,value])\n                                                                                                                                                                                                                    \n                            first_exposure = True \n                            for c in zp_columns:\n                                col_num += 1\n                                #if not degeneracy_break[c['im_rotation']] and c['image'] == exp['name']:\n                                if not per_chip:\n                                    if (first_exposure is not True  and c['image'] == exp['name']):  \n                                        value = 1./sigma\n                                        star_A.append([row_num,col_num,value])\n                                    if calc_illum and same_chips and c.has_key('chip'):\n                                        if (c['chip'] == chip) and chip != CHIPS[0]:  \n                                            value = 1./sigma                       \n                                            star_A.append([row_num,col_num,value])\n                                    first_exposure = False\n                                #if per_chip:\n                                #    if (first_column is not True and c['image'] == exp['name'] and c['chip'] == chip):  \n                                #        value = 1./sigma\n                                #        star_A.append([row_num,col_num,value])\n                           \n                                                                                                                                                                                                                    \n                            ''' fit for the color term dependence for SDSS comparison '''\n                            if match:\n                                if relative_colors:\n                                    for c in color_columns:                            \n                                        col_num += 1\n                                        for chip_num in c['chip_group']:    \n                                            if float(chip_num) == float(chip):\n                                                value = tab['SDSSstdMagColor_corr'][star['table index']]/sigma\n                                                star_A.append([row_num,col_num,value])\n                                else:\n                                    col_num += 1\n                           \n                                                                                                                                                                                                                    \n                            ''' magnitude column -- include the correct/common magnitude '''\n                            col_num += 1\n                            value = 1./sigma\n                            star_A.append([row_num,col_num+supa_num,value])\n                            ra = tab[str(rotation) + '$' + exp['name'] + '$ALPHA_J2000'][star['table index']]\n                            dec = tab[str(rotation) + '$' + exp['name'] + '$DELTA_J2000'][star['table index']]\n\n                            if calc_illum or string.find(sample_size,'uncorr') != -1:\n                                value = tab[str(rotation) + '$' + exp['name'] + '$MAG_AUTO'][star['table index']]/sigma\n                            elif not calc_illum:\n                                ''' correct the input magnitudes using the previously fitted correction '''\n                                epsilon=0\n                                for term in cheby_terms_use:\n                                    epsilon += fitvars[str(rotation)+'$'+term['n']]*term['fx'](x,y)*term['fy'](x,y)\n                                epsilon += float(fitvars['zp_' + str(chip)])\n                                value = (tab[str(rotation) + '$' + exp['name'] + '$MAG_AUTO'][star['table index']] - epsilon)/sigma\n                                #print epsilon, value\n\n                            star_B.append([row_num,value])\n                            sigmas.append([row_num,sigma])\n                                                                                                                                                                                                                    \n                                                                                                                                                                                                                    \n                            catalog_values[col_num+supa_num] = {'inst_value':value*sigma,'ra':ra,'dec':dec,'sigma':sigma} # write into catalog\n                            #print catalog_values, col_num+supa_num\n                                                                                                                                                                                                                    \n                            #x_long = tab[str(rotation) + '$' + exp['name'] + '$Xpos_ABS'][star['table index']]\n                            #y_long = tab[str(rotation) + '$' + exp['name'] + '$Ypos_ABS'][star['table index']]\n                            #x = coord_conv_x(x_long)\n                            #y = coord_conv_y(y_long)\n                            #if fitvars_fiducial:\n                            #    value += add_single_correction(x,y,fitvars_fiducial)\n                                                                                                                                                                                                                    \n                        inst.append({'type':'match','A_array':star_A, 'B_array':star_B, 'sigma_array': sigmas})\n                                           \n                    ''' only include one SDSS observation per star '''\n                                                                                                                                                                                                                    \n                    \n                    #print sample, star['match'], star['match'] and (sample=='all' or sample=='sdss' or sample=='bootstrap') # and tab['SDSSStar_corr'][star['table index']] == 1\n                    if star['match'] and (sample=='sdss' or sample=='bootstrap'): # and tab['SDSSStar_corr'][star['table index']] == 1:\n                                    \n                        star_A = []\n                        star_B = []\n                        sigmas = []\n                        ''' need to filter out bad colored-stars '''\n                        if 1: \n                            row_num += 1                                                                                                                                      \n                            col_num = -1 \n                            exp = star['supa files'][0]\n                            rotation = exp['rotation'] \n                            sigma = tab['SDSSstdMagErr_corr'][star['table index']]\n                            if sigma < 0.03: sigma = 0.03\n                                                                                                                                                                                                                    \n                            for c in position_columns:\n                                col_num += 1\n                            first_column = True\n                            for c in zp_columns:\n                                col_num += 1\n                                ''' remember that the good magnitude does not have any zp dependence!!! '''\n                                if c['image'] == 'match': \n                                    value = 1./sigma\n                                    star_A.append([row_num,col_num,value])\n                                    x_positions[row_num] = x\n                                    y_positions[row_num] = y\n                                                                                                                                                                                                                    \n                                first_column = False\n                                                                                                                                                                              \n                            ''' fit for the color term dependence for SDSS comparison -- '''\n                            if relative_colors:\n                                for c in color_columns:                            \n                                    col_num += 1\n                                    if c['name'] == 'SDSS_color':\n                                        value = tab['SDSSstdMagColor_corr'][star['table index']]/sigma\n                                        star_A.append([row_num,col_num,value])\n                            else:\n                                col_num += 1\n                                value = tab['SDSSstdMagColor_corr'][star['table index']]/sigma\n                                star_A.append([row_num,col_num,value])\n                                                                                                                                                                                                                    \n                            col_num += 1\n                            ''' magnitude column -- include the correct/common magnitude '''\n                            value = 1./sigma\n                            star_A.append([row_num,col_num+supa_num,value])\n                                                                                                                                                                                                                    \n                            value = tab['SDSSstdMag_corr'][star['table index']]/sigma\n                            star_B.append([row_num,value])\n                            sigmas.append([row_num,sigma])\n                            \n                            inst.append({'type':'sdss','A_array':star_A, 'B_array':star_B, 'sigma_array': sigmas})\n                            \n                            ''' record star information '''\n                            if True:\n                                for exp in star['supa files']:\n\n                                    rotation = exp['rotation'] \n                                    x = tab[str(rotation) + '$' + exp['name'] + '$Xpos_ABS'][star['table index']]\n                                    y = tab[str(rotation) + '$' + exp['name'] + '$Ypos_ABS'][star['table index']]\n\n                                    x = coord_conv_x(x)\n                                    y = coord_conv_y(y)\n\n                                    if calc_illum or string.find(sample_size,'uncorr') != -1:                                               \n                                        value = tab[str(rotation) + '$' + exp['name'] + '$MAG_AUTO'][star['table index']]\n                                    elif not calc_illum:\n                                        ''' correct the input magnitudes using the previously fitted correction '''\n                                        epsilon=0\n                                        for term in cheby_terms_use:\n                                            epsilon += fitvars[str(rotation)+'$'+term['n']]*term['fx'](x,y)*term['fy'](x,y)\n                                        epsilon += float(fitvars['zp_' + str(chip)])\n                                        value = (tab[str(rotation) + '$' + exp['name'] + '$MAG_AUTO'][star['table index']] - epsilon)\n\n\n\n\n\n\n\n                                    rotation = str(exp['rotation'])\n                                    data[rotation].append(value - tab['SDSSstdMag_corr'][star['table index']]) \n                                    Star[rotation].append(tab['SDSSStar_corr'][star['table index']])\n                                    magErr[rotation].append(tab['SDSSstdMagErr_corr'][star['table index']])\n                                    whichimage[rotation].append(exp['name'])\n                                    X[rotation].append(tab[str(rotation) + '$' + exp['name'] + '$Xpos_ABS'][star['table index']])\n                                    Y[rotation].append(tab[str(rotation) + '$' + exp['name'] + '$Ypos_ABS'][star['table index']])\n                                    color[rotation].append(tab['SDSSstdMagColor_corr'][star['table index']])\n                                    chipnums[rotation].append(tab[str(rotation) + '$' + exp['name'] + '$CHIP'][star['table index']])\n                                    #if tab[str(rotation) + '$' + exp['name'] + '$CHIP'][star['table index']] == 1:\n                                    #    print str(rotation) + '$' + exp['name'] + '$CHIP'\n                            #print star_A, star_B, sigmas, sigma\n                print data.keys()\n                #print len(data['0']) \n                print EXPS\n                for rot in EXPS.keys():\n                    print data.keys()\n                    print rot, len(data[str(rot)])\n                                                                                                                                                                                                                    \n                ''' save the SDSS matches '''\n                matches = {'data':data,'magErr':magErr,'whichimage':whichimage,'X':X,'Y':Y,'color':color}\n                                                                                                                                                                                                                    \n                uu = open(tmpdir + '/sdss','w')\n                import pickle\n                pickle.dump(matches,uu)\n                uu.close()\n                                                                                                                                                                                                                    \n                ''' do fitting '''\n                #if 1: #not quick:\n                for attempt in ['first','rejected']:\n                    ''' make matrices/vectors '''                                                                                                                           \n                    Ainst_expand = []\n                    for z in inst:\n                        for y in z['A_array']:\n                            Ainst_expand.append(y)\n                                                                                                                                                                            \n                    Binst_expand = []\n                    for z in inst:\n                        for y in z['B_array']:\n                            Binst_expand.append(y)\n                    print len(Binst_expand)\n                    ''' this gives the total number of rows added '''\n                                                                                                                                                                            \n                    sigmas_expand = []\n                    for z in inst:\n                        for y in z['sigma_array']:\n                            sigmas_expand.append(y)\n                    print len(sigmas_expand)\n                                                                                                                                                                            \n                    ylength = len(Binst_expand)\n                    print y_length, x_length\n                    print len(Ainst_expand), len(Binst_expand)\n                    print 'lengths'\n                    A = scipy.zeros([y_length,x_length])\n                    B = scipy.zeros(y_length)\n                    S = scipy.zeros(y_length)\n                                                                                                                                                                                                                    \n                    import copy\n                    if attempt == 'first': rejectlist = 0*copy.copy(B)\n                                                                                                                                                                            \n                    Af = open('A','w')\n                    Bf = open('b','w')\n                                                                                                                                                                                                                    \n                    rejected = 0\n                    rejected_x = []\n                    rejected_y = []\n                    all_x = []\n                    all_y = []\n                    all_resids = []\n                    if attempt == 'rejected':\n                        for ele in Ainst_expand:                                                      \n                            if rejectlist[ele[0]] == 0: \n                                if x_positions.has_key(ele[0]) and y_positions.has_key(ele[0]):  \n                                    all_x.append(float(str(x_positions[ele[0]]))) \n                                    all_y.append(float(str(y_positions[ele[0]])))\n                                    all_resids.append(float(str(resids_sign[ele[0]])))\n                            if rejectlist[ele[0]] == 0: \n                                Af.write(str(ele[0]) + ' ' + str(ele[1]) + ' ' + str(ele[2]) + '\\n') \n                                #print ele, y_length, x_length\n                                #print ele\n                                A[ele[0],ele[1]] = ele[2]\n                            else: \n                                rejected += 1\n                                if x_positions.has_key(ele[0]) and y_positions.has_key(ele[0]): \n                                    rejected_x.append(float(str(x_positions[ele[0]])))\n                                    rejected_y.append(float(str(y_positions[ele[0]])))\n                    else:\n                        for ele in Ainst_expand:                                                      \n                            Af.write(str(ele[0]) + ' ' + str(ele[1]) + ' ' + str(ele[2]) + '\\n') \n                            #print ele, y_length, x_length\n                            #print ele \n                            A[ele[0],ele[1]] = ele[2]\n                                                                                                                                                                                                                    \n                    for ele in Binst_expand:\n                        if rejectlist[ele[0]] == 0: \n                            B[ele[0]] = ele[1]\n\n                    for ele in sigmas_expand:\n                        if rejectlist[ele[0]] == 0: \n                            S[ele[0]] = ele[1]\n\n                    if attempt == 'rejected' and rejected > 0:\n                        print rejected, 'rejected' \n                                                                                                                                                                                                                    \n                        path='/nfs/slac/g/ki/ki05/anja/SUBARU/%(OBJNAME)s/' % {'OBJNAME':OBJNAME}\n                        illum_dir = path + 'PHOTOMETRY/ILLUMINATION/' + FILTER + '/' + PPRUN + '/'\n                        import Numeric\n                        print all_resids[0:100]\n                        print all_x[0:100] \n                        print all_y[0:100] \n                        print 'check'\n                                                                                                                                                                                                                    \n                        os.system('mkdir -p ' + illum_dir)\n                        calcDataIllum(sample + 'reducedchi'+str(ROT)+FILTER,LENGTH1,LENGTH2,Numeric.array(all_resids),Numeric.ones(len(all_resids)),Numeric.array(all_x),Numeric.array(all_y),pth=illum_dir,rot=0) \n                                                                                                                                                                                                                    \n                        dtmp = {} \n                        dtmp['rejected']=rejected\n                        dtmp['totalmeasurements']=rejected\n                                                                                                                                                                                                                    \n                        import Numeric \n                        import ppgplot\n                        print rejected_x, rejected\n                        x_p = Numeric.array(rejected_x)\n                        y_p = Numeric.array(rejected_y)\n                                                                                                                                                                                                                    \n                        import copy\n                        x = sorted(copy.copy(x_p))\n                        y = sorted(copy.copy(y_p))\n                                                                                                                                                                                                                    \n                        illum_dir = path + 'PHOTOMETRY/ILLUMINATION/' + FILTER + '/' + PPRUN + '/'\n                        import os\n                        os.system('mkdir ' + illum_dir)\n                        reject_plot = illum_dir + sample + sample_size + 'rejects.ps'\n                                                                                                                                                                                                                    \n                        dtmp['reject_plot']=reject_plot\n                                                                                                                                                                                                                    \n                        dtmp.update({'PPRUN':PPRUN,'FILTER':FILTER,'OBJNAME':OBJNAME,'sample':sample,'sample_size':sample_size,'linearfit':'1'})\n                        save_fit(dtmp)\n                            \n                        import tempfile \n                        t = tempfile.NamedTemporaryFile(dir='/tmp/').name \n                                                                                                                                                                                                                    \n                        ppgplot.pgbeg(t + '/cps',1,1)                                       \n                        ppgplot.pgiden()\n                                                                 \n                        #print x_p\n                        #print z_p \n                        #print zerr_p\n                                                                 \n                        #pgswin(x[0],x[-1],z[0],z[-1])\n                                                                 \n                        ### plot positions\n                        ppgplot.pgpanl(1,1)\n                        ppgplot.pgswin(x[0],x[-1],y[0],y[-1])\n                        ppgplot.pgbox()\n                        ppgplot.pglab('X','Y','rejected points')     # label the plot\n                        #pgsci(3)\n                        #pgerrb(6,x_p,z_p,zerr_p)\n                        print x_p[0:100], y_p[0:100]\n                        print type(x_p), type(y_p)\n                                                                                                                                                                                                                    \n                        print 'plotting'\n                        ppgplot.pgpt(x_p,y_p,3)\n                        print 'plotted'\n                                                                 \n                        ppgplot.pgend()\n                        print reject_plot\n                        os.system('mv ' + t + ' ' + reject_plot)\n                                                                                                                                                                                                                    \n                    Bstr = reduce(lambda x,y:x+' '+y,[str(z[1]) for z in Binst_expand])\n                    Bf.write(Bstr)\n                    Bf.close()\n                    Af.close()\n                                                                                                                                                                            \n                    print 'finished matrix....'\n                    print len(position_columns), len(zp_columns)\n                    print A[0,0:30], B[0:10], scipy.shape(A), scipy.shape(B)\n                    print A[1,0:30], B[0:10], scipy.shape(A), scipy.shape(B)\n                    print 'hi!'\n                                                                                                                                                                            \n                    Af = open(tmpdir + '/B','w')\n                    for i in range(len(B)):\n                        Af.write(str(B[i]) + '\\n')\n                    Af.close()\n                    \n                    print 'solving matrix...'\n                    import re, os\n                    os.system('rm x')\n                    os.system('sparse < A')\n                    bout = open('x','r').read()\n\n                    res = re.split('\\s+',bout[:-1])\n                    Traw = [float(x) for x in res][:x_length]\n\n\n                    res = re.split('\\s+',bout[:-1].replace('nan','0').replace('inf','0'))\n                    T = [float(x) for x in res][:x_length]\n\n                                                                                                                                                                                                                    \n                    params = {}\n                    for i in range(len(T)):\n                        if i < len(column_names):\n                            params[column_names[i]] = T[i]\n                            import string\n                            if string.find(column_names[i],'mag') == -1:\n                                print column_names[i], T[i]\n                                print column_names[i], Traw[i]\n                            if T[i] == -99:\n                                print column_names[i], T[i]\n                        if catalog_values.has_key(i):\n                            catalog_values[i]['mag'] = T[i]\n\n                                                                                                                                                                                                                    \n                    U = [float(x) for x in res][:x_length]\n                                                                                                                                                                                                                    \n                    print 'finished solving...'\n                    \n                    #from scipy import linalg\n                    #print 'doing linear algebra'\n                    #U = linalg.lstsq(A,B)\n                    #print U[0][0:30]\n                    \n                    ''' calculate reduced chi-squared value'''\n                    print scipy.shape(A), len(U), x_length, len(res)\n                    Bprime = scipy.dot(A,U)  \n                    print scipy.shape(Bprime),scipy.shape(B)\n\n                    ''' number of free parameters is the length of U '''\n                    Bdiff = (abs((B-Bprime)**2.)).sum()/(len(B) - len(U))\n                    resids = abs(B-Bprime)\n                    resids_sign = B-Bprime\n                    rejectlist = [] \n                    rejectnums = 0\n                    for i in range(len(resids)): \n                        if resids[i] > 5: \n                            rejectlist.append(1) \n                            rejectnums += 1\n                        else: rejectlist.append(0)\n                    print (B-Bprime)[:300]\n                    print len(resids), rejectnums\n                    print U[0:20]\n                    print x[0:20]\n                    reducedchi = Bdiff\n\n                    ''' number of free parameters is the length of U , number of data points is B '''\n                    difference = (abs(abs((B-Bprime)*S))).sum()/len(B)\n                    print difference, 'difference'\n                    \n                    print reducedchi, 'reduced chi-squared'\n                    #save_fit({'PPRUN':PPRUN,'FILTER':FILTER,'OBJNAME':OBJNAME,'reducedchi$'+sample+'$'+sample_size:Bdiff})\n                                                                                                 \n                    data_directory = '/nfs/slac/g/ki/ki04/pkelly/illumination/'\n                                                                                                \n                    position_fit = [['Xpos_ABS','Xpos_ABS'],['Xpos_ABS','Ypos_ABS'],['Ypos_ABS','Ypos_ABS'],['Xpos_ABS'],['Ypos_ABS']]\n                    import re\n                    ''' save fit information '''\n                    #print  sample+'$'+sample_size+'$' + str(ROT) + '$positioncolumns',reduce(lambda x,y: x+','+y,[z['name'] for z in position_columns]) \n                                                                                                                                                                                                                    \n                                                                                                                                                                                                                    \n                    if match: \n                        save_columns = position_columns + zp_columns + color_columns \n                    else:\n                        save_columns = position_columns + zp_columns\n                   \n                    dtmp = {}\n                    o = zp_columns + position_columns\n                    #for ROT in EXPS.keys():\n                        #dtmp['zp_' + ROT] = params['zp_' + ROT]\n                    fitvars = {}\n                    zp_images = ''\n                    zp_images_names = ''\n                    for ele in save_columns:                      \n                        print ele\n                        res = re.split('$',ele['name'])\n                        import string\n                        ''' save to own column if not an image zeropoint '''\n                        if string.find(ele['name'],'zp_image') == -1:\n                            fitvars[ele['name']] = U[ele['index']]             \n                            term_name = ele['name']\n                            print term_name\n                            dtmp[term_name]=fitvars[ele['name']]\n                            print ele['name'], fitvars[ele['name']]\n                        else:\n                            zp_images += str(U[ele['index']]) + ','\n                            zp_images_names += ele['name'] + ','\n                    \n                    print 'save_columns', save_columns, \n                    print 'zp_columns', zp_columns\n                                                                                                                                                                                                                    \n                                                                                                                                                                                                                    \n                    zp_images = zp_images[:-1]\n                    zp_images_names = zp_images_names[:-1]\n                                                                                                                                                                                                                    \n                    term_name = 'zp_images'\n                    print term_name\n                    dtmp[term_name]=zp_images\n                    print dtmp[term_name]\n                    \n                    term_name = 'zp_images_names'\n                    print term_name\n                    dtmp[term_name]=zp_images_names\n                    print dtmp[term_name]\n                                                                                                                                                                                                                    \n                    import string\n                                                                                                                                                                                                                    \n                    print dtmp.keys()\n                    use_columns = filter(lambda x: string.find(x,'zp_image') == -1,[z['name'] for z in save_columns] ) + ['zp_images','zp_images_names']\n                                                                                                                                                                                                                    \n                    positioncolumns = reduce(lambda x,y: x+','+y,use_columns)\n                                                                                                                                                                                                                    \n                    print positioncolumns\n                    #save_fit({'PPRUN':PPRUN,'FILTER':FILTER,'OBJNAME':OBJNAME,sample+'$'+sample_size+'$positioncolumns':positioncolumns})\n                    dtmp['positioncolumns'] = positioncolumns\n                    dtmp[attempt + 'reducedchi']=reducedchi\n                    dtmp[attempt + 'difference']=difference\n                    \n                    #term_name = sample+'$'+sample_size+'$0x1y'\n                    #print term_name, '!!!!!'\n                    #if 0: \n                        #print fitvars['1$0x1y'], '1$0x1y'            \n                        #term_name = sample+'$'+sample_size+'$1$0x1y'\n                        #dtmp[term_name] = 1.\n                        #term_name = sample+'$'+sample_size+'$0$1x0y'\n                        #dtmp[term_name] = 1.\n                        #fitvars['1$0x1y'] = 1.\n                        #fitvars['0$1x0y'] = 1.\n                        #print fitvars\n                    \n                    print dtmp.keys()\n                    print 'stop'\n                    print dtmp['positioncolumns'], 'positioncolumns', PPRUN, FILTER, OBJNAME\n                    dtmp.update({'PPRUN':PPRUN,'FILTER':FILTER,'OBJNAME':OBJNAME,'sample':sample,'sample_size':sample_size,'linearfit':'1'})\n                    print dtmp\n                    \n                    save_fit(dtmp)\n                                                                                                                                                                                                                    \n                if 1:\n                    ''' save the corrected catalog '''\n                    \n                    tmp = {}    \n                    import astropy, astropy.io.fits as pyfits\n                    cols = [] \n                                                                                                                                                                                                                    \n                    stdMag_corr = []\n                    stdMagErr_corr = []\n                    stdMagColor_corr = []\n                    stdMagClean_corr = []\n                    ALPHA_J2000 = []\n                    DELTA_J2000 = []\n                    SeqNr = []\n                    Star_corr = []\n                    sn = -1\n                                                                                                                                                                                                                    \n                    for i in catalog_values.keys():\n                        entr = catalog_values[i]\n                        sn += 1\n                        SeqNr.append(sn)\n                        stdMag_corr.append(entr['mag'])\n                        ALPHA_J2000.append(entr['ra'])\n                        DELTA_J2000.append(entr['dec'])\n                        stdMagErr_corr.append(entr['sigma'])\n                        stdMagColor_corr.append(0)\n                        stdMagClean_corr.append(1)\n                        Star_corr.append(1)\n                        #print ALPHA_J2000\n\n                    print 'data start'\n                    import Numeric \n                    cols.append(pyfits.Column(name='stdMag_corr', format='D',array=Numeric.array(stdMag_corr)))\n                    cols.append(pyfits.Column(name='stdMagErr_corr', format='D',array=Numeric.array(stdMagErr_corr)))\n                    cols.append(pyfits.Column(name='stdMagColor_corr', format='D',array=Numeric.array(stdMagColor_corr)))\n                    cols.append(pyfits.Column(name='stdMagClean_corr', format='D',array=Numeric.array(stdMagClean_corr)))\n                    cols.append(pyfits.Column(name='ALPHA_J2000', format='D',array=Numeric.array(ALPHA_J2000)))\n                    cols.append(pyfits.Column(name='DELTA_J2000', format='D',array=Numeric.array(DELTA_J2000)))\n                    cols.append(pyfits.Column(name='SeqNr', format='E',array=Numeric.array(SeqNr)))\n                    cols.append(pyfits.Column(name='Star_corr', format='E',array=Numeric.array(Star_corr)))\n                   \n                    path='/nfs/slac/g/ki/ki05/anja/SUBARU/%(OBJNAME)s/' % {'OBJNAME':OBJNAME}\n                    outcat = path + 'PHOTOMETRY/ILLUMINATION/' + 'catalog_' + PPRUN + '.cat'\n                    print cols\n                    hdu = pyfits.PrimaryHDU()\n                    hdulist = pyfits.HDUList([hdu])\n                    tbhu = pyfits.BinTableHDU.from_columns(cols)\n                    hdulist.append(tbhu)\n                    hdulist[1].header['EXTNAME']='OBJECTS'\n                    os.system('rm ' + outcat)\n                    hdulist.writeto( outcat )\n                    print 'wrote out new cat'\n                    print outcat\n\n                    save_fit({'PPRUN':PPRUN,'OBJNAME':OBJNAME,'FILTER':FILTER,'format':'good','sample':'record','sample_size':'record'},db='try_db')\n\n                    save_fit({'FILTER':FILTER,'OBJNAME':OBJNAME,'PPRUN':PPRUN,'sample':sample,'sample_size':sample_size,'catalog':outcat})\n                    #save_exposure({type + 'atch':outcat},SUPA,FLAT_TYPE)\n                    #tmp[type + 'sdssmatch'] = outcat\n                \n                ''' make diagnostic plots '''\n                if string.find(sample_size,'rand') == -1:                                                                                                                                 \n                    import re, time\n                    d = get_fits(OBJNAME,FILTER,PPRUN, sample, sample_size)                \n                    print d.keys()\n                    column_prefix = '' #sample+'$'+sample_size+'$'\n                    position_columns_names = re.split('\\,',d[column_prefix + 'positioncolumns']) \n                    print position_columns_names, 'position_columns_names'\n                    fitvars = {}\n                    cheby_terms_dict = {}\n                    print column_prefix, position_columns_names\n                    for ele in position_columns_names:                      \n                        print ele\n                        if type(ele) != type({}):\n                            ele = {'name':ele}\n                        res = re.split('$',ele['name'])\n                        if string.find(ele['name'],'zp_image') == -1:\n                            fitvars[ele['name']] = float(d[ele['name']]) \n                            for term in cheby_terms:\n                                if term['n'] == ele['name'][2:]:\n                                    cheby_terms_dict[term['n']] = term \n                                                                                                                                                                                                                    \n                    zp_images = re.split(',',d['zp_images'])\n                    zp_images_names = re.split(',',d['zp_images_names'])\n                                                                                                                                                                                                                    \n                    for i in range(len(zp_images)):\n                        fitvars[zp_images_names[i]] = float(zp_images[i])\n                                                                                                                                                                                                                    \n                    print fitvars\n                                                                                                                                                                                                                    \n                                                                                                                                                                                                                    \n                                                                                                                                                                                                                    \n                                                                                                                                                                                                                    \n                    cheby_terms_use =  [cheby_terms_dict[k] for k in cheby_terms_dict.keys()]\n                                                                                                                                                                                                                    \n                    print cheby_terms_use, fitvars\n                                                                                                                                                                                                                    \n                    ''' make images of illumination corrections '''                                                                  \n                    if calc_illum:\n                        for ROT in EXPS.keys():      \n                            size_x=LENGTH1\n                            size_y=LENGTH2 \n                            bin=100\n                            import numpy, math, pyfits, os                                                                              \n                            x,y = numpy.meshgrid(numpy.arange(0,size_x,bin),numpy.arange(0,size_y,bin))\n                            F=0.1\n                            print 'calculating'\n                            x = coord_conv_x(x)\n                            y = coord_conv_y(y)\n                                                                                                                                                                            \n                            path='/nfs/slac/g/ki/ki05/anja/SUBARU/%(OBJNAME)s/' % {'OBJNAME':OBJNAME}\n                            illum_dir = path + 'PHOTOMETRY/ILLUMINATION/' + FILTER + '/' + PPRUN + '/' + str(ROT)\n                            os.system('mkdir -p ' + illum_dir)\n                            \n                            epsilon = 0\n                            index = 0\n                            for term in cheby_terms_use:\n                                index += 1\n                                print index, ROT, term, fitvars[str(ROT)+'$'+term['n']]\n                                epsilon += fitvars[str(ROT)+'$'+term['n']]*term['fx'](x,y)*term['fy'](x,y)\n                                                                                                                                                                            \n                            ''' save pattern w/o chip zps '''\n                                                                                                                                                                            \n                            print 'writing'\n                            hdu = pyfits.PrimaryHDU(epsilon)\n                                                                                                                                         \n                            im = illum_dir + '/nochipzps' + sample + sample_size +  '.fits'\n                            save_fit({'PPRUN':PPRUN,'FILTER':FILTER,'OBJNAME':OBJNAME,'sample':sample,'sample_size':sample_size,str(ROT)+'$im':im})\n                                                                                                                                         \n                            os.system('rm ' + im)\n                            hdu.writeto(im)\n                                                                                                                                                                            \n                            ''' save pattern w/ chip zps '''\n                                                                                                                                                                            \n                            if per_chip or same_chips:\n                                print CHIPS, 'CHIPS'                                                                                          \n                                for CHIP in CHIPS:\n                                    if str(dt['CRPIX1_' + str(CHIP)]) != 'None':\n                                        if False: #CONFIG == '10_3':\n                                            for sub_chip in ['1','2','3','4']:\n                                                from config_bonn import chip_divide_10_3                                                                   \n                                                import re\n                                                xmin = float(dt['CRPIX1ZERO']) - float(dt['CRPIX1_' + str(CHIP)]) + chip_divide_10_3[sub_chip][0]\n                                                xmax = float(dt['CRPIX1ZERO']) - float(dt['CRPIX1_' + str(CHIP)]) + chip_divide_10_3[sub_chip][1]\n                                                ymin = float(dt['CRPIX2ZERO']) - float(dt['CRPIX2_' + str(CHIP)])\n                                                ymax = ymin + float(dt['NAXIS2_' + str(CHIP)])\n                                                print xmin, xmax, ymin, ymax, CHIP, 'CHIP'\n                                                print int(xmin/bin), int(xmax/bin), int(ymin/bin), int(ymax/bin), CHIP, 'CHIP', bin, scipy.shape(epsilon)\n                                                print epsilon[int(xmin/bin):int(xmax/bin)][int(ymin/bin):int(ymax/bin)]\n                                                print fitvars.keys()\n                                                if fitvars.has_key('zp_' + str(CHIP) + '_' + sub_chip):\n                                                    print 'zp', fitvars['zp_' + str(CHIP) + '_' + sub_chip]                                                                \n                                                    epsilon[int(ymin/bin):int(ymax/bin),int(xmin/bin):int(xmax/bin)] += float(fitvars['zp_' + str(CHIP) + '_' + sub_chip])\n                                        else:\n                                            xmin = float(dt['CRPIX1ZERO']) - float(dt['CRPIX1_' + str(CHIP)])                                           \n                                            xmax = xmin + float(dt['NAXIS1_' + str(CHIP)])\n                                            ymin = float(dt['CRPIX2ZERO']) - float(dt['CRPIX2_' + str(CHIP)]) \n                                            ymax = ymin + float(dt['NAXIS2_' + str(CHIP)])\n                                                                                                                                                  \n                                            print xmin, xmax, ymin, ymax, CHIP, 'CHIP'                                                                 \n                                                                                                                                                      \n                                            print int(xmin/bin), int(xmax/bin), int(ymin/bin), int(ymax/bin), CHIP, 'CHIP', bin, scipy.shape(epsilon)\n                                                                                                                                                      \n                                            print epsilon[int(xmin/bin):int(xmax/bin)][int(ymin/bin):int(ymax/bin)]\n                                            print fitvars.keys()\n                                            print 'zp', fitvars['zp_' + str(CHIP)]\n                                            epsilon[int(ymin/bin):int(ymax/bin),int(xmin/bin):int(xmax/bin)] += float(fitvars['zp_' + str(CHIP)])\n                                                        \n                            print 'writing'\n                            hdu = pyfits.PrimaryHDU(epsilon)\n                                                                                                                                         \n                            im = illum_dir + '/correction' + sample + sample_size +  '.fits'\n                            save_fit({'linearplot':1,'PPRUN':PPRUN,'FILTER':FILTER,'OBJNAME':OBJNAME,'sample':sample,'sample_size':sample_size,str(ROT)+'$im':im})\n                                                                                                                                         \n                            os.system('rm ' + im)\n                            hdu.writeto(im)\n                                                                                                                                                                            \n                            print 'done'\n\n                ''' don't make these plots if it's a random run '''                                                                                                                                                                                                                    \n                if match and sample != 'None' and string.find(sample_size,'rand') == -1:                                                                                                                                 \n                    ''' calculate matched plot differences, before and after '''\n                    for ROT in EXPS.keys():\n                        data[ROT] = scipy.array(data[ROT])\n                        print scipy.array(data[ROT]), ROT\n                        print EXPS\n                                                                                                                                                                                                                    \n                        color[ROT] = scipy.array(color[ROT])\n                        \n                        ''' apply the color term measured from the data '''\n                        zp_correction = scipy.array([float(fitvars['zp_image_'+x]) for x in whichimage[ROT]])\n                        #data1 = data[ROT] - fitvars['SDSS_color']*color[ROT]  - zp_correction \n                    \n                        if 1: \n                            data1 = data[ROT] + fitvars['SDSS_color']*color[ROT] - zp_correction \n                        #else:\n                        #    data1 = data[ROT] - zp_correction \n                                                                                                                                                                                                                    \n                        #print data1, data[ROT], fitvars['SDSS_color'], color[ROT], zp_correction\n                        print len(data1), len(data[ROT]), match, sample\n                        data2 = data1 - (data1/data1*scipy.median(data1))\n                        \n                        path='/nfs/slac/g/ki/ki05/anja/SUBARU/%(OBJNAME)s/' % {'OBJNAME':OBJNAME}\n                        illum_dir = path + 'PHOTOMETRY/ILLUMINATION/' + FILTER + '/' + PPRUN + '/' + str(ROT) + '/'\n                                                                                                                                                                                                                    \n                        for kind,keyvalue in [['star',1]]: #['galaxy',0],\n                                                                                                                                                                                                                    \n                            calcDataIllum(sample + kind + 'nocorr'+str(ROT)+FILTER,10000,8000,data2,magErr[ROT],X[ROT],Y[ROT],pth=illum_dir,rot=0,good=[Star[ROT],keyvalue]) \n                                                                                                                                                                                                                    \n                            dtmp = {}\n                                                                                                                                                                                                                    \n                            var = variance(data2,magErr[ROT])\n                            print 'var'\n                            print var\n                                                                                                                                                                                                                    \n                            dtmp[sample + 'stdnocorr$' + str(ROT)] = var[1]**0.5\n                            dtmp[sample + 'redchinocorr$' + str(ROT)] = var[2]\n                            dtmp[sample + 'pointsnocorr$' + str(ROT)] = len(data2) \n                            print sample + 'redchinocorr$' + str(ROT)\n                                                                                                                                                                                                                    \n                            if calc_illum:\n                                                                                                                                                                                                                    \n                                #plot_color(color[ROT], data2)                                                                                   \n                                                                                                                                                 \n                                import scipy\n                                \n                                #print X[ROT]\n                                x = coord_conv_x(scipy.array(X[ROT]))\n                                y = coord_conv_y(scipy.array(Y[ROT]))\n                                                                                                                                          \n                                #epsilon = 0                                                       \n                                #for term in cheby_terms:\n                                #    data += fitvars[term[str(ROT)+'$'+'n']]*term['fx'](x,y)*term['fy'](x,y)\n                                \n                                epsilon=0\n                                for term in cheby_terms_use:\n                                    epsilon += fitvars[str(ROT)+'$'+term['n']]*term['fx'](x,y)*term['fy'](x,y)\n                                                                                                                                                \n                                chipcorrect = []\n                                #print chipnums\n                                if CONFIG != '10_3':\n                                    for chip in chipnums[ROT]:                                     \n                                        chipcorrect.append(fitvars['zp_' + str(int(float(chip)))])\n                                    chipcorrect = scipy.array(chipcorrect)\n                                    epsilon += chipcorrect\n                                                                                                                                                \n                                                                                                                                                \n                                calcim = sample+kind+'correction'+str(ROT)+FILTER\n                                calcDataIllum(calcim,10000,8000,epsilon,magErr[ROT],X[ROT],Y[ROT],pth=illum_dir,rot=0,good=[Star[ROT],keyvalue])\n                                                                                                                                                 \n                                data2 -= epsilon\n                                                                                                                                                \n                                #print whichimage[ROT][0:100]\n                                #data1 = data[ROT] - zp_correction \n                                #data2 = data1 - (data1/data1*scipy.median(data1))\n                                #plot_color(color[ROT], data2)\n                                \n                                #print magErr[ROT][0:20]\n                                calcim = sample+kind+'rot'+str(ROT)+FILTER\n                                #print illum_dir\n                                calcDataIllum(calcim,10000,8000,data2,magErr[ROT],X[ROT],Y[ROT],pth=illum_dir,rot=0,good=[Star[ROT],keyvalue])\n                                                                                                                                                 \n                                var = variance(data2,magErr[ROT])\n                                print 'second', var\n                                dtmp[sample + 'stdcorr$' + str(ROT)] = var[1]**0.5\n                                dtmp[sample + 'redchicorr$' + str(ROT)] = var[2]\n                                dtmp[sample + 'pointsnocorr$' + str(ROT)] = len(data2) \n\n                                print 'calcDataIllum', im, calcim, len(data[ROT])\n                                                                                                                                                 \n                            dtmp.update({'PPRUN':PPRUN,'FILTER':FILTER,'OBJNAME':OBJNAME,'sample':sample,'sample_size':sample_size})\n                            save_fit(dtmp)\n                                                                                                                                             \n                            #print params['SDSS_color'], 'SDSS_color'\n                            print OBJNAME, FILTER, PPRUN, tmpdir\n\n    return\n\n\ndef construct_correction(OBJNAME,FILTER,PPRUN,sample,sample_size,OBJNAME_use=None,FILTER_use=None,PPRUN_use=None):\n\n    import os \n    ppid = str(os.getppid())\n    import os\n    os.system('mkdir ' + tmpdir)\n\n    import time\n\n    #save_fit({'PPRUN':PPRUN,'FILTER':FILTER,'OBJNAME':OBJNAME,'sample':sample,'sample_size':sample_size,'correction_applied':'started'})\n\n    if OBJNAME_use is None:\n        OBJNAME_use, FILTER_use, PPRUN_use = OBJNAME, FILTER, PPRUN\n\n    save_fit({'PPRUN':PPRUN,'OBJNAME':OBJNAME,'FILTER':FILTER,'sample':'record','sample_size':'record','correction_applied':'corrstarted','OBJNAME_use':OBJNAME_use,'FILTER_use':FILTER_use,'PPRUN_use':PPRUN_use,'sample_use':sample,'time':str(time.localtime())},db='try_db')\n\n    try:\n\n\n        sample = str(sample)                                                                                                                                                                                                                                                      \n        sample_size = str(sample_size)\n                                                                                                                                                                                                                                                                                  \n        import scipy, re, string, os\n                                                                                                                                                                                                                                                                                  \n        ''' create chebychev polynomials '''\n        cheby_x = [{'n':'0x','f':lambda x,y:1.},{'n':'1x','f':lambda x,y:x},{'n':'2x','f':lambda x,y:2*x**2-1},{'n':'3x','f':lambda x,y:4*x**3.-3*x}] \n        cheby_y = [{'n':'0y','f':lambda x,y:1.},{'n':'1y','f':lambda x,y:y},{'n':'2y','f':lambda x,y:2*y**2-1},{'n':'3y','f':lambda x,y:4*y**3.-3*y}]\n        cheby_terms = []\n        cheby_terms_no_linear = []\n        for tx in cheby_x:\n            for ty in cheby_y:\n                if not ((tx['n'] == '0x' and ty['n'] == '0y')): # or (tx['n'] == '0x' and ty['n'] == '1y') or (tx['n'] == '1x' and ty['n'] == '0y')) :\n                    cheby_terms.append({'n':tx['n'] + ty['n'],'fx':tx['f'],'fy':ty['f']})\n                if not ((tx['n'] == '0x' and ty['n'] == '0y') or (tx['n'] == '0x' and ty['n'] == '1y') or (tx['n'] == '1x' and ty['n'] == '0y')) :\n                    cheby_terms_no_linear.append({'n':tx['n'] + ty['n'],'fx':tx['f'],'fy':ty['f']})\n                                                                                                                                                                                                                                                                                  \n        #if cov:\n        #    samples = [['sdss',cheby_terms,True]] #,['None',cheby_terms_no_linear,False]] #[['None',cheby_terms_no_linear],['sdss',cheby_terms]]\n        #else: \n        #    samples = [['None',cheby_terms_no_linear,False]]\n                                                                                                                                                                                                                                                                                  \n        samples = [['sdss',cheby_terms,True],['None',cheby_terms_no_linear,False]] #[['None',cheby_terms_no_linear],['sdss',cheby_terms]]\n        \n        #sample_size = 'all'\n        import re, time                                                                                                                \n        print OBJNAME, FILTER, PPRUN\n        dt = get_a_file(OBJNAME,FILTER,PPRUN)                \n        d = get_fits(OBJNAME_use,FILTER_use,PPRUN_use, sample, sample_size)                \n        print d.keys()\n        \n        #if d['sdss$good'] == 'y':\n        #    sample = 'sdss' \n        #if d['None$good'] == 'y':\n        #    sample = 'None' \n        #if d['bootstrap$good'] == 'y':\n        #    sample = 'bootstrap' \n                                                                                                                                                                                                                                                                                  \n        column_prefix = '' #sample+'$'+sample_size+'$'\n        position_columns_names = re.split('\\,',d[column_prefix + 'positioncolumns']) \n        print position_columns_names, 'position_columns_names'\n        fitvars = {}\n        cheby_terms_dict = {}\n        print column_prefix, position_columns_names\n        ROTS_dict = {} \n        for ele in position_columns_names:                      \n            print ele\n            if type(ele) != type({}):\n                ele = {'name':ele}\n            res = re.split('\\$',ele['name'])\n            if len(res) > 1:\n                ROTS_dict[res[0]] = ''\n                print res\n            if string.find(ele['name'],'zp_image') == -1:\n                print sample, sample_size, ele['name']\n                fitvars[ele['name']] = float(d[ele['name']]) \n                for term in cheby_terms:\n                    if len(res) > 1:\n                        if term['n'] == res[1]:                 \n                            cheby_terms_dict[term['n']] = term \n                                                                                                                                                                                                                                                                                  \n        ROTS = ROTS_dict.keys()\n        print ROTS\n                                                                                         \n        zp_images = re.split(',',d['zp_images'])\n        zp_images_names = re.split(',',d['zp_images_names'])\n                                                                                         \n        for i in range(len(zp_images)):\n            fitvars[zp_images_names[i]] = float(zp_images[i])\n                                                                                         \n        cheby_terms_use =  [cheby_terms_dict[k] for k in cheby_terms_dict.keys()]\n                                                                                                                                                                                                                                                                                  \n        print cheby_terms_use, fitvars\n        print dt \n        print dt['CHIPS']\n                                                                                                                                                                                                                                                                                  \n        CHIPS = [int(x) for x in re.split(',',dt['CHIPS'])]\n        print CHIPS \n        print dt.keys()\n        LENGTH1, LENGTH2 = dt['LENGTH1'], dt['LENGTH2']\n                                                                                                                                                                                                                                                                                  \n        per_chip = True\n                                                                                                                                                                                                                                                                                  \n        coord_conv_x = lambda x:(2.*x-0-LENGTH1)/(LENGTH1-0) \n        coord_conv_y = lambda x:(2.*x-0-LENGTH2)/(LENGTH2-0) \n                                                                                                                                                                                                                                                                                  \n        cleaned = {}\n                                                                                                                                                                                                                                                                                  \n        ''' make images of illumination corrections '''                                                                  \n\n        print ROTS\n        for ROT in ROTS:\n            size_x=LENGTH1\n            size_y=LENGTH2 \n            bin=100\n            import numpy, math, pyfits, os                                                                              \n            x,y = numpy.meshgrid(numpy.arange(0,size_x,bin),numpy.arange(0,size_y,bin))\n            F=0.1\n            print 'calculating'\n            x = coord_conv_x(x)\n            y = coord_conv_y(y)\n                                                                                                                                    \n            path='/nfs/slac/g/ki/ki05/anja/SUBARU/%(OBJNAME)s/' % {'OBJNAME':OBJNAME}\n            illum_dir = path + 'PHOTOMETRY/ILLUMINATION/' + FILTER + '/' + PPRUN + '/' + str(ROT)\n            os.system('mkdir -p ' + illum_dir)\n                                                                                                                                                                                                                                                                                  \n            \n            epsilon = 0\n            index = 0\n            for term in cheby_terms_use:\n                index += 1\n                print index, ROT, term, fitvars[str(ROT)+'$'+term['n']]\n                epsilon += fitvars[str(ROT)+'$'+term['n']]*term['fx'](x,y)*term['fy'](x,y)\n                                                                                                                                    \n            ''' save pattern w/o chip zps '''\n                                                                                                                                    \n            print 'writing'\n            print epsilon\n            hdu = pyfits.PrimaryHDU(epsilon)\n                                                                                                                         \n            im = illum_dir + '/apply_nochipzps' + sample + sample_size +  '.fits'\n            print 'before save'\n            save_fit({'PPRUN':PPRUN,'FILTER':FILTER,'OBJNAME':OBJNAME,'sample':sample,'sample_size':sample_size,sample+'$'+sample_size+'$'+str(ROT)+'$im':im})\n            print 'after save'                                                                                                                     \n            print im\n            os.system('rm ' + im)\n            print im\n            hdu.writeto(im)\n            print 'ROT', ROT\n                                                                                                                                    \n            ''' save pattern w/ chip zps '''\n                                                                                                                                                                                                                                                                                  \n            print 'here'\n            trial = False \n            Test = False\n            children = []\n            if 1: #per_chip or same_chips:\n                child = False                                                            \n                for CHIP in CHIPS:\n                    if not trial:\n                        print 'forking'\n                        child = os.fork()           \n                        if child:\n                            children.append(child)\n                                                                                 \n                    if not child:\n                        if str(dt['CRPIX1_' + str(CHIP)]) != 'None':            \n                            xmin = int(float(dt['CRPIX1ZERO'])) - int(float(dt['CRPIX1_' + str(CHIP)]))   \n                            ymin = int(float(dt['CRPIX2ZERO'])) - int(float(dt['CRPIX2_' + str(CHIP)]))\n                            xmax = xmin + int(dt['NAXIS1_' + str(CHIP)])\n                            ymax = ymin + int(dt['NAXIS2_' + str(CHIP)])\n                            \n                            print xmin, xmax, xmax - xmin, ymin, ymax, ymax-ymin, CHIP, 'CHIP'\n                            print int(xmin/bin), int(xmax/bin), int(ymin/bin), int(ymax/bin), CHIP, 'CHIP', bin, scipy.shape(epsilon)\n                            print epsilon[int(xmin/bin):int(xmax/bin)][int(ymin/bin):int(ymax/bin)]\n                            print fitvars.keys()\n                            print 'zp', fitvars['zp_' + str(CHIP)]\n                            epsilon[int(ymin/bin):int(ymax/bin),int(xmin/bin):int(xmax/bin)] += float(fitvars['zp_' + str(CHIP)])\n                            x,y = numpy.meshgrid(numpy.arange(xmin,xmax,1),numpy.arange(ymin,ymax,1))\n                            \n                            x = coord_conv_x(x)\n                            y = coord_conv_y(y)\n                                                                                                                                                      \n                            ''' correct w/ polynomial '''\n                            epsilonC = 0\n                            index = 0                                                                                                                \n                                                                                                                                                      \n                            #sum = [lambda u,v: fitvars[str(ROT)+'$'+term['n']]*term['fx'](u,v)*term['fy'](u,v) for term in cheby_terms_use]\n                            #print sum\n                            #p = lambda d,e: reduce(lambda a,b: a(d,e) + b(d,e), sum)\n                                    \n                            for term in cheby_terms_use:\n                                index += 1\n                                print index, ROT, term, fitvars[str(ROT)+'$'+term['n']]\n                                \n                                epsilonC += fitvars[str(ROT)+'$'+term['n']]*term['fx'](x,y)*term['fy'](x,y)\n                                                                                                                                                      \n                            ''' add the zeropoint '''\n                            epsilonC += float(fitvars['zp_' + str(CHIP)])\n                                                                                                                                                    \n                            ''' save pattern w/o chip zps '''\n                                                                                                                                                      \n                            import math\n                            print 'writing/converting to linear flux units'\n                            hdu = pyfits.PrimaryHDU(10.**(epsilonC/2.5))\n                            print hdu \n                            im = tmpdir + '/' + str(ROT) + '_' + str(CHIP) + '.fits'\n                            os.system('rm ' + im)\n                            hdu.writeto(im)\n               \n                        import sys\n                        print 'exiting'\n                        #if not trial:\n                        if not trial:\n                            sys.exit(0)\n                for c in children:  \n                    os.waitpid(c,0)\n                                                                                                                                                                                                                                                                                  \n            print 'finished'\n                                                                                                                                                                                                                                                                                  \n            print im\n            print 'writing'\n                                                                                                                                                                                                                                                                                  \n            ''' apply the corrections to the images '''\n                                                                                                                                                                                                                                                                                  \n            import MySQLdb, sys, os, re, time                                                                     \n            db2,c = connect_except()\n            \n            command  =\"select file, ROTATION from illumination_db where SUPA not like '%I' and BADCCD!=1 and OBJNAME='\" + OBJNAME + \"' and PPRUN='\" + PPRUN + \"' \" # and ROTATION='\" + str(ROT) + \"'\"\n\n\n            print command\n            c.execute(command)\n            results=c.fetchall()\n            db_keys = describe_db(c,'illumination_db')\n            files = []\n            for line in results: \n                files.append([float(line[1]),str(line[0])])\n            db2.close()\n                                                                                                                                                                                                                                                                                  \n            print files\n                                                                                                                                                                                                                                                                                  \n            Check = True\n                                                                                                                                                                                                                                                                                  \n            trial = False \n            for rot_dat,file in files: print rot_dat,file\n                                                                                                                                                                                                                                                                                  \n            #files = [[0,'/nfs/slac/g/ki/ki05/anja/SUBARU//MACS0744+39/W-C-RC/SCIENCE/SUPA0008019_10OCFS.fits']]\n                                                                                                                                                                                                                                                                                  \n            for rot_dat,file in files:\n                children = []\n                for CHIP in CHIPS:                                                                                                                                                    \n                \n                    child = False                                                            \n                    if not trial:\n                        child = os.fork()           \n                        if child:\n                            children.append(child)\n                                                                                \n                    if not child:\n                        #try: \n                        if 1:\n                            \n                            RUN = re.split('\\_',PPRUN)[0]\n                            p = re.compile('\\_\\d+O')                                               \n                            file_chip_1 = p.sub('_' + str(CHIP) + 'O',file)#.replace('.fits','.sub.fits')\n\n                                                                                                              \n                            from glob import glob\n                            import string\n                            g = glob(file_chip_1.replace('.fits','*.fits'))\n                            print file_chip_1.replace('.fits','*.fits')\n                            print g\n                            file_chips = []\n                            for l in g:\n                                if string.find(l,'I.')== -1 and len(glob(l.replace('.fits','.weight.fits').replace('SCIENCE','WEIGHTS').replace('.sub','').replace('III.','.').replace('II.','.').replace('IIII.','.'))) > 0:\n                                    f = l.replace('.sub','').replace('III.','.').replace('II.','.').replace('IIII.','.')\n                                    file_chips.append([len(f),f])\n                            file_chips.sort()\n                            file_chip = file_chips[-1][1]\n\n                            print file_chip\n\n                                                                                                              \n                            if len(g) == 0:\n                                    raise TryDb('missing file ') \n                                                                                                              \n                            #file_chip = g[0].replace('.sub','')\n                                                                                                                                                                                                                                                                                  \n                            import commands\n                            info = commands.getoutput('dfits ' + file_chip + ' | fitsort -d ROTATION')\n                            print info, file_chip\n                            #CHIP_ROT = str(int(re.split('\\s+',info)[1]))\n                                                                                                                                                                            \n                            file_short = re.split('\\/',file_chip)[-1] \n                            run_dir = re.split('\\/',file_chip)[-3] \n                                                                                                                                                                                 \n                            SUPA = re.split('\\_',file_short)[0]\n                            print SUPA\n                            if Test:\n                                file_short = file_short.replace(SUPA,SUPA+'I') \n                                file_chip.replace(SUPA,SUPA+'I')\n                                                                                                                                                                                                                                                                                  \n                            ''' if a calibration exposure, put in the CALIB directory '''\n                            if string.find(run_dir,'CALIB') == -1:\n                                use_run_dir = FILTER                                    \n                            else: \n                                use_run_dir = run_dir\n\n\n                            os.system('rm ' + os.environ['subdir'] + '/' + OBJNAME + '/' + use_run_dir + '/SCIENCE/*II.fits')              \n                            os.system('rm ' + os.environ['subdir'] + '/' + OBJNAME + '/' + use_run_dir + '/WEIGHTS/*II.weight.fits')              \n                                                                                                                                                                         \n                            ''' get rid of zero-size files '''\n                            if 1: #not use_run_dir in cleaned: \n                                                                                                                                                                          \n                                #afiles = glob(os.environ['subdir'] + '/' + OBJNAME + '/' + use_run_dir + '/WEIGHTS/*I.weight.fits')               \n                                #for afile in afiles:\n                                    #if  os.path.getsize(afile) == 0: os.system('rm ' + afile)\n                                #    os.system('rm ' + afile)\n                                cleaned[use_run_dir] = 'done' \n                            print use_run_dir, run_dir, string.find(run_dir,'CALIB')\n                            if string.find(run_dir,'CALIB') != -1:\n                                out_file =  os.environ['subdir'] + '/' + OBJNAME + '/' + FILTER + '/SCIENCE/' +  file_short.replace('.fits','I.fits')               \n                                os.system('rm ' + out_file)\n                                out_weight_file =  os.environ['subdir'] + '/' + OBJNAME + '/' + FILTER + '/WEIGHTS/' +  file_short.replace('.fits','I.weight.fits')\n                                os.system('rm ' + out_weight_file)\n                            ''' see if there are different extensions '''\n                            out_file =  os.environ['subdir'] + '/' + OBJNAME + '/' + use_run_dir + '/SCIENCE/' +  file_short.replace('.fits','I.fits')\n                            print os.environ['subdir'] + '/' + OBJNAME + '/' + use_run_dir + '/WEIGHTS/' +  file_short.replace('.fits','I.weight.fits')\n                            out_weight_file = os.environ['subdir'] + '/' + OBJNAME + '/' + use_run_dir + '/WEIGHTS/' +  file_short.replace('.fits','I.weight.fits')\n                            bad_out_weight_file =  os.environ['subdir'] + '/' + OBJNAME + '/' + use_run_dir + '/SCIENCE/' +  file_short.replace('.fits','I.weight.fits')\n                                                                                                                                                                                                                                                                                  \n                                                                                                                                                                                                                                                                                  \n                            if str(dt['CRPIX1_' + str(CHIP)]) == 'None':               \n                                os.system('rm ' + out_file)\n                                os.system('rm ' + out_weight_file)\n                                                                                                                                                                                                                                                                                  \n                            else:        \n                                \n                                CHIP_ROT = int(rot_dat)\n                                print CHIP_ROT, ROT\n\n                                print int(CHIP_ROT) == int(ROT)\n                                print filter(lambda x: int(x)==int(CHIP_ROT),ROTS) is True\n\n                                if not filter(lambda x: int(x)==int(CHIP_ROT),ROTS): CHIP_ROT = ROT\n\n                                from glob import glob\n                                if int(CHIP_ROT) == int(ROT):\n                                    im = tmpdir + '/' + str(CHIP_ROT) + '_' + str(CHIP) + '.fits'        \n                                    print im\n                                    weight_file = file_chip.replace('SCIENCE','WEIGHTS').replace('.fits','.weight.fits')\n                                    flag_file = file_chip.replace('SCIENCE','WEIGHTS').replace('.fits','.flag.fits')\n                                    print file_chip, weight_file\n                                    \n                                    directory = reduce(lambda x,y: x + '/' + y, re.split('\\/',file_chip)[:-1])\n                                    print directory, 'directory' ,file \n                                                                                                                                 \n                                    filter_dir = directory.replace(FILTER+'_'+RUN,FILTER) \n                                                                                                                                                                                \n                                    if 0:\n                                        out_directory = os.environ['subdir'] + '/TEST/' + FILTER + '_' + RUN + '/SCIENCE/' \n                                        out_filter_dir = os.environ['subdir'] + '/TEST/' + FILTER + '/SCIENCE/'\n                                        out_file =  os.environ['subdir'] + '/TEST/' + FILTER + '_' + RUN + '/SCIENCE/' +  file_short.replace('.fits','I.fits')\n                                        out_weight_file = out_file.replace('SCIENCE','WEIGHTS').replace('.fits','.weight.fits')\n                                        out_flag_file = out_file.replace('SCIENCE','WEIGHTS').replace('.fits','.flag.fits')\n                                        os.system('mkdir -p ' + out_directory)\n                                        os.system('mkdir -p ' + out_directory.replace('SCIENCE','WEIGHTS'))\n                                                                                                                                 \n                                        ''' make link to the header information '''                                                   \n                                        from glob import glob\n                                        print directory\n                                        \n                                        os.system('mkdir -p ' + out_filter_dir)\n                                        print filter_dir, directory, out_filter_dir, out_directory, 'dirs'\n                                                                                                                                     \n                                        print filter_dir+ '/head*'\n                                        print glob(filter_dir+ '/head*')\n                                        for file_scamp in glob(filter_dir+ '/head*'):\n                                            command = 'ln -s ' +  file_scamp +  ' ' + out_filter_dir \n                                            print command\n                                            os.system(command)\n                                                                                                                                                                                \n                                        os.system('rm ' + out_weight_file)                                                            \n                                        command = 'ln -s  ' + weight_file + ' ' + out_weight_file\n                                        print command\n                                        os.system(command)\n                                                                                                                                      \n                                        os.system('rm ' + out_flag_file)\n                                        command = 'ln -s  ' + flag_file + ' ' + out_flag_file\n                                        print command\n                                        os.system(command)\n                                                                                                                                     \n                                        command = 'sethead ' + out_file + ' OBJNAME=TEST' \n                                        print command\n                                        os.system(command)\n\n                                        \n                                    else:\n                                        \n                                        print 'glob', glob(out_file), out_file, Check\n                                        \n                                        go = False\n                                        if not len(glob(out_file)): go = True\n                                        elif os.path.getsize(out_file) == 0: go = True\n                                        elif 0.98 < os.path.getsize(file_chip) / os.path.getsize(out_file) < 1.02: go = True\n                                        go = True\n                                        if go: \n                                            os.system('rm ' + out_file)       \n                                            command = \"ic '%1 %2 *' \" + file_chip + \" \" + im + \"> \" + out_file  \n                                            print command\n                                            code = os.system(command)\n                                            if code != 0:\n                                                raise TryDb('failed ic' + file_chip) \n                                            import time\n                                            save_exposure({'illumination_match':sample,'time':str(time.localtime())},dt['SUPA'],dt['FLAT_TYPE'])\n                                                                                                                                                                                     \n                                                                                                                                                                                     \n                                        #os.system('rm ' + bad_out_weight_file) # remove this file which was accidently put there:w \n                                                                                                                                                                                     \n                                        go = False\n                                        print len(glob(out_weight_file)), 'outweightfile' \n                                        if not len(glob(out_weight_file)) : go = True\n                                        elif os.path.getsize(out_weight_file) == 0: go = True\n                                        elif 0.98 < os.path.getsize(out_weight_file) / os.path.getsize(weight_file) < 1.02: go = True\n                                        go = True\n                                        if go: \n                                            os.system('rm ' + out_weight_file)\n                                            command = \"ic '%1 %2 /' \" + weight_file + \" \" + im + \"> \" + out_weight_file \n                                            print command\n                                            print '\\n\\n\\n\\n\\n\\n'\n                                            from glob import glob\n                                            print glob(weight_file), weight_file, 'exists'\n                                            print glob(im), im, 'exists'\n                                            code = os.system(command)\n                                            if code != 0:\n                                                raise TryDb('failed ic' + weight_file) \n                                    #raw_input()\n                                        \n                                    ''' now do a file integrity check '''\n                                    import os\n                                    if len(glob(weight_file)): \n                                        if 0.98 < os.path.getsize(weight_file) - os.path.getsize(out_weight_file) < 1.02: \n                                            print os.path.getsize(weight_file), weight_file, os.path.getsize(out_weight_file), out_weight_file \n                                            raise TryDb('weight file' + str(weight_file)) \n                        try: \n                            print 'hey'\n                            import sys\n                        except KeyboardInterrupt:                                                                 \n                            raise\n                        except TryDb,e:                \n                            import traceback, sys\n                            print traceback.print_exc(file=sys.stdout)\n                            save_fit({'PPRUN':PPRUN,'OBJNAME':OBJNAME,'FILTER':FILTER,'logfile':illum_dir+'logfile','sample':'record','sample_size':'record',str(sample)+'status':'failed','time':str(time.localtime()),'apply_exception':e.value},db='try_db')\n                            if not trial:\n                                sys.exit(1)\n                            else: raise 'exception'\n                        except : \n                            import traceback , sys\n                            print traceback.print_exc(file=sys.stdout)\n                            save_fit({'PPRUN':PPRUN,'OBJNAME':OBJNAME,'FILTER':FILTER,'logfile':illum_dir+'logfile','sample':'record','sample_size':'record',str(sample)+'status':'failed','time':str(time.localtime()),'apply_exception':'corruption'},db='try_db')\n                            if not trial:\n                                sys.exit(1)\n                            else: raise 'exception'\n                        if not trial:\n                            print 'tmpdir', tmpdir\n                            print 'finished exit', child\n                            sys.exit(0)\n               \n                if not trial: \n                    for ch in children:      \n                        print children, ch\n                        a = os.waitpid(ch,0)\n                        print a, 'a'\n                   \n                        ''' without an exception here, then not all of the threads would finish ''' \n                        if a[1]!=0:\n                            print 'failed', a\n                            raise 'failed'\n                                                                                                                                                                                                                                                                                  \n        save_fit({'PPRUN':PPRUN,'OBJNAME':OBJNAME,'FILTER':FILTER,'sample':'record','sample_size':'record','correction_applied':'finished','OBJNAME_use':OBJNAME_use,'FILTER_use':FILTER_use,'PPRUN_use':PPRUN_use,'sample_use':sample,'time':str(time.localtime())},db='try_db')\n    except:\n        import traceback, sys\n        ppid_loc = str(os.getppid())\n        print traceback.print_exc(file=sys.stdout)\n        if ppid_loc != ppid: sys.exit(0) \n        print traceback.print_exc(file=sys.stdout)\n        save_fit({'PPRUN':PPRUN,'OBJNAME':OBJNAME,'FILTER':FILTER,'sample':'record','sample_size':'record','correction_applied':'failed','OBJNAME_use':OBJNAME_use,'FILTER_use':FILTER_use,'PPRUN_use':PPRUN_use,'sample_use':sample,'time':str(time.localtime())},db='try_db')\n    #save_fit({'PPRUN':PPRUN,'OBJNAME':OBJNAME,'FILTER':FILTER,'sample':'record','sample_size':'record',str(sample)+'status':'finished','time':str(time.localtime())},db='try_db')\n    #save_fit({'PPRUN':PPRUN,'FILTER':FILTER,'OBJNAME':OBJNAME,'sample':sample,'sample_size':sample_size,'correction_applied':sample})\n\ndef correct_image():\n    ''' make diagnostic plots '''                                                                                           \n    if 1:\n        import re\n        d = get_fits(CLUSTER,FILTER,PPRUN)                \n        column_prefix = sample+'$'+sample_size+'$'\n        position_columns_names = re.split('\\,',d[column_prefix + 'positioncolumns']) \n        fitvars = {}\n        cheby_terms_dict = {}\n        for ele in position_columns:                      \n            res = re.split('$',ele['name'])\n            fitvars[ele['name']] = float(d[sample+'$'+sample_size+'$'+ele['name']])\n            for term in cheby_terms:\n                if term['n'] == ele['name'][2:]:\n                    cheby_terms_dict[term['n']] = term \n        cheby_terms_use =  [cheby_terms_dict[k] for k in cheby_terms_dict.keys()]\n                                                                                                                          \n        print cheby_terms_use, fitvars\n                                                                                                                          \n        ''' make images of illumination corrections '''                                                                  \n        for ROT in EXPS.keys():\n            size_x=LENGTH1\n            size_y=LENGTH2\n            bin=100\n            import numpy, math, pyfits, os                                                                              \n            x,y = numpy.meshgrid(numpy.arange(0,size_x,bin),numpy.arange(0,size_y,bin))\n            F=0.1\n            print 'calculating'\n            x = coord_conv_x(x)\n            y = coord_conv_y(y)\n            \n            epsilon = 0\n            index = 0\n            for term in cheby_terms_use:\n                index += 1\n                print index, ROT, term, fitvars[str(ROT)+'$'+term['n']]\n                epsilon += fitvars[str(ROT)+'$'+term['n']]*term['fx'](x,y)*term['fy'](x,y)\n                                                                                                                          \n                                                                                                                          \n            print 'writing'\n            hdu = pyfits.PrimaryHDU(epsilon)\n\n\ndef residual_plots():\n    for ROT in EXPS.keys():\n        print 'ROT', ROT\n        fitvars = {} \n        for ele in position_columns:                      \n            res = re.split('$',ele['name'])\n            if res[0] == ROT:\n                fitvars[ele['name'][2:]] = U[ele['index']] \n                save_fit({'PPRUN':PPRUN,'FILTER':FILTER,'CLUSTER':CLUSTER,sample+'$'+sample_size+'$'+ele['name'].replace('$','$'):fitvars[ele['name'][2:]]})\n                print ele['name'], fitvars[ele['name'][2:]]\n        \n        if 0:\n            uu = open(tmpdir + '/fitvars' + ROT,'w')\n            import pickle\n            pickle.dump(fitvars,uu)\n            uu.close()\n        \n        size_x=8000\n        size_y=10000\n        bin=100\n        import numpy, math, pyfits, os                                                                              \n        x,y = numpy.meshgrid(numpy.arange(0,size_x,bin),numpy.arange(0,size_y,bin))\n        F=0.1\n        print 'calculating'\n\n        x = coord_conv_x(x)\n        y = coord_conv_y(y)\n        \n        epsilon = 0\n        for term in cheby_terms_use:\n            epsilon += fitvars[term['n']]*term['fx'](x,y)*term['fy'](x,y)\n        \n        print 'writing'\n        hdu = pyfits.PrimaryHDU(epsilon)\n        #os.system('rm ' + tmpdir + '/correction' + ROT + filter + sample_size + '.fits')\n        #hdu.writeto(tmpdir + '/correction' + ROT + filter + sample_size + '.fits')\n                                                                                                                                               \n        path='/nfs/slac/g/ki/ki05/anja/SUBARU/%(OBJNAME)s/' % {'OBJNAME':CLUSTER}\n        illum_dir = path + 'PHOTOMETRY/ILLUMINATION/' + FILTER + '/' + str(ROT)\n        os.system('mkdir -p ' + illum_dir)\n                                                                                                                                               \n        im = illum_dir + '/correction' + sample + sample_size + '.fits'\n        save_fit({'PPRUN':PPRUN,'FILTER':FILTER,'CLUSTER':CLUSTER,sample+'$'+sample_size+'$'+str(ROT)+'$im':im})\n                                                                                                                                               \n        os.system('rm ' + im)\n        hdu.writeto(im)\n        \n        #print 'done'\n        \n        epsilon = 10.**(epsilon/2.5)\n        \n        #correction = 10.**(epsilon/2.5)\n        # xaxis is always vertical!!!\n        #print 'writing'\n        #hdu = pyfits.PrimaryHDU(epsilon)\n        #os.system('rm ' + tmpdir + '/fcorrection' + ROT + filter + '.fits')\n        #hdu.writeto(tmpdir + '/fcorrection' + ROT + filter + '.fits')\n        #print 'done'\n    return\n\n\n\ndef fit():\n    maxSigIter=50\n    solutions = [] \n\n    import pickle\n    ''' get data '''\n    EXPS = getTableInfo()\n    print EXPS\n    \n    #ROTS, data, err, X, Y, maxVal, classStar = diffCalcNew()\n    #save = {'ROTS': ROTS, 'data':data,'err':err,'X':X,'Y':Y,'maxVal':maxVal,'classStar':classStar}\n    #uu = open(tmpdir + '/store','w')\n    #import pickle\n    #pickle.dump(save,uu)\n    #uu.close()\n   \n    ''' EXPS has all of the image information for different rotations '''\n\n    ''' make model '''\n    fit = make_model(EXPS)\n    print fit\n    star_good = selectGoodStars(EXPS)\n    uu = open(tmpdir + '/store','w')\n    import pickle\n    pickle.dump(star_good,uu)\n    uu.close()\n\n    import pickle\n    f=open(tmpdir + '/store','r')\n    m=pickle.Unpickler(f)\n    star_good=m.load()\n\n\n\n\n\n\n    fit['class'] = phot_funct(fit['model'],fit['fixed'],EXPS,star_good,fit['apply'])\n\n    import astropy, astropy.io.fits as pyfits\n    p = pyfits.open(tmpdir + '/final.cat')\n    table = p[1].data\n\n    import copy\n    table_save = copy.copy(table)\n    for i in range(maxSigIter):\n        fa = {\"table\": table_save}\n        func = fit['class'].calc_model \n\n        #functkw takes input data arrays\n        #parinfo takes initial guess and constraints on parameters \n        #import optimize\n        #params, covar, info, mesg, ier = optimize.leastsq(func,guess,args = (points,vals,errs), full_output=True)\n        \n\n\n\n\n        import mpfit\n        m =  mpfit.mpfit(func, functkw=fa,\n                         parinfo=fit['class'].parstart,\n                         maxiter=1000, quiet=0)\n        print m.params, m.perror \n        if (m.status <= 0):\n            print 'error message = ', m.errmsg\n            condition = Numeric.zeros(len(data))\n            break\n        print m.params,m.perror\n        #fits = [{'vars':['zp','color1coeff','color1coeff2'],'parinfo':[{'value':p[0],'fixed':0},{'value':p[1],'fixed':0},{'value':p[2],'fixed':0},'function':phot_funct_secondorder,'fit_type':'no_airmass'}]\n        fit['class'].fitvars = {}\n        for ele in range(len(fit['class'].smodel)):                              \n            print ele, fit['class'].smodel\n            name = make_name(fit['class'].smodel[ele])\n            print ele, fit['class'].fitvars, name, m.params[ele] \n            fit['class'].fitvars[name] = m.params[ele]          \n            fit['class'].fitvars[name + '_err'] = m.perror[ele]\n        perror = copy.copy(m.perror)\n                                                                                                                                                                                                               \n        # Compute a 3 sigma rejection criterion\n        print m.params, data_rec[0], data[0]\n        #condition, redchisq = SigmaCond(params, data_save, data,\n        #                           airmass_save, airmass,\n        #                           color1_save, color1, color2_save, color2, err_save, err, sigmareject)\n                                                                                                                                                                                                               \n\n\n        calcIllum(10000, 10000, 100, fit)\n\n        if len(data_save) > 1:                                                                     \n            (mo_save, reddm) = fit['class'].calc_sigma(m.params, airmass_save, color1_save, color2_save, data_save, err_save, X_save, Y_save)\n            #reddm = (data-mo)/err\n            redchisq = Numeric.sqrt(Numeric.sum(Numeric.power(reddm, 2)) / (len(reddm) - 1))\n            dm = data_save-mo_save\n            #dm_save = data_save - mo_save\n            print len(data_save), len(mo_save)\n            dm_save = data_save - mo_save\n            mean =  Numeric.sum(dm)/len(dm)\n            sigma = Numeric.sqrt(Numeric.sum(Numeric.power(mean-dm, 2)) / (len(dm) - 1))\n            # you can pick either \n            #condition = Numeric.less(Numeric.fabs(dm_save), float(sigmareject) * sigma)\n            condition = Numeric.less(Numeric.fabs(dm_save), float(sigmareject) * err_save)\n        else:\n            condition = Numeric.zeros(len(data_save))\n          \n        print redchisq \n        # Keep everything (from the full data set!) that is within\n        # the 3 sigma criterion\n        #data_sig = Numeric.compress(condition, data_save)\n        data = Numeric.compress(condition, data_rec)\n        err = Numeric.compress(condition, err_save)\n        X = Numeric.compress(condition, X_save)\n        Y = Numeric.compress(condition, Y_save)\n        new_len = len(data)\n        \n        if float(new_len)/float(save_len) < 0.5:\n            print \"Rejected more than 50% of all measurements.\"\n            print \"Aborting this fit.\"\n            break\n        \n        # No change\n        if new_len == old_len:\n            print \"Converged! (%d iterations)\" % (i+1, )\n            print \"Kept %d/%d stars.\" % (new_len, save_len)\n            break\n    #print params, perror, condition\n    meanerr = Numeric.sum(err_save)/len(err_save)\n\ndef make_name(name): \n    if len(name) > 1:                               \n        name = reduce(lambda x,y: x + 'T' + y,name)\n    else: \n        name = name[0]\n    return name\n\n''' reduce size od SDSS data '''\ndef convert_SDSS_cat(SUPA,FLAT_TYPE):\n    from config_bonn import info\n    import utilities, Numeric, os\n    reload(utilities)\n    from utilities import *\n\n    dict = get_files(SUPA,FLAT_TYPE)\n    print dict.keys()\n    search_params = initialize(dict['filter'],dict['OBJNAME'])\n    search_params.update(dict)\n\n    print dict['starcat']\n    import astropy, astropy.io.fits as pyfits\n    hdulist1 = pyfits.open(dict['starcat'])\n    #print hdulist1[\"STDTAB\"].columns\n    table = hdulist1[\"STDTAB\"].data\n\n    other_info = info[dict['filter']]\n    filters_info = make_filters_info([dict['filter']])                     \n    compband = filters_info[0][1] ## use the SDSS/other comparison band\n    color1which = other_info['color1']\n    print filters_info, compband\n    print dict['OBJNAME']\n    for key in dict.keys():\n        import string\n        if string.find(key,'color') != -1:\n            print key\n    \n    cols = [pyfits.Column(name=column.name, format=column.format,array=Numeric.array(0 + hdulist1[\"STDTAB\"].data.field(column.name))) for column in hdulist1[\"STDTAB\"].columns]\n    cols.append(pyfits.Column(name='stdMag_corr', format='D',array=Numeric.array(0 + hdulist1[\"STDTAB\"].data.field(compband+'mag'))))\n    cols.append(pyfits.Column(name='stdMagErr_corr', format='D',array=Numeric.array(0 + hdulist1[\"STDTAB\"].data.field(compband+'err'))))\n    cols.append(pyfits.Column(name='stdMagColor_corr', format='D',array=Numeric.array(0 + hdulist1[\"STDTAB\"].data.field(color1which))))\n    cols.append(pyfits.Column(name='stdMagClean_corr', format='E',array=Numeric.array(0 + hdulist1[\"STDTAB\"].data.field('Clean'))))\n\n    type = 'star'\n    path='/nfs/slac/g/ki/ki05/anja/SUBARU/%(OBJNAME)s/' % {'OBJNAME':search_params['OBJNAME']}\n    outcat = path + 'PHOTOMETRY/ILLUMINATION/sdssmatch__' + search_params['SUPA'] + '_' +  type + '.cat'\n    print cols\n    hdu = pyfits.PrimaryHDU()\n    hdulist = pyfits.HDUList([hdu])\n    tbhu = pyfits.BinTableHDU.from_columns(cols)\n    hdulist.append(tbhu)\n    hdulist[1].header['EXTNAME']='OBJECTS'\n    os.system('rm ' + outcat)\n    hdulist.writeto( outcat )\n    print 'wrote out new cat'\n\n    save_exposure({'sdssmatch':outcat},SUPA,FLAT_TYPE)\n\ndef apply_photometric_calibration(SUPA,FLAT_TYPE,starcat):\n    from config_bonn import info\n    import utilities, Numeric, os\n    reload(utilities)\n\n    dict = get_files(SUPA,FLAT_TYPE)\n    print dict.keys()\n    search_params = initialize(dict['filter'],dict['OBJNAME'])\n    search_params.update(dict)\n\n    #print dict['starcat']\n    import astropy, astropy.io.fits as pyfits\n    hdulist1 = pyfits.open(starcat)\n    #print hdulist1[\"STDTAB\"].columns\n    table = hdulist1[\"STDTAB\"].data\n\n    other_info = info[dict['filter']]\n    filters_info = utilities.make_filters_info([dict['filter']])                     \n    compband = filters_info[0][1] ## use the SDSS/other comparison band\n    color1which = other_info['color1']\n    print filters_info, compband\n    print dict['OBJNAME']\n    for key in dict.keys():\n        import string\n        if string.find(key,'color') != -1:\n            print key\n    #calib = get_calibrations_threesecond(dict['OBJNAME'],filters_info)\n    #print 'calib', calib\n    model = utilities.convert_modelname_to_array('zpPcolor1') #dict['model_name%'+dict['filter']])\n\n    cols = [] #pyfits.Column(name=column.name, format=column.format,array=Numeric.array(0 + hdulist1[\"STDTAB\"].data.field(column.name))) for column in hdulist1[\"STDTAB\"].columns]\n    print cols\n    #print start\n\n    print 'data start'\n    #data = utilities.color_std_correct(model,dict,table,dict['filter'],compband+'mag',color1which) # correct standard magnitude into instrumntal system -- at least get rid of the color term\n    \n    from copy import copy \n    data = copy(table.field(compband+'mag'))\n    print 'data done'\n    \n    cols.append(pyfits.Column(name='stdMag_corr', format='D',array=data))\n    cols.append(pyfits.Column(name='stdMagErr_corr', format='D',array=Numeric.array(0 + hdulist1[\"STDTAB\"].data.field(compband+'err'))))\n    cols.append(pyfits.Column(name='stdMagColor_corr', format='D',array=Numeric.array(0 + hdulist1[\"STDTAB\"].data.field(color1which))))\n    cols.append(pyfits.Column(name='stdMagClean_corr', format='D',array=Numeric.array(0 + hdulist1[\"STDTAB\"].data.field('Clean'))))\n\n    type = 'star'\n    path='/nfs/slac/g/ki/ki05/anja/SUBARU/%(OBJNAME)s/' % {'OBJNAME':search_params['OBJNAME']}\n    outcat = path + 'PHOTOMETRY/ILLUMINATION/sdssmatch__' + search_params['SUPA'] + '_' +  type + '.cat'\n    print cols\n    hdu = pyfits.PrimaryHDU()\n    hdulist = pyfits.HDUList([hdu])\n    tbhu = pyfits.BinTableHDU.from_columns(cols)\n    hdulist.append(tbhu)\n    hdulist[1].header['EXTNAME']='OBJECTS'\n    os.system('rm ' + outcat)\n    print 'writing out new cat'\n    hdulist.writeto( outcat )\n    print 'wrote out new cat'\n\n    save_exposure({'sdssmatch':outcat},SUPA,FLAT_TYPE)\n\n\n''' read in the photometric calibration and apply it to the data '''\ndef get_cats_ready(SUPA,FLAT_TYPE,galaxycat,starcat):\n\n    from config_bonn import info, wavelength_order\n\n    import utilities, Numeric, os\n    reload(utilities)\n\n    dict = get_files(SUPA,FLAT_TYPE)\n    print dict.keys()\n    search_params = initialize(dict['filter'],dict['OBJNAME'])\n    search_params.update(dict)\n\n    ''' figure out the correct color and magnitudes for the filter '''\n\n    colors_in_cat = ['W-C-RC','W-S-Z+']\n\n    def find_index(color):\n        index = -99\n        for i in range(len(wavelength_order)):\n            if wavelength_order[i] == color:\n                index = i\n        if index == -99: \n            raise CantFindFilter\n        return index\n    \n    colors_indices = [find_index(color) for color in colors_in_cat]\n\n    print colors_indices\n\n    \n\n\n    #print dict['starcat']\n    tmp = {}\n    import astropy, astropy.io.fits as pyfits\n    path='/nfs/slac/g/ki/ki05/anja/SUBARU/%(OBJNAME)s/' % {'OBJNAME':search_params['OBJNAME']}\n    for type,cat in [['star',starcat]]: #['galaxy',galaxycat],\n        hdulist1 = pyfits.open(cat)                                                                                                                                                                \n        #print hdulist1[\"STDTAB\"].columns\n        table = hdulist1[\"STDTAB\"].data\n        \n        other_info = info[dict['filter']]\n        filters_info = utilities.make_filters_info([dict['filter']])                     \n        compband = filters_info[0][1] ## use the SDSS/other comparison band\n        color1which = other_info['color1']\n        print filters_info, compband\n        print dict['OBJNAME']\n        for key in dict.keys():\n            import string\n            if string.find(key,'color') != -1:\n                print key\n        #calib = get_calibrations_threesecond(dict['OBJNAME'],filters_info)\n        #print 'calib', calib\n        model = utilities.convert_modelname_to_array('zpPcolor1') #dict['model_name%'+dict['filter']])\n        \n        cols = [] #pyfits.Column(name=column.name, format=column.format,array=Numeric.array(0 + hdulist1[\"STDTAB\"].data.field(column.name))) for column in hdulist1[\"STDTAB\"].columns]\n        print cols\n        #print start\n        \n        print 'data start'\n        #data = utilities.color_std_correct(model,dict,table,dict['filter'],compband+'mag',color1which) # correct standard magnitude into instrumntal system -- at least get rid of the color term\n        \n        from copy import copy \n        data = copy(table.field(compband+'mag'))\n        print 'data done', 'here'\n       \n        print (data) \n        cols.append(pyfits.Column(name='stdMag_corr', format='D',array=data))\n        #print (Numeric.array(0 + hdulist1[\"STDTAB\"].data.field(compband+'err')))\n        cols.append(pyfits.Column(name='stdMagErr_corr', format='D',array=Numeric.array(0 + hdulist1[\"STDTAB\"].data.field(compband+'err'))))\n\n        #print (Numeric.array(0 + hdulist1[\"STDTAB\"].data.field(color1which)))\n        cols.append(pyfits.Column(name='stdMagColor_corr', format='D',array=Numeric.array(0 + hdulist1[\"STDTAB\"].data.field(color1which))))\n\n        #print (Numeric.array(0 + hdulist1[\"STDTAB\"].data.field('Clean')))\n        cols.append(pyfits.Column(name='stdMagClean_corr', format='D',array=Numeric.array(0 + hdulist1[\"STDTAB\"].data.field('Clean'))))\n\n        #print (Numeric.array(0 + hdulist1[\"STDTAB\"].data.field('Ra')))\n        cols.append(pyfits.Column(name='ALPHA_J2000', format='D',array=Numeric.array(0 + hdulist1[\"STDTAB\"].data.field('Ra'))))\n\n        #print (Numeric.array(0 + hdulist1[\"STDTAB\"].data.field('Dec')))\n        cols.append(pyfits.Column(name='DELTA_J2000', format='D',array=Numeric.array(0 + hdulist1[\"STDTAB\"].data.field('Dec'))))\n\n        #print (Numeric.array(0 + hdulist1[\"STDTAB\"].data.field('SeqNr')))\n        cols.append(pyfits.Column(name='SeqNr', format='E',array=Numeric.array(0 + hdulist1[\"STDTAB\"].data.field('SeqNr'))))\n        length = len(hdulist1[\"STDTAB\"].data.field('SeqNr'))\n        if type == 'star':\n            cols.append(pyfits.Column(name='Star_corr', format='E',array=Numeric.ones(length)))\n        else: \n            cols.append(pyfits.Column(name='Star_corr', format='E',array=Numeric.zeros(length)))\n\n        outcat = path + 'PHOTOMETRY/ILLUMINATION/' + type + 'sdssmatch__' + search_params['SUPA'] + '_' +  type + '.cat'\n        print cols, 'here'\n        hdu = pyfits.PrimaryHDU()\n        print 'hdulist'\n        hdulist = pyfits.HDUList([hdu])\n        print 'tbhu'\n        tbhu = pyfits.BinTableHDU.from_columns(cols)\n        print 'hdulist'\n        hdulist.append(tbhu)\n        print 'headers'\n        hdulist[1].header['EXTNAME']='OBJECTS'\n        os.system('rm ' + outcat)\n        print 'writing out', outcat \n        hdulist.writeto( outcat )\n        print 'wrote out new cat'\n        save_exposure({type + 'sdssmatch':outcat},SUPA,FLAT_TYPE)\n        tmp[type + 'sdssmatch'] = outcat\n\n    import calc_tmpsave\n    outcat = path + 'PHOTOMETRY/ILLUMINATION/sdssmatch__' + search_params['SUPA'] + '_' +  type + '.cat'\n    #calc_tmpsave.paste_cats([tmp['galaxysdssmatch'],tmp['starsdssmatch']],outcat,index=1)\n\n    os.system('rm ' + outcat)\n    calc_tmpsave.paste_cats([tmp['starsdssmatch']],outcat,index=1)\n\n    #calc_tmpsave.paste_cats([tmp['galaxysdssmatch']],outcat,index=1)\n    #print tmp['galaxysdssmatch'],tmp['starsdssmatch']\n\n    #calc_tmpsave.paste_cats([tmp['galaxysdssmatch']],outcat,index=1)\n    print 'added', outcat\n\n    return outcat \n\n\ndef plot_color(color,data,a=None,m=None):\n    import numpy, math, pyfits, os                                                                              \n    import copy\n    from ppgplot   import *\n\n    pgbeg(\"/XTERM\",1,1)\n                                                                                                                                             \n    pgiden()\n    pgpanl(1,1) \n    from Numeric import *\n    x = copy.copy(color) #hdulist1[\"OBJECTS\"].data.field(color1which)\n    y = copy.copy(data) #hdulist1[\"OBJECTS\"].data.field(compband+'mag') - data\n    plotx = copy.copy(x)\n    ploty = copy.copy(y)\n    x.sort()    \n    y.sort()\n    mediany = y[int(len(y)/2.)]\n    lowx=-2 #x[2]\n    highx=2 #x[-2]\n    lowy=mediany + 1.5\n    highy=mediany -1.5\n    pgswin(lowx,highx,lowy,highy)\n    plotx = array(plotx)\n    ploty = array(ploty)\n    if a is not None:\n        print a, m\n        pgline(array(a), array(m))\n    #pylab.scatter(z,x)\n    pglab('Mag','Mag - Mag(Inst)')\n    #print plotx, ploty\n    pgpt(plotx,ploty,3)\n    \n    pgbox()\n    pgend()\n\ndef hold():\n    if 0: #star['sdss']:\n        star_A = []\n        star_B = []\n        sigmas = []\n        for exp in star['supa files']:\n            row_num += 1 \n            col_num = -1 \n            rotation = exp['rotation'] \n            #sigma = tab[str(rotation) + '$' + exp['name'] + '$MAGERR_AUTO'][star['table index']] \n            sigma = tab['SDSSstdMagErr_corr'][star['table index']] \n            for c in position_columns:\n                col_num += 1\n            first_column = True\n            for c in zp_columns:\n                col_num += 1\n                ''' remember that the good magnitude does not have any zp dependence!!! '''\n                #if (first_column is not True  and c['image'] == exp['name']) or c['image'] == 'sdss':\n                if c['image'] == 'sdss': \n                    value = 1./sigma\n                    star_A.append([row_num,col_num,value])\n                first_column = False\n            \n            ''' fit for the color term dependence '''\n            for c in color_columns:\n                col_num += 1\n                value = tab['SDSSstdMagColor_corr'][star['table index']]/sigma\n                star_A.append([row_num,col_num,value])\n            \n            col_num += 1\n            ''' magnitude column -- include the correct/common magnitude '''\n            value = 1./sigma\n            star_A.append([row_num,col_num+supa_num,value])\n            #value = (tab[str(rotation)+'$'+exp['name']+'$MAG_AUTO'][star['table index']] - tab['SDSSstdMag_corr'][star['table index']])/sigma\n            #print  tab[str(rotation)+'$'+exp['name']+'$MAG_AUTO'][star['table index']], tab['SDSSstdMag_corr'][star['table index']]\n            data[rotation].append(tab[str(rotation)+'$'+exp['name']+'$MAG_AUTO'][star['table index']] - tab['SDSSstdMag_corr'][star['table index']])\n            magErr[rotation].append(tab['SDSSstdMagErr_corr'][star['table index']])\n            whichimage[rotation].append(exp['name'])\n            X[rotation].append(tab[str(rotation) + '$' + exp['name'] + '$Xpos_ABS'][star['table index']])\n            Y[rotation].append(tab[str(rotation) + '$' + exp['name'] + '$Ypos_ABS'][star['table index']])\n            color[rotation].append(tab['SDSSstdMagColor_corr'][star['table index']])\n            star_B.append([row_num,value])\n            sigmas.append(sigma)\n        inst.append({'type':'sdss','A_array':star_A, 'B_array':star_B, 'sigma_array': sigmas})\n\ndef save_fit(dict,OBJNAME=None,FILTER=None,PPRUN=None,db='fit_db'):\n    if OBJNAME!= None and FILTER!= None and  PPRUN!=None:\n        dict['OBJNAME'] = OBJNAME \n        dict['FILTER'] = FILTER \n        dict['PPRUN'] = PPRUN \n\n    db2,c = connect_except()\n\n    #db = 'fit_db'\n    \n    #c.execute(\"DROP TABLE IF EXISTS fit_db\")\n    command = \"CREATE TABLE IF NOT EXISTS \" + db + \" ( id MEDIUMINT NOT NULL AUTO_INCREMENT, PRIMARY KEY (id))\"\n    #print command\n    c.execute(command)\n\n    db_keys = describe_db(c,db)\n\n    from copy import copy\n    floatvars = {}  \n    stringvars = {}\n    #copy array but exclude lists                                                   \n    import string, traceback, sys\n    letters = string.ascii_lowercase + string.ascii_uppercase.replace('E','') + '_' + '-' + ','\n    for ele in dict.keys():\n        \n            type = 'float'                               \n            for l in letters:\n                if string.find(str(dict[ele]),l) != -1: \n                    type = 'string'\n            if type == 'float':  \n                floatvars[ele] = str(float(dict[ele])) \n            elif type == 'string':\n                stringvars[ele] = dict[ele] \n                                                                                                                                                                                                           \n    # make database if it doesn't exist\n    #print 'floatvars', floatvars\n    #print 'stringvars', stringvars\n    for column in stringvars: \n        stop = False\n        for key in db_keys:\n            import string\n            if key.lower() == column.lower(): stop = True \n        if not stop:\n            try:                                                                       \n                if string.find(column,'reject_plot') != -1 or string.find(column,'im') != -1 or string.find(column,'positioncolumns') != -1: \n                    command = 'ALTER TABLE ' + db + ' ADD ' + column + ' varchar(1000)'\n                elif string.find(column,'zp_image') != -1:\n                    command = 'ALTER TABLE ' + db + ' ADD ' + column + ' varchar(3000)'\n                else:\n                    command = 'ALTER TABLE ' + db + ' ADD ' + column + ' varchar(100)'\n                c.execute(command)  \n            except:\n                print traceback.print_exc(file=sys.stdout)\n                                                                                                                                                                                                          \n    for column in floatvars: \n        stop = False                                       \n        for key in db_keys:\n            import string\n            if key.lower() == column.lower(): stop = True \n        if not stop:\n            try:                                                                \n                command = 'ALTER TABLE ' + db + ' ADD ' + column + ' float(15)'\n                c.execute(command)  \n            except:\n                print traceback.print_exc(file=sys.stdout)\n\n    # insert new observation \n\n    #print db_keys\n\n\n    OBJNAME = dict['OBJNAME']                                                                                                                                     \n    FILTER = dict['FILTER']\n    PPRUN = dict['PPRUN']\n    sample = dict['sample']\n    sample_size = dict['sample_size']\n\n    command = \"SELECT OBJNAME from \" + db + \" where OBJNAME = '\" + OBJNAME + \"' and FILTER = '\" + FILTER + \"' and PPRUN='\" + PPRUN + \"' and sample='\" + str(sample) + \"' and sample_size='\" + str(sample_size) + \"'\"\n    #print command\n    c.execute(command)\n    #print OBJNAME, FILTER, PPRUN\n    results = c.fetchall() \n    #print results\n    if len(results) > 0:\n        print 'already added'\n    else:\n        command = \"INSERT INTO \" + db + \" (OBJNAME,FILTER,PPRUN,sample,sample_size) VALUES ('\" + dict['OBJNAME'] + \"','\" + dict['FILTER'] + \"','\" + dict['PPRUN'] + \"','\" + dict['sample'] + \"','\" + dict['sample_size'] + \"')\"\n        #print command\n        c.execute(command) \n                                                                                                                                                                  \n    import commands\n                                                                                                                                                                  \n    vals = ''\n    for key in stringvars.keys():\n        #print key, stringvars[key]\n        vals += ' ' + key + \"='\" + str(stringvars[key]) + \"',\"\n                                                                                                                                                                  \n    for key in floatvars.keys():\n        #print key, floatvars[key]\n        vals += ' ' + key + \"='\" + floatvars[key] + \"',\"\n    vals = vals[:-1]\n\n    if len(vals) > 1:\n        command = \"UPDATE \" + db + \" set \" + vals + \" WHERE OBJNAME='\" + dict['OBJNAME'] + \"' AND FILTER='\" + dict['FILTER'] + \"' AND PPRUN='\"  + dict['PPRUN'] + \"' and sample='\" + str(sample) + \"' and sample_size='\" + str(sample_size) + \"'\" \n        print command\n        c.execute(command)\n        \n    #print vals\n    #names = reduce(lambda x,y: x + ',' + y, [x for x in floatvars.keys()])\n    #values = reduce(lambda x,y: str(x) + ',' + str(y), [floatvars[x] for x in floatvars.keys()])\n    #names += ',' + reduce(lambda x,y: x + ',' + y, [x for x in stringvars.keys()])\n    #values += ',' + reduce(lambda x,y: x + ',' + y, [\"'\" + str(stringvars[x]) + \"'\" for x in stringvars.keys()])\n    #command = \"INSERT INTO illumination_db (\" + names + \") VALUES (\" + values + \")\"\n    #print command\n    #os.system(command)\n\ndef gather_exposures_all(filters=None):\n    #if not filters:\n    #    filters =  ['B','W-J-B','W-J-V','W-C-RC','W-C-IC','I','W-S-Z+']        \n\n    import os, re\n    from glob import glob\n    dirs = glob(os.environ['subdir'] + '/*')\n    print len(dirs)\n    for i in range(len(dirs)):\n        dir = dirs[i]\n        print 'dir',dir\n        subdirs = glob(dir + '/*')\n        for subdir in subdirs: \n            try:\n                slash = re.split('/',subdir)[-1]                                                                                                                                                                                                                \n                res = re.split('_',slash)\n                if len(res) > 1:\n                    files = glob(subdir+'/SCIENCE/*fits')\n                    if len(files)>0:\n                        #search_params = initialize(filter,OBJNAME)                                                                                                                                                 \n                        import os, re, bashreader, sys, string, utilities\n                        from glob import glob\n                        from copy import copy\n                        \n                        #files = glob(searchstr)\n                        files.sort()\n                        exposures =  {} \n                        \n                        import MySQLdb, sys, os, re                                                                     \n                        db2 = MySQLdb.connect(db='subaru', user='weaklensing', passwd='darkmatter', host='ki-rh8')\n                        c = db2.cursor()\n                        \n                        for file in files:\n                            if string.find(file,'links') == -1 and string.find(file,'wcs') == -1 and string.find(file,'.sub.fits') == -1:\n                                res = re.split('_',re.split('/',file)[-1])                                        \n                                exp_name = res[0]\n                                if not exposures.has_key(exp_name): exposures[exp_name] = {'images':[],'keywords':{}}\n                                exposures[exp_name]['images'].append(file) # exp_name is the root of the image name\n                                if len(exposures[exp_name]['keywords'].keys()) == 0: #not exposures[exp_name]['keywords'].has_key('ROTATION'): #if exposure does not have keywords yet, then get them -- this makes sure you only record each SUPA file once!!!\n                                    #exposures[exp_name]['keywords']['filter'] = filter\n                                    exposures[exp_name]['keywords']['file'] = file \n                                    res2 = re.split('/',file)   \n                                    #for r in res2:\n                                    #    if string.find(r,filter) != -1:\n                                    #        print r\n                                    #        exposures[exp_name]['keywords']['date'] = r.replace(filter + '_','')\n                                    #        exposures[exp_name]['keywords']['fil_directory'] = r \n                                    #        search_params['fil_directory'] = r\n                                    kws = utilities.get_header_kw(file,['CRVAL1','CRVAL2','ROTATION','OBJECT','GABODSID','CONFIG','EXPTIME','AIRMASS','INSTRUM','PPRUN','BADCCD','FILTER']) # return KEY/NA if not SUBARU \n                                                                                                                                                                                                                                                                \n                                    ''' figure out PPRUN '''\n                                    import commands\n                                    readlink = commands.getoutput('readlink -f ' + file)\n                                    res = re.split('SUBARU/',readlink)\n                                    res = re.split('/',res[1])\n                                    kws['PPRUN'] = res[0]\n                                                                                                                                                                                                                                                                \n                                    ''' firgure out OBJNAME '''\n                                    res = re.split('SUBARU/',file)\n                                    res = re.split('/',res[1])\n                                    kws['OBJNAME'] = res[0]\n                                    print kws['OBJNAME'], 'OBJNAME'\n                                                                                                                                                                                                                                                                \n                                    ''' figure out a way to break into SKYFLAT, DOMEFLAT '''\n                                    ppid = str(os.getppid())\n                                    command = 'dfits ' + file \n                                    file = commands.getoutput(command)\n                                    import string                    \n                                    if string.find(file,'SKYFLAT') != -1: exposures[exp_name]['keywords']['FLAT_TYPE'] = 'SKYFLAT' \n                                    elif string.find(file,'DOMEFLAT') != -1: exposures[exp_name]['keywords']['FLAT_TYPE'] = 'DOMEFLAT' \n                                    import string                    \n                                    file = re.split('\\n',file)\n                                    for line in file:\n                                        print line\n                                        if string.find(line,'Flat frame:') != -1 and string.find(line,'illum') != -1:\n                                            import re                   \n                                            res = re.split('SET',line)\n                                            if len(res) > 1:\n                                                res = re.split('_',res[1])                                                                                                                                 \n                                                set = res[0]\n                                                exposures[exp_name]['keywords']['FLAT_SET'] = set\n\n                                                res = re.split('illum',line)\n                                                res = re.split('\\.',res[1])\n                                                smooth = res[0]\n                                                exposures[exp_name]['keywords']['SMOOTH'] = smooth \n                                            break\n                                                                                                                                                                                                      \n                                    for kw in kws.keys(): \n                                        exposures[exp_name]['keywords'][kw] = kws[kw]\n                                    exposures[exp_name]['keywords']['SUPA'] = exp_name\n                                    #exposures[exp_name]['keywords']['OBJNAME'] = OBJNAME \n                                    print exposures[exp_name]['keywords']\n                                    save_exposure(exposures[exp_name]['keywords'])\n                                                                                                                                                                                                                                                                \n            except KeyboardInterrupt:                                                                \n                raise\n            except: \n                ppid_loc = str(os.getppid())\n                print sys.exc_info()\n                print 'something else failed',ppid, ppid_loc \n\n    return exposures\n\ndef run_telarchive(ra,dec,objname):\n\n    from ephem import *\n    coord = Equatorial(str(ra/15.),str(dec))\n    ra = str(coord.get()[0]).replace(':',' ')\n    dec = str(coord.get()[1]).replace(':',' ')\n   \n    print 'ra','dec',ra,dec \n\n    import commands, re, string\n    command = 'python dosearch.py --coords=\"' + ra + ' ' + dec + '\" 6.0'\n    print command\n    out = commands.getoutput(command)\n    #i = open('ij','w')\n    #i.write(out)\n    #i.close()\n    #out = open('ij','r').read()\n\n    print out\n    res = re.split('\\n',out)\n    print res\n    d = {}\n    for i in res:\n        res_t = re.split('\\t',i)\n\n        if len(res_t) > 1:\n            if res_t[1] != '':                                                   \n                name = re.split('\\s+',re.split(':',res_t[1])[0])[0]\n                d[name + '_info'] = ' '\n                if string.find(re.split(':',res_t[1])[1],'No data found') != -1:\n                    d[name + '_data'] = 0\n                elif  string.find(re.split(':',res_t[1])[0],'Sloan Digital') != -1:\n                    d[name + '_data'] = 1\n                else: \n                    print res_t[1]\n                    a = re.split(':',res_t[1])[1]\n                    print a\n                    b = re.split('\\(',a)[1]\n                    c = re.split('\\s+',b)[0]\n                    d[name + '_data'] = c\n            else: d[name + '_info'] += res_t[2] + '; '\n                \n    print objname, d \n    return d \n\n\ndef get_observations():\n    import MySQLdb, sys, os, re, time, utilities, pyfits\n    from copy import copy\n    db2 = MySQLdb.connect(db='subaru', user='weaklensing', passwd='darkmatter', host='ki-rh8')\n    c = db2.cursor()\n    db_keys = describe_db(c)\n\n    command = \"CREATE TABLE IF NOT EXISTS telarchive_db ( id MEDIUMINT NOT NULL AUTO_INCREMENT, PRIMARY KEY (id))\" \n    print command\n    #c.execute(\"DROP TABLE IF EXISTS telarchive_db\")\n    c.execute(command)\n    \n    keystop = ['PPRUN','ROTATION','OBJNAME']\n    list = reduce(lambda x,y: x + ',' + y, keystop)\n    command=\"SELECT * from illumination_db LEFT OUTER JOIN telarchive_db on telarchive_db.OBJNAME=illumination_db.OBJNAME where illumination_db.OBJNAME is not null and illumination_db.OBJNAME!='HDFN' and illumination_db.OBJNAME!='COSMOS' and telarchive_db.HST_data is NULL GROUP BY illumination_db.OBJNAME\" \n    print command\n    c.execute(command)\n    results=c.fetchall()\n    for line in results: \n        dtop = {}\n        for i in range(len(db_keys)):\n            dtop[db_keys[i]] = str(line[i])\n        print dtop['CRVAL1'],dtop['CRVAL2'],dtop['OBJNAME']\n        dict = run_telarchive(float(dtop['CRVAL1']),dtop['CRVAL2'],dtop['OBJNAME'])\n        OBJNAME = dtop['OBJNAME']\n        dict['OBJNAME'] = OBJNAME\n\n        floatvars = {}          \n        stringvars = {}\n        #copy array but exclude lists                                                   \n        import string\n        letters = string.ascii_lowercase + string.ascii_uppercase.replace('E','') + '_' + '-'\n        for ele in dict.keys():\n            print ele, dict[ele]\n            type = 'float'\n            for l in letters:\n                if string.find(str(dict[ele]),l) != -1 or dict[ele] == ' ': \n                    type = 'string'\n            if type == 'float':  \n                floatvars[ele] = str(float(dict[ele])) \n            elif type == 'string':\n                stringvars[ele] = dict[ele] \n                                                                                                                                                                                                               \n        # make database if it doesn't exist\n        print 'floatvars', floatvars\n        print 'stringvars', stringvars\n                                                                                                                                                                                                              \n        for column in stringvars: \n            try:\n                command = 'ALTER TABLE telarchive_db ADD ' + column + ' varchar(240)'\n                c.execute(command)  \n            except: nope = 1 \n        for column in floatvars: \n            try:\n                command = 'ALTER TABLE telarchive_db ADD ' + column + ' float(30)'\n                c.execute(command)  \n            except: nope = 1 \n\n        c.execute(\"SELECT OBJNAME from telarchive_db where OBJNAME = '\" + OBJNAME + \"'\")\n        results = c.fetchall() \n        print results\n        if len(results) > 0:\n            print 'already added'\n        else:\n            command = \"INSERT INTO telarchive_db (OBJNAME) VALUES ('\" + OBJNAME + \"')\"\n            print command\n            c.execute(command) \n                                                                                                                                         \n        import commands\n        vals = ''\n        for key in stringvars.keys():\n            print key, stringvars[key]\n            vals += ' ' + key + \"='\" + str(stringvars[key]) + \"',\"\n                                                                                                                                         \n        for key in floatvars.keys():\n            print key, floatvars[key]\n            vals += ' ' + key + \"='\" + floatvars[key] + \"',\"\n        vals = vals[:-1]\n                                                                                                                                         \n        command = \"UPDATE telarchive_db set \" + vals + \" WHERE OBJNAME='\" + OBJNAME + \"'\" \n        print command\n        c.execute(command)\n\ndef calcDataIllum(file, LENGTH1, LENGTH2, data,magErr, X, Y, pth='/nfs/slac/g/ki/ki04/pkelly/plots/', rot=0, good=None):\n    \n    import numpy, math, pyfits, os                                                                              \n    from ppgplot   import *\n\n    #print size_x, size_y, bin, size_x/bin\n\n    x = []\n    y = []\n    z = []\n    zerr = []\n\n    from copy import copy\n    X_sort = copy(X)\n    Y_sort = copy(Y)\n    X_sort = numpy.sort(X_sort)\n    Y_sort = numpy.sort(Y_sort)\n\n    X_min = X_sort[0]\n    Y_min = Y_sort[0]\n\n    X_max = X_sort[-1]\n    Y_max = Y_sort[-1]\n\n    X_width = abs(X_max - X_min)\n    Y_width = abs(Y_max - Y_min)\n\n    nbin1 =15 \n    nbin2 =15 \n\n    LENGTH1 = LENGTH1\n    LENGTH2 = LENGTH2\n\n    print LENGTH1, LENGTH2\n\n    bin1 = int(LENGTH1/nbin1)\n    bin2 = int(LENGTH2/nbin2)\n    \n    diff_weightsum = -9999*numpy.ones([nbin1,nbin2])\n    diff_invvar = -9999*numpy.ones([nbin1,nbin2])\n    diff_X = -9999*numpy.ones([nbin1,nbin2])\n    diff_Y = -9999*numpy.ones([nbin1,nbin2])\n\n    X_cen = []\n    Y_cen = []\n    data_cen = []\n    zerr_cen = []\n\n\n    chisq = 0\n    for i in range(len(data)):\n        if good is not None:\n            use = good[0][i] == good[1]    \n        else: \n            use = True\n        if use:\n            if 1: # LENGTH1*0.3 < X[i] < LENGTH1*0.6:                                                                                             \n                X_cen.append(X[i])\n                Y_cen.append(Y[i])\n                data_cen.append(data[i])\n                zerr_cen.append(magErr[i])\n                                                                                                                                                  \n            x.append(X[i])\n            y.append(Y[i])\n            z.append(data[i])\n            zerr.append(magErr[i])\n            chisq += data[i]**2./magErr[i]**2.\n                                                                                                                                                  \n            x_val = int((X[i])/float(bin1))  # + size_x/(2*bin)\n            y_val = int((Y[i])/float(bin2))  #+ size_y/(2*bin)\n            #print LENGTH1, LENGTH2, x_val, y_val, X[i], Y[i]\n            #print size_x/bin+1,size_y/bin+1, x_val, y_val, X[i], Y[i]\n            err = magErr[i]\n            ''' lower limit on error '''\n            if err < 0.04: err = 0.04\n            weightsum = data[i]/err**2.\n            weightX = X[i]/err**2.\n            weightY = Y[i]/err**2.\n            invvar = 1/err**2.\n            \n                                                                                                                                                  \n            #if 1: #0 <= x_val and x_val < int(nbin1) and y_val >= 0 and y_val < int(nbin2):  #0 < x_val < size_x/bin and 0 < y_val < size_y/bin:\n            #print x_val, y_val\n            try:\n                if diff_weightsum[x_val][y_val] == -9999:      \n                    diff_weightsum[x_val][y_val] = weightsum\n                    diff_invvar[x_val][y_val] = invvar \n                    diff_X[x_val][y_val] = weightX \n                    diff_Y[x_val][y_val] = weightY\n                                                                                                                                                  \n                    #print x_val, y_val, weightsum, '!!!!!'\n                else:                 \n                    diff_weightsum[x_val][y_val] += weightsum \n                    diff_invvar[x_val][y_val] += invvar \n                    diff_X[x_val][y_val] += weightX\n                    diff_Y[x_val][y_val] += weightY\n            except: fail = 'fail'\n\n    redchisq = chisq**0.5 / len(data)\n    print 'redchisq', redchisq\n\n    import Numeric\n    x_p = Numeric.array(X_cen)\n    y_p = Numeric.array(Y_cen)\n    z_p = Numeric.array(data_cen)\n    zerr_p = Numeric.array(zerr_cen)\n    x.sort()\n    y.sort()\n    z.sort()\n\n    mean = diff_weightsum/diff_invvar\n    print 'mean'\n    #print mean\n    err = 1/diff_invvar**0.5\n\n    print 'err'\n    #print err \n\n    print 'writing'\n    hdu = pyfits.PrimaryHDU(mean)\n    f = pth + file \n    os.system('rm ' + f +   'diffmap.fits')\n    hdu.writeto( f + 'diffmap.fits')      \n\n    hdu = pyfits.PrimaryHDU(err)\n    os.system('rm ' + f + 'diffinvar.fits')\n    hdu.writeto( f + 'diffinvar.fits')      \n\n\n    ''' now make cuts with binned data '''\n\n    mean_flat = Numeric.array(mean.flatten(1))\n    print mean_flat\n    err_flat = Numeric.array(err.flatten(1))\n    print err_flat\n    mean_X = Numeric.array((diff_X/diff_invvar).flatten(1))\n    print mean_X\n    mean_Y = Numeric.array((diff_Y/diff_invvar).flatten(1))\n    print mean_Y\n\n    file = f + 'diffp.ps'                                      \n                                                            \n    t = tempfile.NamedTemporaryFile(dir='/tmp/').name\n    ### plot residuals\n    pgbeg(t+\"/cps\",1,2)\n    pgiden()\n                                                            \n    #print x_p\n    #print z_p \n    #print zerr_p\n                                                            \n    #pgswin(x[0],x[-1],z[0],z[-1])\n                                                            \n    pgpanl(1,1)\n    pgswin(x[0],x[-1],-0.4,0.4)\n    pgbox()\n    pglab('X axis','SDSS-SUBARU',file)     # label the plot\n    #pgsci(3)\n    #pgerrb(6,x_p,z_p,zerr_p)\n    pgerrb(6,mean_X,mean_flat,err_flat)\n    pgpt(mean_X,mean_flat,3)\n                                                            \n    #pgswin(y[0],y[-1],z[0],z[-1])\n    pgpanl(1,2)\n    pgswin(y[0],y[-1],-0.4,0.4)\n                                                            \n    pgsci(1)\n    pgbox()\n    pglab('Y axis','SDSS-SUBARU',file)     # label the plot\n    #pgsci(3)\n    pgerrb(6,mean_Y,mean_flat,err_flat)\n    pgpt(mean_Y,mean_flat,3)\n    pgsci(1)\n                                                            \n    pgend()\n                                                            \n    os.system('mv ' + t + ' ' + file)\n\n\n    file = f + 'pos.ps'\n\n    t = tempfile.NamedTemporaryFile(dir='/tmp/').name\n    print file\n    os.system('rm ' + file)\n    pgbeg(t + '/cps',1,1)\n    pgiden()\n\n    #print x_p\n    #print z_p \n    #print zerr_p\n\n    #pgswin(x[0],x[-1],z[0],z[-1])\n\n    ### plot positions\n    pgpanl(1,1)\n    pgswin(x[0],x[-1],y[0],y[-1])\n    pgbox()\n    pglab('X','Y',file)     # label the plot\n    #pgsci(3)\n    #pgerrb(6,x_p,z_p,zerr_p)\n    pgpt(x_p,y_p,3)\n\n    pgend()\n\n    os.system('mv ' + t + ' ' + file)\n\n    print f + 'pos.ps'+\"/cps\"\n\n    file = f + 'diff.ps'\n\n    t = tempfile.NamedTemporaryFile(dir='/tmp/').name\n    ### plot residuals\n    pgbeg(t+\"/cps\",1,2)\n    pgiden()\n\n    #print x_p\n    #print z_p \n    #print zerr_p\n\n    #pgswin(x[0],x[-1],z[0],z[-1])\n\n    pgpanl(1,1)\n    pgswin(x[0],x[-1],-0.4,0.4)\n    pgbox()\n    pglab('X axis','SDSS-SUBARU',file)     # label the plot\n    #pgsci(3)\n    #pgerrb(6,x_p,z_p,zerr_p)\n    pgpt(x_p,z_p,3)\n\n    #pgswin(y[0],y[-1],z[0],z[-1])\n    pgpanl(1,2)\n    pgswin(y[0],y[-1],-0.4,0.4)\n\n    pgsci(1)\n    pgbox()\n    pglab('Y axis','SDSS-SUBARU',file)     # label the plot\n    #pgsci(3)\n    #pgerrb(6,y_p,z_p,zerr_p)\n    pgpt(y_p,z_p,3)\n    pgsci(1)\n  \n\n\n    #print x_p\n    #print z_p \n    #print zerr_p\n\n    pgend()\n\n    os.system('mv ' + t + ' ' + file)\n    return\n\nif __name__ == '__main__': \n    import sys, os \n    tmpdir_root = sys.argv[1]   + '/' \n    os.chdir(tmpdir_root)\n    tmpdir = tmpdir_root + '/tmp/'\n    os.system('mkdir -p ' + tmpdir)\n    astrom = 'solve-field'\n    if len(sys.argv)>2:\n        astrom = sys.argv[2]\n    #select_analyze()\n    #match_OBJNAME()\n    #calc_good()\n    run_correction()\n    #test()\n    #run_scamp()\nelse:\n    if not 'loaded' in locals():\n        import tempfile              \n        os.system('mkdir -p /usr/work/pkelly')\n        tmpdir = tempfile.NamedTemporaryFile(dir='/usr/work/pkelly/').name\n        os.system('mkdir -p ' + tmpdir)\n        tmphome = tempfile.NamedTemporaryFile(dir='/usr/work/pkelly/').name\n        os.system('mkdir -p ' + tmphome)\n        ''' make an empty directory so that rmdir -rf does to remove tmphome '''\n        os.system('mkdir -p ' + tmphome + '/keep')\n        os.chdir(tmphome)\n        loaded = 'yes'\n        print 'loaded' in locals()\n", "\nimport datetime\n\nfrom tensorflow.contrib import layers\nfrom tensorflow.core.framework import summary_pb2\n\nimport matplotlib\n# matplotlib.use('Qt5Agg')\n\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\n\nfrom atari_wrappers import *\n\nGAMMA = 0.99\n\n# Entropy weight.\nBETA = 0.01\n\n# Max length of the episode.\nMAX_LENGTH = 500\n\nUSE_HUBER = False\nDELTA = 10.0\n\n# TASK_NAME=\"cartpole\"\n# TASK_NAME=\"pong\"\nTASK_NAME=\"pong-2d\"\n# TASK_NAME=\"breakout\"\n\n\nif TASK_NAME == \"cartpole\":\n    ADD_TIMESTAMP = True\nelse:\n    ADD_TIMESTAMP = False\n\ndef huber_loss(tensor):\n    abs_error = tf.abs(tensor)\n    quadratic = tf.minimum(abs_error, DELTA)\n    linear = (abs_error - quadratic)\n    return 0.5 * quadratic**2 + DELTA * linear\n\n\nclass ValueFunctionVisualizer(object):\n\n    def __init__(self, window_size):\n        self.window_size = window_size\n        self.xdata = range(window_size)\n        self.ydata = [0 for _ in range(window_size)]\n\n        self.fig, self.ax = plt.subplots(figsize=(12, 12), dpi=80)\n        self.ax.set_aspect('equal')\n        self.ax.set_xlim(0, self.window_size)\n        self.ax.set_ylim(-23, 23)\n        self.ax.hold(True)\n        self.line, = self.ax.plot(self.xdata, self.ydata, 'r-')\n\n        self.tick = 0\n\n        plt.show(block=False)\n\n        self.background = self.fig.canvas.copy_from_bbox(self.ax.bbox)\n        self.init_graph()\n\n    def init_graph(self):\n        self.fig.canvas.draw()\n        plt.draw()\n\n    def _redraw(self):\n        self.tick += 1\n        if self.tick % 5 != 1:\n            return\n\n        self.line.set_ydata(self.ydata)\n\n        self.fig.canvas.restore_region(self.background)\n        self.ax.draw_artist(self.line)\n        self.fig.canvas.blit(self.ax.bbox)\n\n    def add_value(self, ts, value):\n        self.ydata.append(value)\n        self.ydata = self.ydata[-self.window_size:]\n        self._redraw()\n\ndef build_shared_policy_and_value_2d(input_state, num_actions, reuse=False):\n    with tf.variable_scope(\"torso\", reuse=reuse):\n        out = input_state\n        out = layers.convolution2d(out, num_outputs=32, kernel_size=8, stride=4, activation_fn=tf.nn.relu)\n        out = layers.convolution2d(out, num_outputs=64, kernel_size=4, stride=2, activation_fn=tf.nn.relu)\n        out = layers.convolution2d(out, num_outputs=64, kernel_size=3, stride=1, activation_fn=tf.nn.relu)\n        out = layers.flatten(out)\n        out = layers.fully_connected(out, num_outputs=512, activation_fn=tf.nn.relu)\n\n        with tf.variable_scope(\"policy\"):\n            policy_out = layers.fully_connected(out, num_outputs=num_actions, activation_fn=None)\n            policy = layers.softmax(policy_out)\n\n        with tf.variable_scope(\"value\"):\n            value_out = layers.fully_connected(out, num_outputs=1, activation_fn=None)\n            value = tf.reshape(value_out, [-1])\n\n    return policy, value\n\ndef build_shared_policy_and_value(input_state, num_actions, reuse=False):\n    with tf.variable_scope(\"torso\", reuse=reuse):\n        out = layers.flatten(input_state)\n        out = layers.fully_connected(out, num_outputs=256, activation_fn=tf.nn.relu)\n        out = layers.fully_connected(out, num_outputs=128, activation_fn=tf.nn.relu)\n        out = layers.fully_connected(out, num_outputs=64, activation_fn=tf.nn.relu)\n        with tf.variable_scope(\"policy\"):\n            policy_out = layers.fully_connected(out, num_outputs=num_actions, activation_fn=None)\n            policy = layers.softmax(policy_out)\n\n        with tf.variable_scope(\"value\"):\n            value_out = layers.fully_connected(out, num_outputs=1, activation_fn=None)\n            value = tf.reshape(value_out, [-1])\n\n    return policy, value\n\ndef build_shared_policy_and_value_simple(input_state, num_actions, reuse=False):\n    with tf.variable_scope(\"torso\", reuse=reuse):\n        out = layers.flatten(input_state)\n        # out = layers.fully_connected(out, num_outputs=32, activation_fn=tf.nn.relu)\n        out = layers.fully_connected(out, num_outputs=256, activation_fn=tf.nn.relu)\n        with tf.variable_scope(\"policy\"):\n            policy_out = layers.fully_connected(out, num_outputs=num_actions, activation_fn=None)\n            policy = layers.softmax(policy_out)\n\n        with tf.variable_scope(\"value\"):\n            value_out = layers.fully_connected(out, num_outputs=1, activation_fn=None)\n            value = tf.reshape(value_out, [-1])\n\n    return policy, value\n\ndef build_separate_policy_and_value(input_state, num_actions, reuse=False):\n    with tf.variable_scope(\"torso\", reuse=reuse):\n        with tf.variable_scope(\"policy\"):\n            out = layers.flatten(input_state)\n            out = layers.fully_connected(out, num_outputs=32, activation_fn=tf.nn.relu)\n            out = layers.fully_connected(out, num_outputs=num_actions, activation_fn=None)\n            policy = layers.softmax(out)\n\n        with tf.variable_scope(\"value\"):\n            out = layers.flatten(input_state)\n            out = layers.fully_connected(out, num_outputs=32, activation_fn=tf.nn.relu)\n            out = layers.fully_connected(out, num_outputs=1, activation_fn=None)\n            value = tf.reshape(out, [-1])\n\n    return policy, value\n\n\ndef scalar_summary(name, value):\n    return summary_pb2.Summary(value=[summary_pb2.Summary.Value(tag=name, simple_value=value)])\n\n\ndef add_timestep(state, ts):\n    if ADD_TIMESTAMP:\n        return list(state) + [ts / MAX_LENGTH]\n    else:\n        return list(state)\n\n\ndef preprocess_state(state, ts):\n    if TASK_NAME == \"cartpole\":\n        return add_timestep(state, ts)\n    else:\n        return add_timestep(state / 256.0, ts)\n\n\n\nclass Actor(object):\n\n    def __init__(self, env, model_builder):\n        self.num_actions = env.action_space.n\n        input_state_shape = list(env.observation_space.shape)\n        # Adding a timestep.\n        input_state_shape[0] += ADD_TIMESTAMP\n        self.input_state_ph = tf.placeholder(dtype=tf.float32, shape=[None] + input_state_shape, name=\"input\")\n        self.policy, self.value = model_builder(self.input_state_ph, self.num_actions, reuse=True)\n\n    def eval_actions(self, session, state):\n        policy, value = session.run([self.policy, self.value], feed_dict={self.input_state_ph: [state]})\n        return policy[0], value[0]\n\n\ndef select(data, indices, num_actions):\n    return tf.reduce_sum(data * tf.one_hot(indices, num_actions, axis=-1), axis=1)\n\n\nclass Learner(object):\n\n    def __init__(self, env, model_builder, depth):\n        self.reset_state()\n\n        self.depth = depth\n\n        num_actions = env.action_space.n\n        input_state_shape = list(env.observation_space.shape)\n        input_state_shape[0] += ADD_TIMESTAMP\n        self.input_state_ph = tf.placeholder(dtype=tf.float32, shape=[None] + input_state_shape, name=\"input\")\n        self.next_input_state_ph = tf.placeholder(dtype=tf.float32, shape=[None] + input_state_shape, name=\"next_input\")\n\n        # tf.summary.image(\"input\", tf.reshape(self.input_state_ph, [-1, 4, 32, 1]))\n        # tf.summary.image(\"input\", tf.reshape(self.input_state_ph, [-1, 1, input_state_shape[0], 1]))\n\n        policy, value_function = model_builder(self.input_state_ph, num_actions)\n        _, next_value_function = model_builder(self.next_input_state_ph, num_actions, reuse=True)\n\n        # Training data placeholders.\n        self.action_ph = tf.placeholder(dtype=tf.int32, shape=[None], name=\"action_taken\")\n        self.rewards_ph = tf.placeholder(dtype=tf.float32, shape=[None], name=\"rewards\")\n        self.is_terminal_ph = tf.placeholder(dtype=tf.float32, shape=[None], name=\"is_terminal\")\n\n        self.gamma_power_ph = tf.placeholder(dtype=tf.float32, shape=[None], name=\"gamma_power\")\n\n        advantage = self.rewards_ph + self.gamma_power_ph * tf.stop_gradient(next_value_function) * (1.0 - self.is_terminal_ph) - value_function\n        tf.summary.histogram(\"value_function\", value_function)\n        tf.summary.histogram(\"advantage\", advantage)\n        with tf.variable_scope(\"action_log_policy\"):\n            action_log_policy = select(tf.log(policy), self.action_ph, num_actions)\n\n        entropy = -tf.reduce_mean(tf.reduce_sum(policy * tf.log(policy), axis=1))\n        tf.summary.scalar(\"entropy\", entropy)\n\n        trainable_vars = tf.trainable_variables()\n        policy_vars = trainable_vars\n        value_vars = trainable_vars\n\n        value_learning_rate = 1e-4\n\n        policy_optimizer = tf.train.AdamOptimizer()\n        value_optimizer = tf.train.RMSPropOptimizer(value_learning_rate)\n\n        with tf.variable_scope(\"loss\"):\n            policy_loss = -tf.reduce_mean(tf.stop_gradient(advantage) * action_log_policy, name=\"policy_loss\")\n            policy_loss_with_entropy = policy_loss - BETA * entropy\n            tf.summary.scalar(\"policy_loss\", policy_loss)\n            if USE_HUBER:\n                value_loss = tf.reduce_mean(huber_loss(advantage), name=\"value_loss\")\n            else:\n                value_loss = tf.reduce_mean(tf.square(advantage), name=\"value_loss\")\n            tf.summary.scalar(\"value_loss\", value_loss)\n\n            def minimize_clipped(optimizer, loss):\n                gvs = optimizer.compute_gradients(loss)\n                clipped_gvs = [(tf.clip_by_value(grad, -10., 10.), var) for grad, var in gvs if grad is not None]\n                # clipped_gvs = gvs\n                return optimizer.apply_gradients(clipped_gvs)\n\n            self.train_op = tf.group(minimize_clipped(policy_optimizer, policy_loss),\n                                     minimize_clipped(value_optimizer, value_loss))\n\n    def add_samples(self, states, rewards, actions, last_is_terminal):\n        N = len(actions)\n\n        for i in range(N):\n            self.states_.append(states[i])\n            j = min(i + self.depth, N)\n            self.next_states_.append(states[j])\n            self.gamma_powers_.append(GAMMA ** (j - i))\n\n            r = 0\n            for k in range(j - 1, i - 1, -1):\n                r = r * GAMMA + rewards[k]\n            self.rewards_.append(r)\n\n            self.is_terminal_.append(last_is_terminal * (j == N))\n\n        # self.states_.extend(states[:-1])\n        # self.next_states_.extend(states[1:])\n        # self.rewards_.extend(rewards)\n        self.actions_.extend(actions)\n\n        # self.is_terminal_.extend([0 for _ in range(N)])\n        # self.is_terminal_[-1] = last_is_terminal\n\n    def sample_count(self):\n        return len(self.states_)\n\n    def reset_state(self):\n        self.states_ = []\n        self.next_states_ = []\n        self.actions_ = []\n        self.rewards_ = []\n        self.is_terminal_ = []\n        self.gamma_powers_ = []\n\n    def train(self, session, summary_op):\n        summary, _ = session.run([summary_op, self.train_op], feed_dict={\n            self.input_state_ph: self.states_,\n            self.next_input_state_ph: self.next_states_,\n            self.action_ph: self.actions_,\n            self.rewards_ph: self.rewards_,\n            self.is_terminal_ph: self.is_terminal_,\n            self.gamma_power_ph: self.gamma_powers_,\n            })\n\n        self.reset_state()\n\n        return summary\n\n\ndef get_env_by_name(env_name):\n    env = gym.make(env_name)\n    seed = 42\n    env.seed(seed)\n    tf.set_random_seed(seed)\n    np.random.seed(seed)\n    return env\n\n\ndef get_env():\n    if TASK_NAME == \"pong-2d\":\n        benchmark = gym.benchmark_spec('Atari40M')\n        task = benchmark.tasks[3]\n        env_id = task.env_id\n\n        env = get_env_by_name(env_id)\n        # env = wrappers.Monitor(env, \"/tmp/{}\".format(TASK_NAME), force=True)\n        env = wrap_deepmind(env)\n        return env\n\n    if TASK_NAME == \"cartpole\":\n        env_name = \"CartPole-v1\"\n    elif TASK_NAME == \"breakout\":\n        env_name = \"Breakout-ram-v0\"\n    elif TASK_NAME == \"pong\":\n        env_name = \"Pong-ram-v0\"\n\n    get_env_by_name(env_name)\n\n\ndef main():\n    # gym.upload(\"/tmp/cartpole-experiment-1\", api_key=\"sk_scDpKlQS7ercGiVUZWEgw\")\n    # return\n\n    env = get_env()\n\n    episode_count = 1000000\n    max_steps = 10000\n\n    summaries_dir = \"/tmp/{}\".format(TASK_NAME)\n\n    tf_config = tf.ConfigProto(log_device_placement=False)\n\n    num_actions = env.action_space.n\n    # builder = build_shared_policy_and_value\n    builder = build_shared_policy_and_value_2d\n\n    batch_size = 256\n    simulation_depth = 20\n\n    learner = Learner(env, builder, simulation_depth)\n    actor = Actor(env, builder)\n\n    saver = tf.train.Saver()\n    merged_summaries = tf.summary.merge_all()\n\n    value_function_visualizer = ValueFunctionVisualizer(500)\n\n    SHOW_FREQUENCY = 100\n    CHECKPOINT_FREQUENCY = 100\n\n    global_step_tensor = tf.Variable(0, trainable=False, name=\"global_step\")\n    tick_global_step = tf.assign_add(global_step_tensor, 1, name=\"tick_global_step\")\n\n    CHECKPOINT_DIR = \"/home/acid/RL/{}\".format(TASK_NAME)\n    saver_hook = tf.train.CheckpointSaverHook(CHECKPOINT_DIR, save_secs=60, saver=saver)\n\n    with tf.train.SingularMonitoredSession(config=tf_config, hooks=[saver_hook]) as session:\n        summary_name = '{:%Y-%m-%d_%H:%M:%S}'.format(datetime.datetime.now())\n        summary_writer = tf.summary.FileWriter(\n                os.path.join(summaries_dir, summary_name), session.graph)\n\n        checkpoint = tf.train.get_checkpoint_state(CHECKPOINT_DIR)\n        if checkpoint and checkpoint.model_checkpoint_path:\n            print(\"Restoring from checkpoint {}\".format(checkpoint.model_checkpoint_path))\n            saver.restore(session, checkpoint.model_checkpoint_path)\n\n        start_time = time.clock()\n        total_frames = 0\n\n        for episode in range(episode_count):\n            state = preprocess_state(env.reset(), 0)\n            session.run(tick_global_step)\n\n            # Episode data.\n            states = []\n            rewards = []\n            actions = []\n\n            total_reward = 0\n            total_rewards = []\n\n            show_episode = episode % SHOW_FREQUENCY == 0\n            if show_episode:\n                value_function_visualizer.init_graph()\n\n            # if episode % CHECKPOINT_FREQUENCY == 0:\n                # saver.save(session.raw_session(), 'cartpole-model', global_step=episode)\n\n            for step in range(max_steps):\n                probs, value = actor.eval_actions(session, state)\n                action = np.random.choice(num_actions, p=probs)\n\n                if show_episode:\n                    env.render()\n                    value_function_visualizer.add_value(step, value)\n\n                next_state, reward, is_terminal, info = env.step(action)\n                next_state = preprocess_state(next_state, step + 1)\n                total_frames += 1\n                states.append(state)\n                actions.append(action)\n                rewards.append(reward)\n\n                total_reward += reward\n\n                if len(states) > batch_size:\n                    learner.add_samples(states + [next_state], rewards, actions, is_terminal)\n                    summary = learner.train(session, merged_summaries)\n                    summary_writer.add_summary(summary, total_frames)\n                    states = []\n                    rewards = []\n                    actions = []\n\n                if is_terminal:\n                    if rewards:\n                        learner.add_samples(states + [next_state], rewards, actions, is_terminal)\n                        summary = learner.train(session, merged_summaries)\n                        summary_writer.add_summary(summary, total_frames)\n\n                    total_rewards.append(total_reward)\n                    if episode % 100 == 0:\n                        total_rewards = total_rewards[-100:]\n                        avg_reward = np.mean(total_rewards)\n                        summary_writer.add_summary(scalar_summary(\"avg_reward\", avg_reward), episode)\n\n                        print(\"ep: {}, avg: {}, fps: {}\".format(\n                            episode,\n                            avg_reward,\n                            total_frames / (time.clock() - start_time)))\n                    break\n                else:\n                    state = next_state\n\nif __name__ == \"__main__\":\n    main()\n", "\n    \n    def __str__(self):\n        return self.name\n\nclass Dog(models.Model):\n    name = models.CharField(max_length=200)\n    call_name = models.CharField(max_length=50, null=True, blank=True)\n    UNKNOWN = 0\n    FEMALE = 1\n    MALE = 2\n    GENDER_CHOICES = ((MALE, 'Male'), (FEMALE, 'Female'), (UNKNOWN, 'Unknown'))\n    gender = models.SmallIntegerField(choices=GENDER_CHOICES, default=UNKNOWN)\n    date_of_birth = models.DateField(null=True, blank=True)\n    # AKC_number = models.CharField(max_length=20)\n    # Titles\n    # Owner\n    # Kennel\n    breed = models.ForeignKey(Breed, null=True, blank=True)\n    # variety = models.ForeignKey(Variety)\n    \n    def __str__(self):\n        return self.name\n    \n    # For grappelli\n    @staticmethod\n    def autocomplete_search_fields():\n        return (\"id__iexact\", \"name__icontains\",)\n\n#class Variety(models.Model):\n#    breed = models.ForeignKey(Breed)\n#    name = models.CharField(max_length=200)\n", " MIT software license, see the accompanying\n# file COPYING or http://www.opensource.org/licenses/mit-license.php.\n\nfrom test_framework.test_framework import BitcoinTestFramework\nfrom test_framework.util import *\n\nclass WalletTest (BitcoinTestFramework):\n\n    def check_fee_amount(self, curr_balance, balance_with_fee, fee_per_byte, tx_size):\n        \"\"\"Return curr_balance after asserting the fee was in range\"\"\"\n        fee = balance_with_fee - curr_balance\n        assert_fee_amount(fee, tx_size, fee_per_byte * 1000)\n        return curr_balance\n\n    def __init__(self):\n        super().__init__()\n        self.setup_clean_chain = True\n        self.num_nodes = 4\n        self.extra_args = [['-usehd={:d}'.format(i%2==0)] for i in range(4)]\n\n    def setup_network(self, split=False):\n        self.nodes = start_nodes(3, self.options.tmpdir, self.extra_args[:3], redirect_stderr=True)\n        connect_nodes_bi(self.nodes,0,1)\n        connect_nodes_bi(self.nodes,1,2)\n        connect_nodes_bi(self.nodes,0,2)\n        self.is_network_split=False\n        self.sync_all()\n\n    def run_test (self):\n\n        # Check that there's no UTXO on none of the nodes\n        assert_equal(len(self.nodes[0].listunspent()), 0)\n        assert_equal(len(self.nodes[1].listunspent()), 0)\n        assert_equal(len(self.nodes[2].listunspent()), 0)\n\n        print(\"Mining blocks...\")\n\n        self.nodes[0].generate(1)\n\n        walletinfo = self.nodes[0].getwalletinfo()\n        assert_equal(walletinfo['immature_balance'], 500)\n        assert_equal(walletinfo['balance'], 0)\n\n        self.sync_all()\n        self.nodes[1].generate(101)\n        self.sync_all()\n\n        assert_equal(self.nodes[0].getbalance(), 500)\n        assert_equal(self.nodes[1].getbalance(), 500)\n        assert_equal(self.nodes[2].getbalance(), 0)\n\n        # Check that only first and second nodes have UTXOs\n        assert_equal(len(self.nodes[0].listunspent()), 1)\n        assert_equal(len(self.nodes[1].listunspent()), 1)\n        assert_equal(len(self.nodes[2].listunspent()), 0)\n\n        # Send 210 DASH from 0 to 2 using sendtoaddress call.\n        # Second transaction will be child of first, and will require a fee\n        self.nodes[0].sendtoaddress(self.nodes[2].getnewaddress(), 110)\n        self.nodes[0].sendtoaddress(self.nodes[2].getnewaddress(), 100)\n\n        walletinfo = self.nodes[0].getwalletinfo()\n        assert_equal(walletinfo['immature_balance'], 0)\n\n        # Have node0 mine a block, thus it will collect its own fee.\n        self.nodes[0].generate(1)\n        self.sync_all()\n\n        # Exercise locking of unspent outputs\n        unspent_0 = self.nodes[2].listunspent()[0]\n        unspent_0 = {\"txid\": unspent_0[\"txid\"], \"vout\": unspent_0[\"vout\"]}\n        self.nodes[2].lockunspent(False, [unspent_0])\n        assert_raises_message(JSONRPCException, \"Insufficient funds\", self.nodes[2].sendtoaddress, self.nodes[2].getnewaddress(), 200)\n        assert_equal([unspent_0], self.nodes[2].listlockunspent())\n        self.nodes[2].lockunspent(True, [unspent_0])\n        assert_equal(len(self.nodes[2].listlockunspent()), 0)\n\n        # Have node1 generate 100 blocks (so node0 can recover the fee)\n        self.nodes[1].generate(100)\n        self.sync_all()\n\n        # node0 should end up with 1000 DASH in block rewards plus fees, but\n        # minus the 210 plus fees sent to node2\n        assert_equal(self.nodes[0].getbalance(), 1000-210)\n        assert_equal(self.nodes[2].getbalance(), 210)\n\n        # Node0 should have two unspent outputs.\n        # Create a couple of transactions to send them to node2, submit them through\n        # node1, and make sure both node0 and node2 pick them up properly:\n        node0utxos = self.nodes[0].listunspent(1)\n        assert_equal(len(node0utxos), 2)\n\n        # create both transactions\n        txns_to_send = []\n        for utxo in node0utxos:\n            inputs = []\n            outputs = {}\n            inputs.append({ \"txid\" : utxo[\"txid\"], \"vout\" : utxo[\"vout\"]})\n            outputs[self.nodes[2].getnewaddress(\"from1\")] = utxo[\"amount\"]\n            raw_tx = self.nodes[0].createrawtransaction(inputs, outputs)\n            txns_to_send.append(self.nodes[0].signrawtransaction(raw_tx))\n\n        # Have node 1 (miner) send the transactions\n        self.nodes[1].sendrawtransaction(txns_to_send[0][\"hex\"], True, False, True)\n        self.nodes[1].sendrawtransaction(txns_to_send[1][\"hex\"], True, False, True)\n\n        # Have node1 mine a block to confirm transactions:\n        self.nodes[1].generate(1)\n        self.sync_all()\n\n        assert_equal(self.nodes[0].getbalance(), 0)\n        assert_equal(self.nodes[2].getbalance(), 1000)\n        assert_equal(self.nodes[2].getbalance(\"from1\"), 1000-210)\n\n        # Send 100 DASH normal\n        address = self.nodes[0].getnewaddress(\"test\")\n        fee_per_byte = Decimal('0.00001') / 1000\n        self.nodes[2].settxfee(fee_per_byte * 1000)\n        txid = self.nodes[2].sendtoaddress(address, 100, \"\", \"\", False)\n        self.nodes[2].generate(1)\n        self.sync_all()\n        node_2_bal = self.check_fee_amount(self.nodes[2].getbalance(), Decimal('900'), fee_per_byte, count_bytes(self.nodes[2].getrawtransaction(txid)))\n        assert_equal(self.nodes[0].getbalance(), Decimal('100'))\n\n        # Send 100 DASH with subtract fee from amount\n        txid = self.nodes[2].sendtoaddress(address, 100, \"\", \"\", True)\n        self.nodes[2].generate(1)\n        self.sync_all()\n        node_2_bal -= Decimal('100')\n        assert_equal(self.nodes[2].getbalance(), node_2_bal)\n        node_0_bal = self.check_fee_amount(self.nodes[0].getbalance(), Decimal('200'), fee_per_byte, count_bytes(self.nodes[2].getrawtransaction(txid)))\n\n        # Sendmany 100 DASH\n        txid = self.nodes[2].sendmany('from1', {address: 100}, 0, False, \"\", [])\n        self.nodes[2].generate(1)\n        self.sync_all()\n        node_0_bal += Decimal('100')\n        node_2_bal = self.check_fee_amount(self.nodes[2].getbalance(), node_2_bal - Decimal('100'), fee_per_byte, count_bytes(self.nodes[2].getrawtransaction(txid)))\n        assert_equal(self.nodes[0].getbalance(), node_0_bal)\n\n        # Sendmany 100 DASH with subtract fee from amount\n        txid = self.nodes[2].sendmany('from1', {address: 100}, 0, False, \"\", [address])\n        self.nodes[2].generate(1)\n        self.sync_all()\n        node_2_bal -= Decimal('100')\n        assert_equal(self.nodes[2].getbalance(), node_2_bal)\n        node_0_bal = self.check_fee_amount(self.nodes[0].getbalance(), node_0_bal + Decimal('100'), fee_per_byte, count_bytes(self.nodes[2].getrawtransaction(txid)))\n\n        # Test ResendWalletTransactions:\n        # Create a couple of transactions, then start up a fourth\n        # node (nodes[3]) and ask nodes[0] to rebroadcast.\n        # EXPECT: nodes[3] should have those transactions in its mempool.\n        txid1 = self.nodes[0].sendtoaddress(self.nodes[1].getnewaddress(), 1)\n        txid2 = self.nodes[1].sendtoaddress(self.nodes[0].getnewaddress(), 1)\n        sync_mempools(self.nodes)\n\n        self.nodes.append(start_node(3, self.options.tmpdir, self.extra_args[3], redirect_stderr=True))\n        connect_nodes_bi(self.nodes, 0, 3)\n        sync_blocks(self.nodes)\n\n        relayed = self.nodes[0].resendwallettransactions()\n        assert_equal(set(relayed), {txid1, txid2})\n        sync_mempools(self.nodes)\n\n        assert(txid1 in self.nodes[3].getrawmempool())\n\n        # Exercise balance rpcs\n        assert_equal(self.nodes[0].getwalletinfo()[\"unconfirmed_balance\"], 1)\n        assert_equal(self.nodes[0].getunconfirmedbalance(), 1)\n\n        #check if we can list zero value tx as available coins\n        #1. create rawtx\n        #2. hex-changed one output to 0.0\n        #3. sign and send\n        #4. check if recipient (node0) can list the zero value tx\n        usp = self.nodes[1].listunspent()\n        inputs = [{\"txid\":usp[0]['txid'], \"vout\":usp[0]['vout']}]\n        outputs = {self.nodes[1].getnewaddress(): 499.998, self.nodes[0].getnewaddress(): 11.11}\n\n        rawTx = self.nodes[1].createrawtransaction(inputs, outputs).replace(\"c0833842\", \"00000000\") #replace 11.11 with 0.0 (int32)\n        decRawTx = self.nodes[1].decoderawtransaction(rawTx)\n        signedRawTx = self.nodes[1].signrawtransaction(rawTx)\n        decRawTx = self.nodes[1].decoderawtransaction(signedRawTx['hex'])\n        zeroValueTxid= decRawTx['txid']\n        sendResp = self.nodes[1].sendrawtransaction(signedRawTx['hex'])\n\n        self.sync_all()\n        self.nodes[1].generate(1) #mine a block\n        self.sync_all()\n\n        unspentTxs = self.nodes[0].listunspent() #zero value tx must be in listunspents output\n        found = False\n        for uTx in unspentTxs:\n            if uTx['txid'] == zeroValueTxid:\n                found = True\n                assert_equal(uTx['amount'], Decimal('0'))\n        assert(found)\n\n        #do some -walletbroadcast tests\n        stop_nodes(self.nodes)\n        self.nodes = start_nodes(3, self.options.tmpdir, [[\"-walletbroadcast=0\"],[\"-walletbroadcast=0\"],[\"-walletbroadcast=0\"]])\n        connect_nodes_bi(self.nodes,0,1)\n        connect_nodes_bi(self.nodes,1,2)\n        connect_nodes_bi(self.nodes,0,2)\n        self.sync_all()\n\n        txIdNotBroadcasted  = self.nodes[0].sendtoaddress(self.nodes[2].getnewaddress(), 2)\n        txObjNotBroadcasted = self.nodes[0].gettransaction(txIdNotBroadcasted)\n        self.nodes[1].generate(1) #mine a block, tx should not be in there\n        self.sync_all()\n        assert_equal(self.nodes[2].getbalance(), node_2_bal) #should not be changed because tx was not broadcasted\n\n        #now broadcast from another node, mine a block, sync, and check the balance\n        self.nodes[1].sendrawtransaction(txObjNotBroadcasted['hex'])\n        self.nodes[1].generate(1)\n        self.sync_all()\n        node_2_bal += 2\n        txObjNotBroadcasted = self.nodes[0].gettransaction(txIdNotBroadcasted)\n        assert_equal(self.nodes[2].getbalance(), node_2_bal)\n\n        #create another tx\n        txIdNotBroadcasted  = self.nodes[0].sendtoaddress(self.nodes[2].getnewaddress(), 2)\n\n        #restart the nodes with -walletbroadcast=1\n        stop_nodes(self.nodes)\n        self.nodes = start_nodes(3, self.options.tmpdir)\n        connect_nodes_bi(self.nodes,0,1)\n        connect_nodes_bi(self.nodes,1,2)\n        connect_nodes_bi(self.nodes,0,2)\n        sync_blocks(self.nodes)\n\n        self.nodes[0].generate(1)\n        sync_blocks(self.nodes)\n        node_2_bal += 2\n\n        #tx should be added to balance because after restarting the nodes tx should be broadcastet\n        assert_equal(self.nodes[2].getbalance(), node_2_bal)\n\n        #send a tx with value in a string (PR#6380 +)\n        txId  = self.nodes[0].sendtoaddress(self.nodes[2].getnewaddress(), \"2\")\n        txObj = self.nodes[0].gettransaction(txId)\n        assert_equal(txObj['amount'], Decimal('-2'))\n\n        txId  = self.nodes[0].sendtoaddress(self.nodes[2].getnewaddress(), \"0.0001\")\n        txObj = self.nodes[0].gettransaction(txId)\n        assert_equal(txObj['amount'], Decimal('-0.0001'))\n\n        #check if JSON parser can handle scientific notation in strings\n        txId  = self.nodes[0].sendtoaddress(self.nodes[2].getnewaddress(), \"1e-4\")\n        txObj = self.nodes[0].gettransaction(txId)\n        assert_equal(txObj['amount'], Decimal('-0.0001'))\n\n        try:\n            txId  = self.nodes[0].sendtoaddress(self.nodes[2].getnewaddress(), \"1f-4\")\n        except JSONRPCException as e:\n            assert(\"Invalid amount\" in e.error['message'])\n        else:\n            raise AssertionError(\"Must not parse invalid amounts\")\n\n\n        try:\n            self.nodes[0].generate(\"2\")\n            raise AssertionError(\"Must not accept strings as numeric\")\n        except JSONRPCException as e:\n            assert(\"not an integer\" in e.error['message'])\n\n        # Import address and private key to check correct behavior of spendable unspents\n        # 1. Send some coins to generate new UTXO\n        address_to_import = self.nodes[2].getnewaddress()\n        txid = self.nodes[0].sendtoaddress(address_to_import, 1)\n        self.nodes[0].generate(1)\n        self.sync_all()\n\n        # 2. Import address from node2 to node1\n        self.nodes[1].importaddress(address_to_import)\n\n        # 3. Validate that the imported address is watch-only on node1\n        assert(self.nodes[1].validateaddress(address_to_import)[\"iswatchonly\"])\n\n        # 4. Check that the unspents after import are not spendable\n        assert_array_result(self.nodes[1].listunspent(),\n                           {\"address\": address_to_import},\n                           {\"spendable\": False})\n\n        # 5. Import private key of the previously imported address on node1\n        priv_key = self.nodes[2].dumpprivkey(address_to_import)\n        self.nodes[1].importprivkey(priv_key)\n\n        # 6. Check that the unspents are now spendable on node1\n        assert_array_result(self.nodes[1].listunspent(),\n                           {\"address\": address_to_import},\n                           {\"spendable\": True})\n\n        # Mine a block from node0 to an address from node1\n        cbAddr = self.nodes[1].getnewaddress()\n        blkHash = self.nodes[0].generatetoaddress(1, cbAddr)[0]\n        cbTxId = self.nodes[0].getblock(blkHash)['tx'][0]\n        self.sync_all()\n\n        # Check that the txid and balance is found by node1\n        self.nodes[1].gettransaction(cbTxId)\n\n        # check if wallet or blockchain maintenance changes the balance\n        self.sync_all()\n        blocks = self.nodes[0].generate(2)\n        self.sync_all()\n        balance_nodes = [self.nodes[i].getbalance() for i in range(3)]\n        block_count = self.nodes[0].getblockcount()\n\n        # Check modes:\n        #   - True: unicode escaped as \\u....\n        #   - False: unicode directly as UTF-8\n        for mode in [True, False]:\n            self.nodes[0].ensure_ascii = mode\n            # unicode check: Basic Multilingual Plane, Supplementary Plane respectively\n            for s in [u'\u0440\u044b\u0431\u0430', u'\ud834\udd61']:\n                addr = self.nodes[0].getaccountaddress(s)\n                label = self.nodes[0].getaccount(addr)\n                assert_equal(label, s)\n                assert(s in self.nodes[0].listaccounts().keys())\n        self.nodes[0].ensure_ascii = True # restore to default\n\n        # maintenance tests\n        maintenance = [\n            '-rescan',\n            '-reindex',\n            '-zapwallettxes=1',\n            '-zapwallettxes=2',\n            # disabled until issue is fixed: https://github.com/bitcoin/bitcoin/issues/7463\n            # '-salvagewallet',\n        ]\n        chainlimit = 6\n        for m in maintenance:\n            print(\"check \" + m)\n            stop_nodes(self.nodes)\n            # set lower ancestor limit for later\n            self.nodes = start_nodes(3, self.options.tmpdir, [[m, \"-limitancestorcount=\"+str(chainlimit)]] * 3)\n            while m == '-reindex' and [block_count] * 3 != [self.nodes[i].getblockcount() for i in range(3)]:\n                # reindex will leave rpc warm up \"early\"; Wait for it to finish\n                time.sleep(0.1)\n            assert_equal(balance_nodes, [self.nodes[i].getbalance() for i in range(3)])\n\n        # Exercise listsinceblock with the last two blocks\n        coinbase_tx_1 = self.nodes[0].listsinceblock(blocks[0])\n        assert_equal(coinbase_tx_1[\"lastblock\"], blocks[1])\n        assert_equal(len(coinbase_tx_1[\"transactions\"]), 1)\n        assert_equal(coinbase_tx_1[\"transactions\"][0][\"blockhash\"], blocks[1])\n        assert_equal(len(self.nodes[0].listsinceblock(blocks[1])[\"transactions\"]), 0)\n\n        # ==Check that wallet prefers to use coins that don't exceed mempool limits =====\n\n        # Get all non-zero utxos together\n        chain_addrs = [self.nodes[0].getnewaddress(), self.nodes[0].getnewaddress()]\n        singletxid = self.nodes[0].sendtoaddress(chain_addrs[0], self.nodes[0].getbalance(), \"\", \"\", True)\n        self.nodes[0].generate(1)\n        node0_balance = self.nodes[0].getbalance()\n        # Split into two chains\n        rawtx = self.nodes[0].createrawtransaction([{\"txid\":singletxid, \"vout\":0}], {chain_addrs[0]:node0_balance/2-Decimal('0.01'), chain_addrs[1]:node0_balance/2-Decimal('0.01')})\n        signedtx = self.nodes[0].signrawtransaction(rawtx)\n        singletxid = self.nodes[0].sendrawtransaction(signedtx[\"hex\"])\n        self.nodes[0].generate(1)\n\n        # Make a long chain of unconfirmed payments without hitting mempool limit\n        # Each tx we make leaves only one output of change on a chain 1 longer\n        # Since the amount to send is always much less than the outputs, we only ever need one output\n        # So we should be able to generate exactly chainlimit txs for each original output\n        sending_addr = self.nodes[1].getnewaddress()\n        txid_list = []\n        for i in range(chainlimit*2):\n            txid_list.append(self.nodes[0].sendtoaddress(sending_addr, Decimal('0.0001')))\n        assert_equal(self.nodes[0].getmempoolinfo()['size'], chainlimit*2)\n        assert_equal(len(txid_list), chainlimit*2)\n\n        # Without walletrejectlongchains, we will still generate a txid\n        # The tx will be stored in the wallet but not accepted to the mempool\n        extra_txid = self.nodes[0].sendtoaddress(sending_addr, Decimal('0.0001'))\n        assert(extra_txid not in self.nodes[0].getrawmempool())\n        assert(extra_txid in [tx[\"txid\"] for tx in self.nodes[0].listtransactions()])\n        self.nodes[0].abandontransaction(extra_txid)\n        total_txs = len(self.nodes[0].listtransactions(\"*\",99999))\n\n        # Try with walletrejectlongchains\n        # Double chain limit but require combining inputs, so we pass SelectCoinsMinConf\n        stop_node(self.nodes[0],0)\n        self.nodes[0] = start_node(0, self.options.tmpdir, [\"-walletrejectlongchains\", \"-limitancestorcount=\"+str(2*chainlimit)])\n\n        # wait for loadmempool\n        timeout = 10\n        while (timeout > 0 and len(self.nodes[0].getrawmempool()) < chainlimit*2):\n            time.sleep(0.5)\n            timeout -= 0.5\n        assert_equal(len(self.nodes[0].getrawmempool()), chainlimit*2)\n\n        node0_balance = self.nodes[0].getbalance()\n        # With walletrejectlongchains we will not create the tx and store it in our wallet.\n        assert_raises_message(JSONRPCException, \"mempool chain\", self.nodes[0].sendtoaddress, sending_addr, node0_balance - Decimal('0.01'))\n\n        # Verify nothing new in wallet\n        assert_equal(total_txs, len(self.nodes[0].listtransactions(\"*\",99999)))\n\nif __name__ == '__main__':\n    WalletTest().main()\n", ")\nmongo = PyMongo(app)\n\n\n@app.route(\"/\")\ndef home_page():\n    unsafe_search = request.args['search']\n    json_search = json.loads(unsafe_search)\n\n    return mongo.db.user.find({'name': json_search})\n\n# if __name__ == \"__main__\":\n#     app.run(debug=True)\n", "", "def index(self):\n      dh = random.randint(1, 1200)\n      of = random.choice([\"jpg\", \"gif\", \"png\", \"webp\"])\n      self.client.get(\"/local/small_light(dh=\" + str(dh) + \",da=l,ds=s,cc=FFFFFF,of=\" + of + \")/images/example.jpg\")\n\nclass WebsiteUser(HttpLocust):\n    task_set = MyTaskSet\n    min_wait=5000\n    max_wait=9000\n", "False\n        l[i] = c.upper()\n    elif c == ' ':\n        cap = True\n\nprint ''.join(l)", "  \r\n    def __init__(self, body, speed):\r\n        \"\"\" Initialize with the speed to jump at \"\"\"\r\n        self.body = body\r\n        self.speed = Vec2(0, speed)\r\n        \r\n    def jump(self, event=None):\r\n        \"\"\" Jump the entity \"\"\"\r\n        impulse = self.speed*self.body.mass\r\n        self.body.applyImpulse(impulse)", "img\n", ""], "perplexity": [2.0627329349517822, 2.5861656665802, 2.875483751296997, 1.4094871282577515, 2.1911752223968506, 2.6264398097991943, 5.006896495819092, 1.7774525880813599, 2.226506233215332, 2.3398547172546387, 2.2342233657836914, 1.0828349590301514, 1.2614737749099731, 3.4235622882843018, 2.841385841369629, 2.0664782524108887, 2.2565038204193115, 1.2184906005859375, 1.4276528358459473, 1.2159662246704102, 1.8419257402420044, 1.5218229293823242, 3.3266050815582275, 10.06508731842041, 2.995373249053955, 1.7077347040176392, 1.1486762762069702, 5.100645542144775, 1.3484586477279663, 1.673787236213684, 4.427168369293213, 2.7207190990448, 1.5226311683654785, 2.262845754623413, 1.1848691701889038, 3.80417799949646, 2.2856342792510986, 2.588259696960449, 2.4464266300201416, 7.506635665893555, 2.3724045753479004, 2.240233898162842, 3.638737916946411, 3.5863518714904785, 1.7992432117462158, 2.2111825942993164, 3.8398630619049072, 1.9495784044265747, 2.47471284866333, 3.126020669937134, 4.173168659210205, 2.9265003204345703, 11.01302719116211, 7.689385890960693, 2.2551910877227783, 3.2677621841430664, 1.7459590435028076, 2.1148431301116943, 2.233361005783081, 3.008251905441284, 1.6837488412857056, 1.1407088041305542, 2.427945852279663, 4.8384175300598145, 3.9855246543884277, 8.61389446258545, 3.1153616905212402, 4.805242538452148, 1.7335315942764282, 3.807865858078003, 7.453574180603027, 3.5908823013305664, 1.7311558723449707, 3.027017116546631, 1.618388295173645, 11.365509033203125, 2.87992000579834, 2.0409066677093506, 1.6677931547164917, 1.6407713890075684, 1.4914265871047974, 2.9640285968780518, 1.5178989171981812, 1.8605015277862549, 3.3444199562072754, 2.6944563388824463, 20.18926429748535, 2.9431633949279785, 1.7091847658157349, 2.6795356273651123, 1.936262845993042, 2.2895967960357666, 3.1225359439849854, 2.346045970916748, 48.18834686279297, 4.8815460205078125, 2.8030612468719482, 3.9299519062042236, 15.067368507385254, 16.28533363342285], "avg_perplexity": 3.8868812012672422}