{"text": [".0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport warnings\nfrom typing import Awaitable, Callable, Dict, Optional, Sequence, Tuple, Union\n\nfrom google.api_core import gapic_v1\nfrom google.api_core import grpc_helpers_async\nfrom google.auth import credentials as ga_credentials  # type: ignore\nfrom google.auth.transport.grpc import SslCredentials  # type: ignore\n\nimport grpc  # type: ignore\nfrom grpc.experimental import aio  # type: ignore\n\nfrom google.cloud.dialogflowcx_v3.types import session_entity_type\nfrom google.cloud.dialogflowcx_v3.types import (\n    session_entity_type as gcdc_session_entity_type,\n)\nfrom google.protobuf import empty_pb2  # type: ignore\nfrom .base import SessionEntityTypesTransport, DEFAULT_CLIENT_INFO\nfrom .grpc import SessionEntityTypesGrpcTransport\n\n\nclass SessionEntityTypesGrpcAsyncIOTransport(SessionEntityTypesTransport):\n    \"\"\"gRPC AsyncIO backend transport for SessionEntityTypes.\n\n    Service for managing\n    [SessionEntityTypes][google.cloud.dialogflow.cx.v3.SessionEntityType].\n\n    This class defines the same methods as the primary client, so the\n    primary client can load the underlying transport implementation\n    and call it.\n\n    It sends protocol buffers over the wire using gRPC (which is built on\n    top of HTTP/2); the ``grpcio`` package must be installed.\n    \"\"\"\n\n    _grpc_channel: aio.Channel\n    _stubs: Dict[str, Callable] = {}\n\n    @classmethod\n    def create_channel(\n        cls,\n        host: str = \"dialogflow.googleapis.com\",\n        credentials: ga_credentials.Credentials = None,\n        credentials_file: Optional[str] = None,\n        scopes: Optional[Sequence[str]] = None,\n        quota_project_id: Optional[str] = None,\n        **kwargs,\n    ) -> aio.Channel:\n        \"\"\"Create and return a gRPC AsyncIO channel object.\n        Args:\n            host (Optional[str]): The host for the channel to use.\n            credentials (Optional[~.Credentials]): The\n                authorization credentials to attach to requests. These\n                credentials identify this application to the service. If\n                none are specified, the client will attempt to ascertain\n                the credentials from the environment.\n            credentials_file (Optional[str]): A file with credentials that can\n                be loaded with :func:`google.auth.load_credentials_from_file`.\n                This argument is ignored if ``channel`` is provided.\n            scopes (Optional[Sequence[str]]): A optional list of scopes needed for this\n                service. These are only used when credentials are not specified and\n                are passed to :func:`google.auth.default`.\n            quota_project_id (Optional[str]): An optional project to use for billing\n                and quota.\n            kwargs (Optional[dict]): Keyword arguments, which are passed to the\n                channel creation.\n        Returns:\n            aio.Channel: A gRPC AsyncIO channel object.\n        \"\"\"\n\n        return grpc_helpers_async.create_channel(\n            host,\n            credentials=credentials,\n            credentials_file=credentials_file,\n            quota_project_id=quota_project_id,\n            default_scopes=cls.AUTH_SCOPES,\n            scopes=scopes,\n            default_host=cls.DEFAULT_HOST,\n            **kwargs,\n        )\n\n    def __init__(\n        self,\n        *,\n        host: str = \"dialogflow.googleapis.com\",\n        credentials: ga_credentials.Credentials = None,\n        credentials_file: Optional[str] = None,\n        scopes: Optional[Sequence[str]] = None,\n        channel: aio.Channel = None,\n        api_mtls_endpoint: str = None,\n        client_cert_source: Callable[[], Tuple[bytes, bytes]] = None,\n        ssl_channel_credentials: grpc.ChannelCredentials = None,\n        client_cert_source_for_mtls: Callable[[], Tuple[bytes, bytes]] = None,\n        quota_project_id=None,\n        client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,\n        always_use_jwt_access: Optional[bool] = False,\n    ) -> None:\n        \"\"\"Instantiate the transport.\n\n        Args:\n            host (Optional[str]):\n                 The hostname to connect to.\n            credentials (Optional[google.auth.credentials.Credentials]): The\n                authorization credentials to attach to requests. These\n                credentials identify the application to the service; if none\n                are specified, the client will attempt to ascertain the\n                credentials from the environment.\n                This argument is ignored if ``channel`` is provided.\n            credentials_file (Optional[str]): A file with credentials that can\n                be loaded with :func:`google.auth.load_credentials_from_file`.\n                This argument is ignored if ``channel`` is provided.\n            scopes (Optional[Sequence[str]]): A optional list of scopes needed for this\n                service. These are only used when credentials are not specified and\n                are passed to :func:`google.auth.default`.\n            channel (Optional[aio.Channel]): A ``Channel`` instance through\n                which to make calls.\n            api_mtls_endpoint (Optional[str]): Deprecated. The mutual TLS endpoint.\n                If provided, it overrides the ``host`` argument and tries to create\n                a mutual TLS channel with client SSL credentials from\n                ``client_cert_source`` or application default SSL credentials.\n            client_cert_source (Optional[Callable[[], Tuple[bytes, bytes]]]):\n                Deprecated. A callback to provide client SSL certificate bytes and\n                private key bytes, both in PEM format. It is ignored if\n                ``api_mtls_endpoint`` is None.\n            ssl_channel_credentials (grpc.ChannelCredentials): SSL credentials\n                for the grpc channel. It is ignored if ``channel`` is provided.\n            client_cert_source_for_mtls (Optional[Callable[[], Tuple[bytes, bytes]]]):\n                A callback to provide client certificate bytes and private key bytes,\n                both in PEM format. It is used to configure a mutual TLS channel. It is\n                ignored if ``channel`` or ``ssl_channel_credentials`` is provided.\n            quota_project_id (Optional[str]): An optional project to use for billing\n                and quota.\n            client_info (google.api_core.gapic_v1.client_info.ClientInfo):\n                The client info used to send a user-agent string along with\n                API requests. If ``None``, then default info will be used.\n                Generally, you only need to set this if you're developing\n                your own client library.\n            always_use_jwt_access (Optional[bool]): Whether self signed JWT should\n                be used for service account credentials.\n\n        Raises:\n            google.auth.exceptions.MutualTlsChannelError: If mutual TLS transport\n              creation failed for any reason.\n          google.api_core.exceptions.DuplicateCredentialArgs: If both ``credentials``\n              and ``credentials_file`` are passed.\n        \"\"\"\n        self._grpc_channel = None\n        self._ssl_channel_credentials = ssl_channel_credentials\n        self._stubs: Dict[str, Callable] = {}\n\n        if api_mtls_endpoint:\n            warnings.warn(\"api_mtls_endpoint is deprecated\", DeprecationWarning)\n        if client_cert_source:\n            warnings.warn(\"client_cert_source is deprecated\", DeprecationWarning)\n\n        if channel:\n            # Ignore credentials if a channel was passed.\n            credentials = False\n            # If a channel was explicitly provided, set it.\n            self._grpc_channel = channel\n            self._ssl_channel_credentials = None\n        else:\n            if api_mtls_endpoint:\n                host = api_mtls_endpoint\n\n                # Create SSL credentials with client_cert_source or application\n                # default SSL credentials.\n                if client_cert_source:\n                    cert, key = client_cert_source()\n                    self._ssl_channel_credentials = grpc.ssl_channel_credentials(\n                        certificate_chain=cert, private_key=key\n                    )\n                else:\n                    self._ssl_channel_credentials = SslCredentials().ssl_credentials\n\n            else:\n                if client_cert_source_for_mtls and not ssl_channel_credentials:\n                    cert, key = client_cert_source_for_mtls()\n                    self._ssl_channel_credentials = grpc.ssl_channel_credentials(\n                        certificate_chain=cert, private_key=key\n                    )\n\n        # The base transport sets the host, credentials and scopes\n        super().__init__(\n            host=host,\n            credentials=credentials,\n            credentials_file=credentials_file,\n            scopes=scopes,\n            quota_project_id=quota_project_id,\n            client_info=client_info,\n            always_use_jwt_access=always_use_jwt_access,\n        )\n\n        if not self._grpc_channel:\n            self._grpc_channel = type(self).create_channel(\n                self._host,\n                # use the credentials which are saved\n                credentials=self._credentials,\n                # Set ``credentials_file`` to ``None`` here as\n                # the credentials that we saved earlier should be used.\n                credentials_file=None,\n                scopes=self._scopes,\n                ssl_credentials=self._ssl_channel_credentials,\n                quota_project_id=quota_project_id,\n                options=[\n                    (\"grpc.max_send_message_length\", -1),\n                    (\"grpc.max_receive_message_length\", -1),\n                ],\n            )\n\n        # Wrap messages. This must be done after self._grpc_channel exists\n        self._prep_wrapped_messages(client_info)\n\n    @property\n    def grpc_channel(self) -> aio.Channel:\n        \"\"\"Create the channel designed to connect to this service.\n\n        This property caches on the instance; repeated calls return\n        the same channel.\n        \"\"\"\n        # Return the channel from cache.\n        return self._grpc_channel\n\n    @property\n    def list_session_entity_types(\n        self,\n    ) -> Callable[\n        [session_entity_type.ListSessionEntityTypesRequest],\n        Awaitable[session_entity_type.ListSessionEntityTypesResponse],\n    ]:\n        r\"\"\"Return a callable for the list session entity types method over gRPC.\n\n        Returns the list of all session entity types in the\n        specified session.\n\n        Returns:\n            Callable[[~.ListSessionEntityTypesRequest],\n                    Awaitable[~.ListSessionEntityTypesResponse]]:\n                A function that, when called, will call the underlying RPC\n                on the server.\n        \"\"\"\n        # Generate a \"stub function\" on-the-fly which will actually make\n        # the request.\n        # gRPC handles serialization and deserialization, so we just need\n        # to pass in the functions for each.\n        if \"list_session_entity_types\" not in self._stubs:\n            self._stubs[\"list_session_entity_types\"] = self.grpc_channel.unary_unary(\n                \"/google.cloud.dialogflow.cx.v3.SessionEntityTypes/ListSessionEntityTypes\",\n                request_serializer=session_entity_type.ListSessionEntityTypesRequest.serialize,\n                response_deserializer=session_entity_type.ListSessionEntityTypesResponse.deserialize,\n            )\n        return self._stubs[\"list_session_entity_types\"]\n\n    @property\n    def get_session_entity_type(\n        self,\n    ) -> Callable[\n        [session_entity_type.GetSessionEntityTypeRequest],\n        Awaitable[session_entity_type.SessionEntityType],\n    ]:\n        r\"\"\"Return a callable for the get session entity type method over gRPC.\n\n        Retrieves the specified session entity type.\n\n        Returns:\n            Callable[[~.GetSessionEntityTypeRequest],\n                    Awaitable[~.SessionEntityType]]:\n                A function that, when called, will call the underlying RPC\n                on the server.\n        \"\"\"\n        # Generate a \"stub function\" on-the-fly which will actually make\n        # the request.\n        # gRPC handles serialization and deserialization, so we just need\n        # to pass in the functions for each.\n        if \"get_session_entity_type\" not in self._stubs:\n            self._stubs[\"get_session_entity_type\"] = self.grpc_channel.unary_unary(\n                \"/google.cloud.dialogflow.cx.v3.SessionEntityTypes/GetSessionEntityType\",\n                request_serializer=session_entity_type.GetSessionEntityTypeRequest.serialize,\n                response_deserializer=session_entity_type.SessionEntityType.deserialize,\n            )\n        return self._stubs[\"get_session_entity_type\"]\n\n    @property\n    def create_session_entity_type(\n        self,\n    ) -> Callable[\n        [gcdc_session_entity_type.CreateSessionEntityTypeRequest],\n        Awaitable[gcdc_session_entity_type.SessionEntityType],\n    ]:\n        r\"\"\"Return a callable for the create session entity type method over gRPC.\n\n        Creates a session entity type.\n\n        Returns:\n            Callable[[~.CreateSessionEntityTypeRequest],\n                    Awaitable[~.SessionEntityType]]:\n                A function that, when called, will call the underlying RPC\n                on the server.\n        \"\"\"\n        # Generate a \"stub function\" on-the-fly which will actually make\n        # the request.\n        # gRPC handles serialization and deserialization, so we just need\n        # to pass in the functions for each.\n        if \"create_session_entity_type\" not in self._stubs:\n            self._stubs[\"create_session_entity_type\"] = self.grpc_channel.unary_unary(\n                \"/google.cloud.dialogflow.cx.v3.SessionEntityTypes/CreateSessionEntityType\",\n                request_serializer=gcdc_session_entity_type.CreateSessionEntityTypeRequest.serialize,\n                response_deserializer=gcdc_session_entity_type.SessionEntityType.deserialize,\n            )\n        return self._stubs[\"create_session_entity_type\"]\n\n    @property\n    def update_session_entity_type(\n        self,\n    ) -> Callable[\n        [gcdc_session_entity_type.UpdateSessionEntityTypeRequest],\n        Awaitable[gcdc_session_entity_type.SessionEntityType],\n    ]:\n        r\"\"\"Return a callable for the update session entity type method over gRPC.\n\n        Updates the specified session entity type.\n\n        Returns:\n            Callable[[~.UpdateSessionEntityTypeRequest],\n                    Awaitable[~.SessionEntityType]]:\n                A function that, when called, will call the underlying RPC\n                on the server.\n        \"\"\"\n        # Generate a \"stub function\" on-the-fly which will actually make\n        # the request.\n        # gRPC handles serialization and deserialization, so we just need\n        # to pass in the functions for each.\n        if \"update_session_entity_type\" not in self._stubs:\n            self._stubs[\"update_session_entity_type\"] = self.grpc_channel.unary_unary(\n                \"/google.cloud.dialogflow.cx.v3.SessionEntityTypes/UpdateSessionEntityType\",\n                request_serializer=gcdc_session_entity_type.UpdateSessionEntityTypeRequest.serialize,\n                response_deserializer=gcdc_session_entity_type.SessionEntityType.deserialize,\n            )\n        return self._stubs[\"update_session_entity_type\"]\n\n    @property\n    def delete_session_entity_type(\n        self,\n    ) -> Callable[\n        [session_entity_type.DeleteSessionEntityTypeRequest], Awaitable[empty_pb2.Empty]\n    ]:\n        r\"\"\"Return a callable for the delete session entity type method over gRPC.\n\n        Deletes the specified session entity type.\n\n        Returns:\n            Callable[[~.DeleteSessionEntityTypeRequest],\n                    Awaitable[~.Empty]]:\n                A function that, when called, will call the underlying RPC\n                on the server.\n        \"\"\"\n        # Generate a \"stub function\" on-the-fly which will actually make\n        # the request.\n        # gRPC handles serialization and deserialization, so we just need\n        # to pass in the functions for each.\n        if \"delete_session_entity_type\" not in self._stubs:\n            self._stubs[\"delete_session_entity_type\"] = self.grpc_channel.unary_unary(\n                \"/google.cloud.dialogflow.cx.v3.SessionEntityTypes/DeleteSessionEntityType\",\n                request_serializer=session_entity_type.DeleteSessionEntityTypeRequest.serialize,\n                response_deserializer=empty_pb2.Empty.FromString,\n            )\n        return self._stubs[\"delete_session_entity_type\"]\n\n    def close(self):\n        return self.grpc_channel.close()\n\n\n__all__ = (\"SessionEntityTypesGrpcAsyncIOTransport\",)\n", "IL import Image\nfrom PIL import ImageDraw\nimport RPi.GPIO as GPIO\n# from Adafruit_LED_Backpack import BicolorMatrix8x8\n\nimport settings\nfrom rotary.rotary_thread import RotaryThread\n#from httpd.server import Server\n\n\nlogger = logging.getLogger('piradio')\n\nGPIO.setmode(GPIO.BCM)\nGPIO.setwarnings(False)\n\n# display = BicolorMatrix8x8.BicolorMatrix8x8()\n# display.begin()\n\nswitch1_lock = threading.Lock()\nswitch2_lock = threading.Lock()\n\n######################################################################\n# Display Functions\n######################################################################\n\n\ndef fill_display(r, g):\n    display.clear()\n    image = Image.new('RGB', (8, 8))\n    draw = ImageDraw.Draw(image)\n    draw.rectangle((0, 0, 7, 7), fill=(r, g, 0))\n    display.set_image(image)\n    display.write_display()\n\n\ndef drawPixel(x, y, c):\n    display.clear()\n    display.set_pixel(x, y, c)\n    display.write_display()\n\n\ndef splash():\n    fill_display(255, 0)\n    time.sleep(1)\n    fill_display(0, 255)\n    time.sleep(1)\n    fill_display(255, 255)\n    time.sleep(1)\n    display.clear()\n    display.write_display()\n\n\n######################################################################\n# MPD Functions\n######################################################################\n\n\ndef getMPDClient():\n    client = mpd.MPDClient(use_unicode=True)\n    client.connect(\"localhost\", 6600)\n    return client\n\n\n######################################################################\n# Callbacks\n######################################################################\n\n\ndef rotary1_left():\n    logger.debug(\"rotary1_left\")\n    client = getMPDClient()\n    logger.debug(\"[ PREVIOUS ]\")\n    client.previous()\n    client.close()\n\n\ndef rotary1_right():\n    logger.debug(\"rotary1_right\")\n    client = getMPDClient()\n    logger.debug(\"[ NEXT ]\")\n    client.next()\n    client.close()\n\n\ndef rotary1_push(ev=None):\n    logger.debug(\"rotary1_push\")\n    if switch1_lock.acquire(False):\n        logger.debug(\"[ STOP ]\")\n        client = getMPDClient()\n        client.stop()\n        client.close()\n        time.sleep(.5)\n        switch1_lock.release()\n\n\ndef rotary2_left():\n    logger.debug(\"rotary2_left\")\n    client = getMPDClient()\n    status = client.status()\n    volume = int(status['volume']) + 1\n    logger.debug(\"[ VOLUME %d ]\" % volume)\n    client.setvol(volume)\n    client.close()\n\n\ndef rotary2_right():\n    logger.debug(\"rotary2_right\")\n    client = getMPDClient()\n    status = client.status()\n    volume = int(status['volume']) - 1\n    logger.debug(\"[ VOLUME %d ]\" % volume)\n    client.setvol(volume)\n    client.close()\n\n\ndef rotary2_push(ev=None):\n    logger.debug(\"rotary2_push\")\n    if switch2_lock.acquire(False):\n        client = getMPDClient()\n        logger.debug(\"[ PLAY ]\")\n        client.play()\n        client.close()\n        time.sleep(.5)\n        switch2_lock.release()\n\n\n######################################################################\n# Main Loop\n######################################################################\n\n\ndef main():\n    # Setup Logging\n    hdlr = logging.FileHandler(settings.LOG)\n    formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n    hdlr.setFormatter(formatter)\n    logger.addHandler(hdlr)\n    hdlr = logging.StreamHandler(sys.stdout)\n    formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n    hdlr.setFormatter(formatter)\n    logger.addHandler(hdlr)\n    if settings.DEBUG:\n        logger.setLevel(logging.DEBUG)\n        logger.info(\"DEBUG level logging\")\n    else:\n        logger.setLevel(logging.INFO)\n        logger.info(\"INFO level logging\")\n        logger.info(\"Starting up...\")\n\n    # Splash the display\n    # splash()\n\n    rotary1_thread = None\n    rotary2_thread = None\n    # server = None\n\n    loops = 0\n    try:\n        while True:\n            loops = loops + 1\n            logger.debug(\"Main Loop \" + str(loops))\n\n            # Make sure our rotary threads are alive and happy\n            if not rotary1_thread or not rotary1_thread.is_alive():\n                logger.info(\"Starting rotary1 thread\")\n                rotary1_thread = RotaryThread(\"Rotary1\", settings.ROTARY1_APin, settings.ROTARY1_BPin, settings.ROTARY1_SPin, logger)\n                rotary1_thread.setLeftCallback(rotary1_left)\n                rotary1_thread.setRightCallback(rotary1_right)\n                rotary1_thread.setPushCallback(rotary1_push)\n                rotary1_thread.start()\n\n            if not rotary2_thread or not rotary2_thread.is_alive():\n                logger.info(\"Starting rotary2 thread\")\n                rotary2_thread = RotaryThread(\"Rotary2\", settings.ROTARY2_APin, settings.ROTARY2_BPin, settings.ROTARY2_SPin, logger)\n                rotary2_thread.setLeftCallback(rotary2_left)\n                rotary2_thread.setRightCallback(rotary2_right)\n                rotary2_thread.setPushCallback(rotary2_push)\n                rotary2_thread.start()\n\n            # if settings.HTTP_SERVER:\n            #     if not server or not server.is_alive():\n            #         logger.info(\"Starting HTTP server\")\n            #         server = Server(queue, logger)\n            #         server.setDaemon(True)\n            #         server.start()\n\n            time.sleep(15)\n    except KeyboardInterrupt:\n        logger.info(\"Keyboard Interuption!\")\n    except Exception as e:\n        logger.exception(e)\n    finally:\n        logger.debug(\"Shutting Down...\")\n        if rotary1_thread:\n            rotary1_thread.stop()\n            rotary1_thread.join()\n        if rotary2_thread:\n            rotary2_thread.stop()\n            rotary2_thread.join()\n\n    # Release GPIO\n    GPIO.cleanup()\n\n\nif __name__ == '__main__':\n    main()\n", "der the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n\nfrom unittest import mock\n\nfrom ironic.cmd import dbsync\nfrom ironic.common import context\nfrom ironic.db import migration\nfrom ironic.tests.unit.db import base as db_base\n\n\nclass DbSyncTestCase(db_base.DbTestCase):\n\n    def test_upgrade_and_version(self):\n        migration.upgrade('head')\n        v = migration.version()\n        self.assertTrue(v)\n\n\nclass OnlineMigrationTestCase(db_base.DbTestCase):\n\n    def setUp(self):\n        super(OnlineMigrationTestCase, self).setUp()\n        self.context = context.get_admin_context()\n        self.db_cmds = dbsync.DBCommand()\n\n    def test_check_obj_versions(self):\n        with mock.patch.object(self.dbapi, 'check_versions',\n                               autospec=True) as mock_check_versions:\n            mock_check_versions.return_value = True\n            msg = self.db_cmds.check_obj_versions()\n            self.assertIsNone(msg)\n            mock_check_versions.assert_called_once_with(\n                permit_initial_version=False)\n\n    def test_check_obj_versions_bad(self):\n        with mock.patch.object(self.dbapi, 'check_versions',\n                               autospec=True) as mock_check_versions:\n            mock_check_versions.return_value = False\n            msg = self.db_cmds.check_obj_versions()\n            self.assertIsNotNone(msg)\n            mock_check_versions.assert_called_once_with(\n                permit_initial_version=False)\n\n    def test_check_obj_versions_ignore_models(self):\n        with mock.patch.object(self.dbapi, 'check_versions',\n                               autospec=True) as mock_check_versions:\n            mock_check_versions.return_value = True\n            msg = self.db_cmds.check_obj_versions(ignore_missing_tables=True)\n            self.assertIsNone(msg)\n            mock_check_versions.assert_called_once_with(\n                permit_initial_version=True)\n\n    @mock.patch.object(dbsync.DBCommand, 'check_obj_versions', autospec=True)\n    def test_check_versions_bad(self, mock_check_versions):\n        mock_check_versions.return_value = 'This is bad'\n        exit = self.assertRaises(SystemExit, self.db_cmds._check_versions)\n        mock_check_versions.assert_called_once_with(\n            mock.ANY, ignore_missing_tables=False)\n        self.assertEqual(2, exit.code)\n\n    @mock.patch.object(dbsync, 'ONLINE_MIGRATIONS', autospec=True)\n    def test__run_migration_functions(self, mock_migrations):\n        mock_migrations.__iter__.return_value = ((self.dbapi, 'foo'),)\n        mock_func = mock.MagicMock(side_effect=((15, 15),), __name__='foo')\n        with mock.patch.object(self.dbapi, 'foo', mock_func, create=True):\n            self.assertTrue(\n                self.db_cmds._run_migration_functions(self.context, 50, {}))\n            mock_func.assert_called_once_with(self.context, 50)\n\n    @mock.patch.object(dbsync, 'ONLINE_MIGRATIONS', autospec=True)\n    def test__run_migration_functions_none(self, mock_migrations):\n        # No migration functions to run\n        mock_migrations.__iter__.return_value = ()\n        self.assertTrue(\n            self.db_cmds._run_migration_functions(self.context, 50, {}))\n\n    @mock.patch.object(dbsync, 'ONLINE_MIGRATIONS', autospec=True)\n    def test__run_migration_functions_exception(self, mock_migrations):\n        mock_migrations.__iter__.return_value = ((self.dbapi, 'foo'),)\n        # Migration function raises exception\n        mock_func = mock.MagicMock(side_effect=TypeError(\"bar\"),\n                                   __name__='foo')\n        with mock.patch.object(self.dbapi, 'foo', mock_func, create=True):\n            self.assertRaises(\n                TypeError, self.db_cmds._run_migration_functions,\n                self.context, 50, {})\n        mock_func.assert_called_once_with(self.context, 50)\n\n    @mock.patch.object(dbsync, 'ONLINE_MIGRATIONS', autospec=True)\n    def test__run_migration_functions_2(self, mock_migrations):\n        # 2 migration functions, migration completed\n        mock_migrations.__iter__.return_value = ((self.dbapi, 'func1'),\n                                                 (self.dbapi, 'func2'))\n        mock_func1 = mock.MagicMock(side_effect=((15, 15),), __name__='func1')\n        mock_func2 = mock.MagicMock(side_effect=((20, 20),), __name__='func2')\n        with mock.patch.object(self.dbapi, 'func1', mock_func1, create=True):\n            with mock.patch.object(self.dbapi, 'func2', mock_func2,\n                                   create=True):\n                options = {'func1': {'key': 'value'},\n                           'func2': {'x': 1, 'y': 2}}\n                self.assertTrue(self.db_cmds._run_migration_functions(\n                    self.context, 50, options))\n        mock_func1.assert_called_once_with(self.context, 50, key='value')\n        mock_func2.assert_called_once_with(self.context, 35, x=1, y=2)\n\n    @mock.patch.object(dbsync, 'ONLINE_MIGRATIONS', autospec=True)\n    def test__run_migration_functions_2_notdone(self, mock_migrations):\n        # 2 migration functions; only first function was run but not completed\n        mock_migrations.__iter__.return_value = ((self.dbapi, 'func1'),\n                                                 (self.dbapi, 'func2'))\n        mock_func1 = mock.MagicMock(side_effect=((15, 10),), __name__='func1')\n        mock_func2 = mock.MagicMock(side_effect=((20, 0),), __name__='func2')\n        with mock.patch.object(self.dbapi, 'func1', mock_func1, create=True):\n            with mock.patch.object(self.dbapi, 'func2', mock_func2,\n                                   create=True):\n                self.assertFalse(self.db_cmds._run_migration_functions(\n                    self.context, 10, {}))\n        mock_func1.assert_called_once_with(self.context, 10)\n        self.assertFalse(mock_func2.called)\n\n    @mock.patch.object(dbsync, 'ONLINE_MIGRATIONS', autospec=True)\n    def test__run_migration_functions_2_onedone(self, mock_migrations):\n        # 2 migration functions; only first function was run and completed\n        mock_migrations.__iter__.return_value = ((self.dbapi, 'func1'),\n                                                 (self.dbapi, 'func2'))\n        mock_func1 = mock.MagicMock(side_effect=((10, 10),), __name__='func1')\n        mock_func2 = mock.MagicMock(side_effect=((20, 0),), __name__='func2')\n        with mock.patch.object(self.dbapi, 'func1', mock_func1, create=True):\n            with mock.patch.object(self.dbapi, 'func2', mock_func2,\n                                   create=True):\n                self.assertFalse(self.db_cmds._run_migration_functions(\n                    self.context, 10, {}))\n        mock_func1.assert_called_once_with(self.context, 10)\n        self.assertFalse(mock_func2.called)\n\n    @mock.patch.object(dbsync, 'ONLINE_MIGRATIONS', autospec=True)\n    def test__run_migration_functions_2_done(self, mock_migrations):\n        # 2 migration functions; migrations completed\n        mock_migrations.__iter__.return_value = ((self.dbapi, 'func1'),\n                                                 (self.dbapi, 'func2'))\n        mock_func1 = mock.MagicMock(side_effect=((10, 10),), __name__='func1')\n        mock_func2 = mock.MagicMock(side_effect=((0, 0),), __name__='func2')\n        with mock.patch.object(self.dbapi, 'func1', mock_func1, create=True):\n            with mock.patch.object(self.dbapi, 'func2', mock_func2,\n                                   create=True):\n                self.assertTrue(self.db_cmds._run_migration_functions(\n                    self.context, 15, {}))\n        mock_func1.assert_called_once_with(self.context, 15)\n        mock_func2.assert_called_once_with(self.context, 5)\n\n    @mock.patch.object(dbsync, 'ONLINE_MIGRATIONS', autospec=True)\n    def test__run_migration_functions_two_calls_done(self, mock_migrations):\n        # 2 migration functions; migrations completed after calling twice\n        mock_migrations.__iter__.return_value = ((self.dbapi, 'func1'),\n                                                 (self.dbapi, 'func2'))\n        mock_func1 = mock.MagicMock(side_effect=((10, 10), (0, 0)),\n                                    __name__='func1')\n        mock_func2 = mock.MagicMock(side_effect=((0, 0), (0, 0)),\n                                    __name__='func2')\n        with mock.patch.object(self.dbapi, 'func1', mock_func1, create=True):\n            with mock.patch.object(self.dbapi, 'func2', mock_func2,\n                                   create=True):\n                self.assertFalse(self.db_cmds._run_migration_functions(\n                    self.context, 10, {}))\n                mock_func1.assert_called_once_with(self.context, 10)\n                self.assertFalse(mock_func2.called)\n                self.assertTrue(self.db_cmds._run_migration_functions(\n                    self.context, 10, {}))\n                mock_func1.assert_has_calls((mock.call(self.context, 10),) * 2)\n                mock_func2.assert_called_once_with(self.context, 10)\n\n    @mock.patch.object(dbsync.DBCommand, '_run_migration_functions',\n                       autospec=True)\n    def test__run_online_data_migrations(self, mock_functions):\n        mock_functions.return_value = True\n        exit = self.assertRaises(SystemExit,\n                                 self.db_cmds._run_online_data_migrations)\n        self.assertEqual(0, exit.code)\n        mock_functions.assert_called_once_with(self.db_cmds, mock.ANY, 50, {})\n\n    @mock.patch.object(dbsync.DBCommand, '_run_migration_functions',\n                       autospec=True)\n    def test__run_online_data_migrations_with_options(self, mock_functions):\n        mock_functions.return_value = True\n        exit = self.assertRaises(SystemExit,\n                                 self.db_cmds._run_online_data_migrations,\n                                 options=[\"m1.key1=value1\", \"m1.key2=value2\",\n                                          \"m2.key3=value3\"])\n        self.assertEqual(0, exit.code)\n        mock_functions.assert_called_once_with(self.db_cmds, mock.ANY, 50,\n                                               {'m1': {'key1': 'value1',\n                                                       'key2': 'value2'},\n                                                'm2': {'key3': 'value3'}})\n\n    @mock.patch.object(dbsync.DBCommand, '_run_migration_functions',\n                       autospec=True)\n    def test__run_online_data_migrations_invalid_option1(self, mock_functions):\n        mock_functions.return_value = True\n        exit = self.assertRaises(SystemExit,\n                                 self.db_cmds._run_online_data_migrations,\n                                 options=[\"m1key1=value1\"])\n        self.assertEqual(127, exit.code)\n        self.assertFalse(mock_functions.called)\n\n    @mock.patch.object(dbsync.DBCommand, '_run_migration_functions',\n                       autospec=True)\n    def test__run_online_data_migrations_invalid_option2(self, mock_functions):\n        mock_functions.return_value = True\n        exit = self.assertRaises(SystemExit,\n                                 self.db_cmds._run_online_data_migrations,\n                                 options=[\"m1.key1value1\"])\n        self.assertEqual(127, exit.code)\n        self.assertFalse(mock_functions.called)\n\n    @mock.patch.object(dbsync.DBCommand, '_run_migration_functions',\n                       autospec=True)\n    def test__run_online_data_migrations_batches(self, mock_functions):\n        mock_functions.side_effect = (False, True)\n        exit = self.assertRaises(SystemExit,\n                                 self.db_cmds._run_online_data_migrations)\n        self.assertEqual(0, exit.code)\n        mock_functions.assert_has_calls(\n            (mock.call(self.db_cmds, mock.ANY, 50, {}),) * 2)\n\n    @mock.patch.object(dbsync.DBCommand, '_run_migration_functions',\n                       autospec=True)\n    def test__run_online_data_migrations_notdone(self, mock_functions):\n        mock_functions.return_value = False\n        exit = self.assertRaises(SystemExit,\n                                 self.db_cmds._run_online_data_migrations,\n                                 max_count=30)\n        self.assertEqual(1, exit.code)\n        mock_functions.assert_called_once_with(self.db_cmds, mock.ANY, 30, {})\n\n    @mock.patch.object(dbsync.DBCommand, '_run_migration_functions',\n                       autospec=True)\n    def test__run_online_data_migrations_max_count_neg(self, mock_functions):\n        mock_functions.return_value = False\n        exit = self.assertRaises(SystemExit,\n                                 self.db_cmds._run_online_data_migrations,\n                                 max_count=-4)\n        self.assertEqual(127, exit.code)\n        self.assertFalse(mock_functions.called)\n\n    @mock.patch.object(dbsync.DBCommand, '_run_migration_functions',\n                       autospec=True)\n    def test__run_online_data_migrations_exception(self, mock_functions):\n        mock_functions.side_effect = TypeError(\"yuck\")\n        self.assertRaises(TypeError, self.db_cmds._run_online_data_migrations)\n        mock_functions.assert_called_once_with(self.db_cmds, mock.ANY, 50, {})\n", "cloud/securitycenter_v1beta1/proto/organization_settings.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode('latin1'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom google.api import annotations_pb2 as google_dot_api_dot_annotations__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name='google/cloud/securitycenter_v1beta1/proto/organization_settings.proto',\n  package='google.cloud.securitycenter.v1beta1',\n  syntax='proto3',\n  serialized_pb=_b('\\nEgoogle/cloud/securitycenter_v1beta1/proto/organization_settings.proto\\x12#google.cloud.securitycenter.v1beta1\\x1a\\x1cgoogle/api/annotations.proto\\\"\\xa8\\x03\\n\\x14OrganizationSettings\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\x1e\\n\\x16\\x65nable_asset_discovery\\x18\\x02 \\x01(\\x08\\x12n\\n\\x16\\x61sset_discovery_config\\x18\\x03 \\x01(\\x0b\\x32N.google.cloud.securitycenter.v1beta1.OrganizationSettings.AssetDiscoveryConfig\\x1a\\xf1\\x01\\n\\x14\\x41ssetDiscoveryConfig\\x12\\x13\\n\\x0bproject_ids\\x18\\x01 \\x03(\\t\\x12t\\n\\x0einclusion_mode\\x18\\x02 \\x01(\\x0e\\x32\\\\.google.cloud.securitycenter.v1beta1.OrganizationSettings.AssetDiscoveryConfig.InclusionMode\\\"N\\n\\rInclusionMode\\x12\\x1e\\n\\x1aINCLUSION_MODE_UNSPECIFIED\\x10\\x00\\x12\\x10\\n\\x0cINCLUDE_ONLY\\x10\\x01\\x12\\x0b\\n\\x07\\x45XCLUDE\\x10\\x02\\x42~\\n\\'com.google.cloud.securitycenter.v1beta1P\\x01ZQgoogle.golang.org/genproto/googleapis/cloud/securitycenter/v1beta1;securitycenterb\\x06proto3')\n  ,\n  dependencies=[google_dot_api_dot_annotations__pb2.DESCRIPTOR,])\n\n\n\n_ORGANIZATIONSETTINGS_ASSETDISCOVERYCONFIG_INCLUSIONMODE = _descriptor.EnumDescriptor(\n  name='InclusionMode',\n  full_name='google.cloud.securitycenter.v1beta1.OrganizationSettings.AssetDiscoveryConfig.InclusionMode',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name='INCLUSION_MODE_UNSPECIFIED', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='INCLUDE_ONLY', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='EXCLUDE', index=2, number=2,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=487,\n  serialized_end=565,\n)\n_sym_db.RegisterEnumDescriptor(_ORGANIZATIONSETTINGS_ASSETDISCOVERYCONFIG_INCLUSIONMODE)\n\n\n_ORGANIZATIONSETTINGS_ASSETDISCOVERYCONFIG = _descriptor.Descriptor(\n  name='AssetDiscoveryConfig',\n  full_name='google.cloud.securitycenter.v1beta1.OrganizationSettings.AssetDiscoveryConfig',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name='project_ids', full_name='google.cloud.securitycenter.v1beta1.OrganizationSettings.AssetDiscoveryConfig.project_ids', index=0,\n      number=1, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name='inclusion_mode', full_name='google.cloud.securitycenter.v1beta1.OrganizationSettings.AssetDiscoveryConfig.inclusion_mode', index=1,\n      number=2, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _ORGANIZATIONSETTINGS_ASSETDISCOVERYCONFIG_INCLUSIONMODE,\n  ],\n  options=None,\n  is_extendable=False,\n  syntax='proto3',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=324,\n  serialized_end=565,\n)\n\n_ORGANIZATIONSETTINGS = _descriptor.Descriptor(\n  name='OrganizationSettings',\n  full_name='google.cloud.securitycenter.v1beta1.OrganizationSettings',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name='name', full_name='google.cloud.securitycenter.v1beta1.OrganizationSettings.name', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b(\"\").decode('utf-8'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name='enable_asset_discovery', full_name='google.cloud.securitycenter.v1beta1.OrganizationSettings.enable_asset_discovery', index=1,\n      number=2, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name='asset_discovery_config', full_name='google.cloud.securitycenter.v1beta1.OrganizationSettings.asset_discovery_config', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[_ORGANIZATIONSETTINGS_ASSETDISCOVERYCONFIG, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax='proto3',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=141,\n  serialized_end=565,\n)\n\n_ORGANIZATIONSETTINGS_ASSETDISCOVERYCONFIG.fields_by_name['inclusion_mode'].enum_type = _ORGANIZATIONSETTINGS_ASSETDISCOVERYCONFIG_INCLUSIONMODE\n_ORGANIZATIONSETTINGS_ASSETDISCOVERYCONFIG.containing_type = _ORGANIZATIONSETTINGS\n_ORGANIZATIONSETTINGS_ASSETDISCOVERYCONFIG_INCLUSIONMODE.containing_type = _ORGANIZATIONSETTINGS_ASSETDISCOVERYCONFIG\n_ORGANIZATIONSETTINGS.fields_by_name['asset_discovery_config'].message_type = _ORGANIZATIONSETTINGS_ASSETDISCOVERYCONFIG\nDESCRIPTOR.message_types_by_name['OrganizationSettings'] = _ORGANIZATIONSETTINGS\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nOrganizationSettings = _reflection.GeneratedProtocolMessageType('OrganizationSettings', (_message.Message,), dict(\n\n  AssetDiscoveryConfig = _reflection.GeneratedProtocolMessageType('AssetDiscoveryConfig', (_message.Message,), dict(\n    DESCRIPTOR = _ORGANIZATIONSETTINGS_ASSETDISCOVERYCONFIG,\n    __module__ = 'google.cloud.securitycenter_v1beta1.proto.organization_settings_pb2'\n    ,\n    __doc__ = \"\"\"The configuration used for Asset Discovery runs.\n    \n    \n    Attributes:\n        project_ids:\n            The project ids to use for filtering asset discovery.\n        inclusion_mode:\n            The mode to use for filtering asset discovery.\n    \"\"\",\n    # @@protoc_insertion_point(class_scope:google.cloud.securitycenter.v1beta1.OrganizationSettings.AssetDiscoveryConfig)\n    ))\n  ,\n  DESCRIPTOR = _ORGANIZATIONSETTINGS,\n  __module__ = 'google.cloud.securitycenter_v1beta1.proto.organization_settings_pb2'\n  ,\n  __doc__ = \"\"\"User specified settings that are attached to the Cloud Security Command\n  Center (Cloud SCC) organization.\n  \n  \n  Attributes:\n      name:\n          The relative resource name of the settings. See: https://cloud\n          .google.com/apis/design/resource\\_names#relative\\_resource\\_na\n          me Example: \"organizations/123/organizationSettings\".\n      enable_asset_discovery:\n          A flag that indicates if Asset Discovery should be enabled. If\n          the flag is set to ``true``, then discovery of assets will\n          occur. If it is set to \\`false, all historical assets will\n          remain, but discovery of future assets will not occur.\n      asset_discovery_config:\n          The configuration used for Asset Discovery runs.\n  \"\"\",\n  # @@protoc_insertion_point(class_scope:google.cloud.securitycenter.v1beta1.OrganizationSettings)\n  ))\n_sym_db.RegisterMessage(OrganizationSettings)\n_sym_db.RegisterMessage(OrganizationSettings.AssetDiscoveryConfig)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b('\\n\\'com.google.cloud.securitycenter.v1beta1P\\001ZQgoogle.golang.org/genproto/googleapis/cloud/securitycenter/v1beta1;securitycenter'))\n# @@protoc_insertion_point(module_scope)\n", "", " patterns('',\n    url(r'^', include('events.urls')),\n)\n", "port unittest\nfrom unittest import mock\n\nimport voluptuous as vol\n\nfrom homeassistant.bootstrap import setup_component\nfrom homeassistant.components import device_tracker\nfrom homeassistant.components.device_tracker import (\n    CONF_CONSIDER_HOME, CONF_TRACK_NEW)\nfrom homeassistant.components.device_tracker.asuswrt import (\n    CONF_PROTOCOL, CONF_MODE, CONF_PUB_KEY, DOMAIN,\n    CONF_PORT, PLATFORM_SCHEMA)\nfrom homeassistant.const import (CONF_PLATFORM, CONF_PASSWORD, CONF_USERNAME,\n                                 CONF_HOST)\n\nfrom tests.common import (\n    get_test_home_assistant, get_test_config_dir, assert_setup_component)\n\nFAKEFILE = None\n\n\ndef setup_module():\n    \"\"\"Setup the test module.\"\"\"\n    global FAKEFILE\n    FAKEFILE = get_test_config_dir('fake_file')\n    with open(FAKEFILE, 'w') as out:\n        out.write(' ')\n\n\ndef teardown_module():\n    \"\"\"Tear down the module.\"\"\"\n    os.remove(FAKEFILE)\n\n\nclass TestComponentsDeviceTrackerASUSWRT(unittest.TestCase):\n    \"\"\"Tests for the ASUSWRT device tracker platform.\"\"\"\n\n    hass = None\n\n    def setup_method(self, _):\n        \"\"\"Setup things to be run when tests are started.\"\"\"\n        self.hass = get_test_home_assistant()\n        self.hass.config.components = set(['zone'])\n\n    def teardown_method(self, _):\n        \"\"\"Stop everything that was started.\"\"\"\n        self.hass.stop()\n        try:\n            os.remove(self.hass.config.path(device_tracker.YAML_DEVICES))\n        except FileNotFoundError:\n            pass\n\n    def test_password_or_pub_key_required(self): \\\n            # pylint: disable=invalid-name\n        \"\"\"Test creating an AsusWRT scanner without a pass or pubkey.\"\"\"\n        with assert_setup_component(0):\n            assert setup_component(\n                self.hass, DOMAIN, {DOMAIN: {\n                    CONF_PLATFORM: 'asuswrt',\n                    CONF_HOST: 'fake_host',\n                    CONF_USERNAME: 'fake_user'\n                }})\n\n    @mock.patch(\n        'homeassistant.components.device_tracker.asuswrt.AsusWrtDeviceScanner',\n        return_value=mock.MagicMock())\n    def test_get_scanner_with_password_no_pubkey(self, asuswrt_mock):  \\\n            # pylint: disable=invalid-name\n        \"\"\"Test creating an AsusWRT scanner with a password and no pubkey.\"\"\"\n        conf_dict = {\n            DOMAIN: {\n                CONF_PLATFORM: 'asuswrt',\n                CONF_HOST: 'fake_host',\n                CONF_USERNAME: 'fake_user',\n                CONF_PASSWORD: 'fake_pass',\n                CONF_TRACK_NEW: True,\n                CONF_CONSIDER_HOME: timedelta(seconds=180)\n            }\n        }\n\n        with assert_setup_component(1):\n            assert setup_component(self.hass, DOMAIN, conf_dict)\n\n        conf_dict[DOMAIN][CONF_MODE] = 'router'\n        conf_dict[DOMAIN][CONF_PROTOCOL] = 'ssh'\n        conf_dict[DOMAIN][CONF_PORT] = 22\n        self.assertEqual(asuswrt_mock.call_count, 1)\n        self.assertEqual(asuswrt_mock.call_args, mock.call(conf_dict[DOMAIN]))\n\n    @mock.patch(\n        'homeassistant.components.device_tracker.asuswrt.AsusWrtDeviceScanner',\n        return_value=mock.MagicMock())\n    def test_get_scanner_with_pubkey_no_password(self, asuswrt_mock):  \\\n            # pylint: disable=invalid-name\n        \"\"\"Test creating an AsusWRT scanner with a pubkey and no password.\"\"\"\n        conf_dict = {\n            device_tracker.DOMAIN: {\n                CONF_PLATFORM: 'asuswrt',\n                CONF_HOST: 'fake_host',\n                CONF_USERNAME: 'fake_user',\n                CONF_PUB_KEY: FAKEFILE,\n                CONF_TRACK_NEW: True,\n                CONF_CONSIDER_HOME: timedelta(seconds=180)\n            }\n        }\n\n        with assert_setup_component(1):\n            assert setup_component(self.hass, DOMAIN, conf_dict)\n\n        conf_dict[DOMAIN][CONF_MODE] = 'router'\n        conf_dict[DOMAIN][CONF_PROTOCOL] = 'ssh'\n        conf_dict[DOMAIN][CONF_PORT] = 22\n        self.assertEqual(asuswrt_mock.call_count, 1)\n        self.assertEqual(asuswrt_mock.call_args, mock.call(conf_dict[DOMAIN]))\n\n    def test_ssh_login_with_pub_key(self):\n        \"\"\"Test that login is done with pub_key when configured to.\"\"\"\n        ssh = mock.MagicMock()\n        ssh_mock = mock.patch('pexpect.pxssh.pxssh', return_value=ssh)\n        ssh_mock.start()\n        self.addCleanup(ssh_mock.stop)\n        conf_dict = PLATFORM_SCHEMA({\n            CONF_PLATFORM: 'asuswrt',\n            CONF_HOST: 'fake_host',\n            CONF_USERNAME: 'fake_user',\n            CONF_PUB_KEY: FAKEFILE\n        })\n        update_mock = mock.patch(\n            'homeassistant.components.device_tracker.asuswrt.'\n            'AsusWrtDeviceScanner.get_asuswrt_data')\n        update_mock.start()\n        self.addCleanup(update_mock.stop)\n        asuswrt = device_tracker.asuswrt.AsusWrtDeviceScanner(conf_dict)\n        asuswrt.ssh_connection()\n        self.assertEqual(ssh.login.call_count, 1)\n        self.assertEqual(\n            ssh.login.call_args,\n            mock.call('fake_host', 'fake_user', port=22, ssh_key=FAKEFILE)\n        )\n\n    def test_ssh_login_with_password(self):\n        \"\"\"Test that login is done with password when configured to.\"\"\"\n        ssh = mock.MagicMock()\n        ssh_mock = mock.patch('pexpect.pxssh.pxssh', return_value=ssh)\n        ssh_mock.start()\n        self.addCleanup(ssh_mock.stop)\n        conf_dict = PLATFORM_SCHEMA({\n            CONF_PLATFORM: 'asuswrt',\n            CONF_HOST: 'fake_host',\n            CONF_USERNAME: 'fake_user',\n            CONF_PASSWORD: 'fake_pass'\n        })\n        update_mock = mock.patch(\n            'homeassistant.components.device_tracker.asuswrt.'\n            'AsusWrtDeviceScanner.get_asuswrt_data')\n        update_mock.start()\n        self.addCleanup(update_mock.stop)\n        asuswrt = device_tracker.asuswrt.AsusWrtDeviceScanner(conf_dict)\n        asuswrt.ssh_connection()\n        self.assertEqual(ssh.login.call_count, 1)\n        self.assertEqual(\n            ssh.login.call_args,\n            mock.call('fake_host', 'fake_user', password='fake_pass', port=22)\n        )\n\n    def test_ssh_login_without_password_or_pubkey(self):  \\\n            # pylint: disable=invalid-name\n        \"\"\"Test that login is not called without password or pub_key.\"\"\"\n        ssh = mock.MagicMock()\n        ssh_mock = mock.patch('pexpect.pxssh.pxssh', return_value=ssh)\n        ssh_mock.start()\n        self.addCleanup(ssh_mock.stop)\n\n        conf_dict = {\n            CONF_PLATFORM: 'asuswrt',\n            CONF_HOST: 'fake_host',\n            CONF_USERNAME: 'fake_user',\n        }\n\n        with self.assertRaises(vol.Invalid):\n            conf_dict = PLATFORM_SCHEMA(conf_dict)\n\n        update_mock = mock.patch(\n            'homeassistant.components.device_tracker.asuswrt.'\n            'AsusWrtDeviceScanner.get_asuswrt_data')\n        update_mock.start()\n        self.addCleanup(update_mock.stop)\n\n        with assert_setup_component(0):\n            assert setup_component(self.hass, DOMAIN,\n                                   {DOMAIN: conf_dict})\n        ssh.login.assert_not_called()\n", "on 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n\nfrom oslo_serialization import jsonutils\nimport six\nimport webob\n\nfrom jacket.compute import cloud\nfrom jacket.objects import compute\nfrom jacket.compute import test\nfrom jacket.tests.compute.unit.api.openstack import fakes\n\nUUID1 = '00000000-0000-0000-0000-000000000001'\nUUID2 = '00000000-0000-0000-0000-000000000002'\nUUID3 = '00000000-0000-0000-0000-000000000003'\nNW_CACHE = [\n    {\n        'address': 'aa:aa:aa:aa:aa:aa',\n        'id': 1,\n        'network': {\n            'bridge': 'br0',\n            'id': 1,\n            'label': 'private',\n            'subnets': [\n                {\n                    'cidr': '192.168.1.0/24',\n                    'ips': [\n                        {\n                            'address': '192.168.1.100',\n                            'type': 'fixed',\n                            'floating_ips': [\n                                {'address': '5.0.0.1', 'type': 'floating'},\n                            ],\n                        },\n                    ],\n                },\n            ]\n        }\n    },\n    {\n        'address': 'bb:bb:bb:bb:bb:bb',\n        'id': 2,\n        'network': {\n            'bridge': 'br1',\n            'id': 2,\n            'label': 'public',\n            'subnets': [\n                {\n                    'cidr': '10.0.0.0/24',\n                    'ips': [\n                        {\n                            'address': '10.0.0.100',\n                            'type': 'fixed',\n                            'floating_ips': [\n                                {'address': '5.0.0.2', 'type': 'floating'},\n                            ],\n                        }\n                    ],\n                },\n            ]\n        }\n    }\n]\nALL_IPS = []\nfor cache in NW_CACHE:\n    for subnet in cache['network']['subnets']:\n        for fixed in subnet['ips']:\n            sanitized = dict(fixed)\n            sanitized.pop('floating_ips')\n            ALL_IPS.append(sanitized)\n            for floating in fixed['floating_ips']:\n                ALL_IPS.append(floating)\nALL_IPS.sort(key=lambda x: str(x))\n\n\ndef fake_compute_get(*args, **kwargs):\n    inst = fakes.stub_instance_obj(None, 1, uuid=UUID3, nw_cache=NW_CACHE)\n    return inst\n\n\ndef fake_compute_get_all(*args, **kwargs):\n    inst_list = [\n        fakes.stub_instance_obj(None, 1, uuid=UUID1, nw_cache=NW_CACHE),\n        fakes.stub_instance_obj(None, 2, uuid=UUID2, nw_cache=NW_CACHE),\n    ]\n    return cloud.InstanceList(cloud=inst_list)\n\n\nclass ExtendedIpsTestV21(test.TestCase):\n    content_type = 'application/json'\n    prefix = 'OS-EXT-IPS:'\n\n    def setUp(self):\n        super(ExtendedIpsTestV21, self).setUp()\n        fakes.stub_out_nw_api(self)\n        self.stubs.Set(cloud.api.API, 'get', fake_compute_get)\n        self.stubs.Set(cloud.api.API, 'get_all', fake_compute_get_all)\n\n    def _make_request(self, url):\n        req = webob.Request.blank(url)\n        req.headers['Accept'] = self.content_type\n        res = req.get_response(fakes.wsgi_app_v21(init_only=('servers',)))\n        return res\n\n    def _get_server(self, body):\n        return jsonutils.loads(body).get('server')\n\n    def _get_servers(self, body):\n        return jsonutils.loads(body).get('servers')\n\n    def _get_ips(self, server):\n        for network in six.itervalues(server['addresses']):\n            for ip in network:\n                yield ip\n\n    def assertServerStates(self, server):\n        results = []\n        for ip in self._get_ips(server):\n            results.append({'address': ip.get('addr'),\n                            'type': ip.get('%stype' % self.prefix)})\n\n        self.assertEqual(ALL_IPS, sorted(results))\n\n    def test_show(self):\n        url = '/v2/fake/servers/%s' % UUID3\n        res = self._make_request(url)\n\n        self.assertEqual(res.status_int, 200)\n        self.assertServerStates(self._get_server(res.body))\n\n    def test_detail(self):\n        url = '/v2/fake/servers/detail'\n        res = self._make_request(url)\n\n        self.assertEqual(res.status_int, 200)\n        for i, server in enumerate(self._get_servers(res.body)):\n            self.assertServerStates(server)\n\n\nclass ExtendedIpsTestV2(ExtendedIpsTestV21):\n\n    def setUp(self):\n        super(ExtendedIpsTestV2, self).setUp()\n        self.flags(\n            osapi_compute_extension=[\n                'cloud.api.openstack.cloud.contrib.select_extensions'],\n            osapi_compute_ext_list=['Extended_ips'])\n\n    def _make_request(self, url):\n        req = webob.Request.blank(url)\n        req.headers['Accept'] = self.content_type\n        res = req.get_response(fakes.wsgi_app(init_only=('servers',)))\n        return res\n", "import logging\nimport sys\nimport time\nimport zlib\n\nfrom io import BytesIO\nfrom tornado.web import RequestHandler\nfrom tornado import httputil\nfrom datetime import datetime\nfrom datetime import timedelta\n\nfrom urllib3.packages.six.moves.http_client import responses\nfrom urllib3.packages.six.moves.urllib.parse import urlsplit\nfrom urllib3.packages.six import binary_type, ensure_str\n\nlog = logging.getLogger(__name__)\n\n\nclass Response(object):\n    def __init__(self, body=\"\", status=\"200 OK\", headers=None):\n        self.body = body\n        self.status = status\n        self.headers = headers or [(\"Content-type\", \"text/plain\")]\n\n    def __call__(self, request_handler):\n        status, reason = self.status.split(\" \", 1)\n        request_handler.set_status(int(status), reason)\n        for header, value in self.headers:\n            request_handler.add_header(header, value)\n\n        # chunked\n        if isinstance(self.body, list):\n            for item in self.body:\n                if not isinstance(item, bytes):\n                    item = item.encode(\"utf8\")\n                request_handler.write(item)\n                request_handler.flush()\n        else:\n            body = self.body\n            if not isinstance(body, bytes):\n                body = body.encode(\"utf8\")\n\n            request_handler.write(body)\n\n\nRETRY_TEST_NAMES = collections.defaultdict(int)\n\n\nclass TestingApp(RequestHandler):\n    \"\"\"\n    Simple app that performs various operations, useful for testing an HTTP\n    library.\n\n    Given any path, it will attempt to load a corresponding local method if\n    it exists. Status code 200 indicates success, 400 indicates failure. Each\n    method has its own conditions for success/failure.\n    \"\"\"\n\n    def get(self):\n        \"\"\" Handle GET requests \"\"\"\n        self._call_method()\n\n    def post(self):\n        \"\"\" Handle POST requests \"\"\"\n        self._call_method()\n\n    def put(self):\n        \"\"\" Handle PUT requests \"\"\"\n        self._call_method()\n\n    def options(self):\n        \"\"\" Handle OPTIONS requests \"\"\"\n        self._call_method()\n\n    def head(self):\n        \"\"\" Handle HEAD requests \"\"\"\n        self._call_method()\n\n    def _call_method(self):\n        \"\"\" Call the correct method in this class based on the incoming URI \"\"\"\n        req = self.request\n        req.params = {}\n        for k, v in req.arguments.items():\n            req.params[k] = next(iter(v))\n\n        path = req.path[:]\n        if not path.startswith(\"/\"):\n            path = urlsplit(path).path\n\n        target = path[1:].replace(\"/\", \"_\")\n        method = getattr(self, target, self.index)\n\n        resp = method(req)\n\n        if dict(resp.headers).get(\"Connection\") == \"close\":\n            # FIXME: Can we kill the connection somehow?\n            pass\n\n        resp(self)\n\n    def index(self, _request):\n        \"Render simple message\"\n        return Response(\"Dummy server!\")\n\n    def certificate(self, request):\n        \"\"\"Return the requester's certificate.\"\"\"\n        cert = request.get_ssl_certificate()\n        subject = dict()\n        if cert is not None:\n            subject = dict((k, v) for (k, v) in [y for z in cert[\"subject\"] for y in z])\n        return Response(json.dumps(subject))\n\n    def source_address(self, request):\n        \"\"\"Return the requester's IP address.\"\"\"\n        return Response(request.remote_ip)\n\n    def set_up(self, request):\n        test_type = request.params.get(\"test_type\")\n        test_id = request.params.get(\"test_id\")\n        if test_id:\n            print(\"\\nNew test %s: %s\" % (test_type, test_id))\n        else:\n            print(\"\\nNew test %s\" % test_type)\n        return Response(\"Dummy server is ready!\")\n\n    def specific_method(self, request):\n        \"Confirm that the request matches the desired method type\"\n        method = request.params.get(\"method\")\n        if method and not isinstance(method, str):\n            method = method.decode(\"utf8\")\n\n        if request.method != method:\n            return Response(\n                \"Wrong method: %s != %s\" % (method, request.method),\n                status=\"400 Bad Request\",\n            )\n        return Response()\n\n    def upload(self, request):\n        \"Confirm that the uploaded file conforms to specification\"\n        # FIXME: This is a huge broken mess\n        param = request.params.get(\"upload_param\", b\"myfile\").decode(\"ascii\")\n        filename = request.params.get(\"upload_filename\", b\"\").decode(\"utf-8\")\n        size = int(request.params.get(\"upload_size\", \"0\"))\n        files_ = request.files.get(param)\n\n        if len(files_) != 1:\n            return Response(\n                \"Expected 1 file for '%s', not %d\" % (param, len(files_)),\n                status=\"400 Bad Request\",\n            )\n        file_ = files_[0]\n\n        data = file_[\"body\"]\n        if int(size) != len(data):\n            return Response(\n                \"Wrong size: %d != %d\" % (size, len(data)), status=\"400 Bad Request\"\n            )\n\n        got_filename = file_[\"filename\"]\n        if isinstance(got_filename, binary_type):\n            got_filename = got_filename.decode(\"utf-8\")\n\n        # Tornado can leave the trailing \\n in place on the filename.\n        if filename != got_filename:\n            return Response(\n                u\"Wrong filename: %s != %s\" % (filename, file_.filename),\n                status=\"400 Bad Request\",\n            )\n\n        return Response()\n\n    def redirect(self, request):\n        \"Perform a redirect to ``target``\"\n        target = request.params.get(\"target\", \"/\")\n        status = request.params.get(\"status\", \"303 See Other\")\n        if len(status) == 3:\n            status = \"%s Redirect\" % status.decode(\"latin-1\")\n\n        headers = [(\"Location\", target)]\n        return Response(status=status, headers=headers)\n\n    def not_found(self, request):\n        return Response(\"Not found\", status=\"404 Not Found\")\n\n    def multi_redirect(self, request):\n        \"Performs a redirect chain based on ``redirect_codes``\"\n        codes = request.params.get(\"redirect_codes\", b\"200\").decode(\"utf-8\")\n        head, tail = codes.split(\",\", 1) if \",\" in codes else (codes, None)\n        status = \"{0} {1}\".format(head, responses[int(head)])\n        if not tail:\n            return Response(\"Done redirecting\", status=status)\n\n        headers = [(\"Location\", \"/multi_redirect?redirect_codes=%s\" % tail)]\n        return Response(status=status, headers=headers)\n\n    def keepalive(self, request):\n        if request.params.get(\"close\", b\"0\") == b\"1\":\n            headers = [(\"Connection\", \"close\")]\n            return Response(\"Closing\", headers=headers)\n\n        headers = [(\"Connection\", \"keep-alive\")]\n        return Response(\"Keeping alive\", headers=headers)\n\n    def echo_params(self, request):\n        params = sorted(\n            [(ensure_str(k), ensure_str(v)) for k, v in request.params.items()]\n        )\n        return Response(repr(params))\n\n    def sleep(self, request):\n        \"Sleep for a specified amount of ``seconds``\"\n        # DO NOT USE THIS, IT'S DEPRECATED.\n        # FIXME: Delete this once appengine tests are fixed to not use this handler.\n        seconds = float(request.params.get(\"seconds\", \"1\"))\n        time.sleep(seconds)\n        return Response()\n\n    def echo(self, request):\n        \"Echo back the params\"\n        if request.method == \"GET\":\n            return Response(request.query)\n\n        return Response(request.body)\n\n    def echo_uri(self, request):\n        \"Echo back the requested URI\"\n        return Response(request.uri)\n\n    def encodingrequest(self, request):\n        \"Check for UA accepting gzip/deflate encoding\"\n        data = b\"hello, world!\"\n        encoding = request.headers.get(\"Accept-Encoding\", \"\")\n        headers = None\n        if encoding == \"gzip\":\n            headers = [(\"Content-Encoding\", \"gzip\")]\n            file_ = BytesIO()\n            with contextlib.closing(\n                gzip.GzipFile(\"\", mode=\"w\", fileobj=file_)\n            ) as zipfile:\n                zipfile.write(data)\n            data = file_.getvalue()\n        elif encoding == \"deflate\":\n            headers = [(\"Content-Encoding\", \"deflate\")]\n            data = zlib.compress(data)\n        elif encoding == \"garbage-gzip\":\n            headers = [(\"Content-Encoding\", \"gzip\")]\n            data = \"garbage\"\n        elif encoding == \"garbage-deflate\":\n            headers = [(\"Content-Encoding\", \"deflate\")]\n            data = \"garbage\"\n        return Response(data, headers=headers)\n\n    def headers(self, request):\n        return Response(json.dumps(dict(request.headers)))\n\n    def successful_retry(self, request):\n        \"\"\" Handler which will return an error and then success\n\n        It's not currently very flexible as the number of retries is hard-coded.\n        \"\"\"\n        test_name = request.headers.get(\"test-name\", None)\n        if not test_name:\n            return Response(\"test-name header not set\", status=\"400 Bad Request\")\n\n        RETRY_TEST_NAMES[test_name] += 1\n\n        if RETRY_TEST_NAMES[test_name] >= 2:\n            return Response(\"Retry successful!\")\n        else:\n            return Response(\"need to keep retrying!\", status=\"418 I'm A Teapot\")\n\n    def chunked(self, request):\n        return Response([\"123\"] * 4)\n\n    def chunked_gzip(self, request):\n        chunks = []\n        compressor = zlib.compressobj(6, zlib.DEFLATED, 16 + zlib.MAX_WBITS)\n\n        for uncompressed in [b\"123\"] * 4:\n            chunks.append(compressor.compress(uncompressed))\n\n        chunks.append(compressor.flush())\n\n        return Response(chunks, headers=[(\"Content-Encoding\", \"gzip\")])\n\n    def nbytes(self, request):\n        length = int(request.params.get(\"length\"))\n        data = b\"1\" * length\n        return Response(data, headers=[(\"Content-Type\", \"application/octet-stream\")])\n\n    def status(self, request):\n        status = request.params.get(\"status\", \"200 OK\")\n\n        return Response(status=status)\n\n    def retry_after(self, request):\n        if datetime.now() - self.application.last_req < timedelta(seconds=1):\n            status = request.params.get(\"status\", b\"429 Too Many Requests\")\n            return Response(\n                status=status.decode(\"utf-8\"), headers=[(\"Retry-After\", \"1\")]\n            )\n\n        self.application.last_req = datetime.now()\n\n        return Response(status=\"200 OK\")\n\n    def redirect_after(self, request):\n        \"Perform a redirect to ``target``\"\n        date = request.params.get(\"date\")\n        if date:\n            retry_after = str(\n                httputil.format_timestamp(datetime.fromtimestamp(float(date)))\n            )\n        else:\n            retry_after = \"1\"\n        target = request.params.get(\"target\", \"/\")\n        headers = [(\"Location\", target), (\"Retry-After\", retry_after)]\n        return Response(status=\"303 See Other\", headers=headers)\n\n    def shutdown(self, request):\n        sys.exit()\n", "ersion 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n\nRULE_TYPE_BANDWIDTH_LIMIT = 'bandwidth_limit'\nRULE_TYPE_DSCP_MARKING = 'dscp_marking'\nRULE_TYPE_MINIMUM_BANDWIDTH = 'minimum_bandwidth'\nVALID_RULE_TYPES = [RULE_TYPE_BANDWIDTH_LIMIT,\n                    RULE_TYPE_DSCP_MARKING,\n                    RULE_TYPE_MINIMUM_BANDWIDTH,\n                    ]\n\nQOS_POLICY_ID = 'qos_policy_id'\n\n# NOTE(slaweq): Value used to calculate burst value for egress bandwidth limit\n# if burst is not given by user. In such case burst value will be calculated\n# as 80% of bw_limit to ensure that at least limits for TCP traffic will work\n# fine.\nDEFAULT_BURST_RATE = 0.8\n", "", "):\n\n    def __init__(self):\n        # this is a VerticalLayout\n        super(VerticalLayoutBasicExample, self).__init__()\n\n        # let's add some components\n        for i in range(5):\n            tf = TextField('Row %d' % (i + 1))\n            tf.setWidth('300px')\n            # Add the component to the VerticalLayout\n            self.addComponent(tf)\n", ".0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# Generated code. DO NOT EDIT!\n#\n# Snippet for ListVersions\n# NOTE: This snippet has been automatically generated for illustrative purposes only.\n# It may require modifications to work in your environment.\n\n# To install the latest published package dependency, execute the following:\n#   python3 -m pip install google-cloud-artifact-registry\n\n\n# [START artifactregistry_v1beta2_generated_ArtifactRegistry_ListVersions_sync]\nfrom google.cloud import artifactregistry_v1beta2\n\n\ndef sample_list_versions():\n    # Create a client\n    client = artifactregistry_v1beta2.ArtifactRegistryClient()\n\n    # Initialize request argument(s)\n    request = artifactregistry_v1beta2.ListVersionsRequest(\n    )\n\n    # Make the request\n    page_result = client.list_versions(request=request)\n\n    # Handle the response\n    for response in page_result:\n        print(response)\n\n# [END artifactregistry_v1beta2_generated_ArtifactRegistry_ListVersions_sync]\n", "ort *\n\nclass CallingConventionProxy:\n\n    def __init__(self, cconv, argv, funcsym):\n        self.argv = argv # viv style (type,name) tuple list\n        self.cconv = cconv\n        self.funcsym = funcsym\n\n    def __call__(self, emu):\n\n        # Get and update the symbolik args\n        args = self.cconv.getSymbolikArgs(emu, self.argv)\n        args = [ arg.update(emu) for arg in args ]\n\n        # If callFunction returns something, snap it back in.\n        # Otherwise, snap in a Call symbol.\n        ret = self.callFunction(emu, *args)\n        if ret == None:\n            ret = Call(self.funcsym, emu.__width__, args)\n\n        # Set the return value into the symbolik state\n        self.cconv.setSymbolikReturn(emu, ret, self.argv)\n\n    def getSymbolikArgs(self, emu):\n        return self.cconv.getSymbolikArgs(emu, self.argv)\n\n    def callFunction(emu, *args):\n        # Each calling convention proxy must implement this to do\n        # the actual call hook...\n        return None\n\nclass ImportCallProxy(CallingConventionProxy):\n    '''\n    A calling convention proxy allows the definition of\n    a pythonic function which may then be called by an emulator\n    during symbolik effect processing.\n    '''\n\n    def __init__(self, func, cconv):\n\n        # Do crazy introspection shit to make calling convention\n        # map function args to names / vstruct types.\n        aspec = inspect.getargspec(func)\n        argn = aspec.args[1:]\n        argt = aspec.defaults\n\n        argv = [ (argt[i],argn[i]) for i in xrange(len(argn)) ]\n\n        modlast = func.__module__.split('.')[-1]\n        funcsym = Var('%s.%s' % (modlast, func.__name__))\n\n        CallingConventionProxy.__init__(self, cconv, argv, funcsym)\n\n        self.func = func\n\n    def callFunction(self, emu, *args):\n        return self.func(emu, *args)\n", "f test_invalid_sql(self):\n        with self.conn.cursor() as cursor:\n            with self.assertRaises(self.conn.ProgrammingError) as cm:\n                cursor.execute(\"UPS\")\n            self.assertEqual(\"Syntax error. Encountered \\\"UPS\\\" at line 1, column 1.\", cm.exception.message)\n            self.assertEqual(601, cm.exception.code)\n            self.assertEqual(\"42P00\", cm.exception.sqlstate)\n\n\nclass IntegrityErrorTest(DatabaseTestCase):\n\n    def test_null_in_pk(self):\n        self.createTable(\"pyphoenix_test_tbl1\", \"id integer primary key\")\n        with self.conn.cursor() as cursor:\n            with self.assertRaises(self.conn.IntegrityError) as cm:\n                cursor.execute(\"UPSERT INTO pyphoenix_test_tbl1 VALUES (NULL)\")\n            self.assertEqual(\"Constraint violation. pyphoenix_TEST_TBL1.ID may not be null\", cm.exception.message)\n            self.assertEqual(218, cm.exception.code)\n            self.assertIn(cm.exception.sqlstate, (\"22018\", \"23018\"))\n\n\nclass DataErrorTest(DatabaseTestCase):\n\n    def test_number_outside_of_range(self):\n        self.createTable(\"pyphoenix_test_tbl1\", \"id tinyint primary key\")\n        with self.conn.cursor() as cursor:\n            with self.assertRaises(self.conn.DataError) as cm:\n                cursor.execute(\"UPSERT INTO pyphoenix_test_tbl1 VALUES (10000)\")\n            self.assertEqual(\"Type mismatch. TINYINT and INTEGER for 10000\", cm.exception.message)\n            self.assertEqual(203, cm.exception.code)\n            self.assertEqual(\"22005\", cm.exception.sqlstate)\n\n    def test_division_by_zero(self):\n        self.createTable(\"pyphoenix_test_tbl1\", \"id integer primary key\")\n        with self.conn.cursor() as cursor:\n            with self.assertRaises(self.conn.DataError) as cm:\n                cursor.execute(\"UPSERT INTO pyphoenix_test_tbl1 VALUES (2/0)\")\n            self.assertEqual(\"Divide by zero.\", cm.exception.message)\n            self.assertEqual(202, cm.exception.code)\n            self.assertEqual(\"22012\", cm.exception.sqlstate)\n", "e\nfrom ray.tune.sample import Categorical, Domain, Float, Integer, Quantized, \\\n    Uniform\nfrom ray.tune.suggest.variant_generator import parse_spec_vars\nfrom ray.tune.utils.util import unflatten_dict\nfrom zoopt import ValueType\n\ntry:\n    import zoopt\nexcept ImportError:\n    zoopt = None\n\nfrom ray.tune.suggest import Searcher\n\nlogger = logging.getLogger(__name__)\n\n\nclass ZOOptSearch(Searcher):\n    \"\"\"A wrapper around ZOOpt to provide trial suggestions.\n\n    ZOOptSearch is a library for derivative-free optimization. It is backed by\n    the `ZOOpt <https://github.com/polixir/ZOOpt>`__ package. Currently,\n    Asynchronous Sequential RAndomized COordinate Shrinking (ASRacos)\n    is implemented in Tune.\n\n    To use ZOOptSearch, install zoopt (>=0.4.0): ``pip install -U zoopt``.\n\n    Tune automatically converts search spaces to ZOOpt\"s format:\n\n    .. code-block:: python\n\n        from ray import tune\n        from ray.tune.suggest.zoopt import ZOOptSearch\n\n        \"config\": {\n            \"iterations\": 10,  # evaluation times\n            \"width\": tune.uniform(-10, 10),\n            \"height\": tune.uniform(-10, 10)\n        }\n\n        zoopt_search = ZOOptSearch(\n            algo=\"Asracos\",  # only support Asracos currently\n            budget=20,  # must match `num_samples` in `tune.run()`.\n            dim_dict=dim_dict,\n            metric=\"mean_loss\",\n            mode=\"min\")\n\n        tune.run(my_objective,\n            config=config,\n            search_alg=zoopt_search,\n            name=\"zoopt_search\",\n            num_samples=20,\n            stop={\"timesteps_total\": 10})\n\n    If you would like to pass the search space manually, the code would\n    look like this:\n\n    .. code-block:: python\n\n        from ray import tune\n        from ray.tune.suggest.zoopt import ZOOptSearch\n        from zoopt import ValueType\n\n        dim_dict = {\n            \"height\": (ValueType.CONTINUOUS, [-10, 10], 1e-2),\n            \"width\": (ValueType.DISCRETE, [-10, 10], False)\n        }\n\n        \"config\": {\n            \"iterations\": 10,  # evaluation times\n        }\n\n        zoopt_search = ZOOptSearch(\n            algo=\"Asracos\",  # only support Asracos currently\n            budget=20,  # must match `num_samples` in `tune.run()`.\n            dim_dict=dim_dict,\n            metric=\"mean_loss\",\n            mode=\"min\")\n\n        tune.run(my_objective,\n            config=config,\n            search_alg=zoopt_search,\n            name=\"zoopt_search\",\n            num_samples=20,\n            stop={\"timesteps_total\": 10})\n\n    Parameters:\n        algo (str): To specify an algorithm in zoopt you want to use.\n            Only support ASRacos currently.\n        budget (int): Number of samples.\n        dim_dict (dict): Dimension dictionary.\n            For continuous dimensions: (continuous, search_range, precision);\n            For discrete dimensions: (discrete, search_range, has_order).\n            More details can be found in zoopt package.\n        metric (str): The training result objective value attribute.\n            Defaults to \"episode_reward_mean\".\n        mode (str): One of {min, max}. Determines whether objective is\n            minimizing or maximizing the metric attribute.\n            Defaults to \"min\".\n\n    \"\"\"\n\n    optimizer = None\n\n    def __init__(self,\n                 algo: str = \"asracos\",\n                 budget: Optional[int] = None,\n                 dim_dict: Optional[Dict] = None,\n                 metric: Optional[str] = None,\n                 mode: Optional[str] = None,\n                 **kwargs):\n        assert zoopt is not None, \"Zoopt not found - please install zoopt.\"\n        assert budget is not None, \"`budget` should not be None!\"\n        if mode:\n            assert mode in [\"min\", \"max\"], \"`mode` must be 'min' or 'max'.\"\n        _algo = algo.lower()\n        assert _algo in [\"asracos\", \"sracos\"\n                         ], \"`algo` must be in ['asracos', 'sracos'] currently\"\n\n        self._algo = _algo\n        self._dim_dict = dim_dict\n        self._budget = budget\n\n        self._metric = metric\n        if mode == \"max\":\n            self._metric_op = -1.\n        elif mode == \"min\":\n            self._metric_op = 1.\n        self._live_trial_mapping = {}\n\n        self._dim_keys = []\n        self.solution_dict = {}\n        self.best_solution_list = []\n        self.optimizer = None\n\n        super(ZOOptSearch, self).__init__(\n            metric=self._metric, mode=mode, **kwargs)\n\n        if self._dim_dict:\n            self.setup_zoopt()\n\n    def setup_zoopt(self):\n        _dim_list = []\n        for k in self._dim_dict:\n            self._dim_keys.append(k)\n            _dim_list.append(self._dim_dict[k])\n\n        dim = zoopt.Dimension2(_dim_list)\n        par = zoopt.Parameter(budget=self._budget)\n        if self._algo == \"sracos\" or self._algo == \"asracos\":\n            from zoopt.algos.opt_algorithms.racos.sracos import SRacosTune\n            self.optimizer = SRacosTune(dimension=dim, parameter=par)\n\n    def set_search_properties(self, metric: Optional[str], mode: Optional[str],\n                              config: Dict) -> bool:\n        if self._dim_dict:\n            return False\n        space = self.convert_search_space(config)\n        self._dim_dict = space\n\n        if metric:\n            self._metric = metric\n        if mode:\n            self._mode = mode\n\n        if self._mode == \"max\":\n            self._metric_op = -1.\n        elif self._mode == \"min\":\n            self._metric_op = 1.\n\n        self.setup_zoopt()\n        return True\n\n    def suggest(self, trial_id: str) -> Optional[Dict]:\n        if not self._dim_dict or not self.optimizer:\n            raise RuntimeError(\n                \"Trying to sample a configuration from {}, but no search \"\n                \"space has been defined. Either pass the `{}` argument when \"\n                \"instantiating the search algorithm, or pass a `config` to \"\n                \"`tune.run()`.\".format(self.__class__.__name__, \"space\"))\n\n        _solution = self.optimizer.suggest()\n        if _solution:\n            self.solution_dict[str(trial_id)] = _solution\n            _x = _solution.get_x()\n            new_trial = dict(zip(self._dim_keys, _x))\n            self._live_trial_mapping[trial_id] = new_trial\n            return unflatten_dict(new_trial)\n\n    def on_trial_complete(self,\n                          trial_id: str,\n                          result: Optional[Dict] = None,\n                          error: bool = False):\n        \"\"\"Notification for the completion of trial.\"\"\"\n        if result:\n            _solution = self.solution_dict[str(trial_id)]\n            _best_solution_so_far = self.optimizer.complete(\n                _solution, self._metric_op * result[self._metric])\n            if _best_solution_so_far:\n                self.best_solution_list.append(_best_solution_so_far)\n\n        del self._live_trial_mapping[trial_id]\n\n    def save(self, checkpoint_path: str):\n        trials_object = self.optimizer\n        with open(checkpoint_path, \"wb\") as output:\n            pickle.dump(trials_object, output)\n\n    def restore(self, checkpoint_path: str):\n        with open(checkpoint_path, \"rb\") as input:\n            trials_object = pickle.load(input)\n        self.optimizer = trials_object\n\n    @staticmethod\n    def convert_search_space(spec: Dict) -> Dict[str, Tuple]:\n        spec = copy.deepcopy(spec)\n        resolved_vars, domain_vars, grid_vars = parse_spec_vars(spec)\n\n        if not domain_vars and not grid_vars:\n            return []\n\n        if grid_vars:\n            raise ValueError(\n                \"Grid search parameters cannot be automatically converted \"\n                \"to a ZOOpt search space.\")\n\n        def resolve_value(domain: Domain) -> Tuple:\n            quantize = None\n\n            sampler = domain.get_sampler()\n            if isinstance(sampler, Quantized):\n                quantize = sampler.q\n                sampler = sampler.sampler\n\n            if isinstance(domain, Float):\n                precision = quantize or 1e-12\n                if isinstance(sampler, Uniform):\n                    return (ValueType.CONTINUOUS, [domain.lower, domain.upper],\n                            precision)\n\n            elif isinstance(domain, Integer):\n                if isinstance(sampler, Uniform):\n                    return (ValueType.DISCRETE, [domain.lower, domain.upper],\n                            True)\n\n            elif isinstance(domain, Categorical):\n                # Categorical variables would use ValjeType.DISCRETE with\n                # has_partial_order=False, however, currently we do not\n                # keep track of category values and cannot automatically\n                # translate back and forth between them.\n                raise ValueError(\n                    \"ZOOpt does not support automatic conversion for \"\n                    \"categorical variables. Please instantiate ZOOpt with \"\n                    \"a manually defined search space.\")\n\n            raise ValueError(\"ZOOpt does not support parameters of type \"\n                             \"`{}` with samplers of type `{}`\".format(\n                                 type(domain).__name__,\n                                 type(domain.sampler).__name__))\n\n        spec = {\n            \"/\".join(path): resolve_value(domain)\n            for path, domain in domain_vars\n        }\n\n        return spec\n", "e, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n\n\"\"\"\nhost interface (1.1 extension).\n\"\"\"\nfrom novaclient import base\n\n\nclass Host(base.Resource):\n    def __repr__(self):\n        return \"<Host: %s>\" % self.host\n\n    def _add_details(self, info):\n        dico = 'resource' in info and info['resource'] or info\n        for (k, v) in dico.items():\n            setattr(self, k, v)\n\n    def update(self, values):\n        return self.manager.update(self.host, values)\n\n    def startup(self):\n        return self.manager.host_action(self.host, 'startup')\n\n    def shutdown(self):\n        return self.manager.host_action(self.host, 'shutdown')\n\n    def reboot(self):\n        return self.manager.host_action(self.host, 'reboot')\n\n    @property\n    def host_name(self):\n        return self.host\n\n    @host_name.setter\n    def host_name(self, value):\n        # A host from hosts.list() has the attribute \"host_name\" instead of\n        # \"host.\" This sets \"host\" if that's the case. Even though it doesn't\n        # exactly mirror the response format, it enables users to work with\n        # host objects from list and non-list operations interchangeably.\n        self.host = value\n\n\nclass HostManager(base.ManagerWithFind):\n    resource_class = Host\n\n    def get(self, host):\n        \"\"\"\n        Describes cpu/memory/hdd info for host.\n\n        :param host: destination host name.\n        \"\"\"\n        return self._list(\"/os-hosts/%s\" % host, \"host\")\n\n    def update(self, host, values):\n        \"\"\"Update status or maintenance mode for the host.\"\"\"\n        return self._update(\"/os-hosts/%s\" % host, values)\n\n    def host_action(self, host, action):\n        \"\"\"\n        Perform an action on a host.\n\n        :param host: The host to perform an action\n        :param action: The action to perform\n        returns: An instance of novaclient.base.TupleWithMeta\n        \"\"\"\n        url = '/os-hosts/{0}/{1}'.format(host, action)\n        resp, body = self.api.client.get(url)\n        return base.TupleWithMeta((resp, body), resp)\n\n    def list(self, zone=None):\n        url = '/os-hosts'\n        if zone:\n            url = '/os-hosts?zone=%s' % zone\n        return self._list(url, \"hosts\")\n\n    list_all = list\n", "gnizant Technology Solutions\n# \n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n# use this file except in compliance with the License.  You may obtain a copy\n# of the License at\n# \n#   http://www.apache.org/licenses/LICENSE-2.0\n# \n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the\n# License for the specific language governing permissions and limitations under\n# the License.\n#-------------------------------------------------------------------------------\n'''\nCreated on 10 April 2017\n\n@author: 446620\n'''\nfrom ....core.BaseAgent import BaseAgent\n\nclass TFSAgent(BaseAgent):\n    \n    @BaseAgent.timed\n    def process(self):\n        BaseUrl = self.config.get(\"baseUrl\", '')\n        UserID = self.getCredential(\"userid\")\n        Passwd = self.getCredential(\"passwd\")\n        Auth = self.config.get(\"auth\", '')\n        getCollectionsUrl = BaseUrl+\"/_apis/projectcollections\"\n        collections = self.getResponse(getCollectionsUrl, 'GET', UserID, Passwd, None, authType=Auth)\n        #print(collections)\n        responseTemplate = self.getResponseTemplate()\n        data = []\n        colCount = collections[\"count\"]\n        for collection in range(colCount):            \n            collectionName = collections[\"value\"][collection][\"name\"]\n            getProjectsUrl = BaseUrl + \"/\" +collectionName+\"/_apis/projects/\"\n            projects = self.getResponse(getProjectsUrl, 'GET', UserID, Passwd, None, authType=Auth)\n            #print(projects)\n            projCount = projects[\"count\"]\n            for project in range(projCount):\n                injectData = {}                \n                projectName = projects[\"value\"][project][\"name\"]\n                injectData['collectionName'] = collectionName\n                injectData['projectName'] = projectName\n                #print(collectionName + \"/\" + projectName)\n                newProject = False                \n                if not self.tracking.get(collectionName + \"/\" + projectName,None):\n                    #getChangesetsUrl = BaseUrl + \"/\" + collectionName + \"/\" + projectName + \"/_apis/tfvc/changesets/?$orderBy=id asc\"\n                    getChangesetsUrl = BaseUrl + \"/\" + collectionName + \"/\" + projectName + \"/_apis/tfvc/changesets\"\n                    newProject = True\n                else:\n                    lastID = self.tracking.get(collectionName+ \"/\" + projectName,None)\n                    getChangesetsUrl = BaseUrl + \"/\" + collectionName + \"/\" + projectName + \"/_apis/tfvc/changesets?fromId=\" + str(lastID)                    \n                #print(getChangesetsUrl)\n                changesets = self.getResponse(getChangesetsUrl, 'GET', UserID, Passwd, None, authType=Auth)\n                #print(changesets)\n                csCount = changesets[\"count\"]\n                #print(csCount)\n                if not newProject:\n                    csCount = csCount-1\n                for changeset in range(csCount):\n                    changesetDetail = changesets[\"value\"][changeset]\n                    #print(changesetDetail)\n                    data += self.parseResponse(responseTemplate, changesetDetail, injectData)\n                    #getChangesetDetailsUrl = BaseUrl + \"/\" +collectionName + \"/\" + projectName + \"/_apis/tfvc/changesets/\" + str(changesets[\"value\"][changeset][\"changesetId\"])\n                    #changesetDetails = self.getResponse(getChangesetDetailsUrl, 'GET', UserID, Passwd, None, authType=Auth)\n                    #print(changesetDetails)\n                self.tracking[collectionName + \"/\" + projectName] = changesets[\"value\"][0][\"changesetId\"]\n        #print(data)\n        self.publishToolsData(data)\n        self.updateTrackingJson(self.tracking)\nif __name__ == \"__main__\":\n    TFSAgent()\n", " Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Test cases for eager execution using XLA.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nfrom tensorflow.compiler.tests import xla_test\nfrom tensorflow.core.protobuf import config_pb2\nfrom tensorflow.python.eager import backprop\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.eager import function\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.layers import convolutional\nfrom tensorflow.python.layers import pooling\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import embedding_ops\nfrom tensorflow.python.ops import gen_random_ops\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import resource_variable_ops\nfrom tensorflow.python.platform import googletest\nfrom tensorflow.python.training import adam\n\n\nclass EagerTest(xla_test.XLATestCase):\n\n  def testBasic(self):\n    with self.test_scope():\n      three = constant_op.constant(3)\n      five = constant_op.constant(5)\n      product = three * five\n      self.assertAllEqual(15, product)\n\n  def testGradientTape(self):\n    with self.test_scope():\n\n      x = constant_op.constant(1.0)\n      y = constant_op.constant(10.0)\n      with backprop.GradientTape(persistent=True) as tape:\n        tape.watch(x)\n        tape.watch(y)\n        a = x + y + x * y\n      da_dx = tape.gradient(a, x)\n      da_dy = tape.gradient(a, y)\n\n    self.assertEqual(11.0, da_dx.numpy())\n    self.assertEqual(2.0, da_dy.numpy())\n\n  def testExecuteListOutputLen0(self):\n    with self.test_scope():\n      empty = constant_op.constant([], dtype=dtypes.float32)\n      result = array_ops.unstack(empty, 0)\n      self.assertTrue(isinstance(result, list))\n      self.assertEqual(0, len(result))\n\n  def testExecuteListOutputLen1(self):\n    with self.test_scope():\n      split_dim = constant_op.constant(1)\n      value = constant_op.constant([[0., 1., 2.], [3., 4., 5.]])\n      result = array_ops.split(value, 1, axis=split_dim)\n      self.assertTrue(isinstance(result, list))\n      self.assertEqual(1, len(result))\n      self.assertAllEqual([[0, 1, 2], [3, 4, 5]], result[0])\n\n  def testExecuteListOutputLen3(self):\n    with self.test_scope():\n      split_dim = constant_op.constant(1)\n      value = constant_op.constant([[0., 1., 2.], [3., 4., 5.]])\n      result = array_ops.split(value, 3, axis=split_dim)\n      self.assertTrue(isinstance(result, list))\n      self.assertEqual(3, len(result))\n      self.assertAllEqual([[0], [3]], result[0])\n      self.assertAllEqual([[1], [4]], result[1])\n      self.assertAllEqual([[2], [5]], result[2])\n\n  def testBasicGraph(self):\n    # Run some ops eagerly\n    with self.test_scope():\n      three = constant_op.constant(3)\n      five = constant_op.constant(5)\n      product = three * five\n      self.assertAllEqual(15, product)\n\n    # Run some ops graphly\n    with context.graph_mode(), self.cached_session():\n      with self.test_scope():\n        three = constant_op.constant(3)\n        five = constant_op.constant(5)\n        product = three * five\n        self.assertAllEqual(15, self.evaluate(product))\n\n  def testDegenerateSlices(self):\n    with self.test_scope():\n      npt = np.arange(1, 19, dtype=np.float32).reshape(3, 2, 3)\n      t = constant_op.constant(npt)\n      # degenerate by offering a forward interval with a negative stride\n      self.assertAllEqual(npt[0:-1:-1, :, :], t[0:-1:-1, :, :])\n      # degenerate with a reverse interval with a positive stride\n      self.assertAllEqual(npt[-1:0, :, :], t[-1:0, :, :])\n      # empty interval in every dimension\n      self.assertAllEqual(npt[-1:0, 2:2, 2:3:-1], t[-1:0, 2:2, 2:3:-1])\n\n  def testIdentity(self):\n    with self.test_scope():\n      self.assertAllEqual(2, array_ops.identity(2))\n\n  def testRandomOps(self):\n    with self.test_scope():\n      tensor = gen_random_ops.random_uniform((2, 2), dtypes.float32)\n      row0 = tensor[0].numpy()\n      row1 = tensor[1].numpy()\n      # It should be very unlikely to rng to generate two equal rows.\n      self.assertFalse((row0 == row1).all())\n\n  def testIdentityOnVariable(self):\n    with self.test_scope():\n      v = resource_variable_ops.ResourceVariable(True)\n      i = array_ops.identity(v)\n    self.assertAllEqual(True, i.numpy())\n\n  def testAssignAddVariable(self):\n    with self.test_scope():\n      v = resource_variable_ops.ResourceVariable(1.0)\n      v.assign_add(2.0)\n    self.assertEqual(3.0, v.numpy())\n\n  def testReadAssignRead(self):\n    with self.test_scope():\n      v = resource_variable_ops.ResourceVariable(1.0)\n      val1 = v.read_value()\n      v.assign_add(2.0)\n      val2 = v.read_value()\n    self.assertEqual(1.0, val1.numpy())\n    self.assertEqual(3.0, val2.numpy())\n\n  def testGradient(self):\n    def f(x):\n      return x\n\n    with self.test_scope():\n      grad_fn = backprop.gradients_function(f)\n      self.assertAllEqual(2., grad_fn(1., dy=2.)[0])\n\n  def testVariableGradient(self):\n    with self.test_scope():\n      v0 = resource_variable_ops.ResourceVariable(1.0)\n\n      def f():\n        x = v0 * v0\n        return x\n\n      grads = backprop.implicit_grad(f)()\n    self.assertEqual(2., grads[0][0].numpy())\n\n  def testMultipleVariableReads(self):\n    # This test makes sure consecutive variable reads don't copy\n    # the underlying memory.\n    with self.test_scope():\n      # Create 128MiB variables\n      var = resource_variable_ops.ResourceVariable(\n          array_ops.ones([32, 1024, 1024]))\n\n      # Read the same variable 100 times. If the underlying tensor\n      # is not copied, this is a trivial operation. If it is copied,\n      # this will eat over 13GB and OOM.\n      values = []\n      for _ in range(100):\n        values.append(var.value())\n\n  # The shape, shape_n, size, and rank are tested here because their\n  # execution kernels (as opposed to compilation only tf2xla kernels)\n  # are distincts from tf2xla kernels.\n\n  def testShape(self):\n    def const(value):\n      return array_ops.shape(\n          constant_op.constant(value)).numpy()\n\n    def ones(value):\n      return array_ops.shape(\n          array_ops.ones(value)).numpy()\n\n    with self.test_scope():\n      # Shapes of directly constructed tensors\n      self.assertAllEqual([], const(3))\n      self.assertAllEqual([3], const([1.0, 2.0, 3.0]))\n      self.assertAllEqual([2, 2], const([[1.0, 2.0], [3.0, 4.0]]))\n      self.assertAllEqual([2, 1, 2], const([[[1.0, 2.0]], [[3.0, 4.0]]]))\n\n      # Shapes of tensors created by op running on device\n      # We make this distinction because directly constructed tensors\n      # are treated differently in a few places that can influence shape:\n      #  - they always have on_host_tensor\n      #  - they and their shapes can be cached\n      #  - they end up on device via a copy, instead of as program output\n      self.assertAllEqual([], ones([]))\n      self.assertAllEqual([3], ones([3]))\n      self.assertAllEqual([2, 2], ones([2, 2]))\n      self.assertAllEqual([2, 1, 2], ones([2, 1, 2]))\n\n  def testShapeN(self):\n    with self.test_scope():\n      # Shapes of directly constructed tensors\n      shapes = array_ops.shape_n([\n          constant_op.constant(1.0),\n          constant_op.constant([1.0, 2.0, 3.0]),\n          constant_op.constant([[1.0, 2.0], [3.0, 4.0]])])\n      self.assertAllEqual(\n          [[], [3], [2, 2]],\n          [x.numpy().tolist() for x in shapes])\n\n      # Shapes of tensors created by op running on device\n      shapes = array_ops.shape_n([\n          array_ops.ones([]),\n          array_ops.ones([3]),\n          array_ops.ones([2, 2])])\n      self.assertAllEqual(\n          [[], [3], [2, 2]],\n          [x.numpy().tolist() for x in shapes])\n\n  def testSize(self):\n    with self.test_scope():\n      self.assertEqual(\n          1, array_ops.size(constant_op.constant(1.0)).numpy())\n      self.assertEqual(\n          3, array_ops.size(constant_op.constant([1.0, 2.0, 3.0])).numpy())\n      self.assertEqual(\n          4, array_ops.size(\n              constant_op.constant([[1.0, 2.0], [3.0, 4.0]])).numpy())\n\n  def testRank(self):\n    with self.test_scope():\n      self.assertEqual(\n          0, array_ops.rank(constant_op.constant(1.0)).numpy())\n      self.assertEqual(\n          1, array_ops.rank(constant_op.constant([1.0, 2.0, 3.0])).numpy())\n      self.assertEqual(\n          2, array_ops.rank(\n              constant_op.constant([[1.0, 2.0], [3.0, 4.0]])).numpy())\n\n  def testAdam(self):\n    with self.test_scope():\n      optimizer = adam.AdamOptimizer(0.1)\n      x = resource_variable_ops.ResourceVariable(10.0)\n      with backprop.GradientTape() as tape:\n        y = x * x\n      dy_dx = tape.gradient(y, x)\n      optimizer.apply_gradients([(dy_dx, x)])\n      self.assertAlmostEqual(9.9, x.numpy(), places=3)\n\n  def testAdamSparse(self):\n    with ops.device('/cpu:0'):\n      # Create 2-D embedding for 3 objects on CPU because sparse/sliced updates\n      # are not implemented on TPU.\n      embedding_matrix = resource_variable_ops.ResourceVariable(\n          array_ops.ones([3, 2]))\n\n    with self.test_scope():\n      with backprop.GradientTape() as tape:\n        embedding = embedding_ops.embedding_lookup(embedding_matrix, [1])\n        y = math_ops.reduce_sum(embedding)\n      dy_dx = tape.gradient(y, embedding_matrix)\n      self.assertIsInstance(dy_dx, ops.IndexedSlices)\n      optimizer = adam.AdamOptimizer(0.1)\n      # The gradient application operations will run on CPU because optimizer\n      # updates are always collocated with the variable.\n      optimizer.apply_gradients([(dy_dx, embedding_matrix)])\n\n      # This assign_add will run on CPU because when an input to an\n      # operation is a resource, this operation is placed on the resource's\n      # device by the eager runtime.\n      embedding_matrix.assign_add(array_ops.ones([3, 2]))\n\n    self.assertAllClose([[2.0, 2.0],\n                         [1.9, 1.9],\n                         [2.0, 2.0]], embedding_matrix.numpy())\n\n\nclass EagerFunctionTest(xla_test.XLATestCase):\n\n  def testBasic(self):\n    with self.test_scope():\n      matmul = function.defun(math_ops.matmul)\n      t = constant_op.constant([[1.0, 2.0], [3.0, 4.0]])\n      sq = matmul(t, t, transpose_a=True)\n      self.assertAllEqual(sq.numpy().reshape(-1), [10, 14, 14, 20])\n\n  def testConv(self):\n    if 'GPU' in self.device:\n      # TODO(b/32333178)\n      self.skipTest('Current implementation of RandomStandardNormal kernel '\n                    'is very slow on GPU, and has been blacklisted.')\n    with self.test_scope():\n      data_format = 'channels_last'\n      conv = convolutional.Conv2D(\n          filters=1, kernel_size=2, padding='VALID',\n          data_format=data_format, activation=nn_ops.relu,\n          kernel_initializer=init_ops.ones_initializer(),\n          bias_initializer=init_ops.zeros_initializer())\n      pool = pooling.MaxPooling2D(2, 2, data_format=data_format)\n\n      def model(x):\n        x = conv(x)\n        return pool(x)\n      model = function.defun(model)\n\n      x = array_ops.ones([1, 4, 4, 1])\n      y = model(x)\n      self.assertAllEqual(y.numpy(), [[[[4.]]]])\n\n  def testReadVariable(self):\n    with self.test_scope():\n      v = resource_variable_ops.ResourceVariable(1.0)\n\n      @function.defun\n      def f():\n        return v.read_value()\n\n      var = f()\n      self.assertEqual(1.0, var.numpy())\n\n  def testUpdateVariable(self):\n    with self.test_scope():\n      v = resource_variable_ops.ResourceVariable(1.0)\n\n      def f(v):\n        v.assign_add(1.0)\n        return v\n\n      f = function.defun(f)\n\n      var = f(v)\n      self.assertEqual(2.0, var.numpy())\n\n  def testReturnResourceHandle(self):\n    with self.test_scope():\n      v = resource_variable_ops.ResourceVariable([[1.0, 2.0], [3.0, 4.0]])\n\n      def f(v):\n        return v.handle\n\n      f = function.defun(f)\n      handle = f(v)\n      self.assertAllEqual(v.numpy(),\n                          resource_variable_ops.read_variable_op(\n                              handle, dtypes.float32).numpy())\n\n  def testReturnMultipleResourceHandles(self):\n    with self.test_scope():\n      v1 = resource_variable_ops.ResourceVariable(1.25)\n      v2 = resource_variable_ops.ResourceVariable(2.0)\n\n      def f(v):\n        return v.handle, 3.0 * v, v2.handle, v + v2\n\n      f = function.defun(f)\n      v1_handle, v1_times_3, v2_handle, variable_sum = f(v1)\n      self.assertAllEqual(v1.numpy(),\n                          resource_variable_ops.read_variable_op(\n                              v1_handle, dtypes.float32).numpy())\n      self.assertEqual(3.75, v1_times_3.numpy())\n      self.assertAllEqual(v2.numpy(),\n                          resource_variable_ops.read_variable_op(\n                              v2_handle, dtypes.float32).numpy())\n      self.assertEqual(3.25, variable_sum.numpy())\n\n  def testAllArgumentKinds(self):\n    \"\"\"Test a complex function that takes different argument kinds.\n\n    tf2xla machinery that translates, compiles, and runs defuns\n    classifies arguments into: compile-time constants, regular tensors,\n    and resources. This test creates a function with a mix of all these\n    kinds. Moreover, the order of function arguments is intentionally mixed up.\n\n    This also tests the case when the same argument is a compile-time constant\n    as well as used in an operation that normally expects its inputs to be\n    in device memory - addition in this case.\n    \"\"\"\n    with self.test_scope():\n      def foo(c1, r1, v1, c2, v2, r2):\n        # c1 and c2 are compile-time constants\n        # r1 and r2 are regular tensors\n        # v1 and v2 are resource variables\n        a = c1 + r1\n        b = math_ops.cast(c2, dtypes.float32) + v2\n        c = array_ops.slice(v1, c1, c2)\n        d = r2 * v2\n        return a, b, c, d\n\n      foo = function.defun(foo)\n\n      c1 = [0, 0]\n      c2 = array_ops.ones([2], dtype=dtypes.int32)\n\n      r1 = array_ops.ones([2])\n      r2 = [[2., 2.], [3., 3.]]\n\n      v1 = resource_variable_ops.ResourceVariable([[1., 2.], [3., 4.]])\n      v2 = resource_variable_ops.ResourceVariable([[10., 20.], [30., 40.]])\n\n      a, b, c, d = foo(c1, r1, v1, c2, v2, r2)\n\n      self.assertAllEqual([1, 1], a.numpy())\n      self.assertAllEqual([[11., 21.], [31., 41.]], b.numpy())\n      self.assertAllEqual([[1.]], c.numpy())\n      self.assertAllEqual([[20., 40.], [90., 120.]], d.numpy())\n\n  def testDefunInGradientTape(self):\n    with self.test_scope():\n      v0 = resource_variable_ops.ResourceVariable(5.0)\n\n      @function.defun\n      def f(x):\n        x = v0 * v0 * x\n        return x\n\n      x = constant_op.constant(3.0)\n      with backprop.GradientTape() as tape:\n        y = f(x)\n      dy = tape.gradient(y, v0)\n\n    self.assertEqual(75, y.numpy())\n    self.assertEqual(30, dy.numpy())\n\n  def testGradientTapeInDefun(self):\n    with self.test_scope():\n      v0 = resource_variable_ops.ResourceVariable(5.0)\n\n      @function.defun\n      def f():\n        x = constant_op.constant(1.0)\n        with backprop.GradientTape() as tape:\n          y = v0 * x\n        dy = tape.gradient(y, v0)\n        return dy\n\n      dy = f()\n      self.assertEqual(1.0, dy.numpy())\n\n  def testSliceInDefun(self):\n    with self.test_scope():\n\n      @function.defun\n      def f(x, y):\n        return x[0::2, y:, ...]\n\n      x = array_ops.ones([2, 3, 4])\n      y = array_ops.ones([], dtype=dtypes.int32)\n      with backprop.GradientTape() as tape:\n        tape.watch(x)\n        tape.watch(y)\n        z = f(x, y)\n      dz = tape.gradient(z, x)\n\n      self.assertAllEqual(np.ones([1, 2, 4]), z.numpy())\n      self.assertAllEqual((2, 3, 4), dz.shape.as_list())\n\n  def testNestedDefun(self):\n    with self.test_scope():\n\n      @function.defun\n      def times_two(x):\n        return 2 * x\n\n      @function.defun\n      def two_x_plus_1(x):\n        return times_two(x) + 1\n\n      x = constant_op.constant([2, 3, 4])\n      y = two_x_plus_1(x)\n      self.assertAllEqual([5, 7, 9], y.numpy())\n\n  def testNestedDefunWithVariable(self):\n    with self.test_scope():\n      v0 = resource_variable_ops.ResourceVariable(5.0)\n\n      @function.defun\n      def g(x):\n        x = v0 * x\n        return x\n\n      @function.defun\n      def f(x):\n        x = g(v0 * x)\n        return x\n\n      x = constant_op.constant(3.0)\n      y = f(x)\n\n    self.assertEqual(75, y.numpy())\n\n  def testNestedDefunInGradientTape(self):\n    with self.test_scope():\n      v0 = resource_variable_ops.ResourceVariable(5.0)\n\n      @function.defun\n      def g(x):\n        x = v0 * x\n        return x\n\n      @function.defun\n      def f(x):\n        x = g(v0 * x)\n        return x\n\n      x = constant_op.constant(3.0)\n      with backprop.GradientTape() as tape:\n        y = f(x)\n      dy = tape.gradient(y, v0)\n\n    self.assertEqual(75, y.numpy())\n    self.assertEqual(30, dy.numpy())\n\n  def testNestedDefunInGradientTapeDifferentVars(self):\n    with self.test_scope():\n      v0 = resource_variable_ops.ResourceVariable(5.0)\n      v1 = resource_variable_ops.ResourceVariable(3.0)\n\n      @function.defun\n      def g(x):\n        x = v1 * x\n        return x\n\n      @function.defun\n      def f(x):\n        x = g(v0 * x)\n        return x\n\n      x = constant_op.constant(3.0)\n      with backprop.GradientTape(persistent=True) as tape:\n        y = f(x)\n      dy_v0 = tape.gradient(y, v0)\n      dy_v1 = tape.gradient(y, v1)\n\n    self.assertEqual(45, y.numpy())\n    self.assertEqual(9, dy_v0.numpy())\n    self.assertEqual(15, dy_v1.numpy())\n\n\nclass ExcessivePaddingTest(xla_test.XLATestCase):\n  \"\"\"Test that eager execution works with TPU flattened tensors.\n\n  Tensors that would normally be excessively padded when written\n  to TPU memory are reshaped to 1-D flat tensors.\n\n  This test case verifies that such tensors work with eager execution.\n\n  The flattening currently only happens on TPU, but tests should work\n  fine with all backends as flattening is transparent.\n  \"\"\"\n\n  def testFromConstant(self):\n    with self.test_scope():\n      # Create constant of shape [100, 2, 1]. This tensor would be\n      # excessively padded on TPU.\n      tensor = constant_op.constant(100 * [[[10.0], [2.0]]])\n      # Use reduce_sum since it requires correctly working with\n      # a particular dimension.\n      reduced = math_ops.reduce_sum(tensor, axis=1)\n      self.assertAllEqual(100 * [[12.0]], reduced)\n\n  def testFromOperation(self):\n    with self.test_scope():\n      tensor = array_ops.ones([3, 100, 2, 2])\n      reduced = math_ops.reduce_sum(tensor, axis=[0, 2, 3])\n      self.assertAllEqual(100 * [12.0], reduced)\n\n  def testAsFunctionInput(self):\n    with self.test_scope():\n\n      @function.defun\n      def f(x):\n        return math_ops.reduce_sum(x, axis=2)\n\n      tensor = constant_op.constant(100 * [[[10.0, 2.0]]])\n      reduced = f(tensor)\n      self.assertAllEqual(100 * [[12.0]], reduced)\n\n  def testAsFunctionOutput(self):\n    with self.test_scope():\n\n      @function.defun\n      def f(x):\n        return x * constant_op.constant(100 * [[[10.0, 2.0]]])\n\n      y = f(3)\n      reduced = math_ops.reduce_sum(y, axis=2)\n      self.assertAllEqual(100 * [[36.0]], reduced)\n\n\ndef multiple_tpus():\n  devices = context.context().devices()\n  return len([d for d in devices if 'device:TPU:' in d]) > 1\n\n\nclass MultiDeviceTest(xla_test.XLATestCase):\n  \"\"\"Test running TPU computation on more than one core.\"\"\"\n\n  def testBasic(self):\n    if not multiple_tpus():\n      self.skipTest('MultiDeviceTest requires multiple TPU devices.')\n\n    # Compute 10 on TPU core 0\n    with ops.device('device:TPU:0'):\n      two = constant_op.constant(2)\n      five = constant_op.constant(5)\n      ten = two * five\n      self.assertAllEqual(10, ten)\n\n    # Compute 6 on TPU core 1\n    with ops.device('device:TPU:1'):\n      two = constant_op.constant(2)\n      three = constant_op.constant(3)\n      six = two * three\n      self.assertAllEqual(6, six)\n\n    # Copy 10 and 6 to CPU and sum them\n    self.assertAllEqual(16, ten + six)\n\n\nif __name__ == '__main__':\n  ops.enable_eager_execution(\n      config=config_pb2.ConfigProto(log_device_placement=True))\n  googletest.main()\n", "se, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# @@license_version:1.7@@\n\nimport os\nimport shutil\n\nimport oca_unittest\nfrom rogerthat.bizz.profile import create_user_profile\nfrom rogerthat.bizz.service import UserWithThisEmailAddressAlreadyExistsException\nfrom rogerthat.dal.profile import get_user_profile\nfrom rogerthat.rpc import users\nfrom rogerthat.rpc.service import ServiceApiException\nfrom rogerthat.utils.app import create_app_user_by_email\nfrom solutions.common.bizz import create_solution_service\n\n\nclass TestException(ServiceApiException):\n\n    def __init__(self):\n        super(TestException, self).__init__(300, \"lekker\")\n\ndef copytree(src, dst, symlinks=False, ignore=None):\n    if not os.path.exists(dst):\n        os.makedirs(dst)\n    for item in os.listdir(src):\n        s = os.path.join(src, item)\n        d = os.path.join(dst, item)\n        if os.path.isdir(s):\n            copytree(s, d, symlinks, ignore)\n        else:\n            if not os.path.exists(d) or os.stat(s).st_mtime - os.stat(d).st_mtime > 1:\n                shutil.copy2(s, d)\n\n\nclass Test(oca_unittest.TestCase):\n\n    def testServiceAPIException(self):\n        try:\n            raise TestException()\n        except ServiceApiException, e:\n            assert unicode(e) == \"lekker\"\n\n    def test_create_service_with_existing_user(self):\n        self.set_datastore_hr_probability(1)\n        rogerthat_service_email = \"service-rogerthat@foo.com\"\n        rogerthat_email = \"rogerthat@foo.com\"\n        rogerthat_user = create_app_user_by_email(rogerthat_email, 'rogerthat')\n\n        be_loc_email = \"be.loc@foo.com\"\n        be_loc_user = create_app_user_by_email(be_loc_email, 'be-loc')\n\n        for u in (rogerthat_user, be_loc_user):\n            create_user_profile(u, u.email())\n\n        with self.assertRaises(UserWithThisEmailAddressAlreadyExistsException) as cm:\n            create_solution_service(rogerthat_email, 'name', solution='flex')\n        self.assertEqual(rogerthat_email, cm.exception.fields['email'])\n\n        _, new_service_sln_settings = create_solution_service(rogerthat_service_email, 'name', solution='flex',\n                                                              owner_user_email=rogerthat_email)\n        new_service_user = new_service_sln_settings.service_user\n        self.assertEqual(rogerthat_service_email, new_service_user.email())\n        self.assertTrue(rogerthat_service_email in get_user_profile(users.User(rogerthat_email), cached=False).owningServiceEmails)\n\n        _, new_service_sln_settings = create_solution_service(be_loc_email, 'name', solution='flex')\n        new_service_user = new_service_sln_settings.service_user\n        self.assertEqual(be_loc_email, new_service_user.email())\n", " Reserved.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n\nimport itertools\n\nfrom nova.api.openstack import common\nfrom nova.openstack.common import log as logging\n\nLOG = logging.getLogger(__name__)\n\n\nclass ViewBuilder(common.ViewBuilder):\n    \"\"\"Models server addresses as a dictionary.\"\"\"\n\n    _collection_name = \"addresses\"\n\n    def basic(self, ip):\n        \"\"\"Return a dictionary describing an IP address.\"\"\"\n        return {\n            \"version\": ip[\"version\"],\n            \"addr\": ip[\"address\"],\n        }\n\n    def show(self, network, label):\n        \"\"\"Returns a dictionary describing a network.\"\"\"\n        all_ips = itertools.chain(network[\"ips\"], network[\"floating_ips\"])\n        return {label: [self.basic(ip) for ip in all_ips]}\n\n    def index(self, networks):\n        \"\"\"Return a dictionary describing a list of networks.\"\"\"\n        addresses = {}\n        for label, network in networks.items():\n            network_dict = self.show(network, label)\n            addresses[label] = network_dict[label]\n        return dict(addresses=addresses)\n", "(the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Input data processing tests for ranking library.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\nimport os\nfrom absl.testing import parameterized\n\nimport tensorflow as tf\n\nfrom google.protobuf import text_format\nfrom tensorflow_ranking.python import data as data_lib\nfrom tensorflow_serving.apis import input_pb2\n\n# Feature name for example list sizes and masks.\n_SIZE = \"example_list_size\"\n_MASK = \"mask\"\n\nEXAMPLE_LIST_PROTO_1 = text_format.Parse(\n    \"\"\"\n    context {\n      features {\n        feature {\n          key: \"query_length\"\n          value { int64_list { value: 3 } }\n        }\n      }\n    }\n    examples {\n      features {\n        feature {\n          key: \"unigrams\"\n          value { bytes_list { value: \"tensorflow\" } }\n        }\n        feature {\n          key: \"utility\"\n          value { float_list { value: 0.0 } }\n        }\n      }\n    }\n    examples {\n      features {\n        feature {\n          key: \"unigrams\"\n          value { bytes_list { value: [\"learning\", \"to\", \"rank\"] } }\n        }\n        feature {\n          key: \"utility\"\n          value { float_list { value: 1.0 } }\n        }\n      }\n    }\n    \"\"\", input_pb2.ExampleListWithContext())\n\nEXAMPLE_LIST_PROTO_2 = text_format.Parse(\n    \"\"\"\n    context {\n      features {\n        feature {\n          key: \"query_length\"\n          value { int64_list { value: 2 } }\n        }\n      }\n    }\n    examples {\n      features {\n        feature {\n          key: \"unigrams\"\n          value { bytes_list { value: \"gbdt\" } }\n        }\n        feature {\n          key: \"utility\"\n          value { float_list { value: 0.0 } }\n        }\n      }\n    }\n    \"\"\", input_pb2.ExampleListWithContext())\n\nCONTEXT_1 = text_format.Parse(\n    \"\"\"\n    features {\n      feature {\n        key: \"query_length\"\n        value { int64_list { value: 3 } }\n      }\n    }\"\"\", tf.train.Example())\n\nEXAMPLES_1 = [\n    text_format.Parse(\n        \"\"\"\n    features {\n      feature {\n        key: \"unigrams\"\n        value { bytes_list { value: \"tensorflow\" } }\n      }\n      feature {\n        key: \"utility\"\n        value { float_list { value: 0.0 } }\n      }\n    }\"\"\", tf.train.Example()),\n    text_format.Parse(\n        \"\"\"\n    features {\n      feature {\n        key: \"unigrams\"\n        value { bytes_list { value: [\"learning\", \"to\", \"rank\"] } }\n      }\n      feature {\n        key: \"utility\"\n        value { float_list { value: 1.0 } }\n      }\n    }\"\"\", tf.train.Example()),\n]\n\n\nCONTEXT_2 = text_format.Parse(\n    \"\"\"\n    features {\n      feature {\n        key: \"query_length\"\n        value { int64_list { value: 2 } }\n      }\n    }\"\"\", tf.train.Example())\n\nEXAMPLES_2 = [\n    text_format.Parse(\n        \"\"\"\n    features {\n      feature {\n        key: \"unigrams\"\n        value { bytes_list { value: \"gbdt\" } }\n      }\n      feature {\n        key: \"utility\"\n        value { float_list { value: 0.0 } }\n      }\n    }\"\"\", tf.train.Example()),\n]\n\nTF_EXAMPLE_PROTO_1 = text_format.Parse(\n    \"\"\"\n    features {\n      feature {\n          key: \"query_length\"\n          value { int64_list { value: 1 } }\n        }\n      feature {\n        key: \"unigrams\"\n        value { bytes_list { value: \"tensorflow\" } }\n      }\n      feature {\n        key: \"utility\"\n        value { float_list { value: 0.0 } }\n      }\n    }\n    \"\"\", tf.train.Example())\n\nTF_EXAMPLE_PROTO_2 = text_format.Parse(\n    \"\"\"\n    features {\n      feature {\n          key: \"query_length\"\n          value { int64_list { value: 3 } }\n        }\n      feature {\n        key: \"unigrams\"\n        value { bytes_list { value: [\"learning\", \"to\", \"rank\"] } }\n      }\n      feature {\n        key: \"utility\"\n        value { float_list { value: 1.0 } }\n      }\n    }\n    \"\"\", tf.train.Example())\n\nSEQ_EXAMPLE_PROTO_1 = text_format.Parse(\n    \"\"\"\n    context {\n      feature {\n        key: \"query_length\"\n        value { int64_list { value: 3 } }\n      }\n    }\n    feature_lists {\n      feature_list {\n        key: \"unigrams\"\n        value {\n          feature { bytes_list { value: \"tensorflow\" } }\n          feature { bytes_list { value: [\"learning\", \"to\", \"rank\"] } }\n        }\n      }\n      feature_list {\n        key: \"utility\"\n        value {\n          feature { float_list { value: 0.0 } }\n          feature { float_list { value: 1.0 } }\n        }\n      }\n    }\n    \"\"\", tf.train.SequenceExample())\n\nSEQ_EXAMPLE_PROTO_2 = text_format.Parse(\n    \"\"\"\n    context {\n      feature {\n        key: \"query_length\"\n        value { int64_list { value: 2 } }\n      }\n    }\n    feature_lists {\n      feature_list {\n        key: \"unigrams\"\n        value {\n          feature { bytes_list { value: \"gbdt\" } }\n        }\n      }\n      feature_list {\n        key: \"utility\"\n        value {\n          feature { float_list { value: 0.0 } }\n        }\n      }\n    }\n    \"\"\", tf.train.SequenceExample())\n\nCONTEXT_FEATURE_SPEC = {\n    \"query_length\": tf.io.FixedLenFeature([1], tf.int64, default_value=[0])\n}\n\nEXAMPLE_FEATURE_SPEC = {\n    \"unigrams\": tf.io.VarLenFeature(tf.string),\n    \"utility\": tf.io.FixedLenFeature([1], tf.float32, default_value=[-1.])\n}\n\nCONTEXT_RAGGED_FEATURE_SPEC = {\"query_length\": tf.io.RaggedFeature(tf.int64)}\n\nEXAMPLE_RAGGED_FEATURE_SPEC = {\n    \"unigrams\": tf.io.RaggedFeature(tf.string),\n    \"utility\": tf.io.FixedLenFeature([1], tf.float32, default_value=[-1.])\n}\n\nCONTEXT_SPARSE_FEATURE_SPEC = {\n    \"query_length\": tf.io.VarLenFeature(tf.int64)\n}\n\nEXAMPLE_SPARSE_FEATURE_SPEC = {\n    \"unigrams\": tf.io.VarLenFeature(tf.string),\n    \"utility\": tf.io.VarLenFeature(tf.float32)\n}\n\n\ndef make_example_list_input_fn():\n  \"\"\"example_list input fn.\"\"\"\n\n  def _example_list_proto_generator():\n    return [\n        EXAMPLE_LIST_PROTO_1.SerializeToString(),\n        EXAMPLE_LIST_PROTO_2.SerializeToString()\n    ] * 100\n\n  def example_list_input_fn():\n    dataset = tf.data.Dataset.from_generator(_example_list_proto_generator,\n                                             (tf.string), (tf.TensorShape([])))\n    kwargs = {\n        \"list_size\": 2,\n        \"context_feature_spec\": CONTEXT_FEATURE_SPEC,\n        \"example_feature_spec\": EXAMPLE_FEATURE_SPEC,\n    }\n    dataset = dataset.map(\n        functools.partial(data_lib.parse_single_example_list,\n                          **kwargs)).batch(2)\n\n    return tf.compat.v1.data.make_one_shot_iterator(dataset).get_next()\n\n  return example_list_input_fn\n\n\nclass ExampleListTest(tf.test.TestCase):\n\n  def test_decode_as_serialized_example_list(self):\n    with tf.Graph().as_default():\n      context_tensor, list_tensor, sizes = (\n          data_lib._decode_as_serialized_example_list(\n              [EXAMPLE_LIST_PROTO_1.SerializeToString()]))\n      with tf.compat.v1.Session() as sess:\n        sess.run(tf.compat.v1.local_variables_initializer())\n        context_, list_, sizes_ = sess.run([context_tensor, list_tensor, sizes])\n        self.assertAllEqual(\n            tf.convert_to_tensor(value=context_).get_shape().as_list(), [1, 1])\n        self.assertAllEqual(\n            tf.convert_to_tensor(value=list_).get_shape().as_list(), [1, 2])\n        self.assertAllEqual(sizes_, [2])\n\n  def test_parse_from_example_list(self):\n    with tf.Graph().as_default():\n      serialized_example_lists = [\n          EXAMPLE_LIST_PROTO_1.SerializeToString(),\n          EXAMPLE_LIST_PROTO_2.SerializeToString()\n      ]\n      features = data_lib.parse_from_example_list(\n          serialized_example_lists,\n          context_feature_spec=CONTEXT_FEATURE_SPEC,\n          example_feature_spec=EXAMPLE_FEATURE_SPEC)\n\n      with tf.compat.v1.Session() as sess:\n        sess.run(tf.compat.v1.local_variables_initializer())\n        features = sess.run(features)\n        # Test dense_shape, indices and values for a SparseTensor.\n        self.assertAllEqual(features[\"unigrams\"].dense_shape, [2, 2, 3])\n        self.assertAllEqual(\n            features[\"unigrams\"].indices,\n            [[0, 0, 0], [0, 1, 0], [0, 1, 1], [0, 1, 2], [1, 0, 0]])\n        self.assertAllEqual(\n            features[\"unigrams\"].values,\n            [b\"tensorflow\", b\"learning\", b\"to\", b\"rank\", b\"gbdt\"])\n        # For Tensors with dense values, values can be directly checked.\n        self.assertAllEqual(features[\"query_length\"], [[3], [2]])\n        self.assertAllEqual(features[\"utility\"], [[[0.], [1.0]], [[0.], [-1.]]])\n\n  def test_parse_from_example_list_padding(self):\n    with tf.Graph().as_default():\n      serialized_example_lists = [\n          EXAMPLE_LIST_PROTO_1.SerializeToString(),\n          EXAMPLE_LIST_PROTO_2.SerializeToString()\n      ]\n      # Padding since list_size 3 is larger than 2.\n      features = data_lib.parse_from_example_list(\n          serialized_example_lists,\n          list_size=3,\n          context_feature_spec=CONTEXT_FEATURE_SPEC,\n          example_feature_spec=EXAMPLE_FEATURE_SPEC)\n\n      with tf.compat.v1.Session() as sess:\n        sess.run(tf.compat.v1.local_variables_initializer())\n        features = sess.run(features)\n        # Test dense_shape, indices and values for a SparseTensor.\n        self.assertAllEqual(features[\"unigrams\"].dense_shape, [2, 3, 3])\n        self.assertAllEqual(\n            features[\"unigrams\"].indices,\n            [[0, 0, 0], [0, 1, 0], [0, 1, 1], [0, 1, 2], [1, 0, 0]])\n        self.assertAllEqual(\n            features[\"unigrams\"].values,\n            [b\"tensorflow\", b\"learning\", b\"to\", b\"rank\", b\"gbdt\"])\n        # For Tensors with dense values, values can be directly checked.\n        self.assertAllEqual(features[\"query_length\"], [[3], [2]])\n        self.assertAllEqual(features[\"utility\"],\n                            [[[0.], [1.0], [-1.]], [[0.], [-1.], [-1.]]])\n\n  def test_parse_example_list_with_sizes(self):\n    with tf.Graph().as_default():\n      serialized_example_lists = [\n          EXAMPLE_LIST_PROTO_1.SerializeToString(),\n          EXAMPLE_LIST_PROTO_2.SerializeToString()\n      ]\n      # Padding since list_size 3 is larger than 2.\n      features = data_lib.parse_from_example_list(\n          serialized_example_lists,\n          list_size=3,\n          context_feature_spec=CONTEXT_FEATURE_SPEC,\n          example_feature_spec=EXAMPLE_FEATURE_SPEC,\n          size_feature_name=_SIZE,\n          mask_feature_name=_MASK)\n\n      with tf.compat.v1.Session() as sess:\n        sess.run(tf.compat.v1.local_variables_initializer())\n        features = sess.run(features)\n        self.assertAllEqual(features[_SIZE], [2, 1])\n        self.assertAllEqual(features[_MASK],\n                            [[True, True, False], [True, False, False]])\n\n  def test_parse_from_example_list_truncate(self):\n    with tf.Graph().as_default():\n      serialized_example_lists = [\n          EXAMPLE_LIST_PROTO_1.SerializeToString(),\n          EXAMPLE_LIST_PROTO_2.SerializeToString()\n      ]\n      # Truncate number of examples from 2 to 1.\n      features = data_lib.parse_from_example_list(\n          serialized_example_lists,\n          list_size=1,\n          context_feature_spec=CONTEXT_FEATURE_SPEC,\n          example_feature_spec=EXAMPLE_FEATURE_SPEC)\n\n      with tf.compat.v1.Session() as sess:\n        sess.run(tf.compat.v1.local_variables_initializer())\n        features = sess.run(features)\n        # Test dense_shape, indices and values for a SparseTensor.\n        self.assertAllEqual(features[\"unigrams\"].dense_shape, [2, 1, 1])\n        self.assertAllEqual(features[\"unigrams\"].indices,\n                            [[0, 0, 0], [1, 0, 0]])\n        self.assertAllEqual(features[\"unigrams\"].values,\n                            [b\"tensorflow\", b\"gbdt\"])\n        # For Tensors with dense values, values can be directly checked.\n        self.assertAllEqual(features[\"query_length\"], [[3], [2]])\n        self.assertAllEqual(features[\"utility\"], [[[0.]], [[0.]]])\n\n  def test_parse_from_example_list_shuffle(self):\n    with tf.Graph().as_default():\n      serialized_example_lists = [\n          EXAMPLE_LIST_PROTO_1.SerializeToString(),\n          EXAMPLE_LIST_PROTO_2.SerializeToString()\n      ]\n      # Trunate number of examples from 2 to 1.\n      features = data_lib.parse_from_example_list(\n          serialized_example_lists,\n          list_size=1,\n          context_feature_spec=CONTEXT_FEATURE_SPEC,\n          example_feature_spec=EXAMPLE_FEATURE_SPEC,\n          shuffle_examples=True,\n          seed=1)\n\n      with tf.compat.v1.Session() as sess:\n        sess.run(tf.compat.v1.local_variables_initializer())\n        features = sess.run(features)\n        # With `shuffle_examples` and seed=1, the example `tensorflow` and the\n        # example `learning to rank` in EXAMPLE_LIST_PROTO_1 switch order. After\n        # truncation at list_size=1, only `learning to rank` in\n        # EXAMPLE_LIST_PROTO_1 and `gbdt` in EXAMPLE_LIST_PROTO_2 are left in\n        # serialized features.\n        # Test dense_shape, indices and values for a SparseTensor.\n        self.assertAllEqual(features[\"unigrams\"].dense_shape, [2, 1, 3])\n        self.assertAllEqual(features[\"unigrams\"].indices,\n                            [[0, 0, 0], [0, 0, 1], [0, 0, 2], [1, 0, 0]])\n        self.assertAllEqual(features[\"unigrams\"].values,\n                            [b\"learning\", b\"to\", b\"rank\", b\"gbdt\"])\n        # For Tensors with dense values, values can be directly checked.\n        self.assertAllEqual(features[\"query_length\"], [[3], [2]])\n        self.assertAllEqual(features[\"utility\"], [[[1.]], [[0.]]])\n\n  def test_parse_from_example_list_static_shape(self):\n    with tf.Graph().as_default():\n      serialized_example_lists = [\n          EXAMPLE_LIST_PROTO_1.SerializeToString(),\n          EXAMPLE_LIST_PROTO_2.SerializeToString()\n      ]\n      feature_map_list = []\n      for list_size in [None, 100, 1]:\n        feature_map_list.append(\n            data_lib.parse_from_example_list(\n                serialized_example_lists,\n                list_size=list_size,\n                context_feature_spec=CONTEXT_FEATURE_SPEC,\n                example_feature_spec=EXAMPLE_FEATURE_SPEC))\n      for features in feature_map_list:\n        self.assertAllEqual([2, 1],\n                            features[\"query_length\"].get_shape().as_list())\n      for features, static_shape in zip(feature_map_list, [\n          [2, 2, 1],\n          [2, 100, 1],\n          [2, 1, 1],\n      ]):\n        self.assertAllEqual(static_shape,\n                            features[\"utility\"].get_shape().as_list())\n\n  def test_parse_example_list_sparse(self):\n    with tf.Graph().as_default():\n      serialized_example_lists = [\n          EXAMPLE_LIST_PROTO_1.SerializeToString(),\n          EXAMPLE_LIST_PROTO_2.SerializeToString()\n      ]\n      features = data_lib.parse_from_example_list(\n          serialized_example_lists,\n          list_size=3,\n          context_feature_spec=CONTEXT_SPARSE_FEATURE_SPEC,\n          example_feature_spec=EXAMPLE_SPARSE_FEATURE_SPEC,\n          size_feature_name=_SIZE,\n          mask_feature_name=_MASK)\n\n      with tf.compat.v1.Session() as sess:\n        sess.run(tf.compat.v1.local_variables_initializer())\n        feature_map = sess.run(features)\n        self.assertAllEqual(feature_map[\"query_length\"].dense_shape, [2, 1])\n        self.assertAllEqual(feature_map[\"query_length\"].indices,\n                            [[0, 0], [1, 0]])\n        self.assertAllEqual(feature_map[\"query_length\"].values, [3, 2])\n        self.assertAllEqual(feature_map[\"unigrams\"].dense_shape, [2, 3, 3])\n        self.assertAllEqual(\n            feature_map[\"unigrams\"].indices,\n            [[0, 0, 0], [0, 1, 0], [0, 1, 1], [0, 1, 2], [1, 0, 0]])\n        self.assertAllEqual(\n            feature_map[\"unigrams\"].values,\n            [b\"tensorflow\", b\"learning\", b\"to\", b\"rank\", b\"gbdt\"])\n        self.assertAllEqual(feature_map[\"utility\"].dense_shape, [2, 3, 1])\n        self.assertAllEqual(feature_map[\"utility\"].indices,\n                            [[0, 0, 0], [0, 1, 0], [1, 0, 0]])\n        self.assertAllEqual(feature_map[\"utility\"].values, [0., 1., 0.])\n        self.assertAllEqual(feature_map[_MASK],\n                            [[True, True, False], [True, False, False]])\n        self.assertAllEqual(feature_map[_SIZE], [2, 1])\n\n\nclass ExampleListWithRaggedTest(tf.test.TestCase):\n\n  def test_parse_from_example_list(self):\n    with tf.Graph().as_default():\n      serialized_example_lists = [\n          EXAMPLE_LIST_PROTO_1.SerializeToString(),\n          EXAMPLE_LIST_PROTO_2.SerializeToString()\n      ]\n      features = data_lib.parse_from_example_list(\n          serialized_example_lists,\n          context_feature_spec=CONTEXT_RAGGED_FEATURE_SPEC,\n          example_feature_spec=EXAMPLE_RAGGED_FEATURE_SPEC)\n\n      with tf.compat.v1.Session() as sess:\n        sess.run(tf.compat.v1.local_variables_initializer())\n        features = sess.run(features)\n        self.assertAllEqual(\n            features[\"unigrams\"],\n            [[[b\"tensorflow\"], [b\"learning\", b\"to\", b\"rank\"]], [[b\"gbdt\"], []]])\n        self.assertAllEqual(features[\"query_length\"], [[3], [2]])\n        self.assertAllEqual(features[\"utility\"], [[[0.], [1.0]], [[0.], [-1.]]])\n\n  def test_parse_from_example_list_padding(self):\n    with tf.Graph().as_default():\n      serialized_example_lists = [\n          EXAMPLE_LIST_PROTO_1.SerializeToString(),\n          EXAMPLE_LIST_PROTO_2.SerializeToString()\n      ]\n      # Padding since list_size 3 is larger than 2.\n      features = data_lib.parse_from_example_list(\n          serialized_example_lists,\n          list_size=3,\n          context_feature_spec=CONTEXT_RAGGED_FEATURE_SPEC,\n          example_feature_spec=EXAMPLE_RAGGED_FEATURE_SPEC)\n\n      with tf.compat.v1.Session() as sess:\n        sess.run(tf.compat.v1.local_variables_initializer())\n        features = sess.run(features)\n        self.assertAllEqual(features[\"unigrams\"],\n                            [[[b\"tensorflow\"], [b\"learning\", b\"to\", b\"rank\"],\n                              []], [[b\"gbdt\"], [], []]])\n        self.assertAllEqual(features[\"query_length\"], [[3], [2]])\n        self.assertAllEqual(features[\"utility\"],\n                            [[[0.], [1.0], [-1.]], [[0.], [-1.], [-1.]]])\n\n  def test_parse_from_example_list_truncate(self):\n    with tf.Graph().as_default():\n      serialized_example_lists = [\n          EXAMPLE_LIST_PROTO_1.SerializeToString(),\n          EXAMPLE_LIST_PROTO_2.SerializeToString()\n      ]\n      # Truncate number of examples from 2 to 1.\n      features = data_lib.parse_from_example_list(\n          serialized_example_lists,\n          list_size=1,\n          context_feature_spec=CONTEXT_RAGGED_FEATURE_SPEC,\n          example_feature_spec=EXAMPLE_RAGGED_FEATURE_SPEC)\n\n      with tf.compat.v1.Session() as sess:\n        sess.run(tf.compat.v1.local_variables_initializer())\n        features = sess.run(features)\n        self.assertAllEqual(features[\"unigrams\"],\n                            [[[b\"tensorflow\"]], [[b\"gbdt\"]]])\n        self.assertAllEqual(features[\"query_length\"], [[3], [2]])\n        self.assertAllEqual(features[\"utility\"], [[[0.]], [[0.]]])\n\n  def test_parse_from_example_list_shuffle(self):\n    with tf.Graph().as_default():\n      serialized_example_lists = [\n          EXAMPLE_LIST_PROTO_1.SerializeToString(),\n          EXAMPLE_LIST_PROTO_2.SerializeToString()\n      ]\n      # Trunate number of examples from 2 to 1.\n      features = data_lib.parse_from_example_list(\n          serialized_example_lists,\n          list_size=1,\n          context_feature_spec=CONTEXT_RAGGED_FEATURE_SPEC,\n          example_feature_spec=EXAMPLE_RAGGED_FEATURE_SPEC,\n          shuffle_examples=True,\n          seed=1)\n\n      with tf.compat.v1.Session() as sess:\n        sess.run(tf.compat.v1.local_variables_initializer())\n        features = sess.run(features)\n        # With `shuffle_examples` and seed=1, the example `tensorflow` and the\n        # example `learning to rank` in EXAMPLE_LIST_PROTO_1 switch order. After\n        # truncation at list_size=1, only `learning to rank` in\n        # EXAMPLE_LIST_PROTO_1 and `gbdt` in EXAMPLE_LIST_PROTO_2 are left in\n        # serialized features.\n        self.assertAllEqual(features[\"unigrams\"],\n                            [[[b\"learning\", b\"to\", b\"rank\"]], [[b\"gbdt\"]]])\n        self.assertAllEqual(features[\"query_length\"], [[3], [2]])\n        self.assertAllEqual(features[\"utility\"], [[[1.]], [[0.]]])\n\n  def test_parse_from_example_list_shape(self):\n    with tf.Graph().as_default():\n      serialized_example_lists = [\n          EXAMPLE_LIST_PROTO_1.SerializeToString(),\n          EXAMPLE_LIST_PROTO_2.SerializeToString()\n      ]\n      feature_map_list = []\n      for list_size in [None, 100, 1]:\n        feature_map_list.append(\n            data_lib.parse_from_example_list(\n                serialized_example_lists,\n                list_size=list_size,\n                context_feature_spec=CONTEXT_RAGGED_FEATURE_SPEC,\n                example_feature_spec=EXAMPLE_RAGGED_FEATURE_SPEC))\n      # Shape can only be checked for non-ragged tensors.\n      for features, static_shape in zip(feature_map_list, [\n          [2, 2, 1],\n          [2, 100, 1],\n          [2, 1, 1],\n      ]):\n        self.assertAllEqual(static_shape,\n                            features[\"utility\"].get_shape().as_list())\n\n\ndef _example_in_example(context, examples):\n  \"\"\"Returns an Example in Example.\"\"\"\n  example_in_example = tf.train.Example()\n  example_in_example.features.feature[\n      \"serialized_context\"].bytes_list.value.append(context.SerializeToString())\n  for ex in examples:\n    example_in_example.features.feature[\n        \"serialized_examples\"].bytes_list.value.append(ex.SerializeToString())\n  return example_in_example\n\n\nclass ExampleInExampleTest(tf.test.TestCase):\n\n  def test_parse_from_example_in_example(self):\n    with tf.Graph().as_default():\n      serialized_example_in_example = [\n          _example_in_example(CONTEXT_1, EXAMPLES_1).SerializeToString(),\n          _example_in_example(CONTEXT_2, EXAMPLES_2).SerializeToString(),\n      ]\n      features = data_lib.parse_from_example_in_example(\n          serialized_example_in_example,\n          context_feature_spec=CONTEXT_FEATURE_SPEC,\n          example_feature_spec=EXAMPLE_FEATURE_SPEC)\n\n      with tf.compat.v1.Session() as sess:\n        sess.run(tf.compat.v1.local_variables_initializer())\n        features = sess.run(features)\n        # Test dense_shape, indices and values for a SparseTensor.\n        self.assertAllEqual(features[\"unigrams\"].dense_shape, [2, 2, 3])\n        self.assertAllEqual(\n            features[\"unigrams\"].indices,\n            [[0, 0, 0], [0, 1, 0], [0, 1, 1], [0, 1, 2], [1, 0, 0]])\n        self.assertAllEqual(\n            features[\"unigrams\"].values,\n            [b\"tensorflow\", b\"learning\", b\"to\", b\"rank\", b\"gbdt\"])\n        # For Tensors with dense values, values can be directly checked.\n        self.assertAllEqual(features[\"query_length\"], [[3], [2]])\n        self.assertAllEqual(features[\"utility\"], [[[0.], [1.0]], [[0.], [-1.]]])\n\n  def test_parse_from_example_in_example_shuffle(self):\n    with tf.Graph().as_default():\n      serialized_example_in_example = [\n          _example_in_example(CONTEXT_1, EXAMPLES_1).SerializeToString(),\n          _example_in_example(CONTEXT_2, EXAMPLES_2).SerializeToString(),\n      ]\n      features = data_lib.parse_from_example_in_example(\n          serialized_example_in_example,\n          list_size=1,\n          context_feature_spec=CONTEXT_FEATURE_SPEC,\n          example_feature_spec=EXAMPLE_FEATURE_SPEC,\n          shuffle_examples=True,\n          seed=1)\n\n      with tf.compat.v1.Session() as sess:\n        sess.run(tf.compat.v1.local_variables_initializer())\n        features = sess.run(features)\n        # With `shuffle_examples` and seed=1, the example `tensorflow` and the\n        # example `learning to rank` in EXAMPLES_1 switch order. After\n        # truncation at list_size=1, only `learning to rank` in EXAMPLES_1\n        # and `gbdt` in EXAMPLES_2 are left in serialized features.\n        # Test dense_shape, indices and values for a SparseTensor.\n        self.assertAllEqual(features[\"unigrams\"].dense_shape, [2, 1, 3])\n        self.assertAllEqual(features[\"unigrams\"].indices,\n                            [[0, 0, 0], [0, 0, 1], [0, 0, 2], [1, 0, 0]])\n        self.assertAllEqual(features[\"unigrams\"].values,\n                            [b\"learning\", b\"to\", b\"rank\", b\"gbdt\"])\n        # For Tensors with dense values, values can be directly checked.\n        self.assertAllEqual(features[\"query_length\"], [[3], [2]])\n        self.assertAllEqual(features[\"utility\"], [[[1.]], [[0.]]])\n\n  def test_parse_example_in_example_with_sizes(self):\n    with tf.Graph().as_default():\n      serialized_example_in_example = [\n          _example_in_example(CONTEXT_1, EXAMPLES_1).SerializeToString(),\n          _example_in_example(CONTEXT_2, EXAMPLES_2).SerializeToString(),\n      ]\n      features = data_lib.parse_from_example_in_example(\n          serialized_example_in_example,\n          list_size=3,\n          context_feature_spec=CONTEXT_FEATURE_SPEC,\n          example_feature_spec=EXAMPLE_FEATURE_SPEC,\n          size_feature_name=_SIZE,\n          mask_feature_name=_MASK)\n\n      with tf.compat.v1.Session() as sess:\n        sess.run(tf.compat.v1.local_variables_initializer())\n        features = sess.run(features)\n        self.assertAllEqual(features[_SIZE], [2, 1])\n        self.assertAllEqual(features[_MASK],\n                            [[True, True, False], [True, False, False]])\n\n  def test_parse_example_in_example_sparse(self):\n    with tf.Graph().as_default():\n      serialized_example_in_example = [\n          _example_in_example(CONTEXT_1, EXAMPLES_1).SerializeToString(),\n          _example_in_example(CONTEXT_2, EXAMPLES_2).SerializeToString(),\n      ]\n      features = data_lib.parse_from_example_in_example(\n          serialized_example_in_example,\n          list_size=3,\n          context_feature_spec=CONTEXT_SPARSE_FEATURE_SPEC,\n          example_feature_spec=EXAMPLE_SPARSE_FEATURE_SPEC,\n          size_feature_name=_SIZE,\n          mask_feature_name=_MASK)\n\n      with tf.compat.v1.Session() as sess:\n        sess.run(tf.compat.v1.local_variables_initializer())\n        feature_map = sess.run(features)\n        self.assertAllEqual(feature_map[\"query_length\"].dense_shape, [2, 1])\n        self.assertAllEqual(feature_map[\"query_length\"].indices,\n                            [[0, 0], [1, 0]])\n        self.assertAllEqual(feature_map[\"query_length\"].values, [3, 2])\n        self.assertAllEqual(feature_map[\"unigrams\"].dense_shape, [2, 3, 3])\n        self.assertAllEqual(\n            feature_map[\"unigrams\"].indices,\n            [[0, 0, 0], [0, 1, 0], [0, 1, 1], [0, 1, 2], [1, 0, 0]])\n        self.assertAllEqual(\n            feature_map[\"unigrams\"].values,\n            [b\"tensorflow\", b\"learning\", b\"to\", b\"rank\", b\"gbdt\"])\n        self.assertAllEqual(feature_map[\"utility\"].dense_shape, [2, 3, 1])\n        self.assertAllEqual(feature_map[\"utility\"].indices,\n                            [[0, 0, 0], [0, 1, 0], [1, 0, 0]])\n        self.assertAllEqual(feature_map[\"utility\"].values, [0., 1., 0.])\n        self.assertAllEqual(feature_map[_MASK],\n                            [[True, True, False], [True, False, False]])\n        self.assertAllEqual(feature_map[_SIZE], [2, 1])\n\n\nclass ExampleInExampleWithRaggedTest(tf.test.TestCase):\n\n  def test_parse_from_example_in_example(self):\n    with tf.Graph().as_default():\n      serialized_example_in_example = [\n          _example_in_example(CONTEXT_1, EXAMPLES_1).SerializeToString(),\n          _example_in_example(CONTEXT_2, EXAMPLES_2).SerializeToString(),\n      ]\n      features = data_lib.parse_from_example_in_example(\n          serialized_example_in_example,\n          context_feature_spec=CONTEXT_RAGGED_FEATURE_SPEC,\n          example_feature_spec=EXAMPLE_RAGGED_FEATURE_SPEC)\n\n      with tf.compat.v1.Session() as sess:\n        sess.run(tf.compat.v1.local_variables_initializer())\n        features = sess.run(features)\n        self.assertAllEqual(\n            features[\"unigrams\"],\n            [[[b\"tensorflow\"], [b\"learning\", b\"to\", b\"rank\"]], [[b\"gbdt\"], []]])\n        self.assertAllEqual(features[\"query_length\"], [[3], [2]])\n        self.assertAllEqual(features[\"utility\"], [[[0.], [1.0]], [[0.], [-1.]]])\n\n  def test_parse_from_example_in_example_shuffle(self):\n    with tf.Graph().as_default():\n      serialized_example_in_example = [\n          _example_in_example(CONTEXT_1, EXAMPLES_1).SerializeToString(),\n          _example_in_example(CONTEXT_2, EXAMPLES_2).SerializeToString(),\n      ]\n      features = data_lib.parse_from_example_in_example(\n          serialized_example_in_example,\n          list_size=1,\n          context_feature_spec=CONTEXT_RAGGED_FEATURE_SPEC,\n          example_feature_spec=EXAMPLE_RAGGED_FEATURE_SPEC,\n          shuffle_examples=True,\n          seed=1)\n\n      with tf.compat.v1.Session() as sess:\n        sess.run(tf.compat.v1.local_variables_initializer())\n        features = sess.run(features)\n        # With `shuffle_examples` and seed=1, the example `tensorflow` and the\n        # example `learning to rank` in EXAMPLES_1 switch order. After\n        # truncation at list_size=1, only `learning to rank` in EXAMPLES_1\n        # and `gbdt` in EXAMPLES_2 are left in serialized features.\n        self.assertAllEqual(features[\"unigrams\"],\n                            [[[b\"learning\", b\"to\", b\"rank\"]], [[b\"gbdt\"]]])\n        self.assertAllEqual(features[\"query_length\"], [[3], [2]])\n        self.assertAllEqual(features[\"utility\"], [[[1.]], [[0.]]])\n\n\nclass SequenceExampleTest(tf.test.TestCase):\n\n  def test_parse_from_sequence_example(self):\n    with tf.Graph().as_default():\n      features = data_lib.parse_from_sequence_example(\n          tf.convert_to_tensor(value=[\n              SEQ_EXAMPLE_PROTO_1.SerializeToString(),\n              SEQ_EXAMPLE_PROTO_2.SerializeToString(),\n          ]),\n          context_feature_spec=CONTEXT_FEATURE_SPEC,\n          example_feature_spec=EXAMPLE_FEATURE_SPEC)\n\n      with tf.compat.v1.Session() as sess:\n        sess.run(tf.compat.v1.local_variables_initializer())\n        feature_map = sess.run(features)\n        self.assertEqual(\n            sorted(feature_map), [\"query_length\", \"unigrams\", \"utility\"])\n        self.assertAllEqual(feature_map[\"unigrams\"].dense_shape, [2, 2, 3])\n        self.assertAllEqual(\n            feature_map[\"unigrams\"].indices,\n            [[0, 0, 0], [0, 1, 0], [0, 1, 1], [0, 1, 2], [1, 0, 0]])\n        self.assertAllEqual(\n            feature_map[\"unigrams\"].values,\n            [b\"tensorflow\", b\"learning\", b\"to\", b\"rank\", b\"gbdt\"])\n        self.assertAllEqual(feature_map[\"query_length\"], [[3], [2]])\n        self.assertAllEqual(feature_map[\"utility\"],\n                            [[[0.], [1.]], [[0.], [-1.]]])\n        # Check static shapes for dense tensors.\n        self.assertAllEqual([2, 1], feature_map[\"query_length\"].shape)\n        self.assertAllEqual([2, 2, 1], feature_map[\"utility\"].shape)\n\n  def test_parse_from_sequence_example_with_sizes(self):\n    with tf.Graph().as_default():\n      features = data_lib.parse_from_sequence_example(\n          tf.convert_to_tensor(value=[\n              SEQ_EXAMPLE_PROTO_1.SerializeToString(),\n              SEQ_EXAMPLE_PROTO_2.SerializeToString(),\n          ]),\n          context_feature_spec=CONTEXT_FEATURE_SPEC,\n          example_feature_spec=EXAMPLE_FEATURE_SPEC,\n          size_feature_name=_SIZE,\n          mask_feature_name=_MASK)\n\n      with tf.compat.v1.Session() as sess:\n        sess.run(tf.compat.v1.local_variables_initializer())\n        features = sess.run(features)\n        self.assertAllEqual(features[_SIZE], [2, 1])\n        self.assertAllEqual(features[_MASK], [[True, True], [True, False]])\n\n  def test_parse_from_sequence_example_with_large_list_size(self):\n    with tf.Graph().as_default():\n      features = data_lib.parse_from_sequence_example(\n          tf.convert_to_tensor(value=[\n              SEQ_EXAMPLE_PROTO_1.SerializeToString(),\n          ]),\n          list_size=3,\n          context_feature_spec=CONTEXT_FEATURE_SPEC,\n          example_feature_spec=EXAMPLE_FEATURE_SPEC)\n\n      with tf.compat.v1.Session() as sess:\n        sess.run(tf.compat.v1.local_variables_initializer())\n        feature_map = sess.run(features)\n        self.assertEqual(\n            sorted(feature_map), [\"query_length\", \"unigrams\", \"utility\"])\n        self.assertAllEqual(feature_map[\"query_length\"], [[3]])\n        self.assertAllEqual(feature_map[\"unigrams\"].dense_shape, [1, 3, 3])\n        self.assertAllEqual(feature_map[\"unigrams\"].indices,\n                            [[0, 0, 0], [0, 1, 0], [0, 1, 1], [0, 1, 2]])\n        self.assertAllEqual(feature_map[\"unigrams\"].values,\n                            [b\"tensorflow\", b\"learning\", b\"to\", b\"rank\"])\n        self.assertAllEqual(feature_map[\"utility\"], [[[0.], [1.], [-1.]]])\n        # Check static shapes for dense tensors.\n        self.assertAllEqual([1, 1], feature_map[\"query_length\"].shape)\n        self.assertAllEqual([1, 3, 1], feature_map[\"utility\"].shape)\n\n  def test_parse_from_sequence_example_with_small_list_size(self):\n    with tf.Graph().as_default():\n      features = data_lib.parse_from_sequence_example(\n          tf.convert_to_tensor(value=[\n              SEQ_EXAMPLE_PROTO_1.SerializeToString(),\n          ]),\n          list_size=1,\n          context_feature_spec=CONTEXT_FEATURE_SPEC,\n          example_feature_spec=EXAMPLE_FEATURE_SPEC)\n\n      with tf.compat.v1.Session() as sess:\n        sess.run(tf.compat.v1.local_variables_initializer())\n        feature_map = sess.run(features)\n        self.assertEqual(\n            sorted(feature_map), [\"query_length\", \"unigrams\", \"utility\"])\n        self.assertAllEqual(feature_map[\"unigrams\"].dense_shape, [1, 1, 3])\n        self.assertAllEqual(feature_map[\"unigrams\"].indices, [[0, 0, 0]])\n        self.assertAllEqual(feature_map[\"unigrams\"].values, [b\"tensorflow\"])\n        self.assertAllEqual(feature_map[\"query_length\"], [[3]])\n        self.assertAllEqual(feature_map[\"utility\"], [[[0.]]])\n        # Check static shapes for dense tensors.\n        self.assertAllEqual([1, 1], feature_map[\"query_length\"].shape)\n        self.assertAllEqual([1, 1, 1], feature_map[\"utility\"].shape)\n\n  def test_parse_from_sequence_example_missing_frame_exception(self):\n    with tf.Graph().as_default():\n      missing_frame_proto = text_format.Parse(\n          \"\"\"\n          feature_lists {\n            feature_list {\n              key: \"utility\"\n              value {\n                feature { float_list { value: 0.0 } }\n                feature { }\n              }\n            }\n          }\n          \"\"\", tf.train.SequenceExample())\n      features = data_lib.parse_from_sequence_example(\n          tf.convert_to_tensor(value=[missing_frame_proto.SerializeToString()]),\n          list_size=2,\n          context_feature_spec=None,\n          example_feature_spec={\"utility\": EXAMPLE_FEATURE_SPEC[\"utility\"]})\n\n      with tf.compat.v1.Session() as sess:\n        sess.run(tf.compat.v1.local_variables_initializer())\n        with self.assertRaisesRegexp(\n            tf.errors.InvalidArgumentError,\n            # Error from ParseSingleExample:\n            r\"Unexpected number of elements in feature utility\"\n            # Error from ParseSequenceExampleV2:\n            r\"|Name: <unknown>, Key: utility, Index: 1.  \"\n            r\"Number of values != expected.  \"\n            r\"values size: 0 but output shape: \\[1\\]\"):\n          sess.run(features)\n\n  def test_parse_from_sequence_example_missing_feature_list(self):\n    with tf.Graph().as_default():\n      empty_proto = text_format.Parse(\n          \"\"\"\n          feature_lists {\n            feature_list {\n              key: \"utility2\"\n              value {\n                feature { float_list { value: 0.0 } }\n              }\n            }\n          }\n          \"\"\", tf.train.SequenceExample())\n      features = data_lib.parse_from_sequence_example(\n          tf.convert_to_tensor(value=[empty_proto.SerializeToString()]),\n          list_size=2,\n          context_feature_spec=None,\n          example_feature_spec={\"utility\": EXAMPLE_FEATURE_SPEC[\"utility\"]})\n\n      features_0 = data_lib.parse_from_sequence_example(\n          tf.convert_to_tensor(value=[empty_proto.SerializeToString()]),\n          context_feature_spec=None,\n          example_feature_spec={\n              \"utility\": EXAMPLE_FEATURE_SPEC[\"utility\"],\n              \"utility2\": EXAMPLE_FEATURE_SPEC[\"utility\"]\n          })\n\n      with tf.compat.v1.Session() as sess:\n        sess.run(tf.compat.v1.local_variables_initializer())\n        feature_map, feature_0_map = sess.run([features, features_0])\n        self.assertAllEqual([1, 2, 1], feature_map[\"utility\"].shape)\n        self.assertAllEqual([1, 1, 1], feature_0_map[\"utility\"].shape)\n\n  def test_parse_from_sequence_example_sparse(self):\n    with tf.Graph().as_default():\n      features = data_lib.parse_from_sequence_example(\n          tf.convert_to_tensor(value=[\n              SEQ_EXAMPLE_PROTO_1.SerializeToString(),\n              SEQ_EXAMPLE_PROTO_2.SerializeToString(),\n          ]),\n          list_size=3,\n          context_feature_spec=CONTEXT_SPARSE_FEATURE_SPEC,\n          example_feature_spec=EXAMPLE_SPARSE_FEATURE_SPEC,\n          size_feature_name=_SIZE,\n          mask_feature_name=_MASK)\n\n      with tf.compat.v1.Session() as sess:\n        sess.run(tf.compat.v1.local_variables_initializer())\n        feature_map = sess.run(features)\n        self.assertAllEqual(feature_map[\"query_length\"].dense_shape, [2, 1])\n        self.assertAllEqual(feature_map[\"query_length\"].indices,\n                            [[0, 0], [1, 0]])\n        self.assertAllEqual(feature_map[\"query_length\"].values, [3, 2])\n        self.assertAllEqual(feature_map[\"unigrams\"].dense_shape, [2, 3, 3])\n        self.assertAllEqual(\n            feature_map[\"unigrams\"].indices,\n            [[0, 0, 0], [0, 1, 0], [0, 1, 1], [0, 1, 2], [1, 0, 0]])\n        self.assertAllEqual(\n            feature_map[\"unigrams\"].values,\n            [b\"tensorflow\", b\"learning\", b\"to\", b\"rank\", b\"gbdt\"])\n        self.assertAllEqual(feature_map[\"utility\"].dense_shape, [2, 3, 1])\n        self.assertAllEqual(feature_map[\"utility\"].indices,\n                            [[0, 0, 0], [0, 1, 0], [1, 0, 0]])\n        self.assertAllEqual(feature_map[\"utility\"].values, [0., 1., 0.])\n        self.assertAllEqual(feature_map[_MASK],\n                            [[True, True, False], [True, False, False]])\n        self.assertAllEqual(feature_map[_SIZE], [2, 1])\n\n\nclass SequenceExampleWithRaggedTest(tf.test.TestCase):\n\n  def test_parse_from_sequence_example(self):\n    with tf.Graph().as_default():\n      serialized_example_in_example = [\n          SEQ_EXAMPLE_PROTO_1.SerializeToString(),\n          SEQ_EXAMPLE_PROTO_2.SerializeToString()\n      ]\n      features = data_lib.parse_from_sequence_example(\n          serialized_example_in_example,\n          context_feature_spec=CONTEXT_RAGGED_FEATURE_SPEC,\n          example_feature_spec=EXAMPLE_RAGGED_FEATURE_SPEC)\n\n      with tf.compat.v1.Session() as sess:\n        sess.run(tf.compat.v1.local_variables_initializer())\n        features = sess.run(features)\n        self.assertAllEqual(\n            features[\"unigrams\"],\n            [[[b\"tensorflow\"], [b\"learning\", b\"to\", b\"rank\"]], [[b\"gbdt\"], []]])\n        self.assertAllEqual(features[\"query_length\"], [[3], [2]])\n        self.assertAllEqual(features[\"utility\"], [[[0.], [1.0]], [[0.], [-1.]]])\n\n  def test_parse_from_sequence_example_padding(self):\n    with tf.Graph().as_default():\n      serialized_example_lists = [\n          SEQ_EXAMPLE_PROTO_1.SerializeToString(),\n          SEQ_EXAMPLE_PROTO_2.SerializeToString()\n      ]\n      # Padding since list_size 3 is larger than 2.\n      features = data_lib.parse_from_sequence_example(\n          serialized_example_lists,\n          list_size=3,\n          context_feature_spec=CONTEXT_RAGGED_FEATURE_SPEC,\n          example_feature_spec=EXAMPLE_RAGGED_FEATURE_SPEC)\n\n      with tf.compat.v1.Session() as sess:\n        sess.run(tf.compat.v1.local_variables_initializer())\n        features = sess.run(features)\n        self.assertAllEqual(features[\"unigrams\"],\n                            [[[b\"tensorflow\"], [b\"learning\", b\"to\", b\"rank\"],\n                              []], [[b\"gbdt\"], [], []]])\n        self.assertAllEqual(features[\"query_length\"], [[3], [2]])\n        self.assertAllEqual(features[\"utility\"],\n                            [[[0.], [1.0], [-1.]], [[0.], [-1.], [-1.]]])\n\n  def test_parse_from_sequence_example_truncate(self):\n    with tf.Graph().as_default():\n      serialized_example_lists = [\n          SEQ_EXAMPLE_PROTO_1.SerializeToString(),\n          SEQ_EXAMPLE_PROTO_2.SerializeToString()\n      ]\n      # Truncate number of examples from 2 to 1.\n      features = data_lib.parse_from_sequence_example(\n          serialized_example_lists,\n          list_size=1,\n          context_feature_spec=CONTEXT_RAGGED_FEATURE_SPEC,\n          example_feature_spec=EXAMPLE_RAGGED_FEATURE_SPEC)\n\n      with tf.compat.v1.Session() as sess:\n        sess.run(tf.compat.v1.local_variables_initializer())\n        features = sess.run(features)\n        self.assertAllEqual(features[\"unigrams\"],\n                            [[[b\"tensorflow\"]], [[b\"gbdt\"]]])\n        self.assertAllEqual(features[\"query_length\"], [[3], [2]])\n        self.assertAllEqual(features[\"utility\"], [[[0.]], [[0.]]])\n\n\nclass RankingDatasetTest(tf.test.TestCase, parameterized.TestCase):\n\n  def test_make_parsing_fn_eie(self):\n    with tf.Graph().as_default():\n      parsing_fn = data_lib.make_parsing_fn(\n          data_lib.EIE,\n          context_feature_spec=CONTEXT_FEATURE_SPEC,\n          example_feature_spec=EXAMPLE_FEATURE_SPEC)\n      serialized_example_in_example = [\n          _example_in_example(CONTEXT_1, EXAMPLES_1).SerializeToString(),\n          _example_in_example(CONTEXT_2, EXAMPLES_2).SerializeToString(),\n      ]\n      features = parsing_fn(serialized_example_in_example)\n\n      with tf.compat.v1.Session() as sess:\n        sess.run(tf.compat.v1.local_variables_initializer())\n        features = sess.run(features)\n        # Test dense_shape, indices and values for a SparseTensor.\n        self.assertAllEqual(features[\"unigrams\"].dense_shape, [2, 2, 3])\n        self.assertAllEqual(\n            features[\"unigrams\"].indices,\n            [[0, 0, 0], [0, 1, 0], [0, 1, 1], [0, 1, 2], [1, 0, 0]])\n        self.assertAllEqual(\n            features[\"unigrams\"].values,\n            [b\"tensorflow\", b\"learning\", b\"to\", b\"rank\", b\"gbdt\"])\n        # For Tensors with dense values, values can be directly checked.\n        self.assertAllEqual(features[\"query_length\"], [[3], [2]])\n        self.assertAllEqual(features[\"utility\"], [[[0.], [1.0]], [[0.], [-1.]]])\n\n  def test_make_parsing_fn_seq(self):\n    with tf.Graph().as_default():\n      parsing_fn = data_lib.make_parsing_fn(\n          data_lib.SEQ,\n          context_feature_spec=CONTEXT_FEATURE_SPEC,\n          example_feature_spec=EXAMPLE_FEATURE_SPEC)\n      sequence_examples = [\n          SEQ_EXAMPLE_PROTO_1.SerializeToString(),\n          SEQ_EXAMPLE_PROTO_2.SerializeToString(),\n      ]\n      features = parsing_fn(sequence_examples)\n\n      with tf.compat.v1.Session() as sess:\n        sess.run(tf.compat.v1.local_variables_initializer())\n        feature_map = sess.run(features)\n        self.assertCountEqual(feature_map,\n                              [\"query_length\", \"unigrams\", \"utility\"])\n        self.assertAllEqual(feature_map[\"unigrams\"].dense_shape, [2, 2, 3])\n        self.assertAllEqual(\n            feature_map[\"unigrams\"].indices,\n            [[0, 0, 0], [0, 1, 0], [0, 1, 1], [0, 1, 2], [1, 0, 0]])\n        self.assertAllEqual(\n            feature_map[\"unigrams\"].values,\n            [b\"tensorflow\", b\"learning\", b\"to\", b\"rank\", b\"gbdt\"])\n        self.assertAllEqual(feature_map[\"query_length\"], [[3], [2]])\n        self.assertAllEqual(feature_map[\"utility\"],\n                            [[[0.], [1.]], [[0.], [-1.]]])\n\n  def test_make_parsing_fn_exception(self):\n    with tf.Graph().as_default():\n      with self.assertRaises(ValueError):\n        data_lib.make_parsing_fn(\n            \"non_existing_format\",\n            context_feature_spec=CONTEXT_FEATURE_SPEC,\n            example_feature_spec=EXAMPLE_FEATURE_SPEC)\n\n  @parameterized.named_parameters((\"with_sloppy_ordering\", True),\n                                  (\"with_deterministic_ordering\", False))\n  def test_build_ranking_dataset(self, sloppy_ordering):\n    with tf.Graph().as_default():\n      # Save EIE protos in a sstable file in a temp folder.\n      serialized_example_in_examples = [\n          _example_in_example(CONTEXT_1, EXAMPLES_1).SerializeToString(),\n          _example_in_example(CONTEXT_2, EXAMPLES_2).SerializeToString(),\n      ] * 5\n      data_dir = tf.compat.v1.test.get_temp_dir()\n      data_file = os.path.join(data_dir, \"test_ranking_data.tfrecord\")\n      if tf.io.gfile.exists(data_file):\n        tf.io.gfile.remove(data_file)\n\n      with tf.io.TFRecordWriter(data_file) as writer:\n        for serialized_eie in serialized_example_in_examples:\n          writer.write(serialized_eie)\n\n      batched_dataset = data_lib.build_ranking_dataset(\n          file_pattern=data_file,\n          data_format=data_lib.EIE,\n          batch_size=2,\n          list_size=2,\n          context_feature_spec=CONTEXT_FEATURE_SPEC,\n          example_feature_spec=EXAMPLE_FEATURE_SPEC,\n          reader=tf.data.TFRecordDataset,\n          shuffle=False,\n          sloppy_ordering=sloppy_ordering)\n      features = tf.compat.v1.data.make_one_shot_iterator(\n          batched_dataset).get_next()\n      self.assertAllEqual([2, 1],\n                          features[\"query_length\"].get_shape().as_list())\n      self.assertAllEqual([2, 2, 1], features[\"utility\"].get_shape().as_list())\n\n      self.assertAllEqual(\n          sorted(features.keys()), [\"query_length\", \"unigrams\", \"utility\"])\n\n      with tf.compat.v1.Session() as sess:\n        sess.run(tf.compat.v1.local_variables_initializer())\n        features = sess.run(features)\n        self.assertAllEqual(features[\"unigrams\"].dense_shape, [2, 2, 3])\n        self.assertAllEqual(\n            features[\"unigrams\"].indices,\n            [[0, 0, 0], [0, 1, 0], [0, 1, 1], [0, 1, 2], [1, 0, 0]])\n        self.assertAllEqual(\n            features[\"unigrams\"].values,\n            [b\"tensorflow\", b\"learning\", b\"to\", b\"rank\", b\"gbdt\"])\n        # For Tensors with dense values, values can be directly checked.\n        self.assertAllEqual(features[\"query_length\"], [[3], [2]])\n        self.assertAllEqual(features[\"utility\"], [[[0.], [1.0]], [[0.], [-1.]]])\n\n  @parameterized.named_parameters(\n      (\"with_reader_num_threads_autotune\", tf.data.experimental.AUTOTUNE),\n      (\"with_fixed_reader_num_threads\", 5))\n  def test_build_ranking_dataset_reader_num_threads(self, reader_num_threads):\n    with tf.Graph().as_default():\n      # Save EIE protos in a sstable file in a temp folder.\n      serialized_example_in_examples = [\n          _example_in_example(CONTEXT_1, EXAMPLES_1).SerializeToString(),\n          _example_in_example(CONTEXT_2, EXAMPLES_2).SerializeToString(),\n      ] * 5\n      data_dir = tf.compat.v1.test.get_temp_dir()\n      data_file = os.path.join(data_dir, \"test_ranking_data.tfrecord\")\n      if tf.io.gfile.exists(data_file):\n        tf.io.gfile.remove(data_file)\n\n      with tf.io.TFRecordWriter(data_file) as writer:\n        for serialized_eie in serialized_example_in_examples:\n          writer.write(serialized_eie)\n\n      batched_dataset = data_lib.build_ranking_dataset(\n          file_pattern=data_file,\n          data_format=data_lib.EIE,\n          batch_size=2,\n          list_size=2,\n          context_feature_spec=CONTEXT_FEATURE_SPEC,\n          example_feature_spec=EXAMPLE_FEATURE_SPEC,\n          reader=tf.data.TFRecordDataset,\n          shuffle=False,\n          reader_num_threads=reader_num_threads)\n      features = tf.compat.v1.data.make_one_shot_iterator(\n          batched_dataset).get_next()\n      self.assertAllEqual([2, 1],\n                          features[\"query_length\"].get_shape().as_list())\n      self.assertAllEqual([2, 2, 1], features[\"utility\"].get_shape().as_list())\n\n      self.assertAllEqual(\n          sorted(features.keys()), [\"query_length\", \"unigrams\", \"utility\"])\n\n      with tf.compat.v1.Session() as sess:\n        sess.run(tf.compat.v1.local_variables_initializer())\n        features = sess.run(features)\n        self.assertAllEqual(features[\"unigrams\"].dense_shape, [2, 2, 3])\n        self.assertAllEqual(\n            features[\"unigrams\"].indices,\n            [[0, 0, 0], [0, 1, 0], [0, 1, 1], [0, 1, 2], [1, 0, 0]])\n        self.assertAllEqual(\n            features[\"unigrams\"].values,\n            [b\"tensorflow\", b\"learning\", b\"to\", b\"rank\", b\"gbdt\"])\n        # For Tensors with dense values, values can be directly checked.\n        self.assertAllEqual(features[\"query_length\"], [[3], [2]])\n        self.assertAllEqual(features[\"utility\"], [[[0.], [1.0]], [[0.], [-1.]]])\n\n  def test_build_ranking_serving_input_receiver_fn(self):\n    with tf.Graph().as_default():\n      serving_input_receiver_fn = (\n          data_lib.build_ranking_serving_input_receiver_fn(\n              data_format=data_lib.EIE,\n              context_feature_spec=CONTEXT_FEATURE_SPEC,\n              example_feature_spec=EXAMPLE_FEATURE_SPEC))\n      serving_input_receiver = serving_input_receiver_fn()\n      self.assertCountEqual(serving_input_receiver.features.keys(),\n                            [\"query_length\", \"unigrams\", \"utility\"])\n      self.assertCountEqual(serving_input_receiver.receiver_tensors.keys(),\n                            [\"input_ranking_data\"])\n      eie_input = [\n          _example_in_example(CONTEXT_1, EXAMPLES_1).SerializeToString()\n      ]\n      with tf.compat.v1.Session() as sess:\n        sess.run(tf.compat.v1.local_variables_initializer())\n        features = sess.run(\n            serving_input_receiver.features,\n            feed_dict={\n                serving_input_receiver.receiver_tensors[\"input_ranking_data\"]\n                .name:\n                    eie_input\n            })\n        # Test dense_shape, indices and values for a SparseTensor.\n        self.assertAllEqual(features[\"unigrams\"].dense_shape, [1, 2, 3])\n        self.assertAllEqual(features[\"unigrams\"].indices,\n                            [[0, 0, 0], [0, 1, 0], [0, 1, 1], [0, 1, 2]])\n        self.assertAllEqual(features[\"unigrams\"].values,\n                            [b\"tensorflow\", b\"learning\", b\"to\", b\"rank\"])\n        # For Tensors with dense values, values can be directly checked.\n        self.assertAllEqual(features[\"query_length\"], [[3]])\n        self.assertAllEqual(features[\"utility\"], [[[0.], [1.]]])\n\n  def test_sequence_example_serving_input_receiver_fn(self):\n    with tf.Graph().as_default():\n      serving_input_receiver_fn = (\n          data_lib.build_sequence_example_serving_input_receiver_fn(\n              input_size=2,\n              context_feature_spec=CONTEXT_FEATURE_SPEC,\n              example_feature_spec=EXAMPLE_FEATURE_SPEC))\n      serving_input_receiver = serving_input_receiver_fn()\n      self.assertCountEqual(serving_input_receiver.features,\n                            [\"query_length\", \"unigrams\", \"utility\"])\n      self.assertCountEqual(serving_input_receiver.receiver_tensors.keys(),\n                            [\"sequence_example\"])\n      with tf.compat.v1.Session() as sess:\n        sess.run(tf.compat.v1.local_variables_initializer())\n        feature_map = sess.run(\n            serving_input_receiver.features,\n            feed_dict={\n                serving_input_receiver.receiver_tensors[\"sequence_example\"]\n                .name: [\n                    SEQ_EXAMPLE_PROTO_1.SerializeToString(),\n                    SEQ_EXAMPLE_PROTO_2.SerializeToString()\n                ]\n            })\n        # Test dense_shape, indices and values for a SparseTensor.\n        self.assertAllEqual(feature_map[\"unigrams\"].dense_shape, [2, 2, 3])\n        self.assertAllEqual(\n            feature_map[\"unigrams\"].indices,\n            [[0, 0, 0], [0, 1, 0], [0, 1, 1], [0, 1, 2], [1, 0, 0]])\n        self.assertAllEqual(\n            feature_map[\"unigrams\"].values,\n            [b\"tensorflow\", b\"learning\", b\"to\", b\"rank\", b\"gbdt\"])\n        # Check values directly for dense tensors.\n        self.assertAllEqual(feature_map[\"query_length\"], [[3], [2]])\n        self.assertAllEqual(feature_map[\"utility\"],\n                            [[[0.], [1.0]], [[0.], [-1.]]])\n\n\nclass SequenceExampleDatasetTest(tf.test.TestCase, parameterized.TestCase):\n\n  @parameterized.named_parameters((\"with_sloppy_ordering\", True),\n                                  (\"with_deterministic_ordering\", False))\n  def test_read_batched_sequence_example_dataset(self, sloppy_ordering):\n    with tf.Graph().as_default():\n      # Save protos in a sstable file in a temp folder.\n      serialized_sequence_examples = [\n          SEQ_EXAMPLE_PROTO_1.SerializeToString(),\n          SEQ_EXAMPLE_PROTO_2.SerializeToString()\n      ] * 100\n      data_dir = tf.compat.v1.test.get_temp_dir()\n      data_file = os.path.join(data_dir, \"test_sequence_example.tfrecord\")\n      if tf.io.gfile.exists(data_file):\n        tf.io.gfile.remove(data_file)\n\n      with tf.io.TFRecordWriter(data_file) as writer:\n        for s in serialized_sequence_examples:\n          writer.write(s)\n\n      batched_dataset = data_lib.read_batched_sequence_example_dataset(\n          file_pattern=data_file,\n          batch_size=2,\n          list_size=2,\n          context_feature_spec=CONTEXT_FEATURE_SPEC,\n          example_feature_spec=EXAMPLE_FEATURE_SPEC,\n          reader=tf.data.TFRecordDataset,\n          shuffle=False,\n          sloppy_ordering=sloppy_ordering)\n\n      features = tf.compat.v1.data.make_one_shot_iterator(\n          batched_dataset).get_next()\n      self.assertAllEqual(\n          sorted(features), [\"query_length\", \"unigrams\", \"utility\"])\n      # Check static shapes for dense tensors.\n      self.assertAllEqual([2, 1],\n                          features[\"query_length\"].get_shape().as_list())\n      self.assertAllEqual([2, 2, 1], features[\"utility\"].get_shape().as_list())\n\n      with tf.compat.v1.Session() as sess:\n        sess.run(tf.compat.v1.local_variables_initializer())\n        feature_map = sess.run(features)\n        # Test dense_shape, indices and values for a SparseTensor.\n        self.assertAllEqual(feature_map[\"unigrams\"].dense_shape, [2, 2, 3])\n        self.assertAllEqual(\n            feature_map[\"unigrams\"].indices,\n            [[0, 0, 0], [0, 1, 0], [0, 1, 1], [0, 1, 2], [1, 0, 0]])\n        self.assertAllEqual(\n            feature_map[\"unigrams\"].values,\n            [b\"tensorflow\", b\"learning\", b\"to\", b\"rank\", b\"gbdt\"])\n        # Check values directly for dense tensors.\n        self.assertAllEqual(feature_map[\"query_length\"], [[3], [2]])\n        self.assertAllEqual(feature_map[\"utility\"],\n                            [[[0.], [1.0]], [[0.], [-1.]]])\n\n  def test_sequence_example_serving_input_receiver_fn(self):\n    with tf.Graph().as_default():\n      serving_input_receiver_fn = (\n          data_lib.build_sequence_example_serving_input_receiver_fn(\n              input_size=2,\n              context_feature_spec=CONTEXT_FEATURE_SPEC,\n              example_feature_spec=EXAMPLE_FEATURE_SPEC))\n      serving_input_receiver = serving_input_receiver_fn()\n      self.assertCountEqual(serving_input_receiver.features,\n                            [\"query_length\", \"unigrams\", \"utility\"])\n      self.assertCountEqual(serving_input_receiver.receiver_tensors.keys(),\n                            [\"sequence_example\"])\n      with tf.compat.v1.Session() as sess:\n        sess.run(tf.compat.v1.local_variables_initializer())\n        feature_map = sess.run(\n            serving_input_receiver.features,\n            feed_dict={\n                serving_input_receiver.receiver_tensors[\"sequence_example\"]\n                .name: [\n                    SEQ_EXAMPLE_PROTO_1.SerializeToString(),\n                    SEQ_EXAMPLE_PROTO_2.SerializeToString()\n                ]\n            })\n        # Test dense_shape, indices and values for a SparseTensor.\n        self.assertAllEqual(feature_map[\"unigrams\"].dense_shape, [2, 2, 3])\n        self.assertAllEqual(\n            feature_map[\"unigrams\"].indices,\n            [[0, 0, 0], [0, 1, 0], [0, 1, 1], [0, 1, 2], [1, 0, 0]])\n        self.assertAllEqual(\n            feature_map[\"unigrams\"].values,\n            [b\"tensorflow\", b\"learning\", b\"to\", b\"rank\", b\"gbdt\"])\n        # Check values directly for dense tensors.\n        self.assertAllEqual(feature_map[\"query_length\"], [[3], [2]])\n        self.assertAllEqual(feature_map[\"utility\"],\n                            [[[0.], [1.0]], [[0.], [-1.]]])\n\n  def test_sequence_example_serving_input_receiver_fn_sparse(self):\n    with tf.Graph().as_default():\n      serving_input_receiver_fn = (\n          data_lib.build_sequence_example_serving_input_receiver_fn(\n              input_size=2,\n              context_feature_spec=CONTEXT_SPARSE_FEATURE_SPEC,\n              example_feature_spec=EXAMPLE_SPARSE_FEATURE_SPEC))\n      serving_input_receiver = serving_input_receiver_fn()\n      self.assertCountEqual(serving_input_receiver.features,\n                            [\"query_length\", \"unigrams\", \"utility\"])\n      self.assertCountEqual(serving_input_receiver.receiver_tensors.keys(),\n                            [\"sequence_example\"])\n      with tf.compat.v1.Session() as sess:\n        sess.run(tf.compat.v1.local_variables_initializer())\n        feature_map = sess.run(\n            serving_input_receiver.features,\n            feed_dict={\n                serving_input_receiver.receiver_tensors[\"sequence_example\"]\n                .name: [\n                    SEQ_EXAMPLE_PROTO_1.SerializeToString(),\n                    SEQ_EXAMPLE_PROTO_2.SerializeToString()\n                ]\n            })\n        self.assertAllEqual(feature_map[\"unigrams\"].dense_shape, [2, 2, 3])\n        self.assertAllEqual(\n            feature_map[\"unigrams\"].indices,\n            [[0, 0, 0], [0, 1, 0], [0, 1, 1], [0, 1, 2], [1, 0, 0]])\n        self.assertAllEqual(\n            feature_map[\"unigrams\"].values,\n            [b\"tensorflow\", b\"learning\", b\"to\", b\"rank\", b\"gbdt\"])\n        self.assertAllEqual(feature_map[\"query_length\"].dense_shape, [2, 1])\n        self.assertAllEqual(feature_map[\"query_length\"].indices,\n                            [[0, 0], [1, 0]])\n        self.assertAllEqual(feature_map[\"query_length\"].values, [3, 2])\n        self.assertAllEqual(feature_map[\"utility\"].dense_shape, [2, 2, 1])\n        self.assertAllEqual(feature_map[\"utility\"].indices,\n                            [[0, 0, 0], [0, 1, 0], [1, 0, 0]])\n        self.assertAllEqual(feature_map[\"utility\"].values, [0., 1., 0.])\n\n\nclass TFExampleDatasetTest(tf.test.TestCase):\n\n  def test_parse_from_tf_example(self):\n    with tf.Graph().as_default():\n      serialized_examples = [\n          TF_EXAMPLE_PROTO_1.SerializeToString(),\n          TF_EXAMPLE_PROTO_2.SerializeToString()\n      ]\n      features = data_lib.parse_from_tf_example(\n          serialized=serialized_examples,\n          context_feature_spec=CONTEXT_FEATURE_SPEC,\n          example_feature_spec=EXAMPLE_FEATURE_SPEC,\n          size_feature_name=_SIZE,\n          mask_feature_name=_MASK)\n\n      with tf.compat.v1.Session() as sess:\n        features = sess.run(features)\n        self.assertAllEqual(features[_SIZE], [1, 1])\n        self.assertAllEqual(features[_MASK], [[True], [True]])\n        # Test dense_shape, indices and values for a SparseTensor.\n        self.assertAllEqual(features[\"unigrams\"].dense_shape, [2, 1, 3])\n        self.assertAllEqual(features[\"unigrams\"].indices,\n                            [[0, 0, 0], [1, 0, 0], [1, 0, 1], [1, 0, 2]])\n        self.assertAllEqual(features[\"unigrams\"].values,\n                            [b\"tensorflow\", b\"learning\", b\"to\", b\"rank\"])\n        # For Tensors with dense values, values can be directly checked.\n        self.assertAllEqual(features[\"query_length\"], [[1], [3]])\n        self.assertAllEqual(features[\"utility\"], [[[0.]], [[1.]]])\n\n  def test_parse_from_tf_example_ragged(self):\n    with tf.Graph().as_default():\n      serialized_examples = [\n          TF_EXAMPLE_PROTO_1.SerializeToString(),\n          TF_EXAMPLE_PROTO_2.SerializeToString()\n      ]\n      features = data_lib.parse_from_tf_example(\n          serialized=serialized_examples,\n          context_feature_spec=CONTEXT_FEATURE_SPEC,\n          example_feature_spec=EXAMPLE_RAGGED_FEATURE_SPEC,\n          size_feature_name=_SIZE,\n          mask_feature_name=_MASK)\n\n      with tf.compat.v1.Session() as sess:\n        features = sess.run(features)\n        self.assertAllEqual(features[_SIZE], [1, 1])\n        self.assertAllEqual(features[_MASK], [[True], [True]])\n        self.assertAllEqual(\n            features[\"unigrams\"],\n            [[[b\"tensorflow\"]], [[b\"learning\", b\"to\", b\"rank\"]]])\n        self.assertAllEqual(features[\"query_length\"], [[1], [3]])\n        self.assertAllEqual(features[\"utility\"], [[[0.]], [[1.]]])\n\n  def test_build_tf_example_serving_input_receiver_fn(self):\n    with tf.Graph().as_default():\n      serving_input_receiver_fn = (\n          data_lib.build_tf_example_serving_input_receiver_fn(\n              context_feature_spec=CONTEXT_FEATURE_SPEC,\n              example_feature_spec=EXAMPLE_FEATURE_SPEC,\n              size_feature_name=_SIZE,\n              mask_feature_name=_MASK))\n      serving_input_receiver = serving_input_receiver_fn()\n      context_features = {\n          name: tensor\n          for name, tensor in serving_input_receiver.features.items()\n          if name in CONTEXT_FEATURE_SPEC\n      }\n      example_features = {\n          name: tensor\n          for name, tensor in serving_input_receiver.features.items()\n          if name in EXAMPLE_FEATURE_SPEC\n      }\n      self.assertAllEqual(sorted(context_features.keys()), [\"query_length\"])\n      self.assertAllEqual(\n          sorted(example_features.keys()), [\"unigrams\", \"utility\"])\n      self.assertCountEqual([\"input_ranking_data\"],\n                            serving_input_receiver.receiver_tensors.keys())\n      with tf.compat.v1.Session() as sess:\n        context_map, example_map = sess.run(\n            [context_features, example_features],\n            feed_dict={\n                serving_input_receiver.receiver_tensors[\"input_ranking_data\"]\n                .name: [\n                    TF_EXAMPLE_PROTO_1.SerializeToString(),\n                    TF_EXAMPLE_PROTO_2.SerializeToString()\n                ]\n            })\n        # Test dense_shape, indices and values for a SparseTensor.\n        self.assertAllEqual(example_map[\"unigrams\"].dense_shape, [2, 1, 3])\n        self.assertAllEqual(example_map[\"unigrams\"].indices,\n                            [[0, 0, 0], [1, 0, 0], [1, 0, 1], [1, 0, 2]])\n        self.assertAllEqual(example_map[\"unigrams\"].values,\n                            [b\"tensorflow\", b\"learning\", b\"to\", b\"rank\"])\n        # For Tensors with dense values, values can be directly checked.\n        self.assertAllEqual(context_map[\"query_length\"], [[1], [3]])\n        self.assertAllEqual(example_map[\"utility\"], [[[0.]], [[1.]]])\n\n  def test_build_tf_example_serving_input_receiver_fn_ragged(self):\n    with tf.Graph().as_default():\n      serving_input_receiver_fn = (\n          data_lib.build_tf_example_serving_input_receiver_fn(\n              context_feature_spec=CONTEXT_FEATURE_SPEC,\n              example_feature_spec=EXAMPLE_RAGGED_FEATURE_SPEC,\n              size_feature_name=_SIZE,\n              mask_feature_name=_MASK))\n      serving_input_receiver = serving_input_receiver_fn()\n      context_features = {\n          name: tensor\n          for name, tensor in serving_input_receiver.features.items()\n          if name in CONTEXT_FEATURE_SPEC\n      }\n      example_features = {\n          name: tensor\n          for name, tensor in serving_input_receiver.features.items()\n          if name in EXAMPLE_FEATURE_SPEC\n      }\n      self.assertAllEqual(sorted(context_features.keys()), [\"query_length\"])\n      self.assertAllEqual(\n          sorted(example_features.keys()), [\"unigrams\", \"utility\"])\n      self.assertCountEqual([\"input_ranking_data\"],\n                            serving_input_receiver.receiver_tensors.keys())\n      with tf.compat.v1.Session() as sess:\n        context_map, example_map = sess.run(\n            [context_features, example_features],\n            feed_dict={\n                serving_input_receiver.receiver_tensors[\"input_ranking_data\"]\n                .name: [\n                    TF_EXAMPLE_PROTO_1.SerializeToString(),\n                    TF_EXAMPLE_PROTO_2.SerializeToString()\n                ]\n            })\n        self.assertAllEqual(\n            example_map[\"unigrams\"],\n            [[[b\"tensorflow\"]], [[b\"learning\", b\"to\", b\"rank\"]]])\n        self.assertAllEqual(context_map[\"query_length\"], [[1], [3]])\n        self.assertAllEqual(example_map[\"utility\"], [[[0.]], [[1.]]])\n\n\nif __name__ == \"__main__\":\n  tf.test.main()\n", "any monkeypatching\n    \"\"\"\n    global _realthread\n    if _realthread:\n        return _realthread\n\n    import imp\n    fp, pathname, description = imp.find_module('thread')\n    try:\n        return imp.load_module('realthread', fp, pathname, description)\n    finally:\n        if fp:\n            fp.close()\n", "exMap(OMBaseObject):\n    tid = 'data_index_map'\n\n    _ATTRIBUTES = OrderedDict()\n    _ATTRIBUTES['_data'] = {\n        'default_value': None,\n        'type': list\n    }\n\n    # TODO: Tornar esta classe _singleton_per_parent\n    # pois somente deve ser possivel existir 1 DataIndexMap por DataObject.\n\n    # TODO: Is this a kind of dummy class?\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        \"\"\"\n        if kwargs.get('_data') is None:\n            if args:\n                kwargs['_data'] = args[0]\n        super().__init__(**kwargs)\n\n    def _get_tree_object_node_properties(self):\n        return None\n\n    def _get_data_indexes(self):\n        return copy.deepcopy(self._data)\n", "License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"This example gets the changes in the account during the last 24 hours.\n\nTags: CustomerSyncService.get\n\"\"\"\n\n__author__ = 'api.kwinter@gmail.com (Kevin Winter)'\n\nimport datetime\nimport os\nimport sys\nsys.path.insert(0, os.path.join('..', '..', '..', '..', '..'))\n\n# Import appropriate classes from the client library.\nfrom adspygoogle import AdWordsClient\n\n\ndef main(client):\n  # Initialize appropriate service.\n  customer_sync_service = client.GetCustomerSyncService(version='v201209')\n  campaign_service = client.GetCampaignService(version='v201209')\n\n  # Construct selector and get all campaigns.\n  selector = {\n      'fields': ['Id', 'Name', 'Status']\n  }\n  campaigns = campaign_service.Get(selector)[0]\n  campaign_ids = []\n  if 'entries' in campaigns:\n    for campaign in campaigns['entries']:\n      campaign_ids.append(campaign['id'])\n  else:\n    print 'No campaigns were found.'\n    sys.exit(0)\n\n  # Construct selector and get all changes.\n  today = datetime.datetime.today()\n  yesterday = today - datetime.timedelta(1)\n  selector = {\n      'dateTimeRange': {\n          'min': yesterday.strftime('%Y%m%d %H%M%S'),\n          'max': today.strftime('%Y%m%d %H%M%S')\n      },\n      'campaignIds': campaign_ids\n  }\n  account_changes = customer_sync_service.Get(selector)[0]\n\n  # Display results.\n  if account_changes:\n    if 'lastChangeTimestamp' in account_changes:\n      print 'Most recent changes: %s' % account_changes['lastChangeTimestamp']\n    if account_changes['changedCampaigns']:\n      for data in account_changes['changedCampaigns']:\n        print ('Campaign with id \\'%s\\' has change status \\'%s\\'.'\n               % (data['campaignId'], data['campaignChangeStatus']))\n        if (data['campaignChangeStatus'] != 'NEW' and\n            data['campaignChangeStatus'] != 'FIELDS_UNCHANGED'):\n          print '  Added ad extensions: %s' % data.get('addedAdExtensions')\n          print '  Deleted ad extensions: %s' % data.get('deletedAdExtensions')\n          print ('  Added campaign criteria: %s'\n                 % data.get('addedCampaignCriteria'))\n          print ('  Deleted campaign criteria: %s'\n                 % data.get('deletedCampaignCriteria'))\n          print ('  Campaign targeting changed: %s'\n                 % data.get('campaignTargetingChanged'))\n          if data.get('changedAdGroups'):\n            for ad_group_data in data['changedAdGroups']:\n              print ('  Ad group with id \\'%s\\' has change status \\'%s\\'.'\n                     % (ad_group_data['adGroupId'],\n                        ad_group_data['adGroupChangeStatus']))\n              if ad_group_data['adGroupChangeStatus'] != 'NEW':\n                print '    Changed ads: %s' % ad_group_data['changedAds']\n                print ('    Changed criteria: %s'\n                       % ad_group_data['changedCriteria'])\n                print ('    Deleted criteria: %s'\n                       % ad_group_data['deletedCriteria'])\n  else:\n    print 'No changes were found.'\n\n  print\n  print ('Usage: %s units, %s operations' % (client.GetUnits(),\n                                             client.GetOperations()))\n\nif __name__ == '__main__':\n  # Initialize client object.\n  client = AdWordsClient(path=os.path.join('..', '..', '..', '..', '..'))\n  main(client)\n", "nse\");\n#   you may not use this file except in compliance with the License.\n#   You may obtain a copy of the License at\n#\n#       http://www.apache.org/licenses/LICENSE-2.0\n#\n#   Unless required by applicable law or agreed to in writing, software\n#   distributed under the License is distributed on an \"AS IS\" BASIS,\n#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#   See the License for the specific language governing permissions and\n#   limitations under the License.\n#\n\nimport pathlib\nimport subprocess\nimport logging\n\nlogger = logging.getLogger()\n\n\ndef vsctl(commands):\n    cmd = 'ovs-vsctl '\n    cmd += \" -- \".join(commands)\n    return run_cmd(cmd)\n\n\ndef ofctl(commands):\n    return [run_cmd('ovs-ofctl ' + cmd) for cmd in commands]\n\n\ndef run_cmd(cmd):\n    \"\"\"\n    Runs specified command in terminal.\n    :param cmd: the command to run\n    :return: stdout+stderr\n    \"\"\"\n    logger.debug(cmd)\n    p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=True)\n    output = p.communicate()[0].decode('utf-8')\n    if p.returncode != 0:\n        raise OSError(\"Command '%s' failed:  %s\" % (cmd, output))\n    return output\n\n\ndef daemon_start(cmd, name):\n    log_destination = pathlib.Path(\"log\") / '{}.log'.format(name)\n    with log_destination.open(\"at\") as output:\n        return subprocess.Popen(cmd, stdout=output, stderr=subprocess.STDOUT)\n", "edPrecisionDecimalType\nfrom pyangbind.lib.yangtypes import RestrictedClassType\nfrom pyangbind.lib.yangtypes import TypedListType\nfrom pyangbind.lib.yangtypes import YANGBool\nfrom pyangbind.lib.yangtypes import YANGListType\nfrom pyangbind.lib.yangtypes import YANGDynClass\nfrom pyangbind.lib.yangtypes import ReferenceType\nfrom pyangbind.lib.base import PybindBase\nfrom collections import OrderedDict\nfrom decimal import Decimal\nfrom bitarray import bitarray\nimport six\n\n# PY3 support of some PY2 keywords (needs improved)\nif six.PY3:\n    import builtins as __builtin__\n\n    long = int\nelif six.PY2:\n    import __builtin__\n\nfrom . import state\n\n\nclass sid(PybindBase):\n    \"\"\"\n  This class was auto-generated by the PythonClass plugin for PYANG\n  from YANG module openconfig-network-instance - based on the path /network-instances/network-instance/protocols/protocol/isis/levels/level/link-state-database/lsp/tlvs/tlv/mt-isn/neighbors/neighbor/subTLVs/subTLVs/adjacency-sid/sid. Each member element of\n  the container is represented as a class variable - with a specific\n  YANG type.\n\n  YANG Description: Adjacency Segment-IDs List. An IGP-Adjacency Segment is an IGP\nsegment attached to a unidirectional adjacency or a set of\nunidirectional adjacencies. By default, an IGP-Adjacency Segment is\nlocal to the node which advertises it.\n  \"\"\"\n    __slots__ = (\"_path_helper\", \"_extmethods\", \"__state\")\n\n    _yang_name = \"sid\"\n\n    _pybind_generated_by = \"container\"\n\n    def __init__(self, *args, **kwargs):\n\n        self._path_helper = False\n\n        self._extmethods = False\n        self.__state = YANGDynClass(\n            base=state.state,\n            is_container=\"container\",\n            yang_name=\"state\",\n            parent=self,\n            path_helper=self._path_helper,\n            extmethods=self._extmethods,\n            register_paths=True,\n            extensions=None,\n            namespace=\"http://openconfig.net/yang/network-instance\",\n            defining_module=\"openconfig-network-instance\",\n            yang_type=\"container\",\n            is_config=False,\n        )\n\n        load = kwargs.pop(\"load\", None)\n        if args:\n            if len(args) > 1:\n                raise TypeError(\"cannot create a YANG container with >1 argument\")\n            all_attr = True\n            for e in self._pyangbind_elements:\n                if not hasattr(args[0], e):\n                    all_attr = False\n                    break\n            if not all_attr:\n                raise ValueError(\"Supplied object did not have the correct attributes\")\n            for e in self._pyangbind_elements:\n                nobj = getattr(args[0], e)\n                if nobj._changed() is False:\n                    continue\n                setmethod = getattr(self, \"_set_%s\" % e)\n                if load is None:\n                    setmethod(getattr(args[0], e))\n                else:\n                    setmethod(getattr(args[0], e), load=load)\n\n    def _path(self):\n        if hasattr(self, \"_parent\"):\n            return self._parent._path() + [self._yang_name]\n        else:\n            return [\n                \"network-instances\",\n                \"network-instance\",\n                \"protocols\",\n                \"protocol\",\n                \"isis\",\n                \"levels\",\n                \"level\",\n                \"link-state-database\",\n                \"lsp\",\n                \"tlvs\",\n                \"tlv\",\n                \"mt-isn\",\n                \"neighbors\",\n                \"neighbor\",\n                \"subTLVs\",\n                \"subTLVs\",\n                \"adjacency-sid\",\n                \"sid\",\n            ]\n\n    def _get_state(self):\n        \"\"\"\n    Getter method for state, mapped from YANG variable /network_instances/network_instance/protocols/protocol/isis/levels/level/link_state_database/lsp/tlvs/tlv/mt_isn/neighbors/neighbor/subTLVs/subTLVs/adjacency_sid/sid/state (container)\n\n    YANG Description: State parameters of Adjacency-SID.\n    \"\"\"\n        return self.__state\n\n    def _set_state(self, v, load=False):\n        \"\"\"\n    Setter method for state, mapped from YANG variable /network_instances/network_instance/protocols/protocol/isis/levels/level/link_state_database/lsp/tlvs/tlv/mt_isn/neighbors/neighbor/subTLVs/subTLVs/adjacency_sid/sid/state (container)\n    If this variable is read-only (config: false) in the\n    source YANG file, then _set_state is considered as a private\n    method. Backends looking to populate this variable should\n    do so via calling thisObj._set_state() directly.\n\n    YANG Description: State parameters of Adjacency-SID.\n    \"\"\"\n        if hasattr(v, \"_utype\"):\n            v = v._utype(v)\n        try:\n            t = YANGDynClass(\n                v,\n                base=state.state,\n                is_container=\"container\",\n                yang_name=\"state\",\n                parent=self,\n                path_helper=self._path_helper,\n                extmethods=self._extmethods,\n                register_paths=True,\n                extensions=None,\n                namespace=\"http://openconfig.net/yang/network-instance\",\n                defining_module=\"openconfig-network-instance\",\n                yang_type=\"container\",\n                is_config=False,\n            )\n        except (TypeError, ValueError):\n            raise ValueError(\n                {\n                    \"error-string\": \"\"\"state must be of a type compatible with container\"\"\",\n                    \"defined-type\": \"container\",\n                    \"generated-type\": \"\"\"YANGDynClass(base=state.state, is_container='container', yang_name=\"state\", parent=self, path_helper=self._path_helper, extmethods=self._extmethods, register_paths=True, extensions=None, namespace='http://openconfig.net/yang/network-instance', defining_module='openconfig-network-instance', yang_type='container', is_config=False)\"\"\",\n                }\n            )\n\n        self.__state = t\n        if hasattr(self, \"_set\"):\n            self._set()\n\n    def _unset_state(self):\n        self.__state = YANGDynClass(\n            base=state.state,\n            is_container=\"container\",\n            yang_name=\"state\",\n            parent=self,\n            path_helper=self._path_helper,\n            extmethods=self._extmethods,\n            register_paths=True,\n            extensions=None,\n            namespace=\"http://openconfig.net/yang/network-instance\",\n            defining_module=\"openconfig-network-instance\",\n            yang_type=\"container\",\n            is_config=False,\n        )\n\n    state = __builtin__.property(_get_state)\n\n    _pyangbind_elements = OrderedDict([(\"state\", state)])\n\n\nfrom . import state\n\n\nclass sid(PybindBase):\n    \"\"\"\n  This class was auto-generated by the PythonClass plugin for PYANG\n  from YANG module openconfig-network-instance-l2 - based on the path /network-instances/network-instance/protocols/protocol/isis/levels/level/link-state-database/lsp/tlvs/tlv/mt-isn/neighbors/neighbor/subTLVs/subTLVs/adjacency-sid/sid. Each member element of\n  the container is represented as a class variable - with a specific\n  YANG type.\n\n  YANG Description: Adjacency Segment-IDs List. An IGP-Adjacency Segment is an IGP\nsegment attached to a unidirectional adjacency or a set of\nunidirectional adjacencies. By default, an IGP-Adjacency Segment is\nlocal to the node which advertises it.\n  \"\"\"\n    __slots__ = (\"_path_helper\", \"_extmethods\", \"__state\")\n\n    _yang_name = \"sid\"\n\n    _pybind_generated_by = \"container\"\n\n    def __init__(self, *args, **kwargs):\n\n        self._path_helper = False\n\n        self._extmethods = False\n        self.__state = YANGDynClass(\n            base=state.state,\n            is_container=\"container\",\n            yang_name=\"state\",\n            parent=self,\n            path_helper=self._path_helper,\n            extmethods=self._extmethods,\n            register_paths=True,\n            extensions=None,\n            namespace=\"http://openconfig.net/yang/network-instance\",\n            defining_module=\"openconfig-network-instance\",\n            yang_type=\"container\",\n            is_config=False,\n        )\n\n        load = kwargs.pop(\"load\", None)\n        if args:\n            if len(args) > 1:\n                raise TypeError(\"cannot create a YANG container with >1 argument\")\n            all_attr = True\n            for e in self._pyangbind_elements:\n                if not hasattr(args[0], e):\n                    all_attr = False\n                    break\n            if not all_attr:\n                raise ValueError(\"Supplied object did not have the correct attributes\")\n            for e in self._pyangbind_elements:\n                nobj = getattr(args[0], e)\n                if nobj._changed() is False:\n                    continue\n                setmethod = getattr(self, \"_set_%s\" % e)\n                if load is None:\n                    setmethod(getattr(args[0], e))\n                else:\n                    setmethod(getattr(args[0], e), load=load)\n\n    def _path(self):\n        if hasattr(self, \"_parent\"):\n            return self._parent._path() + [self._yang_name]\n        else:\n            return [\n                \"network-instances\",\n                \"network-instance\",\n                \"protocols\",\n                \"protocol\",\n                \"isis\",\n                \"levels\",\n                \"level\",\n                \"link-state-database\",\n                \"lsp\",\n                \"tlvs\",\n                \"tlv\",\n                \"mt-isn\",\n                \"neighbors\",\n                \"neighbor\",\n                \"subTLVs\",\n                \"subTLVs\",\n                \"adjacency-sid\",\n                \"sid\",\n            ]\n\n    def _get_state(self):\n        \"\"\"\n    Getter method for state, mapped from YANG variable /network_instances/network_instance/protocols/protocol/isis/levels/level/link_state_database/lsp/tlvs/tlv/mt_isn/neighbors/neighbor/subTLVs/subTLVs/adjacency_sid/sid/state (container)\n\n    YANG Description: State parameters of Adjacency-SID.\n    \"\"\"\n        return self.__state\n\n    def _set_state(self, v, load=False):\n        \"\"\"\n    Setter method for state, mapped from YANG variable /network_instances/network_instance/protocols/protocol/isis/levels/level/link_state_database/lsp/tlvs/tlv/mt_isn/neighbors/neighbor/subTLVs/subTLVs/adjacency_sid/sid/state (container)\n    If this variable is read-only (config: false) in the\n    source YANG file, then _set_state is considered as a private\n    method. Backends looking to populate this variable should\n    do so via calling thisObj._set_state() directly.\n\n    YANG Description: State parameters of Adjacency-SID.\n    \"\"\"\n        if hasattr(v, \"_utype\"):\n            v = v._utype(v)\n        try:\n            t = YANGDynClass(\n                v,\n                base=state.state,\n                is_container=\"container\",\n                yang_name=\"state\",\n                parent=self,\n                path_helper=self._path_helper,\n                extmethods=self._extmethods,\n                register_paths=True,\n                extensions=None,\n                namespace=\"http://openconfig.net/yang/network-instance\",\n                defining_module=\"openconfig-network-instance\",\n                yang_type=\"container\",\n                is_config=False,\n            )\n        except (TypeError, ValueError):\n            raise ValueError(\n                {\n                    \"error-string\": \"\"\"state must be of a type compatible with container\"\"\",\n                    \"defined-type\": \"container\",\n                    \"generated-type\": \"\"\"YANGDynClass(base=state.state, is_container='container', yang_name=\"state\", parent=self, path_helper=self._path_helper, extmethods=self._extmethods, register_paths=True, extensions=None, namespace='http://openconfig.net/yang/network-instance', defining_module='openconfig-network-instance', yang_type='container', is_config=False)\"\"\",\n                }\n            )\n\n        self.__state = t\n        if hasattr(self, \"_set\"):\n            self._set()\n\n    def _unset_state(self):\n        self.__state = YANGDynClass(\n            base=state.state,\n            is_container=\"container\",\n            yang_name=\"state\",\n            parent=self,\n            path_helper=self._path_helper,\n            extmethods=self._extmethods,\n            register_paths=True,\n            extensions=None,\n            namespace=\"http://openconfig.net/yang/network-instance\",\n            defining_module=\"openconfig-network-instance\",\n            yang_type=\"container\",\n            is_config=False,\n        )\n\n    state = __builtin__.property(_get_state)\n\n    _pyangbind_elements = OrderedDict([(\"state\", state)])\n", "atform, please refer to the documentation at\nhttps://home-assistant.io/components/sensor.london_underground/\n\"\"\"\nimport logging\nfrom datetime import timedelta\n\nimport voluptuous as vol\n\nimport homeassistant.helpers.config_validation as cv\nfrom homeassistant.components.sensor import PLATFORM_SCHEMA\nfrom homeassistant.helpers.entity import Entity\n\nREQUIREMENTS = ['london-tube-status==0.2']\n\n_LOGGER = logging.getLogger(__name__)\n\nATTRIBUTION = \"Powered by TfL Open Data\"\nCONF_LINE = 'line'\nSCAN_INTERVAL = timedelta(seconds=30)\nTUBE_LINES = [\n    'Bakerloo',\n    'Central',\n    'Circle',\n    'District',\n    'DLR',\n    'Hammersmith & City',\n    'Jubilee',\n    'London Overground',\n    'Metropolitan',\n    'Northern',\n    'Piccadilly',\n    'TfL Rail',\n    'Victoria',\n    'Waterloo & City']\n\nPLATFORM_SCHEMA = PLATFORM_SCHEMA.extend({\n    vol.Required(CONF_LINE):\n        vol.All(cv.ensure_list, [vol.In(list(TUBE_LINES))]),\n})\n\n\ndef setup_platform(hass, config, add_entities, discovery_info=None):\n    \"\"\"Set up the Tube sensor.\"\"\"\n    from london_tube_status import TubeData\n    data = TubeData()\n    data.update()\n    sensors = []\n    for line in config.get(CONF_LINE):\n        sensors.append(LondonTubeSensor(line, data))\n\n    add_entities(sensors, True)\n\n\nclass LondonTubeSensor(Entity):\n    \"\"\"Sensor that reads the status of a line from TubeData.\"\"\"\n\n    ICON = 'mdi:subway'\n\n    def __init__(self, name, data):\n        \"\"\"Initialize the sensor.\"\"\"\n        self._name = name\n        self._data = data\n        self._state = None\n        self._description = None\n\n    @property\n    def name(self):\n        \"\"\"Return the name of the sensor.\"\"\"\n        return self._name\n\n    @property\n    def state(self):\n        \"\"\"Return the state of the sensor.\"\"\"\n        return self._state\n\n    @property\n    def icon(self):\n        \"\"\"Icon to use in the frontend, if any.\"\"\"\n        return self.ICON\n\n    @property\n    def device_state_attributes(self):\n        \"\"\"Return other details about the sensor state.\"\"\"\n        attrs = {}\n        attrs['Description'] = self._description\n        return attrs\n\n    def update(self):\n        \"\"\"Update the sensor.\"\"\"\n        self._data.update()\n        self._state = self._data.data[self.name]['State']\n        self._description = self._data.data[self.name]['Description']\n", "e, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n\"\"\"Magnum test utilities.\"\"\"\n\n\nfrom magnum.db import api as db_api\n\n\ndef get_test_baymodel(**kw):\n    return {\n        'id': kw.get('id', 32),\n        'project_id': kw.get('project_id', 'fake_project'),\n        'user_id': kw.get('user_id', 'fake_user'),\n        'uuid': kw.get('uuid', 'e74c40e0-d825-11e2-a28f-0800200c9a66'),\n        'name': kw.get('name', 'baymodel1'),\n        'image_id': kw.get('image_id', 'ubuntu'),\n        'flavor_id': kw.get('flavor_id', 'm1.small'),\n        'master_flavor_id': kw.get('master_flavor_id', 'm1.small'),\n        'keypair_id': kw.get('keypair_id', 'keypair1'),\n        'external_network_id': kw.get('external_network_id',\n                                      'd1f02cfb-d27f-4068-9332-84d907cb0e2e'),\n        'fixed_network': kw.get('fixed_network', 'private'),\n        'network_driver': kw.get('network_driver'),\n        'dns_nameserver': kw.get('dns_nameserver', '8.8.1.1'),\n        'apiserver_port': kw.get('apiserver_port', 8080),\n        'docker_volume_size': kw.get('docker_volume_size', 20),\n        'cluster_distro': kw.get('cluster_distro', 'fedora-atomic'),\n        'ssh_authorized_key': kw.get('ssh_authorized_key',\n                                     'ssh-rsa AAAAB3NzaC1ycEAAAADA'\n                                     'v0XRqg3tm+jlsOKGO81lPDH+KaSJ'\n                                     'Q7wvmjUqszP/H6NC/m+qiGp/sTis'\n                                     'DYucqbeuM7nmJi+8Hb55y1xWoOZI'\n                                     'KMa71G5/4EOQxuQ/sgW965OOO2Hq'\n                                     'X8vjlQUnTK0HijrbSTLxp/9kazWW'\n                                     'FrfsdB8RtAAA test1234@magnum'),\n        'coe': kw.get('coe', 'swarm'),\n        'created_at': kw.get('created_at'),\n        'updated_at': kw.get('updated_at'),\n        'labels': kw.get('labels', {'key1': 'val1', 'key2': 'val2'}),\n        'http_proxy': kw.get('http_proxy', 'fake_http_proxy'),\n        'https_proxy': kw.get('https_proxy', 'fake_https_proxy'),\n        'no_proxy': kw.get('no_proxy', 'fake_no_proxy'),\n        'registry_enabled': kw.get('registry_enabled', False),\n        'tls_disabled': kw.get('tls_disabled', False),\n        'public': kw.get('public', False),\n        'server_type': kw.get('server_type', 'vm')\n    }\n\n\ndef create_test_baymodel(**kw):\n    \"\"\"Create test baymodel entry in DB and return BayModel DB object.\n\n    Function to be used to create test BayModel objects in the database.\n    :param kw: kwargs with overriding values for baymodel's attributes.\n    :returns: Test BayModel DB object.\n    \"\"\"\n    baymodel = get_test_baymodel(**kw)\n    # Let DB generate ID if it isn't specified explicitly\n    if 'id' not in kw:\n        del baymodel['id']\n    dbapi = db_api.get_instance()\n    return dbapi.create_baymodel(baymodel)\n\n\ndef get_test_bay(**kw):\n    return {\n        'id': kw.get('id', 42),\n        'uuid': kw.get('uuid', '5d12f6fd-a196-4bf0-ae4c-1f639a523a52'),\n        'name': kw.get('name', 'bay1'),\n        'discovery_url': kw.get('discovery_url', None),\n        'ca_cert_ref': kw.get('ca_cert_ref', None),\n        'magnum_cert_ref': kw.get('magnum_cert_ref', None),\n        'project_id': kw.get('project_id', 'fake_project'),\n        'user_id': kw.get('user_id', 'fake_user'),\n        'baymodel_id': kw.get('baymodel_id',\n                              'e74c40e0-d825-11e2-a28f-0800200c9a66'),\n        'stack_id': kw.get('stack_id', '047c6319-7abd-4bd9-a033-8c6af0173cd0'),\n        'status': kw.get('status', 'CREATE_IN_PROGRESS'),\n        'status_reason': kw.get('status_reason', 'Completed successfully'),\n        'api_address': kw.get('api_address', '172.17.2.3'),\n        'node_addresses': kw.get('node_addresses', ['172.17.2.4']),\n        'node_count': kw.get('node_count', 3),\n        'master_count': kw.get('master_count', 3),\n        'master_addresses': kw.get('master_addresses', ['172.17.2.18']),\n        'created_at': kw.get('created_at'),\n        'updated_at': kw.get('updated_at'),\n    }\n\n\ndef create_test_bay(**kw):\n    \"\"\"Create test bay entry in DB and return Bay DB object.\n\n    Function to be used to create test Bay objects in the database.\n    :param kw: kwargs with overriding values for bay's attributes.\n    :returns: Test Bay DB object.\n    \"\"\"\n    bay = get_test_bay(**kw)\n    # Let DB generate ID if it isn't specified explicitly\n    if 'id' not in kw:\n        del bay['id']\n    dbapi = db_api.get_instance()\n    return dbapi.create_bay(bay)\n\n\ndef get_test_pod(**kw):\n    return {\n        'id': kw.get('id', 42),\n        'uuid': kw.get('uuid', '10a47dd1-4874-4298-91cf-eff046dbdb8d'),\n        'name': kw.get('name', 'pod1'),\n        'project_id': kw.get('project_id', 'fake_project'),\n        'user_id': kw.get('user_id', 'fake_user'),\n        'desc': kw.get('desc', 'test pod'),\n        'bay_uuid': kw.get('bay_uuid', '5d12f6fd-a196-4bf0-ae4c-1f639a523a52'),\n        'images': kw.get('images', ['MyImage']),\n        'labels': kw.get('labels', {'name': 'foo'}),\n        'status': kw.get('status', 'Running'),\n        'host': kw.get('host', '10.0.0.3'),\n        'created_at': kw.get('created_at'),\n        'updated_at': kw.get('updated_at'),\n    }\n\n\ndef create_test_pod(**kw):\n    \"\"\"Create test pod entry in DB and return Pod DB object.\n\n    Function to be used to create test Pod objects in the database.\n    :param kw: kwargs with overriding values for pod's attributes.\n    :returns: Test Pod DB object.\n    \"\"\"\n    pod = get_test_pod(**kw)\n    # Let DB generate ID if it isn't specified explicitly\n    if 'id' not in kw:\n        del pod['id']\n    dbapi = db_api.get_instance()\n    return dbapi.create_pod(pod)\n\n\ndef get_test_service(**kw):\n    return {\n        'id': kw.get('id', 42),\n        'uuid': kw.get('uuid', '10a47dd1-4874-4298-91cf-eff046dbdb8d'),\n        'name': kw.get('name', 'service1'),\n        'project_id': kw.get('project_id', 'fake_project'),\n        'user_id': kw.get('user_id', 'fake_user'),\n        'bay_uuid': kw.get('bay_uuid', '5d12f6fd-a196-4bf0-ae4c-1f639a523a52'),\n        'labels': kw.get('labels', {'name': 'foo'}),\n        'selector': kw.get('selector', {'name': 'foo'}),\n        'ip': kw.get('ip', '172.17.2.2'),\n        'ports': kw.get('ports', [{'port': 80}]),\n        'created_at': kw.get('created_at'),\n        'updated_at': kw.get('updated_at'),\n    }\n\n\ndef create_test_service(**kw):\n    \"\"\"Create test service entry in DB and return Service DB object.\n\n    Function to be used to create test Service objects in the database.\n    :param kw: kwargs with overriding values for service's attributes.\n    :returns: Test Service DB object.\n    \"\"\"\n    service = get_test_service(**kw)\n    # Let DB generate ID if it isn't specified explicitly\n    if 'id' not in kw:\n        del service['id']\n    dbapi = db_api.get_instance()\n    return dbapi.create_service(service)\n\n\ndef get_test_node(**kw):\n    return {\n        'id': kw.get('id', 42),\n        'uuid': kw.get('uuid', 'ea8e2a25-2901-438d-8157-de7ffd68d051'),\n        'type': kw.get('type', 'virt'),\n        'project_id': kw.get('project_id', 'fake_project'),\n        'user_id': kw.get('user_id', 'fake_user'),\n        'image_id': kw.get('image_id', 'ubuntu'),\n        'ironic_node_id': kw.get('ironic_node_id'),\n        'created_at': kw.get('created_at'),\n        'updated_at': kw.get('updated_at'),\n    }\n\n\ndef create_test_node(**kw):\n    \"\"\"Create test node entry in DB and return Node DB object.\n\n    Function to be used to create test Node objects in the database.\n    :param kw: kwargs with overriding values for node's attributes.\n    :returns: Test Node DB object.\n    \"\"\"\n    node = get_test_node(**kw)\n    # Let DB generate ID if it isn't specified explicitly\n    if 'id' not in kw:\n        del node['id']\n    dbapi = db_api.get_instance()\n    return dbapi.create_node(node)\n\n\ndef get_test_container(**kw):\n    return {\n        'id': kw.get('id', 42),\n        'uuid': kw.get('uuid', 'ea8e2a25-2901-438d-8157-de7ffd68d051'),\n        'name': kw.get('name', 'container1'),\n        'project_id': kw.get('project_id', 'fake_project'),\n        'user_id': kw.get('user_id', 'fake_user'),\n        'image': kw.get('image', 'ubuntu'),\n        'created_at': kw.get('created_at'),\n        'updated_at': kw.get('updated_at'),\n        'command': kw.get('command', 'fake_command'),\n        'bay_uuid': kw.get('bay_uuid', 'fff114da-3bfa-4a0f-a123-c0dffad9718e'),\n        'status': kw.get('state', 'Running'),\n        'memory': kw.get('memory', '512m'),\n    }\n\n\ndef create_test_container(**kw):\n    \"\"\"Create test container entry in DB and return Container DB object.\n\n    Function to be used to create test Container objects in the database.\n    :param kw: kwargs with overriding values for container's attributes.\n    :returns: Test Container DB object.\n    \"\"\"\n    container = get_test_container(**kw)\n    # Let DB generate ID if it isn't specified explicitly\n    if 'id' not in kw:\n        del container['id']\n    dbapi = db_api.get_instance()\n    return dbapi.create_container(container)\n\n\ndef get_test_rc(**kw):\n    return {\n        'id': kw.get('id', 42),\n        'uuid': kw.get('uuid', '10a47dd1-4874-4298-91cf-eff046dbdb8d'),\n        'name': kw.get('name', 'replication_controller'),\n        'project_id': kw.get('project_id', 'fake_project'),\n        'user_id': kw.get('user_id', 'fake_user'),\n        'images': kw.get('images', ['steak/for-dinner']),\n        'bay_uuid': kw.get('bay_uuid', '5d12f6fd-a196-4bf0-ae4c-1f639a523a52'),\n        'labels': kw.get('labels', {'name': 'foo'}),\n        'replicas': kw.get('replicas', 3),\n        'manifest_url': kw.get('file:///tmp/rc.yaml'),\n        'created_at': kw.get('created_at'),\n        'updated_at': kw.get('updated_at'),\n    }\n\n\ndef create_test_rc(**kw):\n    \"\"\"Create test rc entry in DB and return ReplicationController DB object.\n\n    Function to be used to create test ReplicationController objects in the\n    database.\n    :param kw: kwargs with overriding values for\n               replication controller's attributes.\n    :returns: Test ReplicationController DB object.\n    \"\"\"\n    replication_controller = get_test_rc(**kw)\n    # Let DB generate ID if it isn't specified explicitly\n    if 'id' not in kw:\n        del replication_controller['id']\n    dbapi = db_api.get_instance()\n    return dbapi.create_rc(replication_controller)\n\n\ndef get_test_baylock(**kw):\n    return {\n        'id': kw.get('id', 42),\n        'bay_uuid': kw.get('bay_uuid', '5d12f6fd-a196-4bf0-ae4c-1f639a523a52'),\n        'conductor_id': kw.get('conductor_id',\n                               '72625085-c507-4410-9b28-cd7cf1fbf1ad'),\n    }\n\n\ndef get_test_x509keypair(**kw):\n    return {\n        'id': kw.get('id', 42),\n        'uuid': kw.get('uuid', '72625085-c507-4410-9b28-cd7cf1fbf1ad'),\n        'name': kw.get('name', 'x509keypair1'),\n        'project_id': kw.get('project_id', 'fake_project'),\n        'user_id': kw.get('user_id', 'fake_user'),\n        'bay_uuid': kw.get('bay_uuid',\n                           '5d12f6fd-a196-4bf0-ae4c-1f639a523a52'),\n        'ca_cert': kw.get('ca_cert', 'client_ca'),\n        'certificate': kw.get('certificate',\n                              'certificate'),\n        'private_key': kw.get('private_key', 'private_key'),\n        'created_at': kw.get('created_at'),\n        'updated_at': kw.get('updated_at'),\n    }\n\n\ndef create_test_x509keypair(**kw):\n    \"\"\"Create test x509keypair entry in DB and return X509KeyPair DB object.\n\n    Function to be used to create test X509KeyPair objects in the database.\n    :param kw: kwargs with overriding values for x509keypair's attributes.\n    :returns: Test X509KeyPair DB object.\n    \"\"\"\n    x509keypair = get_test_x509keypair(**kw)\n    # Let DB generate ID if it isn't specified explicitly\n    if 'id' not in kw:\n        del x509keypair['id']\n    dbapi = db_api.get_instance()\n    return dbapi.create_x509keypair(x509keypair)\n\n\ndef get_test_magnum_service(**kw):\n    return {\n        'id': kw.get('', 13),\n        'report_count': kw.get('report_count', 13),\n        'host': kw.get('host', 'fakehost'),\n        'binary': kw.get('binary', 'fake-bin'),\n        'disabled': kw.get('disabled', False),\n        'disabled_reason': kw.get('disabled_reason', 'fake-reason'),\n        'forced_down': kw.get('forced_down', False),\n        'last_seen_up': kw.get('last_seen_up'),\n        'created_at': kw.get('created_at'),\n        'updated_at': kw.get('updated_at'),\n    }\n\n\ndef create_test_magnum_service(**kw):\n    \"\"\"Create test magnum_service entry in DB and return magnum_service DB object.\n\n    :param kw: kwargs with overriding values for magnum_service's attributes.\n    :returns: Test magnum_service DB object.\n    \"\"\"\n    magnum_service = get_test_magnum_service(**kw)\n    # Let DB generate ID if it isn't specified explicitly\n    if 'id' not in kw:\n        del magnum_service['id']\n    dbapi = db_api.get_instance()\n    return dbapi.create_magnum_service(magnum_service)\n", " you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport subprocess\nimport tempfile\nimport unittest\nimport xml.etree.ElementTree as ET\n\nimport ament_index_python\nfrom launch_testing.junitxml import unittestResultsToXml\nfrom launch_testing.test_result import FailResult\nfrom launch_testing.test_result import SkipResult\nfrom launch_testing.test_result import TestResult as TR\nimport pytest\n\n\nclass TestGoodXmlOutput(unittest.TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        # For performance, we'll run the test once and generate the XML output, then\n        # have multiple test cases assert on it\n\n        cls.tmpdir = tempfile.TemporaryDirectory()\n        cls.xml_file = os.path.join(cls.tmpdir.name, 'junit.xml')\n\n        path = os.path.join(\n            ament_index_python.get_package_share_directory('launch_testing'),\n            'examples',\n            'good_proc_launch_test.py'\n        )\n\n        assert 0 == subprocess.run(\n            args=[\n                'launch_test',\n                path,\n                '--junit-xml', os.path.join(cls.tmpdir.name, 'junit.xml'),\n                '--package-name', 'test_xml_output'\n            ],\n        ).returncode\n\n    @classmethod\n    def tearDownClass(cls):\n        cls.tmpdir.cleanup()\n\n    def test_pre_and_post(self):\n        tree = ET.parse(self.xml_file)\n        root = tree.getroot()\n\n        self.assertEqual(len(root), 1)\n        test_suite = root[0]\n\n        # Expecting an element called '{package}.{test_base_name}.launch_tests' since this\n        # was not parametrized\n        self.assertEqual(\n            test_suite.attrib['name'], 'test_xml_output.good_proc_launch_test.launch_tests'\n        )\n\n        # Drilling down a little further, we expect the class names to show up in the testcase\n        # names\n        case_names = [case.attrib['name'] for case in test_suite]\n        self.assertIn('test_count_to_four', case_names)\n        self.assertIn('test_full_output', case_names)\n\n\n@pytest.mark.usefixtures('source_test_loader_class_fixture')\nclass TestXmlFunctions(unittest.TestCase):\n    # This are closer to unit tests - just call the functions that generate XML\n\n    def unit_test_result_factory(self, test_case_list):\n        # Use the unittest library to run some fake test functions and generate a real TestResult\n        # that we can serialize to XML\n\n        class TestHost(unittest.TestCase):\n            pass\n\n        TestHost.__qualname__ = 'TestHost'  # as if it was defined in global scope\n\n        for n, test_case in enumerate(test_case_list):\n            setattr(TestHost, 'test_{}'.format(n), test_case)\n\n        cases = unittest.TestLoader().loadTestsFromTestCase(TestHost)\n        with open(os.devnull, 'w') as nullstream:\n            runner = unittest.TextTestRunner(\n                stream=nullstream,\n                resultclass=TR\n            )\n            return runner.run(cases)\n\n    def test_fail_results_serialize(self):\n\n        def generate_test_description():\n            raise Exception('This should never be invoked')  # pragma: no cover\n\n        def test_fail_always(self):\n            assert False  # pragma: no cover\n\n        def test_pass_always(self):\n            pass  # pragma: no cover\n\n        test_runs = self.source_test_loader(\n            generate_test_description,\n            pre_shutdown_tests=[\n                test_fail_always,\n            ],\n            post_shutdown_tests=[\n                test_fail_always,\n                test_pass_always\n            ]\n        )\n\n        self.assertEqual(1, len(test_runs))  # Not a parametrized launch, so only 1 run\n\n        xml_tree = unittestResultsToXml(\n            name='fail_xml',\n            test_results={\n                'active_tests': FailResult(test_run=test_runs[0], message='Test Message')\n            }\n        )\n\n        # Simple sanity check - see that there's a child element called active_tests\n        child_names = [chld.attrib['name'] for chld in xml_tree.getroot()]\n        self.assertEqual(set(child_names), {'active_tests'})\n\n        # Make sure failures is non-zero, otherwise colcon test-result won't recognize this\n        # as a failure\n        self.assertGreater(int(xml_tree.getroot().get('failures')), 0)\n\n    def test_skip_results_serialize(self):\n        # This checks the case where all unit tests are skipped because of a skip\n        # decorator on the generate_test_description function\n        @unittest.skip('skip reason string')\n        def generate_test_description():\n            raise Exception('This should never be invoked')  # pragma: no cover\n\n        def test_fail_always(self):\n            assert False  # pragma: no cover\n\n        def test_pass_always(self):\n            pass  # pragma: no cover\n\n        test_runs = self.source_test_loader(\n            generate_test_description,\n            pre_shutdown_tests=[\n                test_fail_always,\n            ],\n            post_shutdown_tests=[\n                test_fail_always,\n                test_pass_always\n            ]\n        )\n\n        self.assertEqual(1, len(test_runs))  # Not a parametrized launch, so only 1 run\n\n        dut_xml = unittestResultsToXml(\n            test_results={\n                'run1': SkipResult(test_run=test_runs[0], skip_reason='skip message')\n            }\n        )\n\n        # Make sure the message got into the 'skip' element\n        testsuites_element = dut_xml.getroot()\n        testsuite_element = testsuites_element.find('testsuite')\n\n        self.assertEqual('0', testsuite_element.attrib['failures'])\n        self.assertEqual('0', testsuite_element.attrib['errors'])\n        self.assertEqual('3', testsuite_element.attrib['skipped'])\n        self.assertEqual('3', testsuite_element.attrib['tests'])\n\n        testcases = testsuite_element.findall('testcase')\n        self.assertEqual(3, len(testcases))\n\n        for testcase_element in testcases:\n            skip_element = testcase_element.find('skipped')\n            self.assertEqual('skip message', skip_element.attrib['message'])\n\n    def test_multiple_test_results(self):\n        xml_tree = unittestResultsToXml(\n            name='multiple_launches',\n            test_results={\n                'launch_1': TR(None, True, 1),\n                'launch_2': TR(None, True, 1),\n                'launch_3': TR(None, True, 1),\n            }\n        )\n\n        child_names = [chld.attrib['name'] for chld in xml_tree.getroot()]\n        self.assertEqual(set(child_names), {'launch_1', 'launch_2', 'launch_3'})\n\n    def test_result_that_ran(self):\n        \"\"\"\n        Test we have output as a result of a test being run.\n\n        The expected XML output for this test is:\n        <testsuites>\n          <testsuite name=\"run1\" . . . >\n            <testcase classname=\"TestHost\" name=\"test_0\" . . . />\n          </testsuite>\n        <testsuites>\n        \"\"\"\n        # This mostly validates the test setup is good for the other tests\n        dut_xml = unittestResultsToXml(\n            test_results={\n                'run1': self.unit_test_result_factory([\n                    lambda self: None\n                ])\n            }\n        )\n\n        testsuites_element = dut_xml.getroot()\n        testsuite_element = testsuites_element.find('testsuite')\n        testcase_element = testsuite_element.find('testcase')\n\n        # The bare minimum XML we require:\n        self.assertEqual('0', testsuite_element.attrib['failures'])\n        self.assertEqual('0', testsuite_element.attrib['errors'])\n        self.assertEqual('1', testsuite_element.attrib['tests'])\n        self.assertEqual('test_0', testcase_element.attrib['name'])\n        # Expect a fully qualified class name\n        self.assertEqual('test_xml_output.TestHost', testcase_element.attrib['classname'])\n\n    def test_result_with_skipped_test(self):\n        \"\"\"\n        Test we have output as a result of a skipped test.\n\n        The expected XML output for this test is:\n        <testsuites>\n          <testsuite name=\"run1\" skipped=\"1\". . . >\n            <testcase classname=\"TestHost\" name=\"test_0\" . . . >\n              <skipped message=\"My reason is foo\" . . . />\n            </testcase>\n          </testsuite>\n        <testsuites>\n\n        Notice the extra 'skipped' child-element of testcase.\n        \"\"\"\n        @unittest.skip('My reason is foo')\n        def test_that_is_skipped(self):\n            pass  # pragma: no cover\n\n        dut_xml = unittestResultsToXml(\n            test_results={\n                'run1': self.unit_test_result_factory([\n                    test_that_is_skipped\n                ])\n            }\n        )\n\n        testsuites_element = dut_xml.getroot()\n        testsuite_element = testsuites_element.find('testsuite')\n        testcase_element = testsuite_element.find('testcase')\n        skip_element = testcase_element.find('skipped')\n\n        self.assertEqual('1', testsuite_element.attrib['skipped'])\n        self.assertEqual('My reason is foo', skip_element.attrib['message'])\n\n    def test_result_with_failure(self):\n        \"\"\"\n        Test we have output as a result of failed test.\n\n        The expected XML output for this test is:\n\n        <testsuites>\n          <testsuite name=\"run1\" failures=\"1\". . . >\n            <testcase classname=\"TestHost\" name=\"test_0\" . . . >\n              <failure message=\"assert 1 == 2\" . . .>\n              </failure>\n            </testcase>\n          </testsuite>\n        <testsuites>\n\n        Notice there's a failure message child-element of the testcase and\n        a count of failed tests.\n        \"\"\"\n        def test_that_fails(self):\n            assert 1 == 2\n\n        dut_xml = unittestResultsToXml(\n            test_results={\n                'run1': self.unit_test_result_factory([\n                    test_that_fails\n                ])\n            }\n        )\n\n        testsuites_element = dut_xml.getroot()\n        testsuite_element = testsuites_element.find('testsuite')\n        testcase_element = testsuite_element.find('testcase')\n        failure_element = testcase_element.find('failure')\n\n        self.assertEqual('1', testsuite_element.attrib['failures'])\n        self.assertIn('1 == 2', failure_element.attrib['message'])\n\n    def test_result_with_error(self):\n        \"\"\"\n        Test we have output as a result of an error in a test.\n\n        The expected XML output for this test is:\n\n        <testsuites>\n          <testsuite name=\"run1\" errors=\"1\". . . >\n            <testcase classname=\"TestHost\" name=\"test_0\" . . . >\n              <error message=\"This is an error\" . . .>\n              </failure>\n            </testcase>\n          </testsuite>\n        <testsuites>\n\n        Python unittest treats exceptions other than AssertionError exceptions\n        as 'errors' not failures so they get a different tag, and count.\n        \"\"\"\n        def test_that_errors(self):\n            raise Exception('This is an error')\n\n        dut_xml = unittestResultsToXml(\n            test_results={\n                'run1': self.unit_test_result_factory([\n                    test_that_errors\n                ])\n            }\n        )\n\n        testsuites_element = dut_xml.getroot()\n        testsuite_element = testsuites_element.find('testsuite')\n        testcase_element = testsuite_element.find('testcase')\n        error_element = testcase_element.find('error')\n\n        self.assertEqual('1', testsuite_element.attrib['errors'])\n        self.assertIn('This is an error', error_element.attrib['message'])\n\n    def test_with_multiple_results(self):\n\n        def good_test(self):\n            pass\n\n        def error_test(self):\n            raise Exception('I am an error')\n\n        def fail_test(self):\n            assert 1 == 2\n\n        dut_xml = unittestResultsToXml(\n            test_results={\n                'run1': self.unit_test_result_factory([\n                    good_test,\n                    error_test,\n                    fail_test,\n                ])\n            }\n        )\n\n        testsuites_element = dut_xml.getroot()\n        testsuite_element = testsuites_element.find('testsuite')\n\n        self.assertEqual('1', testsuite_element.attrib['failures'])\n        self.assertEqual('1', testsuite_element.attrib['errors'])\n        self.assertEqual('3', testsuite_element.attrib['tests'])\n", "loggers'].update({\n        # Only becomes active with DEBUG = True\n        'django.db.backends': {\n            'level': 'DEBUG',\n            'handlers': ['console_sql'],\n            'propagate': False,\n        },\n    })", " (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n# implied. See the License for the specific language governing\n# permissions and limitations under the License.\n\n\"\"\"Tests for tcli.text_buffer.\"\"\"\n\nfrom absl.testing import absltest as unittest\nfrom tcli import text_buffer\n\n\nclass TextBufferTest(unittest.TestCase):\n  \"\"\"Tests the TextBuffer class.\"\"\"\n\n  def setUp(self):\n    super(TextBufferTest, self).setUp()\n    self.buf = text_buffer.TextBuffer()\n\n  def testInit(self):\n    self.assertEqual({}, self.buf._buffers)\n    self.buf.Append('', 'hello')\n    self.assertEqual({}, self.buf._buffers)\n    self.buf.Append('boo', '')\n    self.assertEqual({}, self.buf._buffers)\n\n  def testAppend(self):\n    self.buf.Append('boo', 'hi')\n    self.buf.Append('boo', 'there')\n    self.assertEqual('hi\\nthere', self.buf._buffers['boo'])\n    self.buf.Append('boo', 'hello\\nworld')\n    self.assertEqual('hi\\nthere\\nhello\\nworld', self.buf._buffers['boo'])\n    self.assertEqual({'boo': 'hi\\nthere\\nhello\\nworld'}, self.buf._buffers)\n\n  def testClear(self):\n    self.buf._buffers['boo'] = 'hello\\nworld'\n    self.buf.Clear('boo')\n    self.assertEqual({}, self.buf._buffers)\n    self.buf._buffers['boo'] = 'hello\\nworld'\n    self.buf._buffers['hoo'] = 'hi\\nthere'\n    self.buf.Clear('boo')\n    self.assertEqual({'hoo': 'hi\\nthere'}, self.buf._buffers)\n    self.assertTrue(self.buf.Clear('hoo'))\n    self.assertFalse(self.buf.Clear('non_exist'))\n\n  def testGetBuffer(self):\n    self.buf._buffers['boo'] = 'hello\\nworld'\n    self.assertEqual('hello\\nworld', self.buf.GetBuffer('boo'))\n    self.assertEqual(None, self.buf.GetBuffer('non_exist'))\n\n  def testListBuffer(self):\n    self.buf._buffers['boo'] = 'hello\\nworld'\n    self.buf._buffers['hoo'] = 'hello\\nworld'\n    self.assertEqual('boo hoo', self.buf.ListBuffers())\n    self.buf.Clear('boo')\n    self.assertEqual('hoo', self.buf.ListBuffers())\n\n\nif __name__ == '__main__':\n  unittest.main()\n", "Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nPet API Controller\n\nThis modules provides a REST API for the Pet Model\n\nPaths:\n------\nGET /pets - Lists all of the Pets\nGET /pets/{id} - Retrieves a single Pet with the specified id\nPOST /pets - Creates a new Pet\nPUT /pets/{id} - Updates a single Pet with the specified id\nDELETE /pets/{id} - Deletes a single Pet with the specified id\nPOST /pets/{id}/purchase - Action to purchase a Pet\n\"\"\"\n\nimport os\nimport sys\nimport logging\nfrom flask import Flask, jsonify, request, url_for, make_response, abort\nfrom models import Pet, DataValidationError\n\n# Create Flask application\napp = Flask(__name__)\napp.config['LOGGING_LEVEL'] = logging.INFO\n\n# Pull options from environment\nDEBUG = (os.getenv('DEBUG', 'False') == 'True')\nPORT = os.getenv('PORT', '5000')\n\n# Status Codes\nHTTP_200_OK = 200\nHTTP_201_CREATED = 201\nHTTP_204_NO_CONTENT = 204\nHTTP_400_BAD_REQUEST = 400\nHTTP_404_NOT_FOUND = 404\nHTTP_409_CONFLICT = 409\nHTTP_415_UNSUPPORTED_MEDIA_TYPE = 415\n\n######################################################################\n# Error Handlers\n######################################################################\n@app.errorhandler(DataValidationError)\ndef request_validation_error(error):\n    \"\"\" Handles Value Errors from bad data \"\"\"\n    return bad_request(error)\n\n@app.errorhandler(400)\ndef bad_request(error):\n    \"\"\" Handles bad reuests with 400_BAD_REQUEST \"\"\"\n    message = error.message or str(error)\n    app.logger.info(message)\n    return jsonify(status=400, error='Bad Request', message=message), 400\n\n@app.errorhandler(404)\ndef not_found(error):\n    \"\"\" Handles resources not found with 404_NOT_FOUND \"\"\"\n    message = error.message or str(error)\n    app.logger.info(message)\n    return jsonify(status=404, error='Not Found', message=message), 404\n\n@app.errorhandler(405)\ndef method_not_supported(error):\n    \"\"\" Handles unsuppoted HTTP methods with 405_METHOD_NOT_SUPPORTED \"\"\"\n    message = error.message or str(error)\n    app.logger.info(message)\n    return jsonify(status=405, error='Method not Allowed', message=message), 405\n\n@app.errorhandler(415)\ndef mediatype_not_supported(error):\n    \"\"\" Handles unsuppoted media requests with 415_UNSUPPORTED_MEDIA_TYPE \"\"\"\n    message = error.message or str(error)\n    app.logger.info(message)\n    return jsonify(status=415, error='Unsupported media type', message=message), 415\n\n@app.errorhandler(500)\ndef internal_server_error(error):\n    \"\"\" Handles unexpected server error with 500_SERVER_ERROR \"\"\"\n    message = error.message or str(error)\n    app.logger.info(message)\n    return jsonify(status=500, error='Internal Server Error', message=message), 500\n\n\n######################################################################\n# GET INDEX\n######################################################################\n@app.route('/')\ndef index():\n    \"\"\" Send back the home page \"\"\"\n    return app.send_static_file('index.html')\n\n######################################################################\n# LIST ALL PETS\n######################################################################\n@app.route('/pets', methods=['GET'])\ndef list_pets():\n    \"\"\" Returns all of the Pets \"\"\"\n    pets = []\n    category = request.args.get('category')\n    name = request.args.get('name')\n    available = request.args.get('available')\n    if category:\n        pets = Pet.find_by_category(category)\n    elif name:\n        pets = Pet.find_by_name(name)\n    elif available:\n        pets = Pet.find_by_availability(available)\n    else:\n        pets = Pet.all()\n\n    results = [pet.serialize() for pet in pets]\n    return make_response(jsonify(results), HTTP_200_OK)\n\n######################################################################\n# RETRIEVE A PET\n######################################################################\n@app.route('/pets/<int:pet_id>', methods=['GET'])\ndef get_pets(pet_id):\n    \"\"\"\n    Retrieve a single Pet\n\n    This endpoint will return a Pet based on it's id\n    \"\"\"\n    pet = Pet.find(pet_id)\n    if not pet:\n        abort(HTTP_404_NOT_FOUND, \"Pet with id '{}' was not found.\".format(pet_id))\n    return make_response(jsonify(pet.serialize()), HTTP_200_OK)\n\n######################################################################\n# ADD A NEW PET\n######################################################################\n@app.route('/pets', methods=['POST'])\ndef create_pets():\n    \"\"\"\n    Creates a Pet\n\n    This endpoint will create a Pet based the data in the body that is posted\n    or data that is sent via an html form post.\n    \"\"\"\n    check_content_type('application/json')\n    data = {}\n    # Check for form submission data\n    if request.headers.get('Content-Type') == 'application/x-www-form-urlencoded':\n        app.logger.info('Processing FORM data')\n        data = {\n            'name': request.form['name'],\n            'category': request.form['category'],\n            'available': request.form['available'].lower() in ['true', '1', 't']\n        }\n    else:\n        app.logger.info('Processing JSON data')\n        data = request.get_json()\n    pet = Pet()\n    pet.deserialize(data)\n    pet.save()\n    message = pet.serialize()\n    return make_response(jsonify(message), HTTP_201_CREATED,\n                         {'Location': url_for('get_pets', pet_id=pet.id, _external=True)})\n\n######################################################################\n# UPDATE AN EXISTING PET\n######################################################################\n@app.route('/pets/<int:pet_id>', methods=['PUT'])\ndef update_pets(pet_id):\n    \"\"\"\n    Update a Pet\n\n    This endpoint will update a Pet based the body that is posted\n    \"\"\"\n    check_content_type('application/json')\n    pet = Pet.find(pet_id)\n    if not pet:\n        abort(HTTP_404_NOT_FOUND, \"Pet with id '{}' was not found.\".format(pet_id))\n    pet.deserialize(request.get_json())\n    pet.id = pet_id\n    pet.save()\n    return make_response(jsonify(pet.serialize()), HTTP_200_OK)\n\n\n######################################################################\n# DELETE A PET\n######################################################################\n@app.route('/pets/<int:pet_id>', methods=['DELETE'])\ndef delete_pets(pet_id):\n    \"\"\"\n    Delete a Pet\n\n    This endpoint will delete a Pet based the id specified in the path\n    \"\"\"\n    pet = Pet.find(pet_id)\n    if pet:\n        pet.delete()\n    return make_response('', HTTP_204_NO_CONTENT)\n\n######################################################################\n# PURCHASE A PET\n######################################################################\n@app.route('/pets/<int:pet_id>/purchase', methods=['PUT'])\ndef purchase_pets(pet_id):\n    \"\"\" Purchase a Pet \"\"\"\n    pet = Pet.find(pet_id)\n    if not pet:\n        abort(HTTP_404_NOT_FOUND, \"Pet with id '{}' was not found.\".format(pet_id))\n    if not pet.available:\n        abort(HTTP_400_BAD_REQUEST, \"Pet with id '{}' is not available.\".format(pet_id))\n    pet.available = False\n    pet.save()\n    return make_response(jsonify(pet.serialize()), HTTP_200_OK)\n\n\n######################################################################\n#  U T I L I T Y   F U N C T I O N S\n######################################################################\n\n@app.before_first_request\ndef init_db(redis=None):\n    \"\"\" Initlaize the model \"\"\"\n    Pet.init_db(redis)\n\n# load sample data\ndef data_load(payload):\n    \"\"\" Loads a Pet into the database \"\"\"\n    pet = Pet(0, payload['name'], payload['category'])\n    pet.save()\n\ndef data_reset():\n    \"\"\" Removes all Pets from the database \"\"\"\n    Pet.remove_all()\n\ndef check_content_type(content_type):\n    \"\"\" Checks that the media type is correct \"\"\"\n    if request.headers['Content-Type'] == content_type:\n        return\n    app.logger.error('Invalid Content-Type: %s', request.headers['Content-Type'])\n    abort(HTTP_415_UNSUPPORTED_MEDIA_TYPE, 'Content-Type must be {}'.format(content_type))\n\n# @app.before_first_request\ndef initialize_logging(log_level=logging.INFO):\n    \"\"\" Initialized the default logging to STDOUT \"\"\"\n    if not app.debug:\n        print 'Setting up logging...'\n        # Set up default logging for submodules to use STDOUT\n        # datefmt='%m/%d/%Y %I:%M:%S %p'\n        fmt = '[%(asctime)s] %(levelname)s in %(module)s: %(message)s'\n        logging.basicConfig(stream=sys.stdout, level=log_level, format=fmt)\n        # Make a new log handler that uses STDOUT\n        handler = logging.StreamHandler(sys.stdout)\n        handler.setFormatter(logging.Formatter(fmt))\n        handler.setLevel(log_level)\n        # Remove the Flask default handlers and use our own\n        handler_list = list(app.logger.handlers)\n        for log_handler in handler_list:\n            app.logger.removeHandler(log_handler)\n        app.logger.addHandler(handler)\n        app.logger.setLevel(log_level)\n        app.logger.info('Logging handler established')\n\n\n######################################################################\n#   M A I N\n######################################################################\nif __name__ == \"__main__\":\n    print \"************************************************************\"\n    print \"        P E T   R E S T   A P I   S E R V I C E \"\n    print \"************************************************************\"\n    initialize_logging(app.config['LOGGING_LEVEL'])\n    app.run(host='0.0.0.0', port=int(PORT), debug=DEBUG)\n", "License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n\nfrom tempest import test\n\nfrom neutron.tests.tempest.api import test_trunk\n\n\nclass TestTrunkDetailsJSON(test_trunk.TrunkTestJSONBase):\n\n    extension = 'trunk-details'\n\n    @test.idempotent_id('f0bed24f-d36a-498b-b4e7-0d66e3fb7308')\n    def test_port_resource_trunk_details_no_subports(self):\n        trunk = self._create_trunk_with_network_and_parent([])\n        port = self.client.show_port(trunk['trunk']['port_id'])\n        expected_trunk_details = {'sub_ports': [],\n                                  'trunk_id': trunk['trunk']['id']}\n        observed_trunk_details = port['port'].get('trunk_details')\n        self.assertIsNotNone(observed_trunk_details)\n        self.assertEqual(expected_trunk_details,\n                         observed_trunk_details)\n\n    @test.idempotent_id('544bcaf2-86fb-4930-93ab-ece1c3cc33df')\n    def test_port_resource_trunk_details_with_subport(self):\n        subport_network = self.create_network()\n        subport = self.create_port(subport_network)\n        subport_data = {'port_id': subport['id'],\n                        'segmentation_type': 'vlan',\n                        'segmentation_id': 2}\n        trunk = self._create_trunk_with_network_and_parent([subport_data])\n        subport_data['mac_address'] = subport['mac_address']\n        parent_port = self.client.show_port(trunk['trunk']['port_id'])\n        expected_trunk_details = {'sub_ports': [subport_data],\n                                  'trunk_id': trunk['trunk']['id']}\n        observed_trunk_details = parent_port['port'].get('trunk_details')\n        self.assertIsNotNone(observed_trunk_details)\n        self.assertEqual(expected_trunk_details,\n                         observed_trunk_details)\n\n    @test.idempotent_id('fe6d865f-1d5c-432e-b65d-904157172f24')\n    def test_port_resource_empty_trunk_details(self):\n        network = self.create_network()\n        port = self.create_port(network)\n        port = self.client.show_port(port['id'])\n        observed_trunk_details = port['port'].get('trunk_details')\n        self.assertIsNone(observed_trunk_details)\n", " (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport pytest\n\nimport web_detect\n\nASSET_BUCKET = \"cloud-samples-data\"\n\n\ndef test_detect_file(capsys):\n    file_name = ('../detect/resources/landmark.jpg')\n    web_detect.report(web_detect.annotate(file_name))\n    out, _ = capsys.readouterr()\n    assert 'description' in out.lower()\n\n\n@pytest.mark.flaky(max_runs=3, min_passes=1)\ndef test_detect_web_gsuri(capsys):\n    file_name = ('gs://{}/vision/landmark/pofa.jpg'.format(\n                 ASSET_BUCKET))\n    web_detect.report(web_detect.annotate(file_name))\n    out, _ = capsys.readouterr()\n    assert 'description:' in out.lower()\n", " Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n# pylint: disable=unused-import,g-bad-import-order\n\"\"\"## Activation Functions\n\nThe activation ops provide different types of nonlinearities for use in neural\nnetworks.  These include smooth nonlinearities (`sigmoid`, `tanh`, `elu`,\n`softplus`, and `softsign`), continuous but not everywhere differentiable\nfunctions (`relu`, `relu6`, and `relu_x`), and random regularization\n(`dropout`).\n\nAll activation ops apply componentwise, and produce a tensor of the same\nshape as the input tensor.\n\n@@relu\n@@relu6\n@@elu\n@@softplus\n@@softsign\n@@dropout\n@@bias_add\n@@sigmoid\n@@tanh\n\n## Convolution\n\nThe convolution ops sweep a 2-D filter over a batch of images, applying the\nfilter to each window of each image of the appropriate size.  The different\nops trade off between generic vs. specific filters:\n\n* `conv2d`: Arbitrary filters that can mix channels together.\n* `depthwise_conv2d`: Filters that operate on each channel independently.\n* `separable_conv2d`: A depthwise spatial filter followed by a pointwise filter.\n\nNote that although these ops are called \"convolution\", they are strictly\nspeaking \"cross-correlation\" since the filter is combined with an input window\nwithout reversing the filter.  For details, see [the properties of\ncross-correlation](https://en.wikipedia.org/wiki/Cross-correlation#Properties).\n\nThe filter is applied to image patches of the same size as the filter and\nstrided according to the `strides` argument.  `strides = [1, 1, 1, 1]` applies\nthe filter to a patch at every offset, `strides = [1, 2, 2, 1]` applies the\nfilter to every other image patch in each dimension, etc.\n\nIgnoring channels for the moment, and assume that the 4-D `input` has shape\n`[batch, in_height, in_width, ...]` and the 4-D `filter` has shape\n`[filter_height, filter_width, ...]`, then the spatial semantics of the\nconvolution ops are as follows: first, according to the padding scheme chosen\nas `'SAME'` or `'VALID'`, the output size and the padding pixels are computed.\nFor the `'SAME'` padding, the output height and width are computed as:\n\n    out_height = ceil(float(in_height) / float(strides[1]))\n    out_width  = ceil(float(in_width) / float(strides[2]))\n\nand the padding on the top and left are computed as:\n\n    pad_along_height = ((out_height - 1) * strides[1] +\n                        filter_height - in_height)\n    pad_along_width = ((out_width - 1) * strides[2] +\n                       filter_width - in_width)\n    pad_top = pad_along_height / 2\n    pad_left = pad_along_width / 2\n\nNote that the division by 2 means that there might be cases when the padding on\nboth sides (top vs bottom, right vs left) are off by one. In this case, the\nbottom and right sides always get the one additional padded pixel. For example,\nwhen `pad_along_height` is 5, we pad 2 pixels at the top and 3 pixels at the\nbottom. Note that this is different from existing libraries such as cuDNN and\nCaffe, which explicitly specify the number of padded pixels and always pad the\nsame number of pixels on both sides.\n\nFor the `'VALID`' padding, the output height and width are computed as:\n\n    out_height = ceil(float(in_height - filter_height + 1) / float(strides[1]))\n    out_width  = ceil(float(in_width - filter_width + 1) / float(strides[2]))\n\nand the padding values are always zero. The output is then computed as\n\n    output[b, i, j, :] =\n        sum_{di, dj} input[b, strides[1] * i + di - pad_top,\n                           strides[2] * j + dj - pad_left, ...] *\n                     filter[di, dj, ...]\n\nwhere any value outside the original input image region are considered zero (\ni.e. we pad zero values around the border of the image).\n\nSince `input` is 4-D, each `input[b, i, j, :]` is a vector.  For `conv2d`, these\nvectors are multiplied by the `filter[di, dj, :, :]` matrices to produce new\nvectors.  For `depthwise_conv_2d`, each scalar component `input[b, i, j, k]`\nis multiplied by a vector `filter[di, dj, k]`, and all the vectors are\nconcatenated.\n\n@@conv2d\n@@depthwise_conv2d\n@@separable_conv2d\n@@atrous_conv2d\n@@conv2d_transpose\n@@conv3d\n\n## Pooling\n\nThe pooling ops sweep a rectangular window over the input tensor, computing a\nreduction operation for each window (average, max, or max with argmax).  Each\npooling op uses rectangular windows of size `ksize` separated by offset\n`strides`.  For example, if `strides` is all ones every window is used, if\n`strides` is all twos every other window is used in each dimension, etc.\n\nIn detail, the output is\n\n    output[i] = reduce(value[strides * i:strides * i + ksize])\n\nwhere the indices also take into consideration the padding values. Please refer\nto the `Convolution` section for details about the padding calculation.\n\n@@avg_pool\n@@max_pool\n@@max_pool_with_argmax\n@@avg_pool3d\n@@max_pool3d\n\n## Morphological filtering\n\nMorphological operators are non-linear filters used in image processing.\n\n[Greyscale morphological dilation]\n(https://en.wikipedia.org/wiki/Dilation_(morphology)) is the max-sum counterpart\nof standard sum-product convolution:\n\n    output[b, y, x, c] =\n        max_{dy, dx} input[b,\n                           strides[1] * y + rates[1] * dy,\n                           strides[2] * x + rates[2] * dx,\n                           c] +\n                     filter[dy, dx, c]\n\nThe `filter` is usually called structuring function. Max-pooling is a special\ncase of greyscale morphological dilation when the filter assumes all-zero\nvalues (a.k.a. flat structuring function).\n\n[Greyscale morphological erosion]\n(https://en.wikipedia.org/wiki/Erosion_(morphology)) is the min-sum counterpart\nof standard sum-product convolution:\n\n    output[b, y, x, c] =\n        min_{dy, dx} input[b,\n                           strides[1] * y - rates[1] * dy,\n                           strides[2] * x - rates[2] * dx,\n                           c] -\n                     filter[dy, dx, c]\n\nDilation and erosion are dual to each other. The dilation of the input signal\n`f` by the structuring signal `g` is equal to the negation of the erosion of\n`-f` by the reflected `g`, and vice versa.\n\nStriding and padding is carried out in exactly the same way as in standard\nconvolution. Please refer to the `Convolution` section for details.\n\n@@dilation2d\n@@erosion2d\n\n## Normalization\n\nNormalization is useful to prevent neurons from saturating when inputs may\nhave varying scale, and to aid generalization.\n\n@@l2_normalize\n@@local_response_normalization\n@@sufficient_statistics\n@@normalize_moments\n@@moments\n\n## Losses\n\nThe loss ops measure error between two tensors, or between a tensor and zero.\nThese can be used for measuring accuracy of a network in a regression task\nor for regularization purposes (weight decay).\n\n@@l2_loss\n\n## Classification\n\nTensorFlow provides several operations that help you perform classification.\n\n@@sigmoid_cross_entropy_with_logits\n@@softmax\n@@log_softmax\n@@softmax_cross_entropy_with_logits\n@@sparse_softmax_cross_entropy_with_logits\n@@weighted_cross_entropy_with_logits\n\n## Embeddings\n\nTensorFlow provides library support for looking up values in embedding\ntensors.\n\n@@embedding_lookup\n@@embedding_lookup_sparse\n\n## Evaluation\n\nThe evaluation ops are useful for measuring the performance of a network.\nSince they are nondifferentiable, they are typically used at evaluation time.\n\n@@top_k\n@@in_top_k\n\n## Candidate Sampling\n\nDo you want to train a multiclass or multilabel model with thousands\nor millions of output classes (for example, a language model with a\nlarge vocabulary)?  Training with a full Softmax is slow in this case,\nsince all of the classes are evaluated for every training example.\nCandidate Sampling training algorithms can speed up your step times by\nonly considering a small randomly-chosen subset of contrastive classes\n(called candidates) for each batch of training examples.\n\nSee our [Candidate Sampling Algorithms Reference]\n(../../extras/candidate_sampling.pdf)\n\n### Sampled Loss Functions\n\nTensorFlow provides the following sampled loss functions for faster training.\n\n@@nce_loss\n@@sampled_softmax_loss\n\n### Candidate Samplers\n\nTensorFlow provides the following samplers for randomly sampling candidate\nclasses when using one of the sampled loss functions above.\n\n@@uniform_candidate_sampler\n@@log_uniform_candidate_sampler\n@@learned_unigram_candidate_sampler\n@@fixed_unigram_candidate_sampler\n\n### Miscellaneous candidate sampling utilities\n\n@@compute_accidental_hits\n\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\n\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import candidate_sampling_ops\nfrom tensorflow.python.ops import constant_op\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import embedding_ops\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_grad\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import numerics\nfrom tensorflow.python.ops import random_ops\nfrom tensorflow.python.ops import rnn_cell\nfrom tensorflow.python.ops import seq2seq\nfrom tensorflow.python.ops import sparse_ops\nfrom tensorflow.python.ops import variable_scope as vs\nfrom tensorflow.python.ops.math_ops import sigmoid\nfrom tensorflow.python.ops.math_ops import tanh\nfrom tensorflow.python.util.all_util import make_all\n\n# Bring more nn-associated functionality into this package.\n# go/tf-wildcard-import\n# pylint: disable=wildcard-import\nfrom tensorflow.python.ops.nn_ops import *\nfrom tensorflow.python.ops.candidate_sampling_ops import *\nfrom tensorflow.python.ops.embedding_ops import *\nfrom tensorflow.python.ops.rnn import *\n# pylint: enable=wildcard-import\n\n\ndef sigmoid_cross_entropy_with_logits(logits, targets, name=None):\n  \"\"\"Computes sigmoid cross entropy given `logits`.\n\n  Measures the probability error in discrete classification tasks in which each\n  class is independent and not mutually exclusive.  For instance, one could\n  perform multilabel classification where a picture can contain both an elephant\n  and a dog at the same time.\n\n  For brevity, let `x = logits`, `z = targets`.  The logistic loss is\n\n        z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n      = z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))\n      = z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))\n      = z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))\n      = (1 - z) * x + log(1 + exp(-x))\n      = x - x * z + log(1 + exp(-x))\n\n  For x < 0, to avoid overflow in exp(-x), we reformulate the above\n\n        x - x * z + log(1 + exp(-x))\n      = log(exp(x)) - x * z + log(1 + exp(-x))\n      = - x * z + log(1 + exp(x))\n\n  Hence, to ensure stability and avoid overflow, the implementation uses this\n  equivalent formulation\n\n      max(x, 0) - x * z + log(1 + exp(-abs(x)))\n\n  `logits` and `targets` must have the same type and shape.\n\n  Args:\n    logits: A `Tensor` of type `float32` or `float64`.\n    targets: A `Tensor` of the same type and shape as `logits`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor` of the same shape as `logits` with the componentwise\n    logistic losses.\n\n  Raises:\n    ValueError: If `logits` and `targets` do not have the same shape.\n  \"\"\"\n  with ops.op_scope([logits, targets], name, \"logistic_loss\") as name:\n    logits = ops.convert_to_tensor(logits, name=\"logits\")\n    targets = ops.convert_to_tensor(targets, name=\"targets\")\n    try:\n      targets.get_shape().merge_with(logits.get_shape())\n    except ValueError:\n      raise ValueError(\n          \"logits and targets must have the same shape (%s vs %s)\"\n          % (logits.get_shape(), targets.get_shape()))\n\n    # The logistic loss formula from above is\n    #   x - x * z + log(1 + exp(-x))\n    # For x < 0, a more numerically stable formula is\n    #   -x * z + log(1 + exp(x))\n    # Note that these two expressions can be combined into the following:\n    #   max(x, 0) - x * z + log(1 + exp(-abs(x)))\n    # To allow computing gradients at zero, we define custom versions of max and\n    # abs functions.\n    zeros = array_ops.zeros_like(logits, dtype=logits.dtype)\n    cond = (logits >= zeros)\n    relu_logits = math_ops.select(cond, logits, zeros)\n    neg_abs_logits = math_ops.select(cond, -logits, logits)\n    return math_ops.add(relu_logits - logits * targets,\n                        math_ops.log(1 + math_ops.exp(neg_abs_logits)),\n                        name=name)\n\n\ndef weighted_cross_entropy_with_logits(logits, targets, pos_weight,\n                                       name=None):\n  \"\"\"Computes a weighted cross entropy.\n\n  This is like `sigmoid_cross_entropy_with_logits()` except that `pos_weight`,\n  allows one to trade off recall and precision by up- or down-weighting the\n  cost of a positive error relative to a negative error.\n\n  The usual cross-entropy cost is defined as:\n\n    targets * -log(sigmoid(logits)) + (1 - targets) * -log(1 - sigmoid(logits))\n\n  The argument `pos_weight` is used as a multiplier for the positive targets:\n\n    targets * -log(sigmoid(logits)) * pos_weight +\n        (1 - targets) * -log(1 - sigmoid(logits))\n\n  For brevity, let `x = logits`, `z = targets`, `q = pos_weight`.\n  The loss is:\n\n        qz * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n      = qz * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))\n      = qz * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))\n      = qz * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))\n      = (1 - z) * x + (qz +  1 - z) * log(1 + exp(-x))\n      = (1 - z) * x + (1 + (q - 1) * z) * log(1 + exp(-x))\n\n  Setting `l = (1 + (q - 1) * z)`, to ensure stability and avoid overflow,\n  the implementation uses\n\n      (1 - z) * x + l * (log(1 + exp(-abs(x))) + max(-x, 0))\n\n  `logits` and `targets` must have the same type and shape.\n\n  Args:\n    logits: A `Tensor` of type `float32` or `float64`.\n    targets: A `Tensor` of the same type and shape as `logits`.\n    pos_weight: A coefficient to use on the positive examples.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor` of the same shape as `logits` with the componentwise\n    weightedlogistic losses.\n\n  Raises:\n    ValueError: If `logits` and `targets` do not have the same shape.\n  \"\"\"\n  with ops.op_scope([logits, targets], name, \"logistic_loss\") as name:\n    logits = ops.convert_to_tensor(logits, name=\"logits\")\n    targets = ops.convert_to_tensor(targets, name=\"targets\")\n    try:\n      targets.get_shape().merge_with(logits.get_shape())\n    except ValueError:\n      raise ValueError(\n          \"logits and targets must have the same shape (%s vs %s)\"\n          % (logits.get_shape(), targets.get_shape()))\n\n    # The logistic loss formula from above is\n    #   (1 - z) * x + (1 + (q - 1) * z) * log(1 + exp(-x))\n    # For x < 0, a more numerically stable formula is\n    #   (1 - z) * x + (1 + (q - 1) * z) * log(1 + exp(x)) - l * x\n    # To avoid branching, we use the combined version\n    #   (1 - z) * x + l * (log(1 + exp(-abs(x))) + max(-x, 0))\n    log_weight = 1 + (pos_weight - 1) * targets\n    return math_ops.add(\n        (1 - targets) * logits,\n        log_weight * (math_ops.log(1 + math_ops.exp(-math_ops.abs(logits))) +\n                      nn_ops.relu(-logits)),\n        name=name)\n\n\ndef relu_layer(x, weights, biases, name=None):\n  \"\"\"Computes Relu(x * weight + biases).\n\n  Args:\n    x: a 2D tensor.  Dimensions typically: batch, in_units\n    weights: a 2D tensor.  Dimensions typically: in_units, out_units\n    biases: a 1D tensor.  Dimensions: out_units\n    name: A name for the operation (optional).  If not specified\n      \"nn_relu_layer\" is used.\n\n  Returns:\n    A 2-D Tensor computing relu(matmul(x, weights) + biases).\n    Dimensions typically: batch, out_units.\n  \"\"\"\n  with ops.op_scope([x, weights, biases], name, \"relu_layer\") as name:\n    x = ops.convert_to_tensor(x, name=\"x\")\n    weights = ops.convert_to_tensor(weights, name=\"weights\")\n    biases = ops.convert_to_tensor(biases, name=\"biases\")\n    xw_plus_b = nn_ops.bias_add(math_ops.matmul(x, weights), biases)\n    return nn_ops.relu(xw_plus_b, name=name)\n\n\ndef l2_normalize(x, dim, epsilon=1e-12, name=None):\n  \"\"\"Normalizes along dimension `dim` using an L2 norm.\n\n  For a 1-D tensor with `dim = 0`, computes\n\n      output = x / sqrt(max(sum(x**2), epsilon))\n\n  For `x` with more dimensions, independently normalizes each 1-D slice along\n  dimension `dim`.\n\n  Args:\n    x: A `Tensor`.\n    dim: Dimension along which to normalize.\n    epsilon: A lower bound value for the norm. Will use `sqrt(epsilon)` as the\n      divisor if `norm < sqrt(epsilon)`.\n    name: A name for this operation (optional).\n\n  Returns:\n    A `Tensor` with the same shape as `x`.\n  \"\"\"\n  with ops.op_scope([x], name, \"l2_normalize\") as name:\n    x = ops.convert_to_tensor(x, name=\"x\")\n    square_sum = math_ops.reduce_sum(math_ops.square(x), [dim], keep_dims=True)\n    x_inv_norm = math_ops.rsqrt(math_ops.maximum(square_sum, epsilon))\n    return math_ops.mul(x, x_inv_norm, name=name)\n\n\ndef zero_fraction(value, name=None):\n  \"\"\"Returns the fraction of zeros in `value`.\n\n  If `value` is empty, the result is `nan`.\n\n  This is useful in summaries to measure and report sparsity.  For example,\n\n      z = tf.Relu(...)\n      summ = tf.scalar_summary('sparsity', tf.nn.zero_fraction(z))\n\n  Args:\n    value: A tensor of numeric type.\n    name: A name for the operation (optional).\n\n  Returns:\n    The fraction of zeros in `value`, with type `float32`.\n  \"\"\"\n  with ops.op_scope([value], name, \"zero_fraction\"):\n    value = ops.convert_to_tensor(value, name=\"value\")\n    zero = constant_op.constant(0, dtype=value.dtype, name=\"zero\")\n    return math_ops.reduce_mean(math_ops.cast(math_ops.equal(value, zero),\n                                              dtypes.float32))\n\n\ndef depthwise_conv2d(input, filter, strides, padding, name=None):\n  \"\"\"Depthwise 2-D convolution.\n\n  Given an input tensor of shape `[batch, in_height, in_width, in_channels]`\n  and a filter tensor of shape\n  `[filter_height, filter_width, in_channels, channel_multiplier]`\n  containing `in_channels` convolutional filters of depth 1, `depthwise_conv2d`\n  applies a different filter to each input channel (expanding from 1 channel\n  to `channel_multiplier` channels for each), then concatenates the results\n  together.  The output has `in_channels * channel_multiplier` channels.\n\n  In detail,\n\n      output[b, i, j, k * channel_multiplier + q] =\n          sum_{di, dj} input[b, strides[1] * i + di, strides[2] * j + dj, k] *\n                       filter[di, dj, k, q]\n\n  Must have `strides[0] = strides[3] = 1`.  For the most common case of the\n  same horizontal and vertical strides, `strides = [1, stride, stride, 1]`.\n\n  Args:\n    input: 4-D with shape `[batch, in_height, in_width, in_channels]`.\n    filter: 4-D with shape\n      `[filter_height, filter_width, in_channels, channel_multiplier]`.\n    strides: 1-D of size 4.  The stride of the sliding window for each\n      dimension of `input`.\n    padding: A string, either `'VALID'` or `'SAME'`.  The padding algorithm.\n      See the [comment here](https://www.tensorflow.org/api_docs/python/nn.html#convolution)\n    name: A name for this operation (optional).\n\n  Returns:\n    A 4-D `Tensor` of shape\n    `[batch, out_height, out_width, in_channels * channel_multiplier].`\n  \"\"\"\n  with ops.op_scope([input, filter], name, \"depthwise\") as name:\n    input = ops.convert_to_tensor(input, name=\"tensor_in\")\n    filter = ops.convert_to_tensor(filter, name=\"filter_in\")\n    # A shape is required to statically compute the number of separable filters.\n    if filter.get_shape().ndims is not None:\n      assert len(filter.get_shape()) == 4\n      in_channels = filter.get_shape()[2]\n      # Sanity checks, if shape information is available for the inputs.\n      if input.get_shape().ndims is not None:\n        assert len(input.get_shape()) == 4\n        assert input.get_shape()[3] == in_channels, (\n            \"Mismatched input depth %d and number of depthwise filters %d.\" % (\n                input.get_shape()[3].value, in_channels))\n    else:\n      assert input.get_shape().ndims is not None, (\n          \"Either tensor must provide static shape information.\")\n      assert input.get_shape().ndims == 4\n      in_channels = input.get_shape()[3]\n\n    if in_channels == 1:\n      return nn_ops.conv2d(input, filter, strides, padding, name=name)\n    else:\n      return nn_ops.depthwise_conv2d_native(input, filter, strides, padding,\n                                            name=name)\n\n\ndef separable_conv2d(input, depthwise_filter, pointwise_filter, strides,\n                     padding,\n                     name=None):\n  \"\"\"2-D convolution with separable filters.\n\n  Performs a depthwise convolution that acts separately on channels followed by\n  a pointwise convolution that mixes channels.  Note that this is separability\n  between dimensions `[1, 2]` and `3`, not spatial separability between\n  dimensions `1` and `2`.\n\n  In detail,\n\n      output[b, i, j, k] = sum_{di, dj, q, r]\n          input[b, strides[1] * i + di, strides[2] * j + dj, q] *\n          depthwise_filter[di, dj, q, r] *\n          pointwise_filter[0, 0, q * channel_multiplier + r, k]\n\n  `strides` controls the strides for the depthwise convolution only, since\n  the pointwise convolution has implicit strides of `[1, 1, 1, 1]`.  Must have\n  `strides[0] = strides[3] = 1`.  For the most common case of the same\n  horizontal and vertical strides, `strides = [1, stride, stride, 1]`.\n\n  Args:\n    input: 4-D `Tensor` with shape `[batch, in_height, in_width, in_channels]`.\n    depthwise_filter: 4-D `Tensor` with shape\n      `[filter_height, filter_width, in_channels, channel_multiplier]`.\n      Contains `in_channels` convolutional filters of depth 1.\n    pointwise_filter: 4-D `Tensor` with shape\n      `[1, 1, channel_multiplier * in_channels, out_channels]`.  Pointwise\n      filter to mix channels after `depthwise_filter` has convolved spatially.\n    strides: 1-D of size 4.  The strides for the depthwise convolution for\n      each dimension of `input`.\n    padding: A string, either `'VALID'` or `'SAME'`.  The padding algorithm.\n      See the [comment here](https://www.tensorflow.org/api_docs/python/nn.html#convolution)\n    name: A name for this operation (optional).\n\n  Returns:\n    A 4-D `Tensor` of shape `[batch, out_height, out_width, out_channels]`.\n\n  Raises:\n    ValueError: If channel_multiplier * in_channels > out_channels,\n      which means that the separable convolution is overparameterized.\n  \"\"\"\n  with ops.op_scope([input, depthwise_filter, pointwise_filter],\n                   name, \"separable_conv2d\") as name:\n    input = ops.convert_to_tensor(input, name=\"tensor_in\")\n    depthwise_filter = ops.convert_to_tensor(depthwise_filter,\n                                             name=\"depthwise_filter\")\n    pointwise_filter = ops.convert_to_tensor(pointwise_filter,\n                                             name=\"pointwise_filter\")\n\n    if pointwise_filter.get_shape().ndims is not None:\n      assert len(pointwise_filter.get_shape()) == 4\n      assert pointwise_filter.get_shape()[0] == 1\n      assert pointwise_filter.get_shape()[1] == 1\n      if depthwise_filter.get_shape().ndims and input.get_shape().ndims:\n        channel_multiplier = depthwise_filter.get_shape()[3]\n        in_channels = input.get_shape()[3]\n        out_channels = pointwise_filter.get_shape()[3]\n        if channel_multiplier * in_channels > out_channels:\n          raise ValueError(\n              (\"Refusing to perform an overparameterized separable \"\n               \"convolution: channel_multiplier * in_channels = \"\n               \"%d * %d = %d > %d = out_channels\" %\n               (channel_multiplier, in_channels,\n                channel_multiplier * in_channels, out_channels)))\n    # The layout of the ops in the graph are expected to be as follows:\n    # depthwise_conv2d  // Conv2D op corresponding to native deptwise conv.\n    # separable_conv2d  // Conv2D op corresponding to the pointwise conv.\n    depthwise = nn_ops.depthwise_conv2d_native(input, depthwise_filter, strides,\n                                               padding, name=\"depthwise\")\n    return nn_ops.conv2d(depthwise, pointwise_filter, [1, 1, 1, 1],\n                         padding=\"VALID\", name=name)\n\n\ndef sufficient_statistics(x, axes, shift=None, keep_dims=False, name=None):\n  \"\"\"Calculate the sufficient statistics for the mean and variance of `x`.\n\n  These sufficient statistics are computed using the one pass algorithm on\n  an input that's optionally shifted. See:\n  https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Computing_shifted_data\n\n  Args:\n    x: A `Tensor`.\n    axes: Array of ints. Axes along which to compute mean and variance.\n    shift: A `Tensor` containing the value by which to shift the data for\n      numerical stability, or `None` if no shift is to be performed. A shift\n      close to the true mean provides the most numerically stable results.\n    keep_dims: produce statistics with the same dimensionality as the input.\n    name: Name used to scope the operations that compute the sufficient stats.\n\n  Returns:\n    Four `Tensor` objects of the same type as `x`:\n    * the count (number of elements to average over).\n    * the (possibly shifted) sum of the elements in the array.\n    * the (possibly shifted) sum of squares of the elements in the array.\n    * the shift by which the mean must be corrected or None if `shift` is None.\n  \"\"\"\n  with ops.op_scope([x, axes, shift], name, \"sufficient_statistics\"):\n    x = ops.convert_to_tensor(x, name=\"x\")\n    x_shape = x.get_shape()\n    if x_shape.is_fully_defined():\n      counts = 1\n      m_shape = []\n      for d in xrange(x_shape.ndims):\n        dim = x_shape[d].value\n        if d in set(axes):\n          counts *= dim\n          dim = 1\n        m_shape.append(dim)\n      counts = constant_op.constant(counts, dtype=x.dtype)\n    else:  # shape needs to be inferred at runtime.\n      x_shape = array_ops.shape(x)\n      select_axes = sparse_ops.sparse_to_dense(axes, array_ops.shape(x_shape),\n                                               True, False)\n      m_shape = math_ops.select(select_axes, array_ops.ones_like(x_shape),\n                                x_shape)\n      counts = math_ops.cast(\n          math_ops.reduce_prod(x_shape / m_shape),\n          x.dtype,\n          name=\"count\")\n    if shift is not None:\n      shift = ops.convert_to_tensor(shift, name=\"shift\")\n      m_ss = math_ops.sub(x, shift)\n      v_ss = math_ops.squared_difference(x, shift)\n    else:  # no shift.\n      m_ss = x\n      v_ss = math_ops.square(x)\n    m_ss = math_ops.reduce_sum(m_ss, axes, keep_dims=keep_dims, name=\"mean_ss\")\n    v_ss = math_ops.reduce_sum(v_ss, axes, keep_dims=keep_dims, name=\"var_ss\")\n  return counts, m_ss, v_ss, shift\n\n\ndef normalize_moments(counts, mean_ss, variance_ss, shift, name=None):\n  \"\"\"Calculate the mean and variance of based on the sufficient statistics.\n\n  Args:\n    counts: A `Tensor` containing a the total count of the data (one value).\n    mean_ss: A `Tensor` containing the mean sufficient statistics: the (possibly\n      shifted) sum of the elements to average over.\n    variance_ss: A `Tensor` containing the variance sufficient statistics: the\n      (possibly shifted) squared sum of the data to compute the variance over.\n    shift: A `Tensor` containing the value by which the data is shifted for\n      numerical stability, or `None` if no shift was performed.\n    name: Name used to scope the operations that compute the moments.\n\n  Returns:\n    Two `Tensor` objects: `mean` and `variance`.\n  \"\"\"\n  with ops.op_scope([counts, mean_ss, variance_ss, shift], name, \"normalize\"):\n    divisor = math_ops.inv(counts, name=\"divisor\")\n    if shift is not None:\n      shifted_mean = math_ops.mul(mean_ss, divisor, name=\"shifted_mean\")\n      mean = math_ops.add(shifted_mean, shift, name=\"mean\")\n    else:  # no shift.\n      shifted_mean = math_ops.mul(mean_ss, divisor, name=\"mean\")\n      mean = shifted_mean\n    variance = math_ops.sub(\n        math_ops.mul(variance_ss, divisor),\n        math_ops.square(shifted_mean),\n        name=\"variance\")\n  return (mean, variance)\n\n\ndef moments(x, axes, shift=None, name=None, keep_dims=False):\n  \"\"\"Calculate the mean and variance of `x`.\n\n  The mean and variance are calculated by aggregating the contents of `x`\n  across `axes`.  If `x` is 1-D and `axes = [0]` this is just the mean\n  and variance of a vector.\n\n  When using these moments for batch normalization (see\n  `tf.nn.batch_normalization`):\n    * for so-called \"global normalization\", used with convolutional filters with\n      shape `[batch, height, width, depth]`, pass `axes=[0, 1, 2]`.\n    * for simple batch normalization pass `axes=[0]` (batch only).\n\n  Args:\n    x: A `Tensor`.\n    axes: array of ints.  Axes along which to compute mean and\n      variance.\n    shift: A `Tensor` containing the value by which to shift the data for\n      numerical stability, or `None` if no shift is to be performed. A shift\n      close to the true mean provides the most numerically stable results.\n    keep_dims: produce moments with the same dimensionality as the input.\n    name: Name used to scope the operations that compute the moments.\n\n  Returns:\n    Two `Tensor` objects: `mean` and `variance`.\n  \"\"\"\n  with ops.op_scope([x, axes, shift], name, \"moments\"):\n    counts, m_ss, v_ss, shift = sufficient_statistics(x,\n                                                      axes,\n                                                      shift=shift,\n                                                      keep_dims=keep_dims,\n                                                      name=name)\n    return normalize_moments(counts, m_ss, v_ss, shift, name=name)\n\n\ndef batch_normalization(x,\n                        mean,\n                        variance,\n                        offset,\n                        scale,\n                        variance_epsilon,\n                        name=None):\n  \"\"\"Batch normalization.\n\n  As described in http://arxiv.org/abs/1502.03167.\n  Normalizes a tensor by `mean` and `variance`, and applies (optionally) a\n  `scale` \\\\\\\\(\\gamma\\\\\\\\) to it, as well as an `offset` \\\\\\\\(\\\\beta\\\\\\\\):\n\n  \\\\\\\\(\\\\frac{\\gamma(x-\\mu)}{\\sigma}+\\\\beta\\\\\\\\)\n\n  `mean`, `variance`, `offset` and `scale` are all expected to be of one of two\n  shapes:\n    * In all generality, they can have the same number of dimensions as the\n      input `x`, with identical sizes as `x` for the dimensions that are not\n      normalized over (the 'depth' dimension(s)), and dimension 1 for the\n      others which are being normalized over.\n      `mean` and `variance` in this case would typically be the outputs of\n      `tf.nn.moments(..., keep_dims=True)` during training, or running averages\n      thereof during inference.\n    * In the common case where the 'depth' dimension is the last dimension in\n      the input tensor `x`, they may be one dimensional tensors of the same\n      size as the 'depth' dimension.\n      This is the case for example for the common `[batch, depth]` layout of\n      fully-connected layers, and `[batch, height, width, depth]` for\n      convolutions.\n      `mean` and `variance` in this case would typically be the outputs of\n      `tf.nn.moments(..., keep_dims=False)` during training, or running averages\n      thereof during inference.\n\n  Args:\n    x: Input `Tensor` of arbitrary dimensionality.\n    mean: A mean `Tensor`.\n    variance: A variance `Tensor`.\n    offset: An offset `Tensor`, often denoted \\\\\\\\(\\\\beta\\\\\\\\) in equations, or\n      None. If present, will be added to the normalized tensor.\n    scale: A scale `Tensor`, often denoted \\\\\\\\(\\gamma\\\\\\\\) in equations, or\n      `None`. If present, the scale is applied to the normalized tensor.\n    variance_epsilon: A small float number to avoid dividing by 0.\n    name: A name for this operation (optional).\n\n  Returns:\n    the normalized, scaled, offset tensor.\n  \"\"\"\n  with ops.op_scope([x, mean, variance, scale, offset], name, \"batchnorm\"):\n    inv = math_ops.rsqrt(variance + variance_epsilon)\n    if scale is not None:\n      inv *= scale\n    return x * inv + (\n        offset - mean * inv if offset is not None else -mean * inv)\n\n\ndef batch_norm_with_global_normalization(t,\n                                         m,\n                                         v,\n                                         beta,\n                                         gamma,\n                                         variance_epsilon,\n                                         scale_after_normalization,\n                                         name=None):\n  \"\"\"Batch normalization.\n\n  This op is deprecated. See `tf.nn.batch_normalization`.\n\n  Args:\n    t: A 4D input Tensor.\n    m: A 1D mean Tensor with size matching the last dimension of t.\n      This is the first output from tf.nn.moments,\n      or a saved moving average thereof.\n    v: A 1D variance Tensor with size matching the last dimension of t.\n      This is the second output from tf.nn.moments,\n      or a saved moving average thereof.\n    beta: A 1D beta Tensor with size matching the last dimension of t.\n      An offset to be added to the normalized tensor.\n    gamma: A 1D gamma Tensor with size matching the last dimension of t.\n      If \"scale_after_normalization\" is true, this tensor will be multiplied\n      with the normalized tensor.\n    variance_epsilon: A small float number to avoid dividing by 0.\n    scale_after_normalization: A bool indicating whether the resulted tensor\n      needs to be multiplied with gamma.\n    name: A name for this operation (optional).\n\n   Returns:\n     A batch-normalized `t`.\n  \"\"\"\n  return batch_normalization(t, m, v, beta, gamma if scale_after_normalization\n                             else None, variance_epsilon, name)\n\n\ndef _sum_rows(x):\n  \"\"\"Returns a vector summing up each row of the matrix x.\"\"\"\n  # _sum_rows(x) is equivalent to math_ops.reduce_sum(x, 1) when x is\n  # a matrix.  The gradient of _sum_rows(x) is more efficient than\n  # reduce_sum(x, 1)'s gradient in today's implementation. Therefore,\n  # we use _sum_rows(x) in the nce_loss() computation since the loss\n  # is mostly used for training.\n  cols = array_ops.shape(x)[1]\n  ones_shape = array_ops.pack([cols, 1])\n  ones = array_ops.ones(ones_shape, x.dtype)\n  return array_ops.reshape(math_ops.matmul(x, ones), [-1])\n\n\ndef _compute_sampled_logits(weights, biases, inputs, labels, num_sampled,\n                            num_classes, num_true=1,\n                            sampled_values=None,\n                            subtract_log_q=True,\n                            remove_accidental_hits=False,\n                            partition_strategy=\"mod\",\n                            name=None):\n  \"\"\"Helper function for nce_loss and sampled_softmax_loss functions.\n\n  Computes sampled output training logits and labels suitable for implementing\n  e.g. noise-contrastive estimation (see nce_loss) or sampled softmax (see\n  sampled_softmax_loss).\n\n  Note: In the case where num_true > 1, we assign to each target class\n  the target probability 1 / num_true so that the target probabilities\n  sum to 1 per-example.\n\n  Args:\n    weights: A `Tensor` of shape `[num_classes, dim]`, or a list of `Tensor`\n        objects whose concatenation along dimension 0 has shape\n        `[num_classes, dim]`.  The (possibly-partitioned) class embeddings.\n    biases: A `Tensor` of shape `[num_classes]`.  The class biases.\n    inputs: A `Tensor` of shape `[batch_size, dim]`.  The forward\n        activations of the input network.\n    labels: A `Tensor` of type `int64` and shape `[batch_size,\n        num_true]`. The target classes.  Note that this format differs from\n        the `labels` argument of `nn.softmax_cross_entropy_with_logits`.\n    num_sampled: An `int`.  The number of classes to randomly sample per batch.\n    num_classes: An `int`. The number of possible classes.\n    num_true: An `int`.  The number of target classes per training example.\n    sampled_values: a tuple of (`sampled_candidates`, `true_expected_count`,\n        `sampled_expected_count`) returned by a `*_candidate_sampler` function.\n        (if None, we default to `log_uniform_candidate_sampler`)\n    subtract_log_q: A `bool`.  whether to subtract the log expected count of\n        the labels in the sample to get the logits of the true labels.\n        Default is True.  Turn off for Negative Sampling.\n    remove_accidental_hits:  A `bool`.  whether to remove \"accidental hits\"\n        where a sampled class equals one of the target classes.  Default is\n        False.\n    partition_strategy: A string specifying the partitioning strategy, relevant\n        if `len(weights) > 1`. Currently `\"div\"` and `\"mod\"` are supported.\n        Default is `\"mod\"`. See `tf.nn.embedding_lookup` for more details.\n    name: A name for the operation (optional).\n  Returns:\n    out_logits, out_labels: `Tensor` objects each with shape\n        `[batch_size, num_true + num_sampled]`, for passing to either\n        `nn.sigmoid_cross_entropy_with_logits` (NCE) or\n        `nn.softmax_cross_entropy_with_logits` (sampled softmax).\n  \"\"\"\n\n  if not isinstance(weights, list):\n    weights = [weights]\n\n  with ops.op_scope(\n      weights + [biases, inputs, labels], name, \"compute_sampled_logits\"):\n    if labels.dtype != dtypes.int64:\n      labels = math_ops.cast(labels, dtypes.int64)\n    labels_flat = array_ops.reshape(labels, [-1])\n\n    # Sample the negative labels.\n    #   sampled shape: [num_sampled] tensor\n    #   true_expected_count shape = [batch_size, 1] tensor\n    #   sampled_expected_count shape = [num_sampled] tensor\n    if sampled_values is None:\n      sampled_values = candidate_sampling_ops.log_uniform_candidate_sampler(\n          true_classes=labels,\n          num_true=num_true,\n          num_sampled=num_sampled,\n          unique=True,\n          range_max=num_classes)\n    # NOTE: pylint cannot tell that 'sampled_values' is a sequence\n    # pylint: disable=unpacking-non-sequence\n    sampled, true_expected_count, sampled_expected_count = sampled_values\n    # pylint: enable=unpacking-non-sequence\n\n    # labels_flat is a [batch_size * num_true] tensor\n    # sampled is a [num_sampled] int tensor\n    all_ids = array_ops.concat(0, [labels_flat, sampled])\n\n    # weights shape is [num_classes, dim]\n    all_w = embedding_ops.embedding_lookup(\n        weights, all_ids, partition_strategy=partition_strategy)\n    all_b = embedding_ops.embedding_lookup(biases, all_ids)\n    # true_w shape is [batch_size * num_true, dim]\n    # true_b is a [batch_size * num_true] tensor\n    true_w = array_ops.slice(\n        all_w, [0, 0], array_ops.pack([array_ops.shape(labels_flat)[0], -1]))\n    true_b = array_ops.slice(all_b, [0], array_ops.shape(labels_flat))\n\n    # inputs shape is [batch_size, dim]\n    # true_w shape is [batch_size * num_true, dim]\n    # row_wise_dots is [batch_size, num_true, dim]\n    dim = array_ops.shape(true_w)[1:2]\n    new_true_w_shape = array_ops.concat(0, [[-1, num_true], dim])\n    row_wise_dots = math_ops.mul(\n        array_ops.expand_dims(inputs, 1),\n        array_ops.reshape(true_w, new_true_w_shape))\n    # We want the row-wise dot plus biases which yields a\n    # [batch_size, num_true] tensor of true_logits.\n    dots_as_matrix = array_ops.reshape(row_wise_dots,\n                                       array_ops.concat(0, [[-1], dim]))\n    true_logits = array_ops.reshape(_sum_rows(dots_as_matrix), [-1, num_true])\n    true_b = array_ops.reshape(true_b, [-1, num_true])\n    true_logits += true_b\n\n    # Lookup weights and biases for sampled labels.\n    #   sampled_w shape is [num_sampled, dim]\n    #   sampled_b is a [num_sampled] float tensor\n    sampled_w = array_ops.slice(\n        all_w, array_ops.pack([array_ops.shape(labels_flat)[0], 0]), [-1, -1])\n    sampled_b = array_ops.slice(all_b, array_ops.shape(labels_flat), [-1])\n\n    # inputs has shape [batch_size, dim]\n    # sampled_w has shape [num_sampled, dim]\n    # sampled_b has shape [num_sampled]\n    # Apply X*W'+B, which yields [batch_size, num_sampled]\n    sampled_logits = math_ops.matmul(inputs,\n                                     sampled_w,\n                                     transpose_b=True) + sampled_b\n\n    if remove_accidental_hits:\n      acc_hits = candidate_sampling_ops.compute_accidental_hits(\n          labels, sampled, num_true=num_true)\n      acc_indices, acc_ids, acc_weights = acc_hits\n\n      # This is how SparseToDense expects the indices.\n      acc_indices_2d = array_ops.reshape(acc_indices, [-1, 1])\n      acc_ids_2d_int32 = array_ops.reshape(math_ops.cast(\n          acc_ids, dtypes.int32), [-1, 1])\n      sparse_indices = array_ops.concat(\n          1, [acc_indices_2d, acc_ids_2d_int32], \"sparse_indices\")\n      # Create sampled_logits_shape = [batch_size, num_sampled]\n      sampled_logits_shape = array_ops.concat(\n          0,\n          [array_ops.shape(labels)[:1], array_ops.expand_dims(num_sampled, 0)])\n      if sampled_logits.dtype != acc_weights.dtype:\n        acc_weights = math_ops.cast(acc_weights, sampled_logits.dtype)\n      sampled_logits += sparse_ops.sparse_to_dense(\n          sparse_indices, sampled_logits_shape, acc_weights,\n          default_value=0.0, validate_indices=False)\n\n    if subtract_log_q:\n      # Subtract log of Q(l), prior probability that l appears in sampled.\n      true_logits -= math_ops.log(true_expected_count)\n      sampled_logits -= math_ops.log(sampled_expected_count)\n\n    # Construct output logits and labels. The true labels/logits start at col 0.\n    out_logits = array_ops.concat(1, [true_logits, sampled_logits])\n    # true_logits is a float tensor, ones_like(true_logits) is a float tensor\n    # of ones. We then divide by num_true to ensure the per-example labels sum\n    # to 1.0, i.e. form a proper probability distribution.\n    out_labels = array_ops.concat(\n        1, [array_ops.ones_like(true_logits) / num_true,\n            array_ops.zeros_like(sampled_logits)])\n\n  return out_logits, out_labels\n\n\ndef nce_loss(weights, biases, inputs, labels, num_sampled, num_classes,\n             num_true=1,\n             sampled_values=None,\n             remove_accidental_hits=False,\n             partition_strategy=\"mod\",\n             name=\"nce_loss\"):\n  \"\"\"Computes and returns the noise-contrastive estimation training loss.\n\n  See [Noise-contrastive estimation: A new estimation principle for\n  unnormalized statistical models]\n  (http://www.jmlr.org/proceedings/papers/v9/gutmann10a/gutmann10a.pdf).\n  Also see our [Candidate Sampling Algorithms Reference]\n  (../../extras/candidate_sampling.pdf)\n\n  Note: In the case where `num_true` > 1, we assign to each target class\n  the target probability 1 / `num_true` so that the target probabilities\n  sum to 1 per-example.\n\n  Note: It would be useful to allow a variable number of target classes per\n  example.  We hope to provide this functionality in a future release.\n  For now, if you have a variable number of target classes, you can pad them\n  out to a constant number by either repeating them or by padding\n  with an otherwise unused class.\n\n  Args:\n    weights: A `Tensor` of shape `[num_classes, dim]`, or a list of `Tensor`\n        objects whose concatenation along dimension 0 has shape\n        [num_classes, dim].  The (possibly-partitioned) class embeddings.\n    biases: A `Tensor` of shape `[num_classes]`.  The class biases.\n    inputs: A `Tensor` of shape `[batch_size, dim]`.  The forward\n        activations of the input network.\n    labels: A `Tensor` of type `int64` and shape `[batch_size,\n        num_true]`. The target classes.\n    num_sampled: An `int`.  The number of classes to randomly sample per batch.\n    num_classes: An `int`. The number of possible classes.\n    num_true: An `int`.  The number of target classes per training example.\n    sampled_values: a tuple of (`sampled_candidates`, `true_expected_count`,\n        `sampled_expected_count`) returned by a `*_candidate_sampler` function.\n        (if None, we default to `log_uniform_candidate_sampler`)\n    remove_accidental_hits:  A `bool`.  Whether to remove \"accidental hits\"\n        where a sampled class equals one of the target classes.  If set to\n        `True`, this is a \"Sampled Logistic\" loss instead of NCE, and we are\n        learning to generate log-odds instead of log probabilities.  See\n        our [Candidate Sampling Algorithms Reference]\n        (../../extras/candidate_sampling.pdf).\n        Default is False.\n    partition_strategy: A string specifying the partitioning strategy, relevant\n        if `len(weights) > 1`. Currently `\"div\"` and `\"mod\"` are supported.\n        Default is `\"mod\"`. See `tf.nn.embedding_lookup` for more details.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `batch_size` 1-D tensor of per-example NCE losses.\n  \"\"\"\n  logits, labels = _compute_sampled_logits(\n      weights, biases, inputs, labels, num_sampled, num_classes,\n      num_true=num_true,\n      sampled_values=sampled_values,\n      subtract_log_q=True,\n      remove_accidental_hits=remove_accidental_hits,\n      partition_strategy=partition_strategy,\n      name=name)\n  sampled_losses = sigmoid_cross_entropy_with_logits(logits,\n                                                     labels,\n                                                     name=\"sampled_losses\")\n  # sampled_losses is batch_size x {true_loss, sampled_losses...}\n  # We sum out true and sampled losses.\n  return _sum_rows(sampled_losses)\n\n\ndef sampled_softmax_loss(weights, biases, inputs, labels, num_sampled,\n                         num_classes, num_true=1,\n                         sampled_values=None,\n                         remove_accidental_hits=True,\n                         partition_strategy=\"mod\",\n                         name=\"sampled_softmax_loss\"):\n  \"\"\"Computes and returns the sampled softmax training loss.\n\n  This is a faster way to train a softmax classifier over a huge number of\n  classes.\n\n  This operation is for training only.  It is generally an underestimate of\n  the full softmax loss.\n\n  At inference time, you can compute full softmax probabilities with the\n  expression `tf.nn.softmax(tf.matmul(inputs, tf.transpose(weights)) + biases)`.\n\n  See our [Candidate Sampling Algorithms Reference]\n  (../../extras/candidate_sampling.pdf)\n\n  Also see Section 3 of [Jean et al., 2014](http://arxiv.org/abs/1412.2007)\n  ([pdf](http://arxiv.org/pdf/1412.2007.pdf)) for the math.\n\n  Args:\n    weights: A `Tensor` of shape `[num_classes, dim]`, or a list of `Tensor`\n        objects whose concatenation along dimension 0 has shape\n        [num_classes, dim].  The (possibly-sharded) class embeddings.\n    biases: A `Tensor` of shape `[num_classes]`.  The class biases.\n    inputs: A `Tensor` of shape `[batch_size, dim]`.  The forward\n        activations of the input network.\n    labels: A `Tensor` of type `int64` and shape `[batch_size,\n        num_true]`. The target classes.  Note that this format differs from\n        the `labels` argument of `nn.softmax_cross_entropy_with_logits`.\n    num_sampled: An `int`.  The number of classes to randomly sample per batch.\n    num_classes: An `int`. The number of possible classes.\n    num_true: An `int`.  The number of target classes per training example.\n    sampled_values: a tuple of (`sampled_candidates`, `true_expected_count`,\n        `sampled_expected_count`) returned by a `*_candidate_sampler` function.\n        (if None, we default to `log_uniform_candidate_sampler`)\n    remove_accidental_hits:  A `bool`.  whether to remove \"accidental hits\"\n        where a sampled class equals one of the target classes.  Default is\n        True.\n    partition_strategy: A string specifying the partitioning strategy, relevant\n        if `len(weights) > 1`. Currently `\"div\"` and `\"mod\"` are supported.\n        Default is `\"mod\"`. See `tf.nn.embedding_lookup` for more details.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `batch_size` 1-D tensor of per-example sampled softmax losses.\n\n  \"\"\"\n  logits, labels = _compute_sampled_logits(\n      weights, biases, inputs, labels, num_sampled, num_classes,\n      num_true=num_true,\n      sampled_values=sampled_values,\n      subtract_log_q=True,\n      remove_accidental_hits=remove_accidental_hits,\n      partition_strategy=partition_strategy,\n      name=name)\n  sampled_losses = nn_ops.softmax_cross_entropy_with_logits(logits, labels)\n  # sampled_losses is a [batch_size] tensor.\n  return sampled_losses\n\n\n# TODO(cwhipkey): sigmoid and tanh should not be exposed from tf.nn.\n__all__ = make_all(__name__)\n__all__.append(\"zero_fraction\")  # documented in training.py\n\n# Modules whitelisted for reference through tf.nn.\n# TODO(cwhipkey): migrate callers to use the submodule directly.\n__all__.extend([\"nn_ops\", \"rnn_cell\", \"seq2seq\"])\n\n# Symbols whitelisted for export without documentation.\n# TODO(cwhipkey): review these and move to contrib or expose through\n# documentation.\n__all__.extend([\n    \"all_candidate_sampler\",\n    \"batch_norm_with_global_normalization\",\n    \"batch_normalization\",\n    \"bidirectional_rnn\",\n    \"conv2d_backprop_filter\",\n    \"conv2d_backprop_input\",\n    \"depthwise_conv2d_native\",\n    \"dynamic_rnn\",\n    \"lrn\",\n    \"relu_layer\",\n    \"rnn\",\n    \"state_saving_rnn\",\n    \"xw_plus_b\",\n])\n", "inistration.\n# All Rights Reserved.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n\nfrom __future__ import print_function\n\nimport errno\nimport gc\nimport os\nimport pprint\nimport socket\nimport sys\nimport traceback\n\nimport eventlet\nimport eventlet.backdoor\nimport greenlet\nfrom oslo.config import cfg\n\nfrom designate.openstack.common.gettextutils import _\nfrom designate.openstack.common import log as logging\n\nhelp_for_backdoor_port = (\n    \"Acceptable values are 0, <port>, and <start>:<end>, where 0 results \"\n    \"in listening on a random tcp port number; <port> results in listening \"\n    \"on the specified port number (and not enabling backdoor if that port \"\n    \"is in use); and <start>:<end> results in listening on the smallest \"\n    \"unused port number within the specified range of port numbers.  The \"\n    \"chosen port is displayed in the service's log file.\")\neventlet_backdoor_opts = [\n    cfg.StrOpt('backdoor_port',\n               default=None,\n               help=\"Enable eventlet backdoor.  %s\" % help_for_backdoor_port)\n]\n\nCONF = cfg.CONF\nCONF.register_opts(eventlet_backdoor_opts)\nLOG = logging.getLogger(__name__)\n\n\nclass EventletBackdoorConfigValueError(Exception):\n    def __init__(self, port_range, help_msg, ex):\n        msg = ('Invalid backdoor_port configuration %(range)s: %(ex)s. '\n               '%(help)s' %\n               {'range': port_range, 'ex': ex, 'help': help_msg})\n        super(EventletBackdoorConfigValueError, self).__init__(msg)\n        self.port_range = port_range\n\n\ndef _dont_use_this():\n    print(\"Don't use this, just disconnect instead\")\n\n\ndef _find_objects(t):\n    return [o for o in gc.get_objects() if isinstance(o, t)]\n\n\ndef _print_greenthreads():\n    for i, gt in enumerate(_find_objects(greenlet.greenlet)):\n        print(i, gt)\n        traceback.print_stack(gt.gr_frame)\n        print()\n\n\ndef _print_nativethreads():\n    for threadId, stack in sys._current_frames().items():\n        print(threadId)\n        traceback.print_stack(stack)\n        print()\n\n\ndef _parse_port_range(port_range):\n    if ':' not in port_range:\n        start, end = port_range, port_range\n    else:\n        start, end = port_range.split(':', 1)\n    try:\n        start, end = int(start), int(end)\n        if end < start:\n            raise ValueError\n        return start, end\n    except ValueError as ex:\n        raise EventletBackdoorConfigValueError(port_range, ex,\n                                               help_for_backdoor_port)\n\n\ndef _listen(host, start_port, end_port, listen_func):\n    try_port = start_port\n    while True:\n        try:\n            return listen_func((host, try_port))\n        except socket.error as exc:\n            if (exc.errno != errno.EADDRINUSE or\n               try_port >= end_port):\n                raise\n            try_port += 1\n\n\ndef initialize_if_enabled():\n    backdoor_locals = {\n        'exit': _dont_use_this,      # So we don't exit the entire process\n        'quit': _dont_use_this,      # So we don't exit the entire process\n        'fo': _find_objects,\n        'pgt': _print_greenthreads,\n        'pnt': _print_nativethreads,\n    }\n\n    if CONF.backdoor_port is None:\n        return None\n\n    start_port, end_port = _parse_port_range(str(CONF.backdoor_port))\n\n    # NOTE(johannes): The standard sys.displayhook will print the value of\n    # the last expression and set it to __builtin__._, which overwrites\n    # the __builtin__._ that gettext sets. Let's switch to using pprint\n    # since it won't interact poorly with gettext, and it's easier to\n    # read the output too.\n    def displayhook(val):\n        if val is not None:\n            pprint.pprint(val)\n    sys.displayhook = displayhook\n\n    sock = _listen('localhost', start_port, end_port, eventlet.listen)\n\n    # In the case of backdoor port being zero, a port number is assigned by\n    # listen().  In any case, pull the port number out here.\n    port = sock.getsockname()[1]\n    LOG.info(_('Eventlet backdoor listening on %(port)s for process %(pid)d') %\n             {'port': port, 'pid': os.getpid()})\n    eventlet.spawn_n(eventlet.backdoor.backdoor_server, sock,\n                     locals=backdoor_locals)\n    return port\n", " to the db on standard port\nconnection = pymongo.MongoClient(\"mongodb://localhost\")\n\n\n\ndb = connection.m101                 # attach to db\ncollection = db.funnynumbers         # specify the colllection\n\n\nmagic = 0\n\ntry:\n    iter = collection.find()\n    for item in iter:\n        if ((item['value'] % 3) == 0):\n            magic = magic + item['value']\n\nexcept Exception as e:\n    print (\"Error trying to read collection:\", type(e), e)\n\n\nprint (\"The answer to Homework One, Problem 2 is \" + str(int(magic)))\n\n\n", "st.bao@gmail.com)\n\nCopyright 2012,  Mindboggle team (http://mindboggle.info), Apache v2.0 License\n\n\"\"\"\n\n\ndef prune(Path, Degree, TreeNbr, Terminal, Branching, Special, Vertices):\n    \"\"\"\n    Prune an MST by deleting edges\n\n    Note: Only links between special vertices are left.\n\n    Parameters\n    ----------\n    Path : list of lists (2-tuple) of integers\n        Each element of *Path* is a list of the two terminals of each pair\n        of connected vertices\n        Note: vertices are indexed LOCALLY\n    Degree : list of integers\n        Degrees of nodes in a component\n    TreeNbr : list of list of integers\n        Each element is a list of neighbors of a node. All LOCAL IDs.\n    Terminal : list of integers\n        Local IDs of nodes that are terminals.\n    Branching : list of integers\n        Local IDs of nodes that are branching nodes.\n    Special : list of integers\n        Local IDs of nodes that are special vertices connected by MST\n    Vertices: list of integers\n        A list of global indices to vertices\n\n    Returns\n    -------\n    Path : list of lists (2-tuple) of integers\n        Each element of *Path* is a list of the two terminals of each pair of\n        connected vertices\n        Note: vertices are indexed LOCALLY\n\n    \"\"\"\n\n    NodeColor = {} # to visualize visited nodes of different types\n    for T in Terminal:\n        if len(Branching) < 1:\n            break\n\n        if T in Special:\n            continue\n\n        Trace = [ ] # store the trace visited from a terminal node to a branching node\n        Visited = []  # store nodes that have been visited\n        At = T  # the node of current position in tracing\n        NodeColor[Vertices[T]] = 1\n        while(not At in Branching and not At in Special):\n            Visited.append(At)\n            for Nbr in TreeNbr[At]:\n                if not Nbr in Visited: # search toward the mainstream\n                    Trace.append((Nbr, At))\n                    Previous = At  # the node visited before At in tracing\n                    At = Nbr\n                    break\n        # After the while loop, At stops at a branching node.\n        # Delete all links from non-special terminals\n        NodeColor[Vertices[At]] = 2  # just a regular branching node\n        TreeNbr[At].remove(Previous)\n        for Pair in Trace:\n            (Src, Dst) = Pair\n            if Pair in Path:\n                Path.remove(Pair)\n            else:  # it is possible the order of nodes is reversed in Path\n                Path.remove((Dst, Src))\n\n        if At in Branching:  # may stop at a Special-only node\n            Degree[At] -= 1\n            if Degree[At] < 3:\n                Branching.remove(At)\n\n    return Path, NodeColor\n\nclass Prim:  # modified from the code without license @http://hurring.com/scott/code/python/mst_prim/v0.1/mst_prim.py\n\n    def __init__(self, A, r):\n        \"\"\"\n        Prepare the inputs for mst_prim\n        \"\"\"\n        import numpy as np\n\n        self.INFINITY = np.Inf\n        self.init_adjacency(A)\n        self.remove_route(A, r)\n        self.degree = np.zeros(len(A))\n        self.tree_nbr= [[] for x in A]\n\n    def mst_prim(self, A, w, path, degree, tree_nbr):\n        \"\"\"\n        'A' is the adjacency matrix\n        'w' is the list of all connected vertices (in order of discovery)\n        'path' is a list of tuples showing (from, to)\n        \"\"\"\n        import sys\n        import numpy as np\n\n        sys.setrecursionlimit(30000)\n\n        # Stop when we've added all nodes to the path\n        # (number of 1-D arrays within matrix that contain nonzero elements)\n        if len(w) == sum([any(np.array(x)[0]) for x in A]):\n            return (A, w, path, degree, tree_nbr)\n\n        # Find minimum path coming OUT of the known vertices\n        vfrom, vto, vcost = self.find_min(A, w)\n\n        # Increase the degree for vertices vfrom and vto\n        degree[vfrom] += 1\n        degree[vto] += 1\n\n        # Update tree_nbr list for vfrom and vto\n        tree_nbr[vfrom].append(vto)\n        tree_nbr[vto].append(vfrom)\n\n        # Store this vertex as part of the MST path\n        w.append(vto)\n        path.append((vfrom, vto))\n\n        self.remove_route(A, vto)\n\n        return self.mst_prim(A, w, path, degree, tree_nbr)\n\n\n    def init_adjacency(self, A):\n        \"\"\"\n        Initialize adjacency list - set 0 values to INFINITY\n        \"\"\"\n        A[A==0] = self.INFINITY\n\n    def remove_route(self, A, v):\n        \"\"\"\n        Once we've added a node to our path, set all routes\n        to this node equal to INFINITY - to prevent loops\n        \"\"\"\n        A[:,v] = self.INFINITY\n\n    def find_min(self, A, w):\n        \"\"\"\n        Find the cheapest connection we can possibly make,\n        given the partially-built MST 'w' --\n        'vfrom': vertex to connect from\n        'vto': vertex to connect to\n        'vcost': cost of connection\n        \"\"\"\n        import numpy as np\n\n        vcost = self.INFINITY\n        vto = vfrom = -1\n        for v in w:\n            # Get array offset of minimum of this vertex\n            i = np.argmin(A[v,:])\n            if A[v,i] < vcost:\n                vcost = A[v,i]\n                vto = i\n                vfrom = v\n        return (vfrom, vto, vcost)\n\ndef min_span_tree(adjacency_matrix, indices_to_connect):\n    \"\"\"\n    Use Prim algorithm to connect vertices within surface region\n\n    Parameters\n    ----------\n    indices_region : list of integers\n        indices of vertices of region in a surface mesh\n    indices_to_connect : list of integers\n        indices of vertices we wish to connect\n    adjacency_matrix : list of lists of integers\n        adjacency matrix of vertices\n\n    Examples\n    --------\n    >>> import os\n    >>> import numpy as np\n    >>> import networkx as nx\n    >>> from mindboggle.utils.io_vtk import read_vtk, rewrite_scalars\n    >>> from mindboggle.utils.mesh import find_neighbors, remove_faces\n    >>> from mindboggle.utils.min_span_tree import min_span_tree\n    >>> data_path = os.environ['MINDBOGGLE_DATA']\n    >>> sulci_file = os.path.join(data_path, 'arno', 'features', 'sulci.vtk')\n    >>> faces, lines, indices, points, npoints, sulci, name, input_vtk = read_vtk(sulci_file)\n    >>> sulcus_ID = 1\n    >>> sulcus_indices = [i for i,x in enumerate(sulci) if x == sulcus_ID]\n    >>> sulcus_faces = remove_faces(faces, sulcus_indices)\n    >>> sulcus_neighbor_lists = find_neighbors(sulcus_faces, npoints)\n    >>> G=nx.Graph()\n    >>> G.add_nodes_from(sulcus_indices)\n    >>> for i, sulcus_neighbor_list in enumerate(sulcus_neighbor_lists):\n    >>>     G.add_edges_from([[i,x] for x in sulcus_neighbor_list])\n    >>> adjacency_matrix = nx.adjacency_matrix(G, nodelist=None, weight='weight')\n    >>> indices_to_connect = [0, len(sulcus_indices)-1]\n    >>> adjacency_matrix2, W, Path, Degree, TreeNbr = min_span_tree(adjacency_matrix, indices_to_connect)\n    >>> # Write results to vtk file and view:\n    >>> MST = np.zeros(len(points))\n    >>> MST[W] = 1\n    >>> rewrite_scalars(sulci_file, 'test_min_span_tree.vtk', MST, 'MST', MST)\n    >>> from mindboggle.utils.plots import plot_surfaces\n    >>> plot_surfaces('test_min_span_tree.vtk')\n\n    \"\"\"\n\n    if len(indices_to_connect) > 1:\n        Root = indices_to_connect[0]\n        M = Prim(adjacency_matrix, Root)\n        adjacency_matrix, W, Path, Degree, TreeNbr = M.mst_prim(adjacency_matrix,\n            [Root], [], M.degree, M.tree_nbr)\n\n        return W, Path, Degree, TreeNbr\n\n# Example use of the minimum spanning tree algorithm\nif __name__ == \"__main__\" :\n    import os\n    import networkx as nx\n    from mindboggle.utils.io_vtk import read_vtk, rewrite_scalars\n    from mindboggle.utils.mesh import find_neighbors, remove_faces\n    from mindboggle.utils.mesh import min_span_tree\n    from mindboggle.utils.plots import plot_surfaces\n    data_path = os.environ['MINDBOGGLE_DATA']\n    sulci_file = os.path.join(data_path, 'arno', 'features', 'sulci.vtk')\n    faces, lines, indices, points, npoints, sulci, name, input_vtk = read_vtk(sulci_file)\n    sulcus_ID = 1\n    sulcus_indices = [i for i,x in enumerate(sulci) if x == sulcus_ID]\n    sulcus_faces = remove_faces(faces, sulcus_indices)\n    sulcus_neighbor_lists = find_neighbors(sulcus_faces, len(points))\n    G=nx.Graph()\n    G.add_nodes_from(sulcus_indices)\n    for i, sulcus_neighbor_list in enumerate(sulcus_neighbor_lists):\n        G.add_edges_from([[i,x] for x in sulcus_neighbor_list])\n    adjacency_matrix = nx.adjacency_matrix(G, nodelist=None, weight='weight')\n    indices_to_connect = [0, len(sulcus_indices)-1]\n    adjacency_matrix2, W, Path, Degree, TreeNbr = min_span_tree(adjacency_matrix,\n                                                                indices_to_connect)\n\n    # Write results to vtk file and view:\n    MST = np.zeros(len(points))\n    MST[W] = 1\n    rewrite_scalars(sulci_file, 'test_min_span_tree.vtk', MST, 'MST', MST)\n\n    Terminal, Branching = [], []\n    for vtx in xrange(0,Num):\n        if Degree[vtx] ==1:\n            Terminal.append(vtx)\n        elif Degree[vtx] > 2:\n            Branching.append(vtx)\n\n    Path, NodeColor = prune(Path, Degree, TreeNbr, Terminal, Branching, indices_to_connect, sulcus_indices)\n\n    #endpoints = [i for i,x in enumerate(Degree) if x == 1]\n", "path.join(os.path.dirname(os.path.abspath(__file__)), 'bin', 'TestStack.White.dll')\nclr.AddReference('System')\nclr.AddReference(DLL_PATH)\nfrom System.Windows.Automation import AutomationElement, ControlType    # noqa: E402\nfrom TestStack.White.UIItems.Finders import SearchCriteria    # noqa: E402\nfrom TestStack.White.UIItems import UIItem    # noqa: E402\nfrom WhiteLibrary.keywords import (ApplicationKeywords, KeyboardKeywords, MouseKeywords,\n                                   WindowKeywords, ScreenshotKeywords, WhiteConfigurationKeywords)    # noqa: E402\nfrom WhiteLibrary.keywords.items import (ButtonKeywords,\n                                         LabelKeywords,\n                                         ListKeywords,\n                                         ListViewKeywords,\n                                         MenuKeywords,\n                                         ProgressbarKeywords,\n                                         SliderKeywords,\n                                         TabKeywords,\n                                         ToolStripKeywords,\n                                         TreeKeywords,\n                                         TextBoxKeywords,\n                                         UiItemKeywords)   # noqa: E402\nfrom WhiteLibrary.keywords.robotlibcore import DynamicCore   # noqa: E402\nfrom WhiteLibrary.errors import ItemNotFoundError   # noqa: E402\nfrom WhiteLibrary import version   # noqa: E402\n\n\nSTRATEGIES = dict(id={\"method\": \"ByAutomationId\"},  # noqa: C408\n                  text={\"method\": \"ByText\"},\n                  index={\"method\": \"Indexed\"},\n                  help_text={\"method\": \"ByNativeProperty\", \"property\": \"HelpTextProperty\"},\n                  class_name={\"method\": \"ByClassName\"},\n                  control_type={\"method\": \"ByControlType\"})\n\n\nclass WhiteLibrary(DynamicCore):\n    \"\"\"WhiteLibrary is a Robot Framework library for automating Windows GUI.\n\n    It is a wrapper for [https://github.com/TestStack/White | TestStack.White] automation framework, which is based on\n    [https://docs.microsoft.com/en-us/windows/desktop/WinAuto/entry-uiauto-win32 | Microsoft UI Automation API] (UIA).\n\n    = Applications and windows =\n    To interact with UI items, the correct application and window must be attached to WhiteLibrary.\n\n    When application is started with `Launch Application`, the keyword also attaches the application to WhiteLibrary.\n    Attaching a running application is done with `Attach Application By Name` or `Attach Application By Id`.\n\n    Once the application is attached, the window to interact with is attached with `Attach Window`.\n\n    Examples:\n    | # Launch application, no separate step for attaching application needed | |\n    | `Launch Application` | C:/myApplication.exe |\n    | `Attach Window`      | Main window |\n    | | |\n    | # Switch to an application that is already running | |\n    | `Attach Application By Name` | calc1 |\n    | `Attach Window`              | Calculator |\n\n    = UI items =\n    WhiteLibrary uses the same names for UI items (=controls) as White.\n    See [https://teststackwhite.readthedocs.io/en/latest/UIItems | White's documentation] for details about mapping\n    UIA control types to White's UI item classes.\n\n    For example, the UIA control type ``Text`` maps to the ``Label`` class in White (e.g. in WhiteLibrary's keyword `Verify Label`).\n\n    == Item locators ==\n    Keywords that access UI items (e.g. `Click Button`) use a ``locator`` argument.\n    The locator consists of a locator prefix that specifies the search criteria, and the locator value.\n\n    Locator syntax is ``prefix:value``.\n    The following locator prefixes are available:\n\n    | = Prefix =        | = Description =                    |\n    | id (or no prefix) | Search by AutomationID. If no prefix is given, the item is searched by AutomationID by default. |\n    | text              | Search by exact item text or name. |\n    | index             | Search by item index.              |\n    | help_text         | Search by HelpTextProperty.        |\n    | class_name        | Search by class name.              |\n    | control_type      | Search by control type.            |\n    | partial_text      | Search by text that the item text/name contains. |\n\n    Examples:\n    | `Click Button` | myButton         | # clicks button by its AutomationID |\n    | `Click Button` | id:myButton      | # clicks button by its AutomationID |\n    | `Click Button` | text:Click here! | # clicks button by the button text  |\n    | `Click Button` | index:2          | # clicks button whose index is 2    |\n\n    *Note:* Old locator syntax ``prefix=value`` is also valid but it is recommended to use the ``prefix:value`` syntax\n    since the old syntax *will be deprecated* in the future.\n\n    == Item object as a locator ==\n    It is also possible to use an item object reference as the ``locator`` value.\n    An item object can be obtained with e.g. `Get Item` or `Get Items` keywords.\n\n    The need to use an item object reference can arise for instance when multiple items match the same locator\n    and one of the items is selected for further action.\n    When using an item object, the action on the item can be executed regardless of the window it is in,\n    i.e. the window where the item is located does not necessarily need to be attached.\n    However, this does not change the attached window and the operation continues in the attached window after action on\n    the referred item is complete.\n\n    Example using item object:\n    | @{my_buttons}= | `Get Items`         | class_name:MyButtonClass |\n    | `Click Button` | ${my_buttons[2]}    | # clicks button object at index 2 of the list |\n\n    = Workflow example =\n    | ***** Variables *****   | | | |\n    | ${TEST APPLICATION}     | C:/path/to/my_application.exe | | |\n    | | | | |\n    | ***** Settings *****    | | | |\n    | Library | WhiteLibrary  | | |\n    | | | | |\n    | ***** Test Cases *****  | | | |\n    | Small Example           | | | |\n    | | Launch Application    | ${TEST APPLICATION} | |\n    | | Attach Window         | Window Title        | |\n    | | Button Text Should Be | my_button           | press this button |\n    | | Click Button          | my_button | |\n    | | Close Application     | | |\n\n    = Waiting and timeouts =\n    White handles a lot of the required waiting automatically, including waiting while the window is busy and\n    waiting for a window to appear.\n\n    White's internal waits use timeouts that can be read and configured with keywords:\n    - BusyTimeout defines how long to wait while the window is busy,\n      see `Get White Busy Timeout`, `Set White Busy Timeout`\n    - FindWindowTimeout defines how long to wait until the specified window is found,\n     see `Get White Find Window Timeout`, `Set White Find Window Timeout`.\n\n    In situations that require additional waiting for UI items, see keywords `Wait Until Item Exists`\n    and `Wait Until Item Does Not Exist`.\n\n    \"\"\"\n\n    ROBOT_LIBRARY_VERSION = version.VERSION\n    ROBOT_LIBRARY_SCOPE = \"Global\"\n    ROBOT_LISTENER_API_VERSION = 2\n\n    def __init__(self, screenshot_dir=None):\n        \"\"\"WhiteLibrary can be imported with an optional argument ``screenshot_dir``.\n\n        ``screenshot_dir`` is the directory where screenshots taken by WhiteLibrary are saved.\n        If the given directory does not already exist, it will be created when the first screenshot is taken.\n        The directory can also be set at runtime with `Set Screenshot Directory`.\n        If the argument is not given, the default location for screenshots is the output directory of the Robot run,\n        i.e. the directory where output and log files are generated.\n        \"\"\"\n        self.app = None\n        self.window = None\n        self.screenshooter = None\n        self.ROBOT_LIBRARY_LISTENER = self  # pylint: disable=invalid-name\n        self.screenshots_enabled = True\n        self.libraries = [ApplicationKeywords(self),\n                          ButtonKeywords(self),\n                          KeyboardKeywords(self),\n                          LabelKeywords(self),\n                          ListKeywords(self),\n                          ListViewKeywords(self),\n                          MenuKeywords(self),\n                          MouseKeywords(self),\n                          ProgressbarKeywords(self),\n                          SliderKeywords(self),\n                          TabKeywords(self),\n                          WhiteConfigurationKeywords(self),\n                          TextBoxKeywords(self),\n                          ToolStripKeywords(self),\n                          TreeKeywords(self),\n                          UiItemKeywords(self),\n                          WindowKeywords(self),\n                          ScreenshotKeywords(self, screenshot_dir)]\n        self._running_keyword = None\n        self._running_on_failure_keyword = False\n        DynamicCore.__init__(self, self.libraries)\n\n    def run_keyword(self, name, args, kwargs):  # pylint: disable=signature-differs\n        \"\"\"Reimplemtation of run_keyword.\n\n        calls robot framework's own implementation but handles screenshots if/when exceptions are triggered.\n        \"\"\"\n        self._running_keyword = name\n        try:\n            return DynamicCore.run_keyword(self, name, args, kwargs)\n        except Exception:\n            self._failure_occurred()\n            raise\n        finally:\n            self._running_keyword = None\n\n    def _failure_occurred(self):\n        # this if-guard here is to prevent recursion if there's\n        # error in taking of a screenshot.\n        # Might be safe to remove\n        if self._running_on_failure_keyword:\n            return\n        try:\n            self._running_on_failure_keyword = True\n            if self.screenshots_enabled:\n                self.screenshooter.take_desktop_screenshot()\n        finally:\n            self._running_on_failure_keyword = False\n\n    def _get_typed_item_by_locator(self, item_type, locator):\n        if isinstance(locator, UIItem):\n            if not isinstance(locator, item_type):\n                raise TypeError(\"Item object was not of the expected type\")\n            return locator\n\n        search_strategy, locator_value = self._parse_locator(locator)\n\n        if search_strategy == \"partial_text\":\n            return self._get_item_by_partial_text(locator_value, item_type)\n\n        search_criteria = self._get_search_criteria(search_strategy, locator_value)\n        return self.window.Get[item_type](search_criteria)\n\n    def _get_item_by_locator(self, locator):\n        if isinstance(locator, UIItem):\n            return locator\n\n        search_strategy, locator_value = self._parse_locator(locator)\n\n        if search_strategy == \"partial_text\":\n            return self._get_item_by_partial_text(locator_value)\n\n        search_criteria = self._get_search_criteria(search_strategy, locator_value)\n        return self.window.Get(search_criteria)\n\n    def _get_multiple_items_by_locator(self, locator):\n        search_strategy, locator_value = self._parse_locator(locator)\n\n        if search_strategy == \"partial_text\":\n            return list(self._get_multiple_items_by_partial_text(locator_value))\n\n        search_criteria = self._get_search_criteria(search_strategy, locator_value)\n        return self.window.GetMultiple(search_criteria)\n\n    def _get_item_by_partial_text(self, partial_text, item_type=None):\n        items = self._get_multiple_items_by_partial_text(partial_text)\n        try:\n            if item_type is None:\n                return next(items)\n            return next((item for item in items if item.GetType() == clr.GetClrType(item_type)))\n        except StopIteration:\n            raise ItemNotFoundError(u\"Item with partial text '{}' was not found\".format(partial_text))\n\n    def _get_multiple_items_by_partial_text(self, partial_text):\n        items = self.window.GetMultiple(SearchCriteria.All)\n        return (item for item in items if partial_text in item.Name)\n\n    @staticmethod\n    def _get_search_criteria(search_strategy, locator_value):\n        if search_strategy == \"index\":\n            locator_value = int(locator_value)\n\n        try:\n            search_method = STRATEGIES[search_strategy][\"method\"]\n        except KeyError:\n            raise ValueError(\"'{}' is not a valid locator prefix\".format(search_strategy))\n\n        if search_method == \"ByNativeProperty\":\n            property_name = STRATEGIES[search_strategy][\"property\"]\n            property_name = getattr(AutomationElement, property_name)\n            search_params = (property_name, locator_value)\n        else:\n            if search_method == \"ByControlType\":\n                locator_value = getattr(ControlType, locator_value)\n            search_params = (locator_value,)\n\n        method = getattr(SearchCriteria, search_method)\n        return method(*search_params)\n\n    def _parse_locator(self, locator):\n        if \"=\" not in locator and \":\" not in locator:\n            locator = \"id:\" + locator\n        idx = self._get_locator_delimiter_index(locator)\n        return locator[:idx], locator[idx + 1:]\n\n    @staticmethod\n    def _get_locator_delimiter_index(locator):\n        if \"=\" not in locator:\n            return locator.index(\":\")\n        if \":\" not in locator:\n            return locator.index(\"=\")\n        return min(locator.index(\":\"), locator.index(\"=\"))\n\n    def _end_keyword(self, name, attrs):  # pylint: disable=unused-argument\n        pass\n\n    @staticmethod\n    def _contains_string_value(expected, actual, case_sensitive=True):\n        case_sensitive = is_truthy(case_sensitive)\n        expected_value = expected if case_sensitive else expected.upper()\n        actual_value = actual if case_sensitive else actual.upper()\n        if expected_value not in actual_value:\n            raise AssertionError(u\"Expected value {} not found in {}\".format(expected, actual))\n\n    @staticmethod\n    def _verify_string_value(expected, actual, case_sensitive=True):\n        case_sensitive = is_truthy(case_sensitive)\n        expected_value = expected if case_sensitive else expected.upper()\n        actual_value = actual if case_sensitive else actual.upper()\n        if expected_value != actual_value:\n            raise AssertionError(u\"Expected value {}, but found {}\".format(expected, actual))\n\n    @staticmethod\n    def _verify_value(expected, actual):\n        if expected != actual:\n            raise AssertionError(u\"Expected value {}, but found {}\".format(expected, actual))\n", " Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Benchmarks for `tf.data.experimental.rejection_resample()`.\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport time\n\nimport numpy as np\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\n\nfrom tensorflow.python.client import session\nfrom tensorflow.python.data.experimental.ops import resampling\nfrom tensorflow.python.data.ops import dataset_ops\nfrom tensorflow.python.platform import test\n\n\ndef _time_resampling(data_np, target_dist, init_dist, num_to_sample):  # pylint: disable=missing-docstring\n  dataset = dataset_ops.Dataset.from_tensor_slices(data_np).repeat()\n\n  # Reshape distribution via rejection sampling.\n  dataset = dataset.apply(\n      resampling.rejection_resample(\n          class_func=lambda x: x,\n          target_dist=target_dist,\n          initial_dist=init_dist,\n          seed=142))\n\n  options = dataset_ops.Options()\n  options.experimental_optimization.apply_default_optimizations = False\n  dataset = dataset.with_options(options)\n  get_next = dataset_ops.make_one_shot_iterator(dataset).get_next()\n\n  with session.Session() as sess:\n    start_time = time.time()\n    for _ in xrange(num_to_sample):\n      sess.run(get_next)\n    end_time = time.time()\n\n  return end_time - start_time\n\n\nclass RejectionResampleBenchmark(test.Benchmark):\n  \"\"\"Benchmarks for `tf.data.experimental.rejection_resample()`.\"\"\"\n\n  def benchmarkResamplePerformance(self):\n    init_dist = [0.25, 0.25, 0.25, 0.25]\n    target_dist = [0.0, 0.0, 0.0, 1.0]\n    num_classes = len(init_dist)\n    # We don't need many samples to test a dirac-delta target distribution\n    num_samples = 1000\n    data_np = np.random.choice(num_classes, num_samples, p=init_dist)\n\n    resample_time = _time_resampling(\n        data_np, target_dist, init_dist, num_to_sample=1000)\n\n    self.report_benchmark(iters=1000, wall_time=resample_time, name=\"resample\")\n\n\nif __name__ == \"__main__\":\n  test.main()\n", "ou may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests handlers defined within helpers.py.\"\"\"\n\nfrom datetime import datetime\nimport pytz\nimport unittest\n\nfrom bigquery_slots_monitoring import helpers\n\n\nclass DateStringToObjectUTCTest(unittest.TestCase):\n  \"\"\"Tests conversion of date string to date object in UTC timezone.\"\"\"\n\n  def testDateStringToObjectUTC(self):\n    date_string = '2017-12-27T00:00:00Z'\n    self.assertEqual(\n      helpers.date_string_to_object_utc(date_string),\n      pytz.utc.localize(datetime.strptime(date_string, '%Y-%m-%dT%H:%M:%SZ')))\n\n\nclass DateObjectToRFC3339Test(unittest.TestCase):\n  \"\"\"Tests extraction of RFC3339 compliant date string from date object.\"\"\"\n\n  def testDateObjectToRFC3339(self):\n    date = datetime(2017, 12, 27, 12, 0, 30)\n    self.assertEqual(helpers.date_object_to_rfc3339(date), '2017-12-27T12:00:30Z')\n", "sity.py\n# -----------------------------------------------------------------------------\n# Copyright (c) 2017 tropter authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n# not use this file except in compliance with the License. You may obtain a\n# copy of the License at http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# -----------------------------------------------------------------------------\nimport scipy.sparse\nimport pandas as pd\nimport sys\nimport numpy as np\nimport pylab as pl\n\ndf = pd.read_csv(sys.argv[1], skiprows=2)\n\nwith open(sys.argv[1]) as f:\n    line0 = f.readline()\n    num_rows = line0.split('=')[1]\n    line1 = f.readline()\n    num_cols = line1.split('=')[1]\n\nspmat = scipy.sparse.coo_matrix((np.ones_like(df.index),\n        (df['row_indices'], df['column_indices'])), shape=(num_rows, num_cols))\npl.spy(spmat, markersize=2, markeredgecolor='k', marker='.')\npl.show()\n", "License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom c7n_azure.provider import resources\nfrom c7n_azure.resources.arm import ArmResourceManager\nfrom c7n_azure.tags import TagHelper\n\nfrom c7n.actions import BaseAction\nfrom c7n.filters.core import ValueFilter, type_schema\nfrom c7n.filters.related import RelatedResourceFilter\n\nfrom c7n.filters.offhours import OffHour, OnHour\n\n\n@resources.register('vm')\nclass VirtualMachine(ArmResourceManager):\n\n    class resource_type(ArmResourceManager.resource_type):\n        service = 'azure.mgmt.compute'\n        client = 'ComputeManagementClient'\n        enum_spec = ('virtual_machines', 'list_all', None)\n        default_report_fields = (\n            'name',\n            'location',\n            'resourceGroup',\n            'properties.hardwareProfile.vmSize',\n        )\n\n\n@VirtualMachine.filter_registry.register('instance-view')\nclass InstanceViewFilter(ValueFilter):\n    schema = type_schema('instance-view', rinherit=ValueFilter.schema)\n\n    def __call__(self, i):\n        if 'instanceView' not in i:\n            client = self.manager.get_client()\n            instance = (\n                client.virtual_machines\n                .get(i['resourceGroup'], i['name'], expand='instanceview')\n                .instance_view\n            )\n            i['instanceView'] = instance.serialize()\n\n        return super(InstanceViewFilter, self).__call__(i['instanceView'])\n\n\n@VirtualMachine.filter_registry.register('network-interface')\nclass NetworkInterfaceFilter(RelatedResourceFilter):\n\n    schema = type_schema('network-interface', rinherit=ValueFilter.schema)\n\n    RelatedResource = \"c7n_azure.resources.network_interface.NetworkInterface\"\n    RelatedIdsExpression = \"properties.networkProfile.networkInterfaces[0].id\"\n\n\n@VirtualMachine.action_registry.register('poweroff')\nclass VmPowerOffAction(BaseAction):\n\n    schema = type_schema('poweroff')\n\n    def __init__(self, data=None, manager=None, log_dir=None):\n        super(VmPowerOffAction, self).__init__(data, manager, log_dir)\n        self.client = self.manager.get_client()\n\n    def poweroff(self, resource_group, vm_name):\n        self.client.virtual_machines.power_off(resource_group, vm_name)\n\n    def process(self, vms):\n        for vm in vms:\n            self.poweroff(vm['resourceGroup'], vm['name'])\n\n\n@VirtualMachine.action_registry.register('stop')\nclass VmStopAction(BaseAction):\n\n    schema = type_schema('stop')\n\n    def __init__(self, data=None, manager=None, log_dir=None):\n        super(VmStopAction, self).__init__(data, manager, log_dir)\n        self.client = self.manager.get_client()\n\n    def stop(self, resource_group, vm_name):\n        self.client.virtual_machines.deallocate(resource_group, vm_name)\n\n    def process(self, vms):\n        for vm in vms:\n            self.stop(vm['resourceGroup'], vm['name'])\n\n\n@VirtualMachine.action_registry.register('start')\nclass VmStartAction(BaseAction):\n\n    schema = type_schema('start')\n\n    def __init__(self, data=None, manager=None, log_dir=None):\n        super(VmStartAction, self).__init__(data, manager, log_dir)\n        self.client = self.manager.get_client()\n\n    def start(self, resource_group, vm_name):\n        self.client.virtual_machines.start(resource_group, vm_name)\n\n    def process(self, vms):\n        for vm in vms:\n            self.start(vm['resourceGroup'], vm['name'])\n\n\n@VirtualMachine.action_registry.register('restart')\nclass VmRestartAction(BaseAction):\n\n    schema = type_schema('restart')\n\n    def __init__(self, data=None, manager=None, log_dir=None):\n        super(VmRestartAction, self).__init__(data, manager, log_dir)\n        self.client = self.manager.get_client()\n\n    def restart(self, resource_group, vm_name):\n        self.client.virtual_machines.restart(resource_group, vm_name)\n\n    def process(self, vms):\n        for vm in vms:\n            self.restart(vm['resourceGroup'], vm['name'])\n\n\n@VirtualMachine.filter_registry.register('offhour')\nclass AzureVMOffHour(OffHour):\n\n    # Override get_tag_value because Azure stores tags differently from AWS\n    def get_tag_value(self, i):\n        tag_value = TagHelper.get_tag_value(resource=i,\n                                       tag=self.tag_key,\n                                       utf_8=True)\n\n        if tag_value is not False:\n            tag_value = tag_value.lower().strip(\"'\\\"\")\n        return tag_value\n\n\n@VirtualMachine.filter_registry.register('onhour')\nclass AzureVMOnHour(OnHour):\n\n    # Override get_tag_value because Azure stores tags differently from AWS\n    def get_tag_value(self, i):\n        tag_value = TagHelper.get_tag_value(resource=i,\n                                       tag=self.tag_key,\n                                       utf_8=True)\n\n        if tag_value is not False:\n            tag_value = tag_value.lower().strip(\"'\\\"\")\n        return tag_value\n", "ou may\n# not use this file except in compliance with the License. You may obtain\n# a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations\n# under the License.\n\nimport mock\nfrom oslo_utils.fixture import uuidsentinel as uuids\n\nfrom nova import exception\nfrom nova import test\nfrom nova.tests import fixtures as nova_fixtures\nfrom nova.tests.functional import integrated_helpers\nfrom nova.tests.unit import fake_notifier\n\n\nclass FakeCinderError(object):\n    \"\"\"Poor man's Mock because we're stubbing out and not mock.patching. Stubs\n    out attachment_delete. We keep a raise and call count to simulate a single\n    volume error while being able to assert that we still got called for all\n    of an instance's volumes.\n    \"\"\"\n\n    def __init__(self):\n        self.raise_count = 0\n        self.call_count = 0\n\n    def __call__(self, *args, **kwargs):\n        self.call_count += 1\n        if self.raise_count == 0:\n            self.raise_count += 1\n            raise exception.CinderConnectionFailed(reason='Fake Cinder error')\n\n\nclass LiveMigrationCinderFailure(integrated_helpers._IntegratedTestBase):\n    # Default self.api to the self.admin_api as live migration is admin only\n    ADMIN_API = True\n    api_major_version = 'v2.1'\n    microversion = 'latest'\n\n    def setUp(self):\n        super(LiveMigrationCinderFailure, self).setUp()\n        fake_notifier.stub_notifier(self)\n        self.addCleanup(fake_notifier.reset)\n        # Start a second compute node (the first one was started for us by\n        # _IntegratedTestBase. set_nodes() is needed to avoid duplicate\n        # nodenames. See comments in test_bug_1702454.py.\n        self.compute2 = self.start_service('compute', host='host2')\n\n    def test_live_migrate_attachment_delete_fails(self):\n        server = self.api.post_server({\n            'server': {\n                'flavorRef': 1,\n                'imageRef': '155d900f-4e14-4e4c-a73d-069cbf4541e6',\n                'name': 'live-migrate-attachment-delete-fail-test',\n                'networks': 'none',\n                'block_device_mapping_v2': [\n                    {'boot_index': 0,\n                     'uuid': uuids.broken_volume,\n                     'source_type': 'volume',\n                     'destination_type': 'volume'},\n                    {'boot_index': 1,\n                     'uuid': uuids.working_volume,\n                     'source_type': 'volume',\n                     'destination_type': 'volume'}]}})\n        server = self._wait_for_state_change(server, 'ACTIVE')\n\n        source = server['OS-EXT-SRV-ATTR:host']\n        if source == self.compute.host:\n            dest = self.compute2.host\n        else:\n            dest = self.compute.host\n\n        post = {\n            'os-migrateLive': {\n                'host': dest,\n                'block_migration': False,\n            }\n        }\n        stub_attachment_delete = FakeCinderError()\n        self.stub_out('nova.volume.cinder.API.attachment_delete',\n                      stub_attachment_delete)\n        self.api.post_server_action(server['id'], post)\n        self._wait_for_server_parameter(server,\n                                        {'OS-EXT-SRV-ATTR:host': dest,\n                                         'status': 'ACTIVE'})\n        self.assertEqual(2, stub_attachment_delete.call_count)\n        self.assertEqual(1, stub_attachment_delete.raise_count)\n\n\nclass TestVolAttachmentsDuringLiveMigration(\n    integrated_helpers._IntegratedTestBase\n):\n    \"\"\"Assert the lifecycle of volume attachments during LM rollbacks\n    \"\"\"\n\n    # Default self.api to the self.admin_api as live migration is admin only\n    ADMIN_API = True\n    microversion = 'latest'\n\n    def _setup_compute_service(self):\n        self._start_compute('src')\n        self._start_compute('dest')\n\n    @mock.patch('nova.virt.fake.FakeDriver.live_migration')\n    def test_vol_attachments_during_driver_live_mig_failure(self, mock_lm):\n        \"\"\"Assert volume attachments during live migration rollback\n\n        * Mock live_migration to always rollback and raise a failure within the\n          fake virt driver\n        * Launch a boot from volume instance\n        * Assert that the volume is attached correctly to the instance\n        * Live migrate the instance to another host invoking the mocked\n          live_migration method\n        * Assert that the instance is still on the source host\n        * Assert that the original source host volume attachment remains\n        \"\"\"\n        # Mock out driver.live_migration so that we always rollback\n        def _fake_live_migration_with_rollback(\n                context, instance, dest, post_method, recover_method,\n                block_migration=False, migrate_data=None):\n            # Just call the recover_method to simulate a rollback\n            recover_method(context, instance, dest, migrate_data)\n            # raise test.TestingException here to imitate a virt driver\n            raise test.TestingException()\n        mock_lm.side_effect = _fake_live_migration_with_rollback\n\n        volume_id = nova_fixtures.CinderFixture.IMAGE_BACKED_VOL\n        server = self._build_server(\n            name='test_bfv_live_migration_failure', image_uuid='',\n            networks='none'\n        )\n        server['block_device_mapping_v2'] = [{\n            'source_type': 'volume',\n            'destination_type': 'volume',\n            'boot_index': 0,\n            'uuid': volume_id\n        }]\n        server = self.api.post_server({'server': server})\n        self._wait_for_state_change(server, 'ACTIVE')\n\n        # Fetch the source host for use later\n        server = self.api.get_server(server['id'])\n        src_host = server['OS-EXT-SRV-ATTR:host']\n\n        # Assert that the volume is connected to the instance\n        self.assertIn(\n            volume_id, self.cinder.volume_ids_for_instance(server['id']))\n\n        # Assert that we have an active attachment in the fixture\n        attachments = self.cinder.volume_to_attachment.get(volume_id)\n        self.assertEqual(1, len(attachments))\n\n        # Fetch the attachment_id for use later once we have migrated\n        src_attachment_id = list(attachments.keys())[0]\n\n        # Migrate the instance and wait until the migration errors out thanks\n        # to our mocked version of live_migration raising TestingException\n        self._live_migrate(server, 'error', server_expected_state='ERROR')\n\n        # Assert that we called the fake live_migration method\n        mock_lm.assert_called_once()\n\n        # Assert that the instance is on the source\n        server = self.api.get_server(server['id'])\n        self.assertEqual(src_host, server['OS-EXT-SRV-ATTR:host'])\n\n        # Assert that the src attachment is still present\n        attachments = self.cinder.volume_to_attachment.get(volume_id)\n        self.assertIn(src_attachment_id, attachments.keys())\n        self.assertEqual(1, len(attachments))\n\n\nclass LiveMigrationNeutronInteractionsTest(\n        integrated_helpers._IntegratedTestBase):\n    # NOTE(artom) We need the admin API to force the host when booting the test\n    # server.\n    ADMIN_API = True\n    microversion = 'latest'\n\n    def _setup_compute_service(self):\n        self._start_compute('src')\n        self._start_compute('dest')\n\n    def test_live_migrate_vifs_from_info_cache(self):\n        \"\"\"Test that bug 1879787 can no longer manifest itself because we get\n        the network_info from the instance info cache, and not Neutron.\n        \"\"\"\n        def stub_notify(context, instance, event_suffix,\n                        network_info=None, extra_usage_info=None, fault=None):\n            vif = network_info[0]\n            # Make sure we have the correct VIF (the NeutronFixture\n            # deterministically uses port_2 for networks=auto) and that the\n            # profile does not contain `migrating_to`, indicating that we did\n            # not obtain it from the Neutron API.\n            self.assertEqual(self.neutron.port_2['id'], vif['id'])\n            self.assertNotIn('migrating_to', vif['profile'])\n\n        server = self._create_server(networks='auto',\n                                     host=self.computes['src'].host)\n\n        with mock.patch.object(self.computes['src'].manager,\n                              '_notify_about_instance_usage',\n                              side_effect=stub_notify) as mock_notify:\n            self._live_migrate(server, 'completed')\n            server = self.api.get_server(server['id'])\n            self.assertEqual('dest', server['OS-EXT-SRV-ATTR:host'])\n            # We don't care about call arguments here, we just want to be sure\n            # our stub actually got called.\n            mock_notify.assert_called()\n", "rce code\n\"\"\"\n\ntry:\n    import cPickle as pickle\nexcept:\n    import pickle\nimport time\nimport re\nimport logging\nimport thread\nimport random\nfrom gluon import current\nfrom gluon.cache import CacheAbstract\nfrom gluon.contrib.redis_utils import acquire_lock, release_lock\nfrom gluon.contrib.redis_utils import register_release_lock, RConnectionError\n\nlogger = logging.getLogger(\"web2py.cache.redis\")\n\nlocker = thread.allocate_lock()\n\n\ndef RedisCache(redis_conn=None, debug=False, with_lock=False, fail_gracefully=False, db=None):\n    \"\"\"\n    Usage example: put in models::\n\n    First of all install Redis\n    Ubuntu :\n    sudo apt-get install redis-server\n    sudo pip install redis\n\n    Then\n\n        from gluon.contrib.redis_utils import RConn\n        rconn = RConn()\n        from gluon.contrib.redis_cache import RedisCache\n        cache.redis = RedisCache(redis_conn=rconn, debug=True, with_lock=True)\n\n    Args:\n        redis_conn: a redis-like connection object\n        debug: if True adds to stats() the total_hits and misses\n        with_lock: sets the default locking mode for creating new keys.\n            By default is False (usualy when you choose Redis you do it\n            for performances reason)\n            When True, only one thread/process can set a value concurrently\n        fail_gracefully: if redis is unavailable, returns the value computing it\n            instead of raising an exception\n\n    It can be used pretty much the same as cache.ram()\n    When you use cache.redis directly you can use :\n\n        redis_key_and_var_name = cache.redis('redis_key_and_var_name', lambda or function,\n                                             time_expire=time.time(), with_lock=True)\n\n    to enforce locking. The with_lock parameter overrides the one set in the\n    cache.redis instance creation\n\n    cache.redis.stats()\n        returns a dictionary with statistics of Redis server\n        with one additional key ('w2p_keys') showing all keys currently set\n        from web2py with their TTL\n\n    A little wording on how keys are stored (and why the cache_it() function\n    and the clear() one look a little bit convoluted): there are a lot of\n    libraries that just store values and then use the KEYS command to delete it.\n    Until recent releases of this module, that technique was used here too.\n    In the need of deleting specific keys in a database with zillions keys in it\n    (other web2py apps, other applications in the need of a Redis stack) the\n    KEYS command is slow (it needs to scan every key in the database).\n    So, we use Redis 'sets' to store keys in \"buckets\"...\n    - every key created gets \"indexed\" in a bucket\n    - all buckets are indexed in a fixed key that never expires\n    - all keys generated within the same minute go in the same bucket\n    - every bucket is then set to expire when every key within it is expired\n    When we need to clear() cached keys:\n    - we tell Redis to SUNION all buckets\n       - gives us just the keys that are not expired yet\n    - buckets that are expired are removed from the fixed set\n    - we scan the keys and then delete them\n    \"\"\"\n\n    locker.acquire()\n    try:\n        instance_name = 'redis_instance_' + current.request.application\n        if not hasattr(RedisCache, instance_name):\n            setattr(RedisCache, instance_name,\n                    RedisClient(redis_conn=redis_conn, debug=debug,\n                                with_lock=with_lock, fail_gracefully=fail_gracefully))\n        return getattr(RedisCache, instance_name)\n    finally:\n        locker.release()\n\n\nclass RedisClient(object):\n\n    meta_storage = {}\n    MAX_RETRIES = 5\n    RETRIES = 0\n\n    def __init__(self, redis_conn=None, debug=False,\n                 with_lock=False, fail_gracefully=False):\n        self.request = current.request\n        self.debug = debug\n        self.with_lock = with_lock\n        self.fail_gracefully = fail_gracefully\n        self.prefix = \"w2p:cache:%s:\" % self.request.application\n        if self.request:\n            app = self.request.application\n        else:\n            app = ''\n\n        if app not in self.meta_storage:\n            self.storage = self.meta_storage[app] = {\n                CacheAbstract.cache_stats_name: {\n                    'hit_total': 0,\n                    'misses': 0,\n                }}\n        else:\n            self.storage = self.meta_storage[app]\n\n        self.cache_set_key = 'w2p:%s:___cache_set' % self.request.application\n\n        self.r_server = redis_conn\n        self._release_script = register_release_lock(self.r_server)\n\n    def initialize(self):\n        pass\n\n    def __call__(self, key, f, time_expire=300, with_lock=None):\n        if with_lock is None:\n            with_lock = self.with_lock\n        if time_expire is None:\n            time_expire = 24 * 60 * 60\n        newKey = self.__keyFormat__(key)\n        value = None\n        ttl = 0\n        try:\n            if f is None:\n                # delete and never look back\n                self.r_server.delete(newKey)\n                return None\n            # is there a value\n            obj = self.r_server.get(newKey)\n            # what's its ttl\n            if obj:\n                ttl = self.r_server.ttl(newKey)\n            if ttl > time_expire:\n                obj = None\n            if obj:\n                # was cached\n                if self.debug:\n                    self.r_server.incr('web2py_cache_statistics:hit_total')\n                value = pickle.loads(obj)\n            else:\n                # naive distributed locking\n                if with_lock:\n                    lock_key = '%s:__lock' % newKey\n                    randomvalue = time.time()\n                    al = acquire_lock(self.r_server, lock_key, randomvalue)\n                    # someone may have computed it\n                    obj = self.r_server.get(newKey)\n                    if obj is None:\n                        value = self.cache_it(newKey, f, time_expire)\n                    else:\n                        value = pickle.loads(obj)\n                    release_lock(self, lock_key, al)\n                else:\n                    # without distributed locking\n                    value = self.cache_it(newKey, f, time_expire)\n            return value\n        except RConnectionError:\n            return self.retry_call(key, f, time_expire, with_lock)\n\n    def cache_it(self, key, f, time_expire):\n        if self.debug:\n            self.r_server.incr('web2py_cache_statistics:misses')\n        cache_set_key = self.cache_set_key\n        expire_at = int(time.time() + time_expire) + 120\n        bucket_key = \"%s:%s\" % (cache_set_key, expire_at / 60)\n        value = f()\n        value_ = pickle.dumps(value, pickle.HIGHEST_PROTOCOL)\n        if time_expire == 0:\n            time_expire = 1\n        self.r_server.setex(key, time_expire, value_)\n        # print '%s will expire on %s: it goes in bucket %s' % (key, time.ctime(expire_at))\n        # print 'that will expire on %s' % (bucket_key, time.ctime(((expire_at / 60) + 1) * 60))\n        p = self.r_server.pipeline()\n        # add bucket to the fixed set\n        p.sadd(cache_set_key, bucket_key)\n        # sets the key\n        p.setex(key, time_expire, value_)\n        # add the key to the bucket\n        p.sadd(bucket_key, key)\n        # expire the bucket properly\n        p.expireat(bucket_key, ((expire_at / 60) + 1) * 60)\n        p.execute()\n        return value\n\n    def retry_call(self, key, f, time_expire, with_lock):\n        self.RETRIES += 1\n        if self.RETRIES <= self.MAX_RETRIES:\n            logger.error(\"sleeping %s seconds before reconnecting\" % (2 * self.RETRIES))\n            time.sleep(2 * self.RETRIES)\n            if self.fail_gracefully:\n                self.RETRIES = 0\n                return f()\n            return self.__call__(key, f, time_expire, with_lock)\n        else:\n            self.RETRIES = 0\n            if self.fail_gracefully:\n                return f\n            raise RConnectionError('Redis instance is unavailable')\n\n    def increment(self, key, value=1):\n        try:\n            newKey = self.__keyFormat__(key)\n            return self.r_server.incr(newKey, value)\n        except RConnectionError:\n            return self.retry_increment(key, value)\n\n    def retry_increment(self, key, value):\n        self.RETRIES += 1\n        if self.RETRIES <= self.MAX_RETRIES:\n            logger.error(\"sleeping some seconds before reconnecting\")\n            time.sleep(2 * self.RETRIES)\n            return self.increment(key, value)\n        else:\n            self.RETRIES = 0\n            raise RConnectionError('Redis instance is unavailable')\n\n    def clear(self, regex):\n        \"\"\"\n        Auxiliary function called by `clear` to search and\n        clear cache entries\n        \"\"\"\n        r = re.compile(regex)\n        # get all buckets\n        buckets = self.r_server.smembers(self.cache_set_key)\n        # get all keys in buckets\n        if buckets:\n            keys = self.r_server.sunion(buckets)\n        else:\n            return\n        prefix = self.prefix\n        pipe = self.r_server.pipeline()\n        for a in keys:\n            if r.match(str(a).replace(prefix, '', 1)):\n                pipe.delete(a)\n        if random.randrange(0, 100) < 10:\n            # do this just once in a while (10% chance)\n            self.clear_buckets(buckets)\n        pipe.execute()\n\n    def clear_buckets(self, buckets):\n        p = self.r_server.pipeline()\n        for b in buckets:\n            if not self.r_server.exists(b):\n                p.srem(self.cache_set_key, b)\n        p.execute()\n\n    def delete(self, key):\n        newKey = self.__keyFormat__(key)\n        return self.r_server.delete(newKey)\n\n    def stats(self):\n        stats_collector = self.r_server.info()\n        if self.debug:\n            stats_collector['w2p_stats'] = dict(\n                hit_total=self.r_server.get(\n                    'web2py_cache_statistics:hit_total'),\n                misses=self.r_server.get('web2py_cache_statistics:misses')\n            )\n        stats_collector['w2p_keys'] = dict()\n\n        for a in self.r_server.keys(\"w2p:%s:*\" % (\n                self.request.application)):\n            stats_collector['w2p_keys'][\"%s_expire_in_sec\" % a] = self.r_server.ttl(a)\n        return stats_collector\n\n    def __keyFormat__(self, key):\n        return '%s%s' % (self.prefix, key.replace(' ', '_'))\n", "o):\n        if (proto.task_info.command.value and\n            proto.task_info.command.value.startswith('{')):\n            # Hack by duendex. The command is in fact a serialized json\n            # containing the Docker image and the actual command to run.\n            command = proto.task_info.command\n            params = json.loads(command.value)\n            command.value = params['command']\n            command.container.image = params['image']\n            command.container.options.extend(params.get('options', []))\n            for k, v in params.get('envs', {}).iteritems():\n                command.environment.variables.add(name=k, value=v)\n        underlying = LaunchProto(proto)\n        self._underlying = underlying\n        _Struct.__init__(self, executor_id=underlying.executor_id(),\n                               container_id=underlying.container_id(),\n                               container=underlying.container(),\n                               argv=underlying.argv(),\n                               env=underlying.env(),\n                               uris=underlying.uris(),\n                               ports=underlying.ports(),\n                               cpu_and_mem=underlying.cpu_and_mem(),\n                               directory=underlying.directory(),\n                               user=underlying.user(),\n                               needs_observer=underlying.needs_observer())\n\n\nclass LaunchProto(object):\n\n    \"\"\"Wraps launch proto to simplify handling of format variations\n\n    For example, the resources can be in either the task_info or the\n    executor_info.\n    \"\"\"\n\n    def __init__(self, proto):\n        self.proto = proto\n\n    def executor(self):\n        if self.proto.HasField(\"task_info\"):\n            return None\n        if self.proto.HasField(\"executor_info\"):\n            return self.proto.executor_info\n        if self.proto.task_info.HasField(\"executor\"):\n            return self.proto.task_info.executor\n\n    def command(self):\n        if self.executor() is not None:\n            return self.executor().command\n        else:\n            return self.proto.task_info.command\n\n    def container(self):\n        if self.command().HasField(\"container\"):\n            container = self.command().container\n            return container.image, list(container.options)\n        return \"docker:///\", []\n\n    def resources(self):\n        # NB: We only want the executor resources when there is no task.\n        if self.proto.HasField(\"task_info\"):\n            return self.proto.task_info.resources\n        else:\n            return self.executor().resources\n\n    def executor_id(self):\n        if self.executor() is not None:\n            return self.executor().executor_id.value\n        else:\n            return self.proto.task_info.task_id.value\n\n    def container_id(self):\n        return self.proto.container_id.value\n\n    def cpu_and_mem(self):\n        cpu, mem = None, None\n        for r in self.resources():\n            if r.name == \"cpus\":\n                cpu = str(int(r.scalar.value * 1024))\n            if r.name == \"mem\":\n                mem = str(int(r.scalar.value)) + \"m\"\n        return (cpu, mem)\n\n    def env(self):\n        cmd = self.command()\n        return [(_.name, _.value) for _ in cmd.environment.variables]\n\n    def ports(self):\n        resources = [_.ranges.range for _ in self.resources()\n                                           if _.name == 'ports']\n        ranges = [_ for __ in resources for _ in __]\n        # NB: Casting long() to int() so there's no trailing 'L' in later\n        #     stringifications. Ports should only ever be shorts, anyways.\n        ports = [range(int(_.begin), int(_.end) + 1) for _ in ranges]\n        return [port for r in ports for port in r]\n\n    def argv(self):\n        cmd = self.command()\n        if cmd.HasField(\"value\") and cmd.value != \"\":\n            if cmd.value.startswith('[\"'):\n                return json.loads(cmd.value)\n            else:\n                return [\"sh\", \"-c\", cmd.value]\n        return []\n\n    def uris(self):\n        return list(self.command().uris)\n\n    def needs_observer(self):\n        return self.executor() is None\n\n    def user(self):\n        if self.proto.HasField(\"user\"):\n            return self.proto.user\n\n    def directory(self):\n        if self.proto.HasField(\"directory\"):\n            return self.proto.directory\n", "e in open(filepath, 'r'):\n        if (line.startswith('(') or line == '\\n'):\n            print(\"header skipped\")\n        else:\n            row = parseline(line)\n            result.append(row)\n    return result\n\n\ndef parseline(line):\n    line = line.replace(' ', '')\n    line = line.replace('\\n', '')\n    line = line.replace(';', '')\n    line = line.split(':')[1]\n    rawattr = line.split(',')\n    size = rawattr[0]\n    occupied = rawattr[1]\n    price = rawattr[2]\n    music = rawattr[3]\n    location = rawattr[4]\n    vip = rawattr[5]\n    beer = rawattr[6]\n    enjoyed = rawattr[7]\n    result = Row(size, occupied, price, music, location, vip, beer, enjoyed)\n    return result", "cense, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n\nfrom oslo_log import log\n\nfrom manila.common import constants\nfrom manila import utils\n\nLOG = log.getLogger(__name__)\n\n\nclass ShareSnapshotInstanceAccess(object):\n\n    def __init__(self, db, driver):\n        self.db = db\n        self.driver = driver\n\n    def update_access_rules(self, context, snapshot_instance_id,\n                            delete_all_rules=False, share_server=None):\n        \"\"\"Update driver and database access rules for given snapshot instance.\n\n        :param context: current context\n        :param snapshot_instance_id: Id of the snapshot instance model\n        :param delete_all_rules: Whether all rules should be deleted.\n        :param share_server: Share server model or None\n        \"\"\"\n        snapshot_instance = self.db.share_snapshot_instance_get(\n            context, snapshot_instance_id, with_share_data=True)\n        snapshot_id = snapshot_instance['snapshot_id']\n\n        @utils.synchronized(\n            \"update_access_rules_for_snapshot_%s\" % snapshot_id, external=True)\n        def _update_access_rules_locked(*args, **kwargs):\n            return self._update_access_rules(*args, **kwargs)\n\n        _update_access_rules_locked(\n            context=context,\n            snapshot_instance=snapshot_instance,\n            delete_all_rules=delete_all_rules,\n            share_server=share_server,\n        )\n\n    def _update_access_rules(self, context, snapshot_instance,\n                             delete_all_rules=None, share_server=None):\n\n        # NOTE(ganso): First let's get all the rules and the mappings.\n\n        rules = self.db.share_snapshot_access_get_all_for_snapshot_instance(\n            context, snapshot_instance['id'])\n\n        add_rules = []\n        delete_rules = []\n\n        if delete_all_rules:\n            # NOTE(ganso): We want to delete all rules.\n            delete_rules = rules\n            rules_to_be_on_snapshot = []\n            # NOTE(ganso): We select all deletable mappings.\n            for rule in rules:\n                # NOTE(ganso): No need to update the state if already set.\n                if rule['state'] != constants.ACCESS_STATE_DENYING:\n                    self.db.share_snapshot_instance_access_update(\n                        context, rule['access_id'], snapshot_instance['id'],\n                        {'state': constants.ACCESS_STATE_DENYING})\n\n        else:\n\n            # NOTE(ganso): error'ed rules are to be left alone until\n            # reset back to \"queued_to_deny\" by API.\n            rules_to_be_on_snapshot = [\n                r for r in rules if r['state'] not in (\n                    constants.ACCESS_STATE_QUEUED_TO_DENY,\n                    # NOTE(ganso): We select denying rules as a recovery\n                    # mechanism for invalid rules during a restart.\n                    constants.ACCESS_STATE_DENYING,\n                    # NOTE(ganso): We do not re-send error-ed access rules to\n                    # drivers.\n                    constants.ACCESS_STATE_ERROR\n                )\n            ]\n\n            # NOTE(ganso): Process queued rules\n            for rule in rules:\n                # NOTE(ganso): We are barely handling recovery, so if any rule\n                # exists in 'applying' or 'denying' state, we add them again.\n                if rule['state'] in (constants.ACCESS_STATE_QUEUED_TO_APPLY,\n                                     constants.ACCESS_STATE_APPLYING):\n                    if rule['state'] == (\n                            constants.ACCESS_STATE_QUEUED_TO_APPLY):\n                        self.db.share_snapshot_instance_access_update(\n                            context, rule['access_id'],\n                            snapshot_instance['id'],\n                            {'state': constants.ACCESS_STATE_APPLYING})\n                    add_rules.append(rule)\n                elif rule['state'] in (\n                        constants.ACCESS_STATE_QUEUED_TO_DENY,\n                        constants.ACCESS_STATE_DENYING):\n                    if rule['state'] == (\n                            constants.ACCESS_STATE_QUEUED_TO_DENY):\n                        self.db.share_snapshot_instance_access_update(\n                            context, rule['access_id'],\n                            snapshot_instance['id'],\n                            {'state': constants.ACCESS_STATE_DENYING})\n                    delete_rules.append(rule)\n\n        try:\n            self.driver.snapshot_update_access(\n                context,\n                snapshot_instance,\n                rules_to_be_on_snapshot,\n                add_rules=add_rules,\n                delete_rules=delete_rules,\n                share_server=share_server)\n\n            # NOTE(ganso): successfully added rules transition to \"active\".\n            for rule in add_rules:\n                self.db.share_snapshot_instance_access_update(\n                    context, rule['access_id'], snapshot_instance['id'],\n                    {'state': constants.STATUS_ACTIVE})\n\n        except Exception:\n            # NOTE(ganso): if we failed, we set all the transitional rules\n            # to ERROR.\n            for rule in add_rules + delete_rules:\n                self.db.share_snapshot_instance_access_update(\n                    context, rule['access_id'], snapshot_instance['id'],\n                    {'state': constants.STATUS_ERROR})\n            raise\n\n        self._remove_access_rules(\n            context, delete_rules, snapshot_instance['id'])\n\n        if self._check_needs_refresh(context, snapshot_instance['id']):\n            self._update_access_rules(context, snapshot_instance,\n                                      share_server=share_server)\n        else:\n            LOG.info(\"Access rules were successfully applied for \"\n                     \"snapshot instance: %s\", snapshot_instance['id'])\n\n    def _check_needs_refresh(self, context, snapshot_instance_id):\n\n        rules = self.db.share_snapshot_access_get_all_for_snapshot_instance(\n            context, snapshot_instance_id)\n\n        return (any(rule['state'] in (\n            constants.ACCESS_STATE_QUEUED_TO_APPLY,\n            constants.ACCESS_STATE_QUEUED_TO_DENY)\n            for rule in rules))\n\n    def _remove_access_rules(self, context, rules, snapshot_instance_id):\n        if not rules:\n            return\n\n        for rule in rules:\n            self.db.share_snapshot_instance_access_delete(\n                context, rule['access_id'], snapshot_instance_id)\n\n    def get_snapshot_instance_access_rules(self, context,\n                                           snapshot_instance_id):\n        return self.db.share_snapshot_access_get_all_for_snapshot_instance(\n            context, snapshot_instance_id)\n", "p.StateClass import State\n\nclass GridGameState(State):\n    ''' Class for two player Grid Game States '''\n\n    def __init__(self, a_x, a_y, b_x, b_y):\n        State.__init__(self, data=[a_x, a_y, b_x, b_y])\n        self.a_x = a_x\n        self.a_y = a_y\n        self.b_x = b_x\n        self.b_y = b_y\n\n    def __hash__(self):\n        # The X coordinate takes the first three digits.\n        if len(str(self.a_x)) < 3:\n            a_x_str = str(self.a_x)\n            while len(a_x_str) < 3:\n                a_x_str = \"0\" + a_x_str\n\n        # The Y coordinate takes the next three digits.\n        if len(str(self.a_y)) < 3:\n            a_y_str = str(self.a_y)\n            while len(a_y_str) < 3:\n                a_y_str = \"0\" + a_y_str\n\n        # The X coordinate takes the first three digits.\n        if len(str(self.b_x)) < 3:\n            b_x_str = str(self.b_x)\n            while len(b_x_str) < 3:\n                b_x_str = \"0\" + b_x_str\n\n        # The Y coordinate takes the next three digits.\n        if len(str(self.b_y)) < 3:\n            b_y_str = str(self.b_y)\n            while len(b_y_str) < 3:\n                b_y_str = \"0\" + b_y_str\n\n        # Concatenate and return.\n        return int(a_x_str + a_y_str + \"0\" + b_x_str + b_y_str)\n\n    def __str__(self):\n        return \"s: (\" + str(self.a_x) + \",\" + str(self.a_y) + \")_a (\" + str(self.b_x) + \",\" + str(self.b_y) + \")_b\"\n\n    def __eq__(self, other):\n        return isinstance(other, GridGameState) and self.a_x == other.a_x and self.a_y == other.a_y and \\\n            self.b_x == other.b_x and self.b_y == other.b_y\n", " an actual path.\n'''\ntoken_file = \"$TOKEN_FILE_PATH\"\n\nfile_ids = [\n    \"11443f3c-9b8b-4e47-b5b7-529468fec098\",\n    \"1f103620-bb34-46f1-b565-94f0027e396d\",\n    \"ca549554-a244-4209-9086-92add7bb7109\"\n    ]\n\nfor file_id in file_ids:\n\n    data_endpt = \"https://api.gdc.cancer.gov/slicing/view/{}\".format(file_id)\n\n    with open(token_file, \"r\") as token:\n        token_string = str(token.read().strip())\n\n    params = {\n        \"regions\": [\"chr1:1-20000\", \"chr10:129000-160000\"]\n        }\n\n    response = requests.post(data_endpt,\n                            data = json.dumps(params),\n                            headers = {\n                                \"Content-Type\": \"application/json\",\n                                \"X-Auth-Token\": token_string\n                                })\n\n    file_name = \"{}_region_slices.bam\".format(file_id)\n\n    with open(file_name, \"wb\") as output_file:\n        output_file.write(response.content)\n", "rom flask import request, jsonify, g\nfrom uuid import uuid4\nfrom redis import Redis\n\n# project imports\nfrom project import app\n\n\nredis = Redis()\n\n\ndef encode_data(data):\n    return json.dumps(data)\n\ndef decode_data(encoded):\n    return json.loads(encoded)\n\n\ndef generate_token(data):\n    token = str(uuid4())\n    redis.setex(token, encode_data(data), app.config['TOKEN_EXPIRE_TIME'])\n    return token\n\n\ndef expire_token():\n    redis.delete(request.headers['TOKEN'])\n\n\ndef login_required(f):\n    @wraps(f)\n    def decorated(*args, **kwargs):\n\n        if not 'TOKEN' in request.headers:\n            return jsonify(errors='set token to access protected routes'), 401\n\n        token = request.headers['TOKEN']\n        token_data = redis.get(token)\n        if not token_data:\n            return jsonify(errors='token is invalid or has expired'), 401\n        \n        g.token_data = decode_data(token_data)\n\n        return f(*args, **kwargs)\n\n    return decorated\n", "  assert tmwrap.server is not None\n    assert isinstance(tmwrap.server, tmuxp.server.Server)\n\ndef test_session_factory():\n    tmwrap.session_factory()\n    sessions = tmwrap.get_sessions()\n    assert sessions is not None\n    for s in sessions:\n    tmwrap.close_sessions()\n\ndef test_window_factory():\n    for i in range(10):\n        tmwrap.session_factory()\n\n    for session in tmwrap.get_sessions():\n        tmwrap.window_factory(session)\n\n# TODO: remake this part of the test, it's invalid.\n# get_windows returns an empty list.\n    for s in tmwrap.get_sessions():\n        windows = tmwrap.get_windows(s)\n        for w in windows:\n            assert isinstance(w, tmuxp.window.Window)\n\n    tmwrap.close_sessions()\n", "cir crea un nuevo archivo _geo.csv con columnas lat y long.\n# Utiliza el API de Google Maps\n# Instrucciones en como obtener la API Key para Google Maps: https://github.com/slawek87/geolocation-python\n\nfrom geolocation.main import GoogleMaps\nfrom geolocation.distance_matrix.client import DistanceMatrixApiClient\nimport os\nimport argparse\nimport csv\nimport sys\nimport logging\nimport time\n\n## SAVE THE FILE TO A GEOJSON\n\n# the template. where data from the csv will be formatted to geojson\ntemplate = \\\n    ''' \\\n    { \"type\" : \"Feature\",\n        \"geometry\" : {\n            \"type\" : \"Point\",\n            \"coordinates\" : [%s, %s]},\n        \"properties\" : %s\n        },\n    '''\n\n## GEOLOCALIZATION\n\n# get address to geolocalize\n# geolocalize\n# save it in file_geo.csv\n\n# environment variables:\n#                   GOOGLE_MAPS_API_KEY\n#                   COUNTRY\n\n# arguments:\n#                   COLUMN NAMES\n\ndef convert_row(row):\n    properties = {}\n    for i in row:\n        if i != 'lng' or i != 'lat':\n            properties[i] = row[i]\n    return template % (row['lng'], row['lat'],  properties)\n\ndef get_environment_variables():\n    if not os.environ.has_key('COUNTRY') or not os.environ.has_key('GOOGLE_MAPS_API_KEY'):\n        sys.exit('Variables de ambiente COUNTRY o GOOGLE_MAPS_API_KEY no estan definidas.')\n    # look for environment variables\n    return {'country': os.environ['COUNTRY'], 'api_key': os.environ['GOOGLE_MAPS_API_KEY']}\n\ndef get_address(row, fields, country):\n    address = ', '.join(map(lambda x: row[x], fields.split(','))) + ', ' + country\n    return address\n\ndef main():\n\n    logging_file = 'geolocation_%s.log' % time.strftime(\"%H_%M_%S\")\n    logging.basicConfig(filename=logging_file,level=logging.DEBUG)\n\n    parser = argparse.ArgumentParser(description='Geolocalizer un archivo CSV.')\n    parser.add_argument('--csv', help='nombre del archivo CSV a geolocalizar')\n    parser.add_argument('--columnas', help='las columnas (en orden) de la direcci\u00f3n')\n\n    args = parser.parse_args()\n\n    # get file name and column names from arguments\n    csv_file = args.csv\n    new_csv_file = '_'.join([csv_file.split('.')[0], 'geo.csv'])\n\n    new_geojson_file = '_'.join([csv_file.split('.')[0], '.geojson'])\n\n    # get the columns names where the address is\n    fields = args.columnas\n\n    # get api key and country from environment variables\n    env_variables = get_environment_variables()\n\n    # get an instance of google maps\n    google_maps = GoogleMaps(api_key=env_variables['api_key'])\n\n    # the head of the geojson file\n    output = \\\n        ''' \\\n    { \"type\" : \"Feature Collection\",\n        \"features\" : [\n        '''\n    with open(csv_file, 'rb') as csvfile:\n        reader = csv.DictReader(csvfile)\n        fieldnames = reader.fieldnames\n        fieldnames.append('lat')\n        fieldnames.append('lng')\n        with open(new_csv_file, 'wb') as newcsvfile:\n            writer = csv.DictWriter(newcsvfile, fieldnames=fieldnames)\n            for row in reader:\n                address = get_address(row, fields, env_variables['country'])\n                try:\n                    location = google_maps.search(location=address) # sends search to Google Maps.\n                    my_location = location.first() # returns only first location.\n\n                    if my_location != None:\n                        row['lat'] = my_location.lat\n                        row['lng'] = my_location.lng\n                        writer.writerow(row)\n                except:\n                    e = sys.exc_info()[0]\n                    logging.warning(\"<p>LOG: no pudo encontrar la direcci\u00f3n: '%s' . Error: %s</p>\", address, e)\n\n                output += convert_row(row)\n\n        # convert new file into a geojson file\n        # the tail of the geojson file\n        output += \\\n            ''' \\\n            ]\n        }\n            '''\n        # opens an geoJSON file to write the output to\n        outFileHandle = open(new_geojson_file, \"w\")\n        outFileHandle.write(output)\n        outFileHandle.close()\n\nif __name__ == '__main__':\n    main()\n", "# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nRetrieve tiles from different tile servers (TMS/TileCache/etc.).\n\"\"\"\n\nimport sys\nfrom mapproxy.image.opts import ImageOptions\nfrom mapproxy.source import SourceError\nfrom mapproxy.client.http import HTTPClientError\nfrom mapproxy.source import InvalidSourceQuery\nfrom mapproxy.layer import BlankImage, map_extent_from_grid, CacheMapLayer, MapLayer\nfrom mapproxy.util.py import reraise_exception\n\nimport logging\nlog = logging.getLogger('mapproxy.source.tile')\nlog_config = logging.getLogger('mapproxy.config')\n\nclass TiledSource(MapLayer):\n    def __init__(self, grid, client, coverage=None, image_opts=None, error_handler=None,\n        res_range=None):\n        MapLayer.__init__(self, image_opts=image_opts)\n        self.grid = grid\n        self.client = client\n        self.image_opts = image_opts or ImageOptions()\n        self.coverage = coverage\n        self.extent = coverage.extent if coverage else map_extent_from_grid(grid)\n        self.res_range = res_range\n        self.error_handler = error_handler\n\n    def get_map(self, query):\n        if self.grid.tile_size != query.size:\n            ex = InvalidSourceQuery(\n                'tile size of cache and tile source do not match: %s != %s'\n                 % (self.grid.tile_size, query.size)\n            )\n            log_config.error(ex)\n            raise ex\n\n        if self.grid.srs != query.srs:\n            ex = InvalidSourceQuery(\n                'SRS of cache and tile source do not match: %r != %r'\n                % (self.grid.srs, query.srs)\n            )\n            log_config.error(ex)\n            raise ex\n\n        if self.res_range and not self.res_range.contains(query.bbox, query.size,\n                                                          query.srs):\n            raise BlankImage()\n        if self.coverage and not self.coverage.intersects(query.bbox, query.srs):\n            raise BlankImage()\n\n        _bbox, grid, tiles = self.grid.get_affected_tiles(query.bbox, query.size)\n\n        if grid != (1, 1):\n            raise InvalidSourceQuery('BBOX does not align to tile')\n\n        tile_coord = next(tiles)\n\n        try:\n            return self.client.get_tile(tile_coord, format=query.format)\n        except HTTPClientError as e:\n            if self.error_handler:\n                resp = self.error_handler.handle(e.response_code, query)\n                if resp:\n                    return resp\n            log.warn('could not retrieve tile: %s', e)\n            reraise_exception(SourceError(e.args[0]), sys.exc_info())\n\nclass CacheSource(CacheMapLayer):\n    def __init__(self, tile_manager, extent=None, image_opts=None,\n        max_tile_limit=None, tiled_only=False):\n        CacheMapLayer.__init__(self, tile_manager, extent=extent, image_opts=image_opts,\n            max_tile_limit=max_tile_limit)\n        self.supports_meta_tiles = not tiled_only\n        self.tiled_only = tiled_only\n\n    def get_map(self, query):\n        if self.tiled_only:\n            query.tiled_only = True\n        return CacheMapLayer.get_map(self, query)\n\n", "lldbtest import *\nfrom lldbsuite.test import lldbutil\n\n\nclass TestCppScopes(TestBase):\n\n    mydir = TestBase.compute_mydir(__file__)\n\n    @expectedFailureAll(oslist=[\"windows\"], bugnumber=\"llvm.org/pr24764\")\n    def test_all_but_c(self):\n        self.do_test(False)\n\n    @expectedFailureAll(oslist=[\"windows\"], bugnumber=\"llvm.org/pr24764\")\n    def test_c(self):\n        self.do_test(True)\n\n    def do_test(self, test_c):\n        self.build()\n\n        # Get main source file\n        src_file = os.path.join(self.getSourceDir(), \"main.cpp\")\n        src_file_spec = lldb.SBFileSpec(src_file)\n        self.assertTrue(src_file_spec.IsValid(), \"Main source file\")\n\n        # Get the path of the executable\n        exe_path = self.getBuildArtifact(\"a.out\")\n\n        # Load the executable\n        target = self.dbg.CreateTarget(exe_path)\n        self.assertTrue(target.IsValid(), VALID_TARGET)\n\n        # Break on main function\n        main_breakpoint = target.BreakpointCreateBySourceRegex(\n            \"// break here\", src_file_spec)\n        self.assertTrue(\n            main_breakpoint.IsValid() and main_breakpoint.GetNumLocations() >= 1,\n            VALID_BREAKPOINT)\n\n        # Launch the process\n        args = None\n        env = None\n        process = target.LaunchSimple(\n            args, env, self.get_process_working_directory())\n        self.assertTrue(process.IsValid(), PROCESS_IS_VALID)\n\n        # Get the thread of the process\n        self.assertTrue(\n            process.GetState() == lldb.eStateStopped,\n            PROCESS_STOPPED)\n        thread = lldbutil.get_stopped_thread(\n            process, lldb.eStopReasonBreakpoint)\n\n        # Get current fream of the thread at the breakpoint\n        frame = thread.GetSelectedFrame()\n\n        # Test result for scopes of variables\n\n        global_variables = frame.GetVariables(True, True, True, False)\n        global_variables_assert = {\n            'A::a': 1111,\n            'B::a': 2222,\n            'C::a': 3333,\n            '::a': 4444,\n            'a': 4444\n        }\n\n        self.assertTrue(\n            global_variables.GetSize() == 4,\n            \"target variable returns all variables\")\n        for variable in global_variables:\n            name = variable.GetName()\n            self.assertTrue(\n                name in global_variables_assert,\n                \"target variable returns wrong variable \" + name)\n\n        for name in global_variables_assert:\n            if name is \"C::a\" and not test_c:\n                continue\n            if name is not \"C::a\" and test_c:\n                continue\n\n            value = frame.EvaluateExpression(name)\n            assert_value = global_variables_assert[name]\n            self.assertTrue(\n                value.IsValid() and value.GetValueAsSigned() == assert_value,\n                name + \" = \" + str(assert_value))\n", "or import webhook_view\nfrom zerver.lib.request import REQ, has_request_variables\nfrom zerver.lib.response import json_success\nfrom zerver.lib.webhooks.common import check_send_webhook_message\nfrom zerver.models import UserProfile\n\nERRBIT_TOPIC_TEMPLATE = \"{project_name}\"\nERRBIT_MESSAGE_TEMPLATE = '[{error_class}]({error_url}): \"{error_message}\" occurred.'\n\n\n@webhook_view(\"Errbit\")\n@has_request_variables\ndef api_errbit_webhook(\n    request: HttpRequest,\n    user_profile: UserProfile,\n    payload: Dict[str, Any] = REQ(argument_type=\"body\"),\n) -> HttpResponse:\n    subject = get_subject(payload)\n    body = get_body(payload)\n    check_send_webhook_message(request, user_profile, subject, body)\n    return json_success()\n\n\ndef get_subject(payload: Dict[str, Any]) -> str:\n    project = payload[\"problem\"][\"app_name\"] + \" / \" + payload[\"problem\"][\"environment\"]\n    return ERRBIT_TOPIC_TEMPLATE.format(project_name=project)\n\n\ndef get_body(payload: Dict[str, Any]) -> str:\n    data = {\n        \"error_url\": payload[\"problem\"][\"url\"],\n        \"error_class\": payload[\"problem\"][\"error_class\"],\n        \"error_message\": payload[\"problem\"][\"message\"],\n    }\n    return ERRBIT_MESSAGE_TEMPLATE.format(**data)\n", "rsion 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"AE Transformer.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\nimport math\nimport os\nfrom six.moves import range  # pylint: disable=redefined-builtin\n\nfrom tensor2tensor.layers import common_attention\nfrom tensor2tensor.layers import common_image_attention as cia\nfrom tensor2tensor.layers import common_layers\nfrom tensor2tensor.layers import discretization\nfrom tensor2tensor.layers import latent_layers\nfrom tensor2tensor.layers import modalities\nfrom tensor2tensor.models import transformer\nfrom tensor2tensor.utils import beam_search\nfrom tensor2tensor.utils import expert_utils\nfrom tensor2tensor.utils import registry\nfrom tensor2tensor.utils import t2t_model\n\nimport tensorflow as tf\n\n\n_DO_SUMMARIES = True\n\n\ndef residual_conv(x, repeat, k, hparams, name, reuse=None):\n  \"\"\"A stack of convolution blocks with residual connections.\"\"\"\n  with tf.variable_scope(name, reuse=reuse):\n    dilations_and_kernels = [((1, 1), k) for _ in range(3)]\n    for i in range(repeat):\n      with tf.variable_scope(\"repeat_%d\" % i):\n        y = common_layers.conv_block(\n            common_layers.layer_norm(x, hparams.hidden_size, name=\"lnorm\"),\n            hparams.hidden_size,\n            dilations_and_kernels,\n            padding=\"SAME\",\n            name=\"residual_conv\")\n        y = tf.nn.dropout(y, 1.0 - hparams.dropout)\n        x += y\n    return x\n\n\ndef attend(x, source, hparams, name):\n  \"\"\"Self-attention layer with source as memory antecedent.\"\"\"\n  with tf.variable_scope(name):\n    x = tf.squeeze(x, axis=2)\n    if len(source.get_shape()) > 3:\n      source = tf.squeeze(source, axis=2)\n    source = common_attention.add_timing_signal_1d(source)\n    y = common_attention.multihead_attention(\n        common_layers.layer_preprocess(x, hparams), source, None,\n        hparams.attention_key_channels or hparams.hidden_size,\n        hparams.attention_value_channels or hparams.hidden_size,\n        hparams.hidden_size, hparams.num_heads,\n        hparams.attention_dropout)\n    res = common_layers.layer_postprocess(x, y, hparams)\n    return tf.expand_dims(res, axis=2)\n\n\ndef decompress_step(source, hparams, first_relu, is_2d, name):\n  \"\"\"Decompression function.\"\"\"\n  with tf.variable_scope(name):\n    shape = common_layers.shape_list(source)\n    multiplier = 4 if is_2d else 2\n    kernel = (1, 1) if is_2d else (1, 1)\n    thicker = common_layers.conv_block(\n        source, hparams.hidden_size * multiplier, [((1, 1), kernel)],\n        first_relu=first_relu, name=\"decompress_conv\")\n    if is_2d:\n      return tf.depth_to_space(thicker, 2)\n    return tf.reshape(thicker, [shape[0], shape[1] * 2, 1, hparams.hidden_size])\n\n\ndef top_k_softmax(x, k):\n  \"\"\"Calculate softmax(x), select top-k and rescale to sum to 1.\"\"\"\n  x = tf.nn.softmax(x)\n  top_x, _ = tf.nn.top_k(x, k=k+1)\n  min_top = tf.reduce_min(top_x, axis=-1, keepdims=True)\n  x = tf.nn.relu((x - min_top) + 1e-12)\n  x /= tf.reduce_sum(x, axis=-1, keepdims=True)\n  return x, tf.reduce_max(top_x, axis=-1)\n\n\ndef top_k_experts(x, k, hparams):\n  x_shape = common_layers.shape_list(x)\n  x_flat = tf.reshape(x, [-1, common_layers.shape_list(x)[-1]])\n  is_training = hparams.mode == tf.contrib.learn.ModeKeys.TRAIN\n  gates, load = expert_utils.noisy_top_k_gating(\n      x_flat, 2 ** hparams.z_size, is_training, k)\n  gates_shape = [x_shape[0], x_shape[1], x_shape[2], 2 ** hparams.z_size]\n  gates = tf.reshape(gates, gates_shape)\n  load_loss = expert_utils.cv_squared(load)\n  return gates, load_loss\n\n\ndef compress(x, c, is_2d, hparams, name):\n  \"\"\"Compress.\"\"\"\n  with tf.variable_scope(name):\n    # Run compression by strided convs.\n    cur = x\n    k1 = (3, 3) if is_2d else (3, 1)\n    k2 = (2, 2) if is_2d else (2, 1)\n    cur = residual_conv(cur, hparams.num_compress_steps, k1, hparams, \"rc\")\n    if c is not None and hparams.do_attend_compress:\n      cur = attend(cur, c, hparams, \"compress_attend\")\n    for i in range(hparams.num_compress_steps):\n      if hparams.do_residual_compress:\n        cur = residual_conv(cur, hparams.num_compress_steps, k1, hparams,\n                            \"rc_%d\" % i)\n      cur = common_layers.conv_block(\n          cur, hparams.hidden_size, [((1, 1), k2)],\n          strides=k2, name=\"compress_%d\" % i)\n    return cur\n\n\ndef encode(x, x_space, hparams, name):\n  \"\"\"Transformer preparations and encoder.\"\"\"\n  with tf.variable_scope(name):\n    (encoder_input, encoder_self_attention_bias,\n     ed) = transformer.transformer_prepare_encoder(x, x_space, hparams)\n    encoder_input = tf.nn.dropout(encoder_input, 1.0 - hparams.dropout)\n    return transformer.transformer_encoder(\n        encoder_input, encoder_self_attention_bias, hparams), ed\n\n\ndef decode_transformer(encoder_output,\n                       encoder_decoder_attention_bias,\n                       targets,\n                       hparams,\n                       name,\n                       task=None,\n                       causal=True):\n  \"\"\"Original Transformer decoder.\"\"\"\n  orig_hparams = hparams\n  with tf.variable_scope(name):\n    if task is None:\n      task = hparams.task\n    if task == \"translate\":\n      targets = common_layers.flatten4d3d(targets)\n\n      decoder_input, decoder_self_bias = (\n          transformer.transformer_prepare_decoder(targets, hparams))\n\n      decoder_input = tf.nn.dropout(decoder_input,\n                                    1.0 - hparams.layer_prepostprocess_dropout)\n\n      if not causal:\n        decoder_self_bias *= 0.\n\n      decoder_output = transformer.transformer_decoder(\n          decoder_input,\n          encoder_output,\n          decoder_self_bias,\n          encoder_decoder_attention_bias,\n          hparams)\n      decoder_output = tf.expand_dims(decoder_output, axis=2)\n    else:\n      assert task == \"image\"\n      inputs = None\n      # have to reshape targets as b, 32, 32, 3 * hidden size] beacuse otherwise\n      # prepare_image will choke\n      targets = tf.reshape(targets, [tf.shape(targets)[0], hparams.img_len,\n                                     hparams.img_len,\n                                     hparams.num_channels*hparams.hidden_size])\n\n      # Prepare decoder inputs and bias.\n      # TODO(nikip): Make prepare_decoder return bias\n      decoder_input, _, _ = cia.prepare_decoder(targets, hparams)\n      bias = None\n\n      # Add class label to decoder input.\n      if not hparams.drop_inputs:\n        decoder_input += tf.reshape(\n            inputs,\n            [common_layers.shape_list(targets)[0], 1, 1, hparams.hidden_size])\n      decoder_output = cia.transformer_decoder_layers(\n          decoder_input,\n          encoder_output=None,\n          num_layers=hparams.num_decoder_layers or hparams.num_hidden_layers,\n          hparams=hparams,\n          self_attention_bias=bias,\n          attention_type=hparams.dec_attention_type,\n          name=\"decoder\")\n    decoder_output_shape = common_layers.shape_list(decoder_output)\n    decoder_output = tf.reshape(decoder_output, [decoder_output_shape[0], -1, 1,\n                                                 hparams.hidden_size])\n    # Expand since t2t expects 4d tensors.\n    hparams = orig_hparams\n    return decoder_output\n\n\ndef multinomial_sample(x, vocab_size, temperature):\n  \"\"\"Multinomial sampling from a n-dimensional tensor.\"\"\"\n  if temperature > 0:\n    samples = tf.multinomial(tf.reshape(x, [-1, vocab_size]) / temperature, 1)\n  else:\n    samples = tf.argmax(x, axis=-1)\n  reshaped_samples = tf.reshape(samples, common_layers.shape_list(x)[:-1])\n  return tf.to_int32(reshaped_samples)\n\n\ndef ae_latent_softmax(latents_pred, latents_discrete, hparams):\n  \"\"\"Latent prediction and loss.\"\"\"\n  vocab_size = 2 ** hparams.z_size\n  if hparams.num_decode_blocks < 2:\n    latents_logits = tf.layers.dense(latents_pred, vocab_size,\n                                     name=\"extra_logits\")\n    if hparams.logit_normalization:\n      latents_logits *= tf.rsqrt(1e-8 +\n                                 tf.reduce_mean(tf.square(latents_logits)))\n\n    loss = None\n    if latents_discrete is not None:\n      if hparams.soft_em:\n        # latents_discrete is actually one-hot of multinomial samples\n        assert hparams.num_decode_blocks == 1\n        loss = tf.nn.softmax_cross_entropy_with_logits_v2(\n            labels=latents_discrete, logits=latents_logits)\n      else:\n        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n            labels=latents_discrete, logits=latents_logits)\n    sample = multinomial_sample(\n        latents_logits, vocab_size, hparams.sampling_temp)\n    return sample, loss\n\n  # Multi-block case.\n  vocab_bits = int(math.log(vocab_size, 2))\n  assert vocab_size == 2**vocab_bits\n  assert vocab_bits % hparams.num_decode_blocks == 0\n  block_vocab_size = 2**(vocab_bits // hparams.num_decode_blocks)\n  latents_logits = [\n      tf.layers.dense(\n          latents_pred, block_vocab_size, name=\"extra_logits_%d\" % i)\n      for i in range(hparams.num_decode_blocks)\n  ]\n  loss = None\n  if latents_discrete is not None:\n    losses = []\n    for i in range(hparams.num_decode_blocks):\n      d = tf.floormod(tf.floordiv(latents_discrete,\n                                  block_vocab_size**i), block_vocab_size)\n      losses.append(tf.nn.sparse_softmax_cross_entropy_with_logits(\n          labels=d, logits=latents_logits[i]))\n    loss = sum(losses)\n  samples = [multinomial_sample(l, block_vocab_size, hparams.sampling_temp)\n             for l in latents_logits]\n  sample = sum([s * block_vocab_size**i for i, s in enumerate(samples)])\n  return sample, loss\n\n\ndef ae_latent_sample_beam(latents_dense_in, inputs, ed, embed, hparams):\n  \"\"\"Sample from the latent space in the autoencoder.\"\"\"\n  vocab_size = 2**hparams.z_size\n  beam_size = 1  # TODO(lukaszkaiser): larger beam sizes seem to work bad.\n  inputs = tf.tile(inputs, [beam_size, 1, 1])\n  ed = tf.tile(ed, [beam_size, 1, 1, 1])\n\n  def symbols_to_logits_fn(ids):\n    \"\"\"Go from ids to logits.\"\"\"\n    ids = tf.expand_dims(ids, axis=2)  # Ids start with added all-zeros.\n    latents_discrete = tf.pad(ids[:, 1:], [[0, 0], [0, 1], [0, 0]])\n\n    with tf.variable_scope(tf.get_variable_scope(), reuse=False):\n      latents_dense = embed(latents_discrete)\n      latents_pred = decode_transformer(\n          inputs, ed, latents_dense, hparams, \"extra\")\n      logits = tf.layers.dense(latents_pred, vocab_size, name=\"extra_logits\")\n      current_output_position = common_layers.shape_list(ids)[1] - 1\n      logits = logits[:, current_output_position, :, :]\n    return tf.squeeze(logits, axis=[1])\n\n  initial_ids = tf.zeros([tf.shape(latents_dense_in)[0]], dtype=tf.int32)\n  length = tf.shape(latents_dense_in)[1]\n  ids, _ = beam_search.beam_search(\n      symbols_to_logits_fn, initial_ids, beam_size, length,\n      vocab_size, alpha=0.0, eos_id=-1, stop_early=False)\n\n  res = tf.expand_dims(ids[:, 0, :], axis=2)  # Pick first beam.\n  return res[:, 1:]  # Remove the added all-zeros from ids.\n\n\ndef ae_latent_sample(latents_dense, inputs, ed, embed, iters, hparams):\n  \"\"\"Sample from the latent space in the autoencoder.\"\"\"\n  if hparams.num_decode_blocks < 2 and hparams.sampling_temp == 0.0:\n    # TODO(lukaszkaiser): beam-search only works in non-blocked mode for now.\n    tf.logging.info(\"Running beam-search for latents with beam size 1.\")\n    return ae_latent_sample_beam(latents_dense, inputs, ed, embed, hparams)\n  latents_pred = decode_transformer(inputs, ed, latents_dense, hparams, \"extra\")\n  latents_discrete, _ = ae_latent_softmax(latents_pred, None, hparams)\n\n  def next_bit(latents_discrete, i):\n    latents_discrete_prev = latents_discrete\n    with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n      latents_dense = embed(latents_discrete)\n      latents_pred = decode_transformer(\n          inputs, ed, latents_dense, hparams, \"extra\")\n      latents_discrete, _ = ae_latent_softmax(latents_pred, None, hparams)\n      return tf.concat([latents_discrete_prev[:, :(i+1), :],\n                        latents_discrete[:, (i+1):, :]], axis=1)\n\n  for i in range(iters):\n    latents_discrete = next_bit(latents_discrete, i)\n  return latents_discrete\n\n\ndef ae_transformer_internal(inputs,\n                            targets,\n                            target_space,\n                            hparams,\n                            cache=None,\n                            predict_mask=1.0):\n  \"\"\"AE Transformer, main step used for training.\"\"\"\n  # Summaries break with the do_refine cond, turn them off in that case.\n  global _DO_SUMMARIES\n  if hparams.do_refine:\n    _DO_SUMMARIES = False\n\n  # Prepare.\n  if inputs is not None:\n    batch_size = common_layers.shape_list(inputs)[0]\n  else:\n    batch_size = common_layers.shape_list(targets)[0]\n  targets = tf.reshape(targets, [batch_size, -1, 1, hparams.hidden_size])\n\n  # Encoder.\n  if inputs is not None:\n    inputs = common_layers.flatten4d3d(inputs)\n    inputs, ed = encode(inputs, target_space, hparams, \"input_enc\")\n    inputs_ex, ed_ex = inputs, ed\n  else:\n    ed, inputs_ex, ed_ex = None, None, None\n\n  # Autoencoding.\n  losses = {\"extra\": tf.constant(0.0), \"latent_pred\": tf.constant(0.0),\n            \"neg_q_entropy\": tf.constant(0.0)}\n  if hparams.do_ae:\n    # flatten here\n    original_targets = targets\n    original_targets_shape = tf.shape(original_targets)\n    if hparams.task == \"image\":\n      cia.maybe_reshape_4d_to_3d(targets)\n    if hparams.task == \"translate\":\n      if inputs is not None:\n        max_targets_len_from_inputs = tf.concat([inputs, inputs], axis=1)\n      else:\n        max_targets_len_from_inputs = targets\n    else:\n      assert hparams.task == \"image\"\n      max_targets_len_from_inputs = targets\n    if hparams.word_shuffle:\n      tf.logging.info(\"Using word shuffle with rate = {}\".format(\n          hparams.word_shuffle))\n      targets_idx = tf.range(start=0,\n                             limit=common_layers.shape_list(targets)[1],\n                             delta=1)\n      targets_idx = tf.to_float(targets_idx)\n      noise = tf.random_uniform(shape=common_layers.shape_list(targets_idx),\n                                minval=0,\n                                maxval=1 + hparams.word_shuffle)\n      targets_idx += noise\n      permutation = tf.contrib.framework.argsort(targets_idx)\n      targets_permuted = tf.gather(targets, indices=permutation, axis=1)\n      targets = targets_permuted\n    targets, _ = common_layers.pad_to_same_length(\n        targets, max_targets_len_from_inputs,\n        final_length_divisible_by=2**hparams.num_compress_steps)\n    # Add positional information\n    targets_shape = common_layers.shape_list(targets)\n    targets = tf.reshape(targets, [targets_shape[0], targets_shape[1],\n                                   targets_shape[3]])\n    targets = common_attention.add_positional_embedding(\n        targets, hparams.max_length, name=\"targets_position\")\n    targets = tf.reshape(targets, shape=targets_shape)\n    if hparams.word_dropout:\n      mask = tf.random_uniform(shape=common_layers.shape_list(targets),\n                               minval=0.0, maxval=1.0)\n      targets_noisy = tf.where(mask > hparams.word_dropout, targets,\n                               tf.zeros_like(targets))\n    else:\n      targets_noisy = targets\n\n    targets_c = compress(targets_noisy, inputs, False, hparams, \"compress\")\n    if hparams.mode != tf.estimator.ModeKeys.PREDICT:\n      # Compress and bottleneck.\n      latents_dense, latents_discrete, extra_loss, embed, neg_q_entropy = (\n          hparams.bottleneck(inputs=targets_c,\n                             filter_size=hparams.compress_filter_size,\n                             mode=hparams.mode,\n                             name=\"vc\"))\n      if _DO_SUMMARIES:\n        tf.summary.histogram(\"b0\", tf.reshape(latents_discrete[:, 0, :], [-1]))\n      pc = common_layers.inverse_exp_decay(hparams.startup_steps)\n      pc = pc if hparams.mode == tf.estimator.ModeKeys.TRAIN else 1.0\n      cond = tf.less(tf.random_uniform([batch_size]), pc)\n      latents_dense = tf.where(cond, latents_dense, targets_c)\n      # TODO(lukaszkaiser): return extra losses batchwise, multiply before mean.\n      losses[\"extra\"] = extra_loss * tf.reduce_mean(tf.to_float(cond))\n      # Extra loss predicting latent code from input. Discrete only.\n      if hparams.bottleneck_kind not in [\"dense\", \"vae\"]:\n        latents_pred = decode_transformer(\n            inputs_ex, ed_ex,\n            embed(latents_discrete), hparams, \"extra\",\n            task=\"translate\")\n        _, latent_pred_loss = ae_latent_softmax(\n            latents_pred, tf.stop_gradient(latents_discrete), hparams)\n\n        # Scale by latent dimension for summary so we can compare across\n        # batches.\n        if _DO_SUMMARIES:\n          tf.summary.scalar(\"latent_pred_loss_mean\",\n                            tf.reduce_mean(latent_pred_loss))\n        if hparams.sum_over_latents:\n          latent_pred_loss = tf.reduce_sum(latent_pred_loss, [1, 2])\n\n        losses[\"latent_pred\"] = tf.reduce_mean(\n            latent_pred_loss * tf.to_float(cond)) * hparams.prior_scale\n        losses[\"neg_q_entropy\"] = neg_q_entropy * hparams.entropy_scale\n      else:\n        inputs_c = decode_transformer(inputs, ed, targets_c, hparams, \"dec_c\")\n        losses[\"latent_pred\"] = tf.reduce_mean((inputs_c - targets_c)**2) * 20\n        def bn_inputs():\n          with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n            bn, _, _, _, _ = hparams.bottleneck(\n                inputs=inputs_c,\n                filter_size=hparams.compress_filter_size,\n                mode=hparams.mode,\n                name=\"vc\")\n          return bn\n        inputs_c = bn_inputs()\n        ptc = 1.0 - common_layers.inverse_lin_decay(200000) * 0.5\n        ptc = ptc if hparams.mode == tf.estimator.ModeKeys.TRAIN else 1.0\n        latents_dense = tf.where(tf.less(tf.random_uniform([batch_size]), ptc),\n                                 latents_dense, inputs_c)\n    else:\n      if hparams.bottleneck_kind in [\"dense\", \"vae\"]:\n        inputs_c = decode_transformer(inputs, ed, targets_c, hparams, \"dec_c\")\n        latents_dense, _, _, _, _ = hparams.bottleneck(\n            inputs=inputs_c,\n            filter_size=hparams.compress_filter_size,\n            mode=hparams.mode,\n            name=\"vc\")\n      else:\n        latent_len = common_layers.shape_list(targets_c)[1]\n        _, _, _, embed, _ = hparams.bottleneck(\n            inputs=targets_c,\n            filter_size=hparams.compress_filter_size,\n            name=\"vc\")\n        latents_dense = tf.zeros_like(targets_c[:, :latent_len, :, :])\n        if cache is None:\n          cache = ae_latent_sample(\n              latents_dense, inputs_ex, ed_ex, embed, 16, hparams)\n        latents_dense = embed(cache)\n    # Postprocess.\n    d = latents_dense\n    d_shape = common_layers.shape_list(d)\n    d = tf.reshape(d, [d_shape[0], d_shape[1], d_shape[3]])\n    d = common_attention.add_positional_embedding(\n        d, hparams.max_length, name=\"latents_position\")\n    d = tf.reshape(d, shape=d_shape)\n\n    # decompressing the dense latents\n    for i in range(hparams.num_compress_steps):\n      j = hparams.num_compress_steps - i - 1\n      d = residual_conv(d, 1, (3, 1), hparams, \"decompress_rc_%d\" % j)\n      if inputs is not None and hparams.do_attend_decompress:\n        d = attend(d, inputs, hparams, \"decompress_attend_%d\" % j)\n      d = decompress_step(d, hparams, i > 0, False, \"decompress_%d\" % j)\n\n    # Masking.\n    if hparams.do_mask:\n      masking = common_layers.inverse_lin_decay(hparams.mask_startup_steps)\n      masking *= common_layers.inverse_exp_decay(\n          hparams.mask_startup_steps // 4)  # Not much at start.\n      if not hparams.do_refine:\n        masking -= tf.random_uniform([]) * hparams.unmasked_percentage\n      masking = tf.minimum(tf.maximum(masking, 0.0), 1.0)\n      if hparams.use_predict_mask:\n        masking = predict_mask\n      if hparams.mode == tf.estimator.ModeKeys.PREDICT:\n        masking = predict_mask\n      mask = tf.less(masking, tf.random_uniform(\n          common_layers.shape_list(targets)[:-1]))\n      mask = tf.expand_dims(tf.to_float(mask), 3)\n\n      # targets is always [batch, length, 1, depth]\n      targets = mask * targets + (1.0 - mask) * d\n      # reshape back to 4d here\n      if hparams.task == \"image\":\n        targets = tf.reshape(targets, original_targets_shape)\n\n  res = decode_transformer(inputs, ed, targets, hparams, \"decoder\",\n                           causal=hparams.causal)\n  if hparams.do_ae:\n    if hparams.do_mask and hparams.do_refine:\n      def refine_res():\n        # return residual_conv(res, 1, (5, 1), hparams, \"refine\")\n        r, _ = encode(tf.squeeze(res, axis=[2]),\n                      target_space, hparams, \"refine_enc\")\n        return tf.expand_dims(r, axis=2)\n      masked_batches = tf.reduce_sum(mask, axis=[1, 2, 3])\n      all_masked = tf.less(masked_batches, 0.1)\n      res = tf.where(all_masked, refine_res(), res)\n    # We'll start training the extra model of latents after mask_startup_steps.\n    nonlatent_steps = hparams.mask_startup_steps\n    latent_time = tf.less(nonlatent_steps,\n                          tf.to_int32(tf.train.get_global_step()))\n    losses[\"latent_pred\"] *= tf.to_float(latent_time)\n\n  # res was generated from padded targets, which means it has some extra\n  # elements. These can cause shape problems when computing loss with respect to\n  # the original (unpadded) targets. So we remove their extra elements here.\n  res = res[:, :original_targets_shape[1], :, :]\n\n  data_dim = common_layers.shape_list(res)[1]\n  latent_dim = common_layers.shape_list(targets_c)[1]\n  return res, losses, cache, data_dim, latent_dim\n\n\n@registry.register_model\nclass TransformerAE(t2t_model.T2TModel):\n  \"\"\"Autoencoder-augmented Transformer.\"\"\"\n\n  def __init__(self, *args, **kwargs):\n    super(TransformerAE, self).__init__(*args, **kwargs)\n    self.predict_mask = 1.0\n\n    # Define bottleneck function\n    self._hparams.bottleneck = functools.partial(\n        discretization.discrete_bottleneck,\n        hidden_size=self._hparams.hidden_size,\n        z_size=self._hparams.z_size,\n        filter_size=self._hparams.filter_size,\n        bottleneck_kind=self._hparams.bottleneck_kind,\n        num_blocks=self._hparams.num_blocks,\n        num_residuals=self.hparams.num_residuals,\n        reshape_method=self._hparams.reshape_method,\n        beta=self._hparams.beta,\n        ema=self._hparams.ema,\n        epsilon=self._hparams.epsilon,\n        decay=self._hparams.decay,\n        random_top_k=self._hparams.random_top_k,\n        soft_em=self.hparams.soft_em,\n        num_samples=self.hparams.num_samples,\n        softmax_k=self._hparams.softmax_k,\n        temperature_warmup_steps=self._hparams.temperature_warmup_steps,\n        do_hard_gumbel_softmax=self._hparams.do_hard_gumbel_softmax,\n        num_flows=self._hparams.num_flows,\n        approximate_gs_entropy=self._hparams.approximate_gs_entropy,\n        discrete_mix=self._hparams.d_mix,\n        noise_dev=self._hparams.noise_dev,\n        startup_steps=self.hparams.startup_steps,\n        summary=_DO_SUMMARIES)\n    # Set the discretization bottleneck specific things here\n    if self._hparams.bottleneck_kind in [\"dvq\", \"gumbel-softmax-dvq\"]:\n      z_size_per_residual = self._hparams.z_size / self._hparams.num_residuals\n      block_dim = int(self._hparams.hidden_size // self._hparams.num_blocks)\n      block_v_size = 2**(z_size_per_residual / self._hparams.num_blocks)\n      block_v_size = int(block_v_size)\n\n      if self._hparams.reshape_method == \"project\":\n        tf.logging.info(\"Using projections for DVQ\")\n        tf.logging.info(\"Trainable projections = {}\".format(\n            self._hparams.trainable_projections))\n\n        projection_tensors = tf.get_variable(\n            name=\"projection\",\n            shape=[\n                self._hparams.num_residuals, self._hparams.num_blocks,\n                self._hparams.hidden_size, block_dim\n            ],\n            initializer=tf.contrib.layers.xavier_initializer(),\n            trainable=self._hparams.trainable_projections)\n\n        self._hparams.bottleneck = functools.partial(\n            self._hparams.bottleneck, projection_tensors=projection_tensors)\n      elif self._hparams.reshape_method == \"slice\":\n        tf.logging.info(\"Using slices for DVQ\")\n      else:\n        raise ValueError(\"Unknown reshape method\")\n\n      means = tf.get_variable(\n          name=\"means\",\n          shape=[\n              self._hparams.num_residuals, self._hparams.num_blocks,\n              block_v_size, block_dim\n          ],\n          initializer=tf.uniform_unit_scaling_initializer())\n\n      # Create the shadow variables if we are using EMA\n      ema_count = None\n      ema_means = None\n      if self._hparams.ema:\n        ema_count = []\n        for i in range(self._hparams.num_residuals):\n          ema_count_i = tf.get_variable(\n              \"ema_count_{}\".format(i),\n              [self._hparams.num_blocks, block_v_size],\n              initializer=tf.constant_initializer(0),\n              trainable=False)\n          ema_count.append(ema_count_i)\n        with tf.colocate_with(means):\n          ema_means = []\n          for i in range(self._hparams.num_residuals):\n            ema_means_i = tf.get_variable(\n                \"ema_means_{}\".format(i),\n                [self._hparams.num_blocks, block_v_size, block_dim],\n                initializer=(lambda shape, dtype=None, partition_info=None,  # pylint: disable=g-long-lambda\n                                    verify_shape=None:\n                             means.initialized_value()[i]),\n                trainable=False)\n            ema_means.append(ema_means_i)\n\n      # Update bottleneck\n      self._hparams.bottleneck = functools.partial(\n          self._hparams.bottleneck,\n          means=means,\n          ema_count=ema_count,\n          ema_means=ema_means)\n\n  def body(self, features):\n    inputs = features[\"inputs\"] if \"inputs\" in features else None\n    if self._hparams.drop_inputs:\n      inputs = None\n    reuse = \"cache_raw\" in features\n    with tf.variable_scope(tf.get_variable_scope(), reuse=reuse):\n      res, loss, _, self._data_dim, self._latent_dim = ae_transformer_internal(\n          inputs,\n          features[\"targets\"],\n          features[\"target_space_id\"],\n          self._hparams,\n          features.get(\"cache_raw\", None),\n          predict_mask=self.predict_mask)\n      return res, loss\n\n  def prepare_features_for_infer(self, features):\n    if self._hparams.do_mask or not self._hparams.do_ae:\n      return features\n    beam_batch_size = self._decode_hparams.beam_size\n    beam_batch_size *= self._decode_hparams.batch_size\n    inputs = tf.zeros([beam_batch_size, 1, 1, self._hparams.hidden_size])\n    inputs = inputs if \"inputs\" in features else None\n    if self._hparams.drop_inputs or not self.has_input:\n      inputs = None\n    targets = tf.zeros([beam_batch_size, 1, 1, self._hparams.hidden_size])\n    with tf.variable_scope(\"body\"):\n      _, _, cache, _, _ = ae_transformer_internal(\n          inputs, targets, features[\"target_space_id\"], self._hparams)\n    features[\"cache_raw\"] = cache\n\n  def infer(self, features=None, decode_length=50, beam_size=1, top_beams=1,\n            alpha=0.0, use_tpu=False):\n    \"\"\"Produce predictions from the model.\"\"\"\n    if not self._hparams.do_mask:\n      infer_out = super(TransformerAE, self).infer(\n          features, decode_length, beam_size, top_beams, alpha, use_tpu=use_tpu)\n      return infer_out[\"outputs\"]\n    if not features:\n      features = {}\n    inputs_old = None\n    if \"inputs\" in features and len(features[\"inputs\"].shape) < 4:\n      inputs_old = features[\"inputs\"]\n      features[\"inputs\"] = tf.expand_dims(features[\"inputs\"], 2)\n\n    # Create an initial targets tensor.\n    if \"partial_targets\" in features:\n      initial_output = tf.convert_to_tensor(features[\"partial_targets\"])\n    else:\n      # inputs might not be present in features (e.g.: language modeling),\n      # in which case we fallback to 'infer_targets' for calculating initial\n      # input shape, type, etc.\n      inputs_or_targets = features.get(\"inputs\", features.get(\"infer_targets\"))\n      batch_size = common_layers.shape_list(inputs_or_targets)[0]\n      length = common_layers.shape_list(inputs_or_targets)[1]\n      hidden_dim = common_layers.shape_list(inputs_or_targets)[-1]\n      target_length = tf.to_int32(2.0 * tf.to_float(length))\n      initial_output = tf.zeros((batch_size, target_length, 1, hidden_dim),\n                                dtype=inputs_or_targets.dtype)\n\n    features[\"targets\"] = initial_output\n    logits, _ = self(features)  # pylint: disable=not-callable\n    # this should only happen if we're doing target_modality not real\n    if inputs_or_targets.dtype == tf.float32:\n      samples = logits\n    else:\n      samples = tf.argmax(logits, axis=-1)\n\n    # More steps.\n    self.predict_mask = 0.0  # Use the provided targets this time.\n    how_many_more_steps = 0  # Set to 1 or more for Gibbs-like sampling.\n    for _ in range(how_many_more_steps):\n      with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n        features[\"targets\"] = samples\n        logits, _ = self(features)  # pylint: disable=not-callable\n        if inputs_or_targets.dtype == tf.float32:\n          # When target_modality is real, the last axis does not represent\n          # classes, so it should not be argmax'ed\n          samples = logits\n        else:\n          samples = tf.argmax(logits, axis=-1)\n\n    self.predict_mask = 1.0\n    if inputs_old is not None:  # Restore to not confuse Estimator.\n      features[\"inputs\"] = inputs_old\n    return samples\n\n  def estimator_spec_eval(self, features, logits, labels, loss, losses_dict):\n    \"\"\"Constructs `tf.estimator.EstimatorSpec` for EVAL (evaluation) mode.\"\"\"\n    estimator_spec = super(TransformerAE, self).estimator_spec_eval(\n        features, logits, labels, loss, losses_dict)\n    if common_layers.is_xla_compiled():\n      # For TPUs (and XLA more broadly?), do not add summary hooks that depend\n      # on losses; they are not supported.\n      return estimator_spec\n\n    summary_op = tf.get_collection(tf.GraphKeys.SUMMARIES, scope=\"losses\")\n    summary_op.extend(tf.get_collection(tf.GraphKeys.SUMMARIES, scope=\"loss\"))\n    summary_op.append(tf.summary.scalar(\"loss\", loss))\n    summary_saver_hook = tf.train.SummarySaverHook(\n        save_steps=100,\n        summary_op=summary_op,\n        output_dir=os.path.join(self.hparams.model_dir, \"eval\"))\n\n    hooks = list(estimator_spec.evaluation_hooks)\n    hooks.append(summary_saver_hook)\n    return estimator_spec._replace(evaluation_hooks=hooks)\n\n  def _summarize_losses(self, losses_dict):\n    \"\"\"Adds `tf.summary`s to all terms in the losses dictionary.\"\"\"\n    super(TransformerAE, self)._summarize_losses(losses_dict)\n    nats_per_dim, bits_per_dim = latent_layers.compute_nats_and_bits_per_dim(\n        data_dim=self._data_dim,\n        latent_dim=self._latent_dim,\n        average_reconstruction=losses_dict[\"training\"],\n        average_prior=losses_dict[\"latent_pred\"])\n    tf.summary.scalar(\"loss/nats_per_dim\", nats_per_dim)\n    tf.summary.scalar(\"loss/bits_per_dim\", bits_per_dim)\n\n\n@registry.register_hparams\ndef transformer_ae_small():\n  \"\"\"Set of hyperparameters.\"\"\"\n  hparams = transformer.transformer_small()\n  hparams.batch_size = 2048\n  hparams.learning_rate = 0.2\n  hparams.learning_rate_warmup_steps = 4000\n  hparams.num_hidden_layers = 3\n  hparams.hidden_size = 384\n  hparams.filter_size = 2048\n  hparams.add_hparam(\"compress_filter_size\", 2048 * 2)\n  hparams.label_smoothing = 0.0\n  hparams.optimizer = \"Adam\"  # Can be unstable, maybe try Adam.\n  hparams.optimizer_adam_epsilon = 1e-9\n  hparams.optimizer_adam_beta1 = 0.9\n  hparams.optimizer_adam_beta2 = 0.997  # Needs tuning, try 0.98 to 0.999.\n  hparams.add_hparam(\"z_size\", 14)\n  hparams.add_hparam(\"noise_dev\", 0.5)\n  hparams.add_hparam(\"d_mix\", 0.5)\n  hparams.add_hparam(\"logit_normalization\", True)\n  hparams.add_hparam(\"word_dropout\", 0.)\n  # Bottleneck kinds supported: dense, vae, semhash, gumbel-softmax, dvq.\n  hparams.add_hparam(\"bottleneck_kind\", \"semhash\")\n  hparams.add_hparam(\"num_blocks\", 1)\n  hparams.add_hparam(\"num_decode_blocks\", 1)\n  # Add an hparam for number of reiduals\n  hparams.add_hparam(\"num_residuals\", 1)\n  # Reshape method for DVQ: slice, project\n  hparams.add_hparam(\"word_shuffle\", 0.5)\n  hparams.add_hparam(\"causal\", True)\n  hparams.add_hparam(\"reshape_method\", \"slice\")\n  hparams.add_hparam(\"trainable_projections\", False)\n  hparams.add_hparam(\"unmasked_percentage\", 0.1)\n  hparams.add_hparam(\"do_ae\", True)\n  hparams.add_hparam(\"do_mask\", True)\n  hparams.add_hparam(\"use_predict_mask\", True)\n  hparams.add_hparam(\"do_refine\", False)\n  hparams.add_hparam(\"do_attend_compress\", False)\n  hparams.add_hparam(\"do_attend_decompress\", True)\n  hparams.add_hparam(\"do_residual_compress\", False)\n  hparams.add_hparam(\"drop_inputs\", False)\n  hparams.add_hparam(\"v_size\", 1024*64)\n  hparams.add_hparam(\"max_context_length\", 64)\n  hparams.add_hparam(\"num_compress_steps\", 3)\n  hparams.add_hparam(\"startup_steps\", 10000)\n  hparams.add_hparam(\"mask_startup_steps\", 50000)\n  hparams.add_hparam(\"z_dropout\", 0.1)\n  hparams.add_hparam(\"is_2d\", 0)\n  hparams.add_hparam(\"softmax_k\", 0)\n  hparams.add_hparam(\"decode_autoregressive\", True)\n  hparams.add_hparam(\"do_vae\", True)\n  hparams.add_hparam(\"bit_vae\", True)\n  hparams.add_hparam(\"beta\", 0.25)\n  hparams.add_hparam(\"epsilon\", 1e-5)\n  hparams.add_hparam(\"decay\", 0.999)\n  hparams.add_hparam(\"ema\", True)\n  hparams.add_hparam(\"random_top_k\", 1)\n  hparams.add_hparam(\"soft_em\", False)\n  hparams.add_hparam(\"num_samples\", 10)\n  hparams.add_hparam(\"inv_temp\", 1.0)\n  hparams.add_hparam(\"entropy_scale\", 0.0)\n  hparams.add_hparam(\"prior_scale\", 1.0)\n  hparams.add_hparam(\"do_hard_gumbel_softmax\", False)\n  hparams.add_hparam(\"num_flows\", 0)\n  hparams.add_hparam(\"approximate_gs_entropy\", False)\n  hparams.add_hparam(\"temperature_warmup_steps\", 150000)\n  hparams.add_hparam(\"sum_over_latents\", False)\n  hparams.force_full_predict = True\n\n  # task params\n  hparams.add_hparam(\"task\", \"translate\")  # translate or image tasks supported\n  return hparams\n\n\n@registry.register_hparams\ndef imagetransformer_ae_cifar():\n  \"\"\"Hyperparameters for CIFAR-10 experiments.\"\"\"\n  hparams = transformer_ae_small()\n  hparams.filter_size = 512\n  hparams.num_compress_steps = 3\n  hparams.startup_steps = 10000\n  hparams.is_2d = 0\n  hparams.learning_rate_warmup_steps = 8000\n  hparams.learning_rate = 0.2\n  hparams.hidden_size = 512\n  hparams.batch_size = 1\n  hparams.max_length = 256\n  hparams.dropout = 0.0\n  hparams.clip_grad_norm = 0.  # i.e. no gradient clipping\n  hparams.optimizer_adam_epsilon = 1e-9\n  hparams.learning_rate_decay_scheme = \"noam\"\n  hparams.learning_rate = 0.1\n  hparams.initializer_gain = 0.2\n  hparams.num_hidden_layers = 6\n  hparams.initializer = \"uniform_unit_scaling\"\n  hparams.weight_decay = 0.0\n  hparams.optimizer_adam_beta1 = 0.9\n  hparams.optimizer_adam_beta2 = 0.98\n  hparams.label_smoothing = 0.0\n  hparams.norm_type = \"layer\"\n  hparams.layer_prepostprocess_dropout = 0.0\n  hparams.num_heads = 8\n  hparams.task = \"image\"\n  hparams.ffn_layer = \"conv_hidden_relu\"\n  # All hyperparameters ending in \"dropout\" are automatically set to 0.0\n  # when not in training mode.\n  hparams.attention_dropout = 0.0\n  hparams.relu_dropout = 0.\n  hparams.pos = \"timing\"  # timing, none\n  hparams.nbr_decoder_problems = 1\n  hparams.num_output_layers = 3\n  # TODO(trandustin): semhash doesn't work if filter_size != hidden_size. For\n  # now, set default to dvq.\n  hparams.bottleneck_kind = \"dvq\"\n  hparams.add_hparam(\"block_size\", 1)\n\n  # dilated attention based flags\n  hparams.add_hparam(\"gap_sizes\", [2, 4, 8, 16, 32, 64, 2, 4, 8, 16, 32, 64])\n  hparams.add_hparam(\"dilated_attention\", False)\n\n  # image size related flags\n  # assuming that the image has same height and width\n  hparams.add_hparam(\"img_len\", 32)\n  hparams.add_hparam(\"num_channels\", 3)\n  # Local attention params\n  hparams.add_hparam(\"local_and_global_att\", False)\n  hparams.add_hparam(\"block_length\", 256)\n  hparams.add_hparam(\"block_width\", 128)\n  hparams.num_encoder_layers = 4\n  hparams.num_decoder_layers = 12\n  hparams.add_hparam(\"dec_attention_type\", cia.AttentionType.LOCAL_1D)\n  hparams.add_hparam(\"block_raster_scan\", False)\n  hparams.add_hparam(\"shared_rel\", False)\n\n  # multipos attention params\n  hparams.add_hparam(\"q_filter_width\", 1)\n  hparams.add_hparam(\"kv_filter_width\", 1)\n\n  hparams.add_hparam(\"unconditional\", False)  # unconditional generation\n\n  hparams.modality[\"targets\"] = modalities.ImageChannelEmbeddingsBottom\n  hparams.drop_inputs = True\n  hparams.do_attend_compress = False\n  hparams.do_attend_decompress = False\n  return hparams\n\n\ndef imagetransformer_ae_imagenet():\n  \"\"\"For 64x64 ImageNet. ~56M trainable variables.\"\"\"\n  hparams = imagetransformer_ae_cifar()\n  hparams.max_length = int(64 * 64 * 3)\n  hparams.img_len = 64\n  hparams.num_heads = 4  # Heads are expensive on TPUs.\n  # Reduce architecture from 32x32 CIFAR-10 in order to fit in memory.\n  hparams.num_decoder_layers = 8\n  hparams.num_compress_steps = 2\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_ae_base():\n  \"\"\"Set of hyperparameters.\"\"\"\n  hparams = transformer_ae_small()\n  hparams.batch_size = 2048\n  hparams.hidden_size = 512\n  hparams.filter_size = 4096\n  hparams.num_hidden_layers = 6\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_ae_a3():\n  \"\"\"Set of hyperparameters.\"\"\"\n  hparams = transformer_ae_base()\n  hparams.batch_size = 4096\n  hparams.layer_prepostprocess_dropout = 0.3\n  hparams.optimizer = \"Adafactor\"\n  hparams.learning_rate = 0.25\n  hparams.learning_rate_warmup_steps = 10000\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_ae_a6():\n  \"\"\"Best hparams for transformer with semhash.\"\"\"\n  hparams = transformer_ae_a3()\n  hparams.optimizer = \"Adam\"\n  hparams.noise_dev = 0.5\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_ae_a8():\n  \"\"\"Set of hyperparameters.\"\"\"\n  hparams = transformer_ae_a3()\n  hparams.optimizer = \"Adafactor\"\n  hparams.noise_dev = 0.5\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_ae_base_tpu():\n  \"\"\"Base config adjusted for TPU.\"\"\"\n  hparams = transformer_ae_base()\n  transformer.update_hparams_for_tpu(hparams)\n  hparams.batch_size = 512\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_ae_base_noatt():\n  \"\"\"Set of hyperparameters.\"\"\"\n  hparams = transformer_ae_base()\n  hparams.reshape_method = \"slice\"\n  hparams.bottleneck_kind = \"dvq\"\n  hparams.hidden_size = 512\n  hparams.num_blocks = 1\n  hparams.num_decode_blocks = 1\n  hparams.z_size = 12\n  hparams.do_attend_decompress = False\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_ae_small_noatt():\n  \"\"\"Set of hyperparameters.\"\"\"\n  hparams = transformer_ae_small()\n  hparams.reshape_method = \"slice\"\n  hparams.bottleneck_kind = \"dvq\"\n  hparams.hidden_size = 512\n  hparams.num_blocks = 1\n  hparams.num_decode_blocks = 1\n  hparams.z_size = 12\n  hparams.do_attend_decompress = False\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_ae_base_ablation_1():\n  hparams = transformer_ae_base_noatt()\n  hparams.soft_em = True\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_ae_base_ablation_2():\n  hparams = transformer_ae_base_ablation_1()\n  hparams.entropy_scale = 0.1\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_ae_base_ablation_3():\n  hparams = transformer_ae_base_ablation_2()\n  hparams.prior_scale = 0.1\n  hparams.entropy_scale = 0.1\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_ae_base_ablation_4():\n  hparams = transformer_ae_base_ablation_3()\n  hparams.entropy_scale = 0.0\n  hparams.prior_scale = 1.0\n  hparams.bottleneck_kind = \"gumbel-softmax-dvq\"\n  hparams.do_hard_gumbel_softmax = True\n  hparams.approximate_gs_entropy = True\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_ae_base_ablation_5():\n  hparams = transformer_ae_base_ablation_4()\n  hparams.do_hard_gumbel_softmax = False\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_ae_base_iaf():\n  hparams = transformer_ae_base_ablation_5()\n  hparams.num_flows = 1\n  hparams.num_samples = 1\n  return hparams\n", " rest_framework import permissions\n\nfrom rest_framework.response import Response\nfrom django.http import JsonResponse\nfrom django.http import Http404\n\nfrom api_indy.indy.issuer import IssuerManager, IssuerException\nfrom api_indy.indy.credential_offer import CredentialOfferManager\nfrom api_indy.indy.credential import Credential, CredentialManager\nfrom api_indy.indy.proof_request import ProofRequest\nfrom api_indy.indy.proof import ProofManager\n\nfrom api_v2.decorators.jsonschema import validate\nfrom api_v2.jsonschema.issuer import ISSUER_JSON_SCHEMA\nfrom api_v2.jsonschema.credential_offer import CREDENTIAL_OFFER_JSON_SCHEMA\nfrom api_v2.jsonschema.credential import CREDENTIAL_JSON_SCHEMA\nfrom api_v2.jsonschema.construct_proof import CONSTRUCT_PROOF_JSON_SCHEMA\n\nfrom api_indy.tob_anchor.boot import indy_client, indy_holder_id\nfrom vonx.common.eventloop import run_coro\nfrom vonx.indy.messages import (\n    ProofRequest as VonxProofRequest,\n    ConstructedProof as VonxConstructedProof,\n)\n\nlogger = logging.getLogger(__name__)\n\n\n@api_view([\"POST\"])\n#@permission_classes((IsSignedRequest,))\n@validate(CREDENTIAL_OFFER_JSON_SCHEMA)\ndef generate_credential_request(request, *args, **kwargs):\n    \"\"\"\n    Processes a credential definition and responds with a credential request\n    which can then be used to submit a credential.\n\n    Example request payload:\n\n    ```json\n    {\n        'credential_offer': <credential offer json>,\n        'credential_definition': <credential definition json>\n    }\n    ```\n\n    returns:\n\n    ```\n    {\n        \"credential_request\": <credential request json>,\n        \"credential_request_metadata\": <credential request metadata json>\n    }\n    ```\n    \"\"\"\n\n    logger.warn(\">>> Generate credential request\")\n\n    credential_offer = request.data[\"credential_offer\"]\n    credential_definition_id = request.data[\"credential_definition_id\"]\n    credential_offer_manager = CredentialOfferManager(\n        credential_offer, credential_definition_id\n    )\n\n    credential_request, credential_request_metadata = (\n        credential_offer_manager.generate_credential_request()\n    )\n\n    result = {\n        \"credential_request\": credential_request,\n        \"credential_request_metadata\": credential_request_metadata,\n    }\n\n    logger.warn(\"<<< Generate credential request\")\n    return JsonResponse({\"success\": True, \"result\": result})\n\n\n@api_view([\"POST\"])\n#@permission_classes((IsSignedRequest,))\n@validate(CREDENTIAL_JSON_SCHEMA)\ndef store_credential(request, *args, **kwargs):\n    \"\"\"\n    Stores a verifiable credential in wallet.\n\n    The data in the credential is parsed and stored in the database\n    for search/display purposes based on the issuer's processor config.\n    The data is then made available through a REST API as well as a\n    search API.\n\n    Example request payload:\n\n    ```json\n    {\n        \"credential_data\": <credential data>,\n        \"credential_request_metadata\": <credential request metadata>\n    }\n    ```\n\n    returns: created verified credential model\n    \"\"\"\n    logger.warn(\">>> Store Credential\")\n\n    credential_data = request.data[\"credential_data\"]\n    credential_request_metadata = request.data[\"credential_request_metadata\"]\n\n    logger.info(credential_data)\n\n    credential = Credential(credential_data)\n    credential_manager = CredentialManager(\n        credential, credential_request_metadata\n    )\n\n    credential_wallet_id = credential_manager.process()\n\n    return Response({\"success\": True, \"result\": credential_wallet_id})\n\n\n@api_view([\"POST\"])\n#@permission_classes((IsSignedRequest,))\n@validate(ISSUER_JSON_SCHEMA)\n# TODO: Clean up abstraction. IssuerManager writes only \u2013\n#       use serializer in view to return created models?\ndef register_issuer(request, *args, **kwargs):\n    \"\"\"\n    Processes an issuer definition and creates or updates the\n    corresponding records. Responds with the updated issuer\n    definition including record IDs.\n\n    Example request payload:\n\n    ```json\n    {\n        \"issuer\": {\n            \"did\": \"6qnvgJtqwK44D8LFYnV5Yf\", // required\n            \"name\": \"BC Corporate Registry\", // required\n            \"abbreviation\": \"BCReg\",\n            \"email\": \"bcreg.test.issuer@example.ca\",\n            \"url\": \"http://localhost:5000\"\n        },\n        \"credential_types\": [\n            {\n            \"name\": \"Incorporation\",\n            \"schema\": \"incorporation.bc_registries\",\n            \"version\": \"1.0.31\",\n            \"endpoint\": \"http://localhost:5000/bcreg/incorporation\",\n            \"topic\": {\n                \"source_id\": {\n                    \"input\": \"corp_num\",\n                    \"from\": \"claim\"\n                },\n                \"type\": {\n                    \"input\": \"incorporation\",\n                    \"from\": \"value\"\n                }\n            },\n            \"mapping\": [\n                {\n                \"model\": \"name\",\n                \"fields\": {\n                    \"text\": {\n                        \"input\": \"legal_name\",\n                        \"from\": \"claim\"\n                    },\n                    \"type\": {\n                        \"input\": \"legal_name\",\n                        \"from\": \"value\"\n                    }\n                }\n                }\n            ]\n            },\n            {\n            \"name\": \"Doing Business As\",\n            \"schema\": \"doing_business_as.bc_registries\",\n            \"version\": \"1.0.31\",\n            \"endpoint\": \"http://localhost:5000/bcreg/dba\",\n            \"topic\": {\n                \"parent_source_id\": {\n                    \"input\": \"org_registry_id\",\n                    \"from\": \"claim\"\n                },\n                \"parent_type\": {\n                    \"input\": \"incorporation\",\n                    \"from\": \"value\"\n                },\n                \"source_id\": {\n                    \"input\": \"dba_corp_num\",\n                    \"from\": \"claim\"\n                },\n                \"type\": {\n                    \"input\": \"doing_business_as\",\n                    \"from\": \"value\"\n                }\n            },\n            \"mapping\": [\n                {\n                \"model\": \"name\",\n                \"fields\": {\n                    \"text\": {\n                        \"input\": \"dba_name\",\n                        \"from\": \"claim\"\n                    },\n                    \"type\": {\n                        \"input\": \"dba_name\",\n                        \"from\": \"value\"\n                    }\n                }\n                }\n            ]\n            },\n            {\n            \"name\": \"Corporate Address\",\n            \"schema\": \"address.bc_registries\",\n            \"version\": \"1.0.31\",\n            \"endpoint\": \"http://localhost:5000/bcreg/address\",\n            \"topic\": [\n                {\n                    \"parent_source_id\": {\n                        \"input\": \"org_registry_id\",\n                        \"from\": \"claim\"\n                    },\n                    \"parent_type\": {\n                        \"input\": \"incorporation\",\n                        \"from\": \"value\"\n                    },\n                    \"source_id\": {\n                        \"input\": \"dba_corp_num\",\n                        \"from\": \"claim\"\n                    },\n                    \"type\": {\n                        \"input\": \"doing_business_as\",\n                        \"from\": \"value\"\n                    }\n                },\n                {\n                    \"source_id\": {\n                        \"input\": \"org_registry_id\",\n                        \"from\": \"claim\"\n                    },\n                    \"type\": {\n                        \"input\": \"incorporation\",\n                        \"from\": \"value\"\n                    }\n                }\n            ],\n            \"cardinality_fields\": [\"addr_type\"],\n            \"mapping\": [\n                {\n                    \"model\": \"address\",\n                    \"fields\": {\n                        \"addressee\": {\n                            \"input\": \"addressee\",\n                            \"from\": \"claim\"\n                        },\n                        \"civic_address\": {\n                            \"input\": \"local_address\",\n                            \"from\": \"claim\"\n                        },\n                        \"city\": {\n                            \"input\": \"municipality\",\n                            \"from\": \"claim\"\n                        },\n                        \"province\": {\n                            \"input\": \"province\",\n                            \"from\": \"claim\"\n                        },\n                        \"postal_code\": {\n                            \"input\": \"postal_code\",\n                            \"from\": \"claim\",\n                            \"processor\": [\"string_helpers.uppercase\"]\n                        },\n                        \"country\": {\n                            \"input\": \"country\",\n                            \"from\": \"claim\"\n                        },\n                        \"type\": {\n                            \"input\": \"addr_type\",\n                            \"from\": \"claim\"\n                        },\n                        \"end_date\": {\n                            \"input\": \"end_date\",\n                            \"from\": \"claim\"\n                        }\n                    }\n                }\n            ]\n            }\n        ]\n        }\n    ```\n\n    returns:\n    ```\n    {\n        \"success\": true,\n        \"result\": {\n            \"issuer\": {\n                \"id\": 1,\n                \"did\": \"6qnvgJtqwK44D8LFYnV5Yf\",\n                \"name\": \"BC Corporate Registry\",\n                \"abbreviation\": \"BCReg\",\n                \"email\": \"bcreg.test.issuer@example.ca\",\n                \"url\": \"http://localhost:5000\"\n            },\n            \"schemas\": [\n                {\n                    \"id\": 1,\n                    \"name\": \"incorporation.bc_registries\",\n                    \"version\": \"1.0.31\",\n                    \"origin_did\": \"6qnvgJtqwK44D8LFYnV5Yf\"\n                },\n                {\n                    \"id\": 2,\n                    \"name\": \"doing_business_as.bc_registries\",\n                    \"version\": \"1.0.31\",\n                    \"origin_did\": \"6qnvgJtqwK44D8LFYnV5Yf\"\n                }\n            ],\n            \"credential_types\": [\n                {\n                    \"id\": 1,\n                    \"schema_id\": 1,\n                    \"issuer_id\": 1,\n                    \"description\": \"Incorporation\",\n                    \"processor_config\": null\n                },\n                {\n                    \"id\": 2,\n                    \"schema_id\": 2,\n                    \"issuer_id\": 1,\n                    \"description\": \"Doing Business As\",\n                    \"processor_config\": null\n                }\n            ]\n        }\n    }\n    ```\n    \"\"\"\n\n    logger.warn(\">>> Register issuer\")\n    try:\n        issuer_manager = IssuerManager()\n        updated = issuer_manager.register_issuer(request, request.data)\n        response = {\"success\": True, \"result\": updated}\n    except IssuerException as e:\n        logger.exception(\"Issuer request not accepted:\")\n        response = {\"success\": False, \"result\": str(e)}\n    logger.warn(\"<<< Register issuer\")\n    return JsonResponse(response)\n\n\n@api_view([\"POST\"])\n#@permission_classes((IsSignedRequest,))\n@validate(CONSTRUCT_PROOF_JSON_SCHEMA)\ndef construct_proof(request, *args, **kwargs):\n    \"\"\"\n    Constructs a proof given a proof request\n\n    ```json\n    {\n        \"proof_request\": <HL Indy proof request>\n    }\n    ```\n\n    returns: HL Indy proof data\n    \"\"\"\n    logger.warn(\">>> Construct Proof\")\n\n    proof_request = request.data.get(\"proof_request\")\n    cred_ids = request.data.get(\"credential_ids\")\n\n    if isinstance(cred_ids, str):\n        cred_ids = (c.strip() for c in 'a, b'.split(','))\n    if isinstance(cred_ids, list):\n        cred_ids = set(filter(None, cred_ids))\n    else:\n        cred_ids = None\n\n    proof_manager = ProofManager(proof_request, cred_ids)\n    proof = proof_manager.construct_proof()\n\n    return JsonResponse({\"success\": True, \"result\": proof})\n\n\n@api_view([\"GET\"])\n#@permission_classes((IsSignedRequest,))\ndef verify_credential(request, *args, **kwargs):\n    \"\"\"\n    Constructs a proof request for a credential stored in the\n    application database, constructs a proof for that proof\n    request, and then verifies it.\n\n    returns:\n\n    ```json\n    {\n        \"verified\": <verification successful boolean>,\n        \"proof\": <proof json>,\n        \"proof_request\": <proof_request json>,\n    }\n    ```\n    \"\"\"\n    logger.warn(\">>> Verify Credential\")\n    credential_id = kwargs.get(\"id\")\n\n    if not credential_id:\n        raise Http404\n\n    try:\n        credential = CredentialModel.objects.get(id=credential_id)\n    except CredentialModel.DoesNotExist as error:\n        logger.warn(error)\n        raise Http404\n\n    proof_request = ProofRequest(name=\"the-org-book\", version=\"1.0.0\")\n    proof_request.build_from_credential(credential)\n\n    proof_manager = ProofManager(proof_request.dict, {credential.wallet_id})\n    proof = proof_manager.construct_proof()\n\n    async def verify():\n        return await indy_client().verify_proof(\n            indy_holder_id(),\n            VonxProofRequest(proof_request.dict),\n            VonxConstructedProof(proof))\n    verified = run_coro(verify())\n\n    verified = verified.verified\n\n    return JsonResponse(\n        {\n            \"success\": verified,\n            \"result\": {\n                \"verified\": verified,\n                \"proof\": proof,\n                \"proof_request\": proof_request.dict,\n            },\n        }\n    )\n\n@api_view([\"GET\"])\ndef status(request, *args, **kwargs):\n    async def get_status():\n        return await indy_client().get_status()\n    return JsonResponse(run_coro(get_status()))\n", "chunksize=1024):\n    \"\"\"\n    An iterator over the decompressed bytes of a zlib-compressed file object.\n\n    Args:\n        fobj: The input file object. The file can be at any position, as long\n            as the current position is the start of the zlib stream.\n        chunksize (int): The number of bytes to read from the file at a time.\n\n    Returns:\n        iterator: An iterator yielding each byte as it's decompressed from the\n            file object.\n    \"\"\"\n    decompressor = zlib.decompressobj()\n    while True:\n        chunk = fobj.read(chunksize)\n        if not chunk:\n            yield from decompressor.flush()\n            break\n        yield from decompressor.decompress(chunk)\n\n\nSIZES = 'xmin', 'xmax', 'ymin', 'ymax'\nSIGNATURE_COMPRESSION = {\n    b'FWS': None,\n    b'CWS': 'zlib',\n    b'ZWS': 'lzma',\n}\n\n\ndef parse_flash_header(fobj):\n    \"\"\"\n    Parse (parts of) the header of a flash object.\n\n    The following information will be parsed out of the flash object:\n\n    ``compression``\n      A value indicating the compression format used. This can be either\n      :py:data:`None`, the string ``'zlib'``, or the string ``'lzma'``.\n\n    ``version``\n      The file version number.\n\n    ``size``\n      The decompressed (if applicable) size of the file.\n\n    ``width``, ``height``\n      The size of the on-screen display.\n\n    Args:\n        fobj: The input file object. Will not be closed.\n\n    Returns:\n        dict: A dict containing the following keys: ``compression``,\n            ``version``, ``size``, ``width``, ``height``.\n    \"\"\"\n    signature = fobj.read(3)\n    if signature not in SIGNATURE_COMPRESSION:\n        raise ValueError('not a SWF file')\n\n    ret = {}\n    ret['compression'] = SIGNATURE_COMPRESSION[signature]\n    ret['version'] = ord(fobj.read(1))\n    ret['size'], = struct.unpack(b'<I', fobj.read(4))\n\n    if ret['compression'] == 'zlib':\n        stream = iter_decompressed_zlib(fobj)\n    elif ret['compression'] == 'lzma':\n        fobj.seek(5, os.SEEK_CUR)\n        dict_size, = struct.unpack(b'<I', fobj.read(4))\n        filters = [{\n            'id': lzma.FILTER_LZMA1,\n            'dict_size': dict_size,\n        }]\n        lzma_fobj = lzma.LZMAFile(\n            fobj, 'rb', format=lzma.FORMAT_RAW, filters=filters)\n        stream = iter(lambda: ord(lzma_fobj.read(1)), b'')\n    else:\n        stream = iter(lambda: ord(fobj.read(1)), b'')\n\n    first_byte = next(stream)\n    bits_per_value = first_byte >> 3\n    mask = (1 << bits_per_value) - 1\n    nbits = 5 + bits_per_value * len(SIZES)\n    bytes_to_read = (nbits + 7) // 8 - 1\n    value_buffer = first_byte & 0b111\n    for byte in itertools.islice(stream, bytes_to_read):\n        value_buffer = (value_buffer << 8) | byte\n    stray_bits = 8 - nbits % 8\n    if stray_bits != 8:\n        value_buffer >>= stray_bits\n    bbox = {}\n    for name in reversed(SIZES):\n        bbox[name] = (value_buffer & mask) / 20\n        value_buffer >>= bits_per_value\n\n    if bbox['xmin'] != 0 or bbox['ymin'] != 0:\n        raise ValueError('invalid SWF file')\n    ret['width'] = bbox['xmax']\n    ret['height'] = bbox['ymax']\n\n    return ret\n", "figured\nfrom django.test.utils import captured_stdout\n\nfrom peps.converters import get_pep0_page, get_pep_page, add_pep_image\n\nfrom . import FAKE_PEP_REPO\n\n\nclass PEPConverterTests(TestCase):\n\n    def test_source_link(self):\n        pep = get_pep_page(FAKE_PEP_REPO, '0525')\n        self.assertEqual(pep.title, 'PEP 525 -- Asynchronous Generators')\n        self.assertIn(\n            'Source: <a href=\"https://github.com/python/peps/blob/master/'\n            'pep-0525.txt\">https://github.com/python/peps/blob/master/pep-0525.txt</a>',\n            pep.content.rendered\n        )\n\n    def test_source_link_rst(self):\n        pep = get_pep_page(FAKE_PEP_REPO, '0012')\n        self.assertEqual(pep.title, 'PEP 12 -- Sample reStructuredText PEP Template')\n        self.assertIn(\n            'Source: <a href=\"https://github.com/python/peps/blob/master/'\n            'pep-0012.rst\">https://github.com/python/peps/blob/master/pep-0012.rst</a>',\n            pep.content.rendered\n        )\n\n    def test_invalid_pep_number(self):\n        with captured_stdout() as stdout:\n            get_pep_page(FAKE_PEP_REPO, '9999999')\n        self.assertRegex(\n            stdout.getvalue(),\n            r\"PEP Path '(.*)9999999(.*)' does not exist, skipping\"\n        )\n\n    def test_add_image_not_found(self):\n        with captured_stdout() as stdout:\n            add_pep_image(FAKE_PEP_REPO, '0525', '/path/that/does/not/exist')\n        self.assertRegex(\n            stdout.getvalue(),\n            r\"Image Path '(.*)/path/that/does/not/exist(.*)' does not exist, skipping\"\n        )\n\n    def test_html_do_not_prettify(self):\n        pep = get_pep_page(FAKE_PEP_REPO, '3001')\n        self.assertEqual(\n            pep.title,\n            'PEP 3001 -- Procedure for reviewing and improving standard library modules'\n        )\n        self.assertIn(\n            '<tr class=\"field\"><th class=\"field-name\">Title:</th>'\n            '<td class=\"field-body\">Procedure for reviewing and improving '\n            'standard library modules</td>\\n</tr>',\n            pep.content.rendered\n        )\n\n    def test_strip_html_and_body_tags(self):\n        pep = get_pep_page(FAKE_PEP_REPO, '0525')\n        self.assertNotIn('<html>', pep.content.rendered)\n        self.assertNotIn('</html>', pep.content.rendered)\n        self.assertNotIn('<body>', pep.content.rendered)\n        self.assertNotIn('</body>', pep.content.rendered)\n", "umber2))\nprint(\"Difference: %d\" % (number1 - number2))\nprint(\"Product: %d\" % (number1 * number2))\nprint(\"Average: %.2f\" % ((number1 + number2) / 2))\nprint(\"Distance: %d\" % abs(number1 - number2))\nprint(\"Maximum: %d\" % max(number1, number2))\nprint(\"Minimum: %d\" % min(number1, number2))\n", " in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport csv\nimport tempfile\n\nfrom bndl.compute.tests import ComputeTest\nimport numpy as np\nimport pandas as pd\n\n\nclass DataFrameTest(ComputeTest):\n    df_size = 100\n\n    @classmethod\n    def setUpClass(cls):\n        super().setUpClass()\n        cls.df = cls.ctx.range(cls.df_size).map(lambda i: dict(num=i, txt=str(i))).as_dataframe()\n\n    def test_from_range(self):\n        df = self.df.collect()\n        self.assertTrue((df['num'] == np.arange(100)).all())\n        self.assertTrue((df['txt'] == list(map(str, range(100)))).all())\n        self.assertTrue((self.df['num'].collect() == list(range(self.df_size))).all())\n\n    def test_assign(self):\n        nums2 = self.df['num'].map(lambda i: i ** 2)\n        with_nums2 = self.df.assign('num2', nums2).collect()\n\n        self.assertIn('num', with_nums2)\n        self.assertIn('num2', with_nums2)\n\n        self.assertTrue((with_nums2['num'] == np.arange(100)).all())\n        self.assertTrue((with_nums2['num2'] == np.arange(100) ** 2).all())\n\n    def test_csv(self):\n        n_files = 10\n        n_rows_per_file = 100\n        n_rows = n_rows_per_file * n_files\n        files = [tempfile.NamedTemporaryFile('w+t')\n                 for _ in range(n_files)]\n        for offset, file in enumerate(files):\n            offset *= n_rows_per_file\n            writer = csv.writer(file.file)\n            rows = [[i, i ** 2, i // 2] for i in range(offset, offset + n_rows_per_file)]\n            writer.writerow(list('abc'))\n            for row in rows:\n                writer.writerow(row)\n            file.file.flush()\n\n        df = self.ctx.files([file.name for file in files],\n                             psize_bytes=1024, psize_files=3).parse_csv().collect()\n\n        expected = pd.DataFrame(dict(\n            a=np.arange(n_rows),\n            b=np.arange(n_rows) ** 2,\n            c=np.arange(n_rows) // 2,\n        ))\n\n        self.assertTrue((df == expected).all().all())\n\n        for file in files:\n            file.close()\n", "r\nfrom django.utils.translation import ugettext as _\n\nfrom daiquiri.core.utils import get_detail_fields\n\nfrom .models import Profile\n\n\nclass UserForm(forms.ModelForm):\n    class Meta:\n        model = User\n        fields = ('first_name', 'last_name')\n        widgets = {\n            'first_name': forms.TextInput(attrs={'placeholder': _('First name')}),\n            'last_name': forms.TextInput(attrs={'placeholder': _('Last name')})\n        }\n\n\nclass ProfileForm(forms.ModelForm):\n\n    class Meta:\n        model = Profile\n        fields = ()\n\n    def __init__(self, *args, **kwargs):\n        super(ProfileForm, self).__init__(*args, **kwargs)\n\n        for key, field in get_detail_fields(settings.AUTH_DETAIL_KEYS):\n            if self.instance.details and key in self.instance.details:\n                field.initial = self.instance.details[key]\n\n            self.fields[key] = field\n\n    def save(self, *args, **kwargs):\n        # create an empty details dict if it does not exist\n        if not self.instance.details:\n            self.instance.details = {}\n\n        # store the form date for each detail key\n        for detail_key in settings.AUTH_DETAIL_KEYS:\n            self.instance.details[detail_key['key']] = self.cleaned_data[detail_key['key']]\n\n        return super(ProfileForm, self).save(*args, **kwargs)\n\n\nclass SignupForm(ProfileForm):\n\n    first_name = forms.CharField(max_length=30,\n        label=_('First name'), widget=forms.TextInput(attrs={'placeholder': _('First name')}))\n    last_name = forms.CharField(max_length=30,\n        label=_('Last name'), widget=forms.TextInput(attrs={'placeholder': _('Last name')}))\n\n    field_order = ['username', 'email', 'password1', 'password2']\n\n    def __init__(self, *args, **kwargs):\n        super(SignupForm, self).__init__(*args, **kwargs)\n\n        # add a consent field, the label is added in the template\n        if settings.AUTH_TERMS_OF_USE:\n            self.fields['consent'] = forms.BooleanField(required=True)\n\n    def signup(self, request, user):\n        # create an empty details dict\n        user.profile.details = {}\n\n        # store the form date for each detail key\n        for detail_key in settings.AUTH_DETAIL_KEYS:\n            user.profile.details[detail_key['key']] = self.cleaned_data[detail_key['key']]\n\n        # store the consent field\n        if settings.AUTH_TERMS_OF_USE:\n            user.profile.consent = self.cleaned_data['consent']\n\n        # save the profile model\n        user.profile.save()\n", "ou may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"A Project Resource.\n\nSee: https://cloud.google.com/resource-manager/reference/rest/v1/projects\n\"\"\"\n\nfrom google.cloud.security.common.gcp_api import cloud_resource_manager as crm\nfrom google.cloud.security.common.gcp_type import resource\n\n\n# pylint: disable=too-few-public-methods\nclass ProjectLifecycleState(resource.LifecycleState):\n    \"\"\"Project lifecycle state.\"\"\"\n\n    DELETE_REQUESTED = 'DELETE_REQUESTED'\n\n\nclass Project(resource.Resource):\n    \"\"\"Project resource.\"\"\"\n\n    RESOURCE_NAME_FMT = 'projects/%s'\n\n    def __init__(\n            self,\n            project_id,\n            project_number=None,\n            name=None,\n            display_name=None,\n            parent=None,\n            lifecycle_state=ProjectLifecycleState.UNSPECIFIED):\n        \"\"\"Initialize.\n\n        Args:\n            project_id: The project string id.\n            project_number: The project number.\n            name: The full unique GCP name, i.e. \"projects/{projectId}\".\n            display_name: The display name.\n            parent: The parent Resource.\n            lifecycle_state: The project's lifecycle state.\n        \"\"\"\n        super(Project, self).__init__(\n            resource_id=project_id,\n            resource_type=resource.ResourceType.PROJECT,\n            name=name,\n            display_name=display_name,\n            parent=parent,\n            lifecycle_state=lifecycle_state)\n        self.project_number = project_number\n\n    def get_project_number(self):\n        \"\"\"Returns the project number.\"\"\"\n        return self.project_number\n\n    def exists(self):\n        \"\"\"Verify that the project exists.\n\n        Returns:\n            True if we can get the project from GCP, otherwise False.\n        \"\"\"\n        crm_client = crm.CloudResourceManagerClient()\n        project = crm_client.get_project(self.id)\n\n        return project is not None\n", "tributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom airflow.contrib.hooks.bigquery_hook import BigQueryHook\nfrom airflow.operators.check_operator import \\\n    CheckOperator, ValueCheckOperator, IntervalCheckOperator\nfrom airflow.utils.decorators import apply_defaults\n\n\nclass BigQueryCheckOperator(CheckOperator):\n    \"\"\"\n    Performs checks against BigQuery. The ``BigQueryCheckOperator`` expects\n    a sql query that will return a single row. Each value on that\n    first row is evaluated using python ``bool`` casting. If any of the\n    values return ``False`` the check is failed and errors out.\n\n    Note that Python bool casting evals the following as ``False``:\n\n    * ``False``\n    * ``0``\n    * Empty string (``\"\"``)\n    * Empty list (``[]``)\n    * Empty dictionary or set (``{}``)\n\n    Given a query like ``SELECT COUNT(*) FROM foo``, it will fail only if\n    the count ``== 0``. You can craft much more complex query that could,\n    for instance, check that the table has the same number of rows as\n    the source table upstream, or that the count of today's partition is\n    greater than yesterday's partition, or that a set of metrics are less\n    than 3 standard deviation for the 7 day average.\n\n    This operator can be used as a data quality check in your pipeline, and\n    depending on where you put it in your DAG, you have the choice to\n    stop the critical path, preventing from\n    publishing dubious data, or on the side and receive email alerts\n    without stopping the progress of the DAG.\n\n    :param sql: the sql to be executed\n    :type sql: str\n    :param bigquery_conn_id: reference to the BigQuery database\n    :type bigquery_conn_id: str\n    :param use_legacy_sql: Whether to use legacy SQL (true)\n        or standard SQL (false).\n    :type use_legacy_sql: bool\n    \"\"\"\n\n    template_fields = ('sql',)\n    template_ext = ('.sql', )\n\n    @apply_defaults\n    def __init__(self,\n                 sql,\n                 bigquery_conn_id='bigquery_default',\n                 use_legacy_sql=True,\n                 *args, **kwargs):\n        super(BigQueryCheckOperator, self).__init__(sql=sql, *args, **kwargs)\n        self.bigquery_conn_id = bigquery_conn_id\n        self.sql = sql\n        self.use_legacy_sql = use_legacy_sql\n\n    def get_db_hook(self):\n        return BigQueryHook(bigquery_conn_id=self.bigquery_conn_id,\n                            use_legacy_sql=self.use_legacy_sql)\n\n\nclass BigQueryValueCheckOperator(ValueCheckOperator):\n    \"\"\"\n    Performs a simple value check using sql code.\n\n    :param sql: the sql to be executed\n    :type sql: str\n    :param use_legacy_sql: Whether to use legacy SQL (true)\n        or standard SQL (false).\n    :type use_legacy_sql: bool\n    \"\"\"\n\n    template_fields = ('sql', 'pass_value',)\n    template_ext = ('.sql', )\n\n    @apply_defaults\n    def __init__(self, sql,\n                 pass_value,\n                 tolerance=None,\n                 bigquery_conn_id='bigquery_default',\n                 use_legacy_sql=True,\n                 *args, **kwargs):\n        super(BigQueryValueCheckOperator, self).__init__(\n            sql=sql, pass_value=pass_value, tolerance=tolerance,\n            *args, **kwargs)\n        self.bigquery_conn_id = bigquery_conn_id\n        self.use_legacy_sql = use_legacy_sql\n\n    def get_db_hook(self):\n        return BigQueryHook(bigquery_conn_id=self.bigquery_conn_id,\n                            use_legacy_sql=self.use_legacy_sql)\n\n\nclass BigQueryIntervalCheckOperator(IntervalCheckOperator):\n    \"\"\"\n    Checks that the values of metrics given as SQL expressions are within\n    a certain tolerance of the ones from days_back before.\n\n    This method constructs a query like so ::\n\n        SELECT {metrics_threshold_dict_key} FROM {table}\n        WHERE {date_filter_column}=<date>\n\n    :param table: the table name\n    :type table: str\n    :param days_back: number of days between ds and the ds we want to check\n        against. Defaults to 7 days\n    :type days_back: int\n    :param metrics_threshold: a dictionary of ratios indexed by metrics, for\n        example 'COUNT(*)': 1.5 would require a 50 percent or less difference\n        between the current day, and the prior days_back.\n    :type metrics_threshold: dict\n    :param use_legacy_sql: Whether to use legacy SQL (true)\n        or standard SQL (false).\n    :type use_legacy_sql: bool\n    \"\"\"\n\n    template_fields = ('table',)\n\n    @apply_defaults\n    def __init__(self, table, metrics_thresholds, date_filter_column='ds',\n                 days_back=-7, bigquery_conn_id='bigquery_default',\n                 use_legacy_sql=True, *args, **kwargs):\n        super(BigQueryIntervalCheckOperator, self).__init__(\n            table=table, metrics_thresholds=metrics_thresholds,\n            date_filter_column=date_filter_column, days_back=days_back,\n            *args, **kwargs)\n        self.bigquery_conn_id = bigquery_conn_id\n        self.use_legacy_sql = use_legacy_sql\n\n    def get_db_hook(self):\n        return BigQueryHook(bigquery_conn_id=self.bigquery_conn_id,\n                            use_legacy_sql=self.use_legacy_sql)\n", "y\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\nfrom oslo_policy import policy\n\nfrom mistral.policies import base\n\nWORKFLOWS = 'workflows:%s'\n\nrules = [\n    policy.DocumentedRuleDefault(\n        name=WORKFLOWS % 'create',\n        check_str=base.RULE_ADMIN_OR_OWNER,\n        description='Create a new workflow.',\n        operations=[\n            {\n                'path': '/v2/workflows',\n                'method': 'POST'\n            }\n        ]\n    ),\n    policy.DocumentedRuleDefault(\n        name=WORKFLOWS % 'delete',\n        check_str=base.RULE_ADMIN_OR_OWNER,\n        description='Delete a workflow.',\n        operations=[\n            {\n                'path': '/v2/workflows',\n                'method': 'DELETE'\n            }\n        ]\n    ),\n    policy.DocumentedRuleDefault(\n        name=WORKFLOWS % 'get',\n        check_str=base.RULE_ADMIN_OR_OWNER,\n        description='Return the named workflow.',\n        operations=[\n            {\n                'path': '/v2/workflows/{workflow_id}',\n                'method': 'GET'\n            }\n        ]\n    ),\n    policy.DocumentedRuleDefault(\n        name=WORKFLOWS % 'list',\n        check_str=base.RULE_ADMIN_OR_OWNER,\n        description='Return a list of workflows.',\n        operations=[\n            {\n                'path': '/v2/workflows',\n                'method': 'GET'\n            }\n        ]\n    ),\n    policy.DocumentedRuleDefault(\n        name=WORKFLOWS % 'list:all_projects',\n        check_str=base.RULE_ADMIN_ONLY,\n        description='Return a list of workflows from all projects.',\n        operations=[\n            {\n                'path': '/v2/workflows',\n                'method': 'GET'\n            }\n        ]\n    ),\n    policy.DocumentedRuleDefault(\n        name=WORKFLOWS % 'publicize',\n        check_str=base.RULE_ADMIN_OR_OWNER,\n        description='Make a workflow publicly available',\n        operations=[\n            {\n                'path': '/v2/workflows',\n                'method': 'POST'\n            },\n            {\n                'path': '/v2/workflows',\n                'method': 'PUT'\n            }\n        ]\n    ),\n    policy.DocumentedRuleDefault(\n        name=WORKFLOWS % 'update',\n        check_str=base.RULE_ADMIN_OR_OWNER,\n        description='Update one or more workflows.',\n        operations=[\n            {\n                'path': '/v2/workflows',\n                'method': 'PUT'\n            }\n        ]\n    )\n]\n\n\ndef list_rules():\n    return rules\n", "encrypt.plugins import common\n\nfrom letsencrypt_nginx import obj\nfrom letsencrypt_nginx import nginxparser\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass NginxDvsni(common.Dvsni):\n    \"\"\"Class performs DVSNI challenges within the Nginx configurator.\n\n    :ivar configurator: NginxConfigurator object\n    :type configurator: :class:`~nginx.configurator.NginxConfigurator`\n\n    :ivar list achalls: Annotated :class:`~letsencrypt.achallenges.DVSNI`\n        challenges.\n\n    :param list indices: Meant to hold indices of challenges in a\n        larger array. NginxDvsni is capable of solving many challenges\n        at once which causes an indexing issue within NginxConfigurator\n        who must return all responses in order.  Imagine NginxConfigurator\n        maintaining state about where all of the SimpleHTTP Challenges,\n        Dvsni Challenges belong in the response array.  This is an optional\n        utility.\n\n    :param str challenge_conf: location of the challenge config file\n\n    \"\"\"\n\n    def perform(self):\n        \"\"\"Perform a DVSNI challenge on Nginx.\n\n        :returns: list of :class:`letsencrypt.acme.challenges.DVSNIResponse`\n        :rtype: list\n\n        \"\"\"\n        if not self.achalls:\n            return []\n\n        self.configurator.save()\n\n        addresses = []\n        default_addr = \"{0} default_server ssl\".format(\n            self.configurator.config.dvsni_port)\n\n        for achall in self.achalls:\n            vhost = self.configurator.choose_vhost(achall.domain)\n            if vhost is None:\n                logger.error(\n                    \"No nginx vhost exists with server_name matching: %s. \"\n                    \"Please specify server_names in the Nginx config.\",\n                    achall.domain)\n                return None\n\n            for addr in vhost.addrs:\n                if addr.default:\n                    addresses.append([obj.Addr.fromstring(default_addr)])\n                    break\n            else:\n                addresses.append(list(vhost.addrs))\n\n        # Create challenge certs\n        responses = [self._setup_challenge_cert(x) for x in self.achalls]\n\n        # Set up the configuration\n        self._mod_config(addresses)\n\n        # Save reversible changes\n        self.configurator.save(\"SNI Challenge\", True)\n\n        return responses\n\n    def _mod_config(self, ll_addrs):\n        \"\"\"Modifies Nginx config to include challenge server blocks.\n\n        :param list ll_addrs: list of lists of\n            :class:`letsencrypt_nginx.obj.Addr` to apply\n\n        :raises .MisconfigurationError:\n            Unable to find a suitable HTTP block to include DVSNI hosts.\n\n        \"\"\"\n        # Add the 'include' statement for the challenges if it doesn't exist\n        # already in the main config\n        included = False\n        directive = ['include', self.challenge_conf]\n        root = self.configurator.parser.loc[\"root\"]\n        main = self.configurator.parser.parsed[root]\n        for entry in main:\n            if entry[0] == ['http']:\n                body = entry[1]\n                if directive not in body:\n                    body.append(directive)\n                included = True\n                break\n        if not included:\n            raise errors.MisconfigurationError(\n                'LetsEncrypt could not find an HTTP block to include DVSNI '\n                'challenges in %s.' % root)\n\n        config = [self._make_server_block(pair[0], pair[1])\n                  for pair in itertools.izip(self.achalls, ll_addrs)]\n\n        self.configurator.reverter.register_file_creation(\n            True, self.challenge_conf)\n\n        with open(self.challenge_conf, \"w\") as new_conf:\n            nginxparser.dump(config, new_conf)\n\n    def _make_server_block(self, achall, addrs):\n        \"\"\"Creates a server block for a DVSNI challenge.\n\n        :param achall: Annotated DVSNI challenge.\n        :type achall: :class:`letsencrypt.achallenges.DVSNI`\n\n        :param list addrs: addresses of challenged domain\n            :class:`list` of type :class:`~nginx.obj.Addr`\n\n        :returns: server block for the challenge host\n        :rtype: list\n\n        \"\"\"\n        document_root = os.path.join(\n            self.configurator.config.work_dir, \"dvsni_page\")\n\n        block = [['listen', str(addr)] for addr in addrs]\n\n        block.extend([['server_name',\n                       achall.gen_response(achall.account.key).z_domain],\n                      ['include', self.configurator.parser.loc[\"ssl_options\"]],\n                      # access and error logs necessary for\n                      # integration testing (non-root)\n                      ['access_log', os.path.join(\n                          self.configurator.config.work_dir, 'access.log')],\n                      ['error_log', os.path.join(\n                          self.configurator.config.work_dir, 'error.log')],\n                      ['ssl_certificate', self.get_cert_path(achall)],\n                      ['ssl_certificate_key', self.get_key_path(achall)],\n                      [['location', '/'], [['root', document_root]]]])\n\n        return [['server'], block]\n", "ort re\n\ntry:\n  from urllib.parse import quote  # Python 3\nexcept ImportError:\n  from urllib import quote  # Python 2\n\n# An empty string which is 'str' type in Python 2 (i.e. a bytestring)\n# and 'str' type in Python 3 (i.e. a unicode string).\nACTUALLY_STR_EMPTY_STRING = str()\n\ntry:\n  str = unicode\nexcept NameError:\n  pass\n\n\ndef escape_uri_helper(v):\n  return quote(str(v), ACTUALLY_STR_EMPTY_STRING)\n\n_ESCAPE_MAP_FOR_ESCAPE_HTML__AND__NORMALIZE_HTML__AND__ESCAPE_HTML_NOSPACE__AND__NORMALIZE_HTML_NOSPACE = {\n  '\\x00': '&#0;',\n  '\\x09': '&#9;',\n  '\\x0a': '&#10;',\n  '\\x0b': '&#11;',\n  '\\x0c': '&#12;',\n  '\\x0d': '&#13;',\n  ' ': '&#32;',\n  '\\\"': '&quot;',\n  '&': '&amp;',\n  '\\'': '&#39;',\n  '-': '&#45;',\n  '/': '&#47;',\n  '<': '&lt;',\n  '=': '&#61;',\n  '>': '&gt;',\n  '`': '&#96;',\n  '\\x85': '&#133;',\n  '\\xa0': '&#160;',\n  '\\u2028': '&#8232;',\n  '\\u2029': '&#8233;',\n}\n\ndef _replacer_for_escape_html__and__normalize_html__and__escape_html_nospace__and__normalize_html_nospace(match):\n  ch = match.group(0)\n  return _ESCAPE_MAP_FOR_ESCAPE_HTML__AND__NORMALIZE_HTML__AND__ESCAPE_HTML_NOSPACE__AND__NORMALIZE_HTML_NOSPACE[ch]\n\n_ESCAPE_MAP_FOR_ESCAPE_JS_STRING__AND__ESCAPE_JS_REGEX = {\n  '\\x00': '\\\\x00',\n  '\\x08': '\\\\x08',\n  '\\x09': '\\\\t',\n  '\\x0a': '\\\\n',\n  '\\x0b': '\\\\x0b',\n  '\\x0c': '\\\\f',\n  '\\x0d': '\\\\r',\n  '\\\"': '\\\\x22',\n  '$': '\\\\x24',\n  '&': '\\\\x26',\n  '\\'': '\\\\x27',\n  '(': '\\\\x28',\n  ')': '\\\\x29',\n  '*': '\\\\x2a',\n  '+': '\\\\x2b',\n  ',': '\\\\x2c',\n  '-': '\\\\x2d',\n  '.': '\\\\x2e',\n  '/': '\\\\/',\n  ':': '\\\\x3a',\n  '<': '\\\\x3c',\n  '=': '\\\\x3d',\n  '>': '\\\\x3e',\n  '?': '\\\\x3f',\n  '[': '\\\\x5b',\n  '\\\\': '\\\\\\\\',\n  ']': '\\\\x5d',\n  '^': '\\\\x5e',\n  '{': '\\\\x7b',\n  '|': '\\\\x7c',\n  '}': '\\\\x7d',\n  '\\x85': '\\\\x85',\n  '\\u2028': '\\\\u2028',\n  '\\u2029': '\\\\u2029',\n}\n\ndef _replacer_for_escape_js_string__and__escape_js_regex(match):\n  ch = match.group(0)\n  return _ESCAPE_MAP_FOR_ESCAPE_JS_STRING__AND__ESCAPE_JS_REGEX[ch]\n\n_ESCAPE_MAP_FOR_ESCAPE_CSS_STRING = {\n  '\\x00': '\\\\0 ',\n  '\\x08': '\\\\8 ',\n  '\\x09': '\\\\9 ',\n  '\\x0a': '\\\\a ',\n  '\\x0b': '\\\\b ',\n  '\\x0c': '\\\\c ',\n  '\\x0d': '\\\\d ',\n  '\\\"': '\\\\22 ',\n  '&': '\\\\26 ',\n  '\\'': '\\\\27 ',\n  '(': '\\\\28 ',\n  ')': '\\\\29 ',\n  '*': '\\\\2a ',\n  '/': '\\\\2f ',\n  ':': '\\\\3a ',\n  ';': '\\\\3b ',\n  '<': '\\\\3c ',\n  '=': '\\\\3d ',\n  '>': '\\\\3e ',\n  '@': '\\\\40 ',\n  '\\\\': '\\\\5c ',\n  '{': '\\\\7b ',\n  '}': '\\\\7d ',\n  '\\x85': '\\\\85 ',\n  '\\xa0': '\\\\a0 ',\n  '\\u2028': '\\\\2028 ',\n  '\\u2029': '\\\\2029 ',\n}\n\ndef _replacer_for_escape_css_string(match):\n  ch = match.group(0)\n  return _ESCAPE_MAP_FOR_ESCAPE_CSS_STRING[ch]\n\n_ESCAPE_MAP_FOR_NORMALIZE_URI__AND__FILTER_NORMALIZE_URI__AND__FILTER_NORMALIZE_MEDIA_URI = {\n  '\\x00': '%00',\n  '\\x01': '%01',\n  '\\x02': '%02',\n  '\\x03': '%03',\n  '\\x04': '%04',\n  '\\x05': '%05',\n  '\\x06': '%06',\n  '\\x07': '%07',\n  '\\x08': '%08',\n  '\\x09': '%09',\n  '\\x0a': '%0A',\n  '\\x0b': '%0B',\n  '\\x0c': '%0C',\n  '\\x0d': '%0D',\n  '\\x0e': '%0E',\n  '\\x0f': '%0F',\n  '\\x10': '%10',\n  '\\x11': '%11',\n  '\\x12': '%12',\n  '\\x13': '%13',\n  '\\x14': '%14',\n  '\\x15': '%15',\n  '\\x16': '%16',\n  '\\x17': '%17',\n  '\\x18': '%18',\n  '\\x19': '%19',\n  '\\x1a': '%1A',\n  '\\x1b': '%1B',\n  '\\x1c': '%1C',\n  '\\x1d': '%1D',\n  '\\x1e': '%1E',\n  '\\x1f': '%1F',\n  ' ': '%20',\n  '\\\"': '%22',\n  '\\'': '%27',\n  '(': '%28',\n  ')': '%29',\n  '<': '%3C',\n  '>': '%3E',\n  '\\\\': '%5C',\n  '{': '%7B',\n  '}': '%7D',\n  '\\x7f': '%7F',\n  '\\x85': '%C2%85',\n  '\\xa0': '%C2%A0',\n  '\\u2028': '%E2%80%A8',\n  '\\u2029': '%E2%80%A9',\n  '\\uff01': '%EF%BC%81',\n  '\\uff03': '%EF%BC%83',\n  '\\uff04': '%EF%BC%84',\n  '\\uff06': '%EF%BC%86',\n  '\\uff07': '%EF%BC%87',\n  '\\uff08': '%EF%BC%88',\n  '\\uff09': '%EF%BC%89',\n  '\\uff0a': '%EF%BC%8A',\n  '\\uff0b': '%EF%BC%8B',\n  '\\uff0c': '%EF%BC%8C',\n  '\\uff0f': '%EF%BC%8F',\n  '\\uff1a': '%EF%BC%9A',\n  '\\uff1b': '%EF%BC%9B',\n  '\\uff1d': '%EF%BC%9D',\n  '\\uff1f': '%EF%BC%9F',\n  '\\uff20': '%EF%BC%A0',\n  '\\uff3b': '%EF%BC%BB',\n  '\\uff3d': '%EF%BC%BD',\n}\n\ndef _replacer_for_normalize_uri__and__filter_normalize_uri__and__filter_normalize_media_uri(match):\n  ch = match.group(0)\n  return _ESCAPE_MAP_FOR_NORMALIZE_URI__AND__FILTER_NORMALIZE_URI__AND__FILTER_NORMALIZE_MEDIA_URI[ch]\n\n\n_MATCHER_FOR_ESCAPE_HTML = re.compile(r'[\\x00\\x22\\x26\\x27\\x3c\\x3e]', re.U)\n\n_MATCHER_FOR_NORMALIZE_HTML = re.compile(r'[\\x00\\x22\\x27\\x3c\\x3e]', re.U)\n\n_MATCHER_FOR_ESCAPE_HTML_NOSPACE = re.compile(r'[\\x00\\x09-\\x0d \\x22\\x26\\x27\\x2d\\/\\x3c-\\x3e`\\x85\\xa0\\u2028\\u2029]', re.U)\n\n_MATCHER_FOR_NORMALIZE_HTML_NOSPACE = re.compile(r'[\\x00\\x09-\\x0d \\x22\\x27\\x2d\\/\\x3c-\\x3e`\\x85\\xa0\\u2028\\u2029]', re.U)\n\n_MATCHER_FOR_ESCAPE_JS_STRING = re.compile(r'[\\x00\\x08-\\x0d\\x22\\x26\\x27\\/\\x3c-\\x3e\\x5b-\\x5d\\x7b\\x7d\\x85\\u2028\\u2029]', re.U)\n\n_MATCHER_FOR_ESCAPE_JS_REGEX = re.compile(r'[\\x00\\x08-\\x0d\\x22\\x24\\x26-\\/\\x3a\\x3c-\\x3f\\x5b-\\x5e\\x7b-\\x7d\\x85\\u2028\\u2029]', re.U)\n\n_MATCHER_FOR_ESCAPE_CSS_STRING = re.compile(r'[\\x00\\x08-\\x0d\\x22\\x26-\\x2a\\/\\x3a-\\x3e@\\\\\\x7b\\x7d\\x85\\xa0\\u2028\\u2029]', re.U)\n\n_MATCHER_FOR_NORMALIZE_URI__AND__FILTER_NORMALIZE_URI__AND__FILTER_NORMALIZE_MEDIA_URI = re.compile(r'[\\x00- \\x22\\x27-\\x29\\x3c\\x3e\\\\\\x7b\\x7d\\x7f\\x85\\xa0\\u2028\\u2029\\uff01\\uff03\\uff04\\uff06-\\uff0c\\uff0f\\uff1a\\uff1b\\uff1d\\uff1f\\uff20\\uff3b\\uff3d]', re.U)\n\n_FILTER_FOR_FILTER_CSS_VALUE = re.compile(r\"\"\"^(?!-*(?:expression|(?:moz-)?binding))(?:(?:[.#]?-?(?:[_a-z0-9-]+)(?:-[_a-z0-9-]+)*-?|(?:rgb|rgba|hsl|hsla|calc)\\([- \t,+.!#%_0-9a-zA-Z]+\\)|[-+]?(?:[0-9]+(?:\\.[0-9]*)?|\\.[0-9]+)(?:e-?[0-9]+)?(?:[a-z]{1,4}|%)?|!important)(?:\\s*[, ]\\s*|\\Z))*\\Z\"\"\", re.U | re.I)\n\n_FILTER_FOR_FILTER_NORMALIZE_URI = re.compile(r\"\"\"^(?:(?:https?|mailto|ftp):|[^&:/?#]*(?:[/?#]|\\Z))\"\"\", re.U | re.I)\n\n_FILTER_FOR_FILTER_NORMALIZE_MEDIA_URI = re.compile(r\"\"\"^[^&:/?#]*(?:[/?#]|\\Z)|^https?:|^ftp:|^data:image/[a-z0-9+]+;base64,[a-z0-9+/]+=*\\Z|^blob:\"\"\", re.U | re.I)\n\n_FILTER_FOR_FILTER_IMAGE_DATA_URI = re.compile(r\"\"\"^data:image/(?:bmp|gif|jpe?g|png|tiff|webp);base64,[a-z0-9+/]+=*\\Z\"\"\", re.U | re.I)\n\n_FILTER_FOR_FILTER_SIP_URI = re.compile(r\"\"\"^sip:[0-9a-z;=\\-+._!~*' /():&$#?@,]+\\Z\"\"\", re.U | re.I)\n\n_FILTER_FOR_FILTER_SMS_URI = re.compile(r\"\"\"^sms:[0-9a-z;=\\-+._!~*' /():&$#?@,]+\\Z\"\"\", re.U | re.I)\n\n_FILTER_FOR_FILTER_TEL_URI = re.compile(r\"\"\"^tel:[0-9a-z;=\\-+._!~*' /():&$#?@,]+\\Z\"\"\", re.U | re.I)\n\n_FILTER_FOR_FILTER_HTML_ATTRIBUTES = re.compile(r\"\"\"^(?!on|src|(?:action|archive|background|cite|classid|codebase|content|data|dsync|href|http-equiv|longdesc|style|usemap)\\s*$)(?:[a-z0-9_$:-]*)\\Z\"\"\", re.U | re.I)\n\n_FILTER_FOR_FILTER_HTML_ELEMENT_NAME = re.compile(r\"\"\"^(?!base|iframe|link|noframes|noscript|object|script|style|textarea|title|xmp)[a-z0-9_$:-]*\\Z\"\"\", re.U | re.I)\n\n_FILTER_FOR_FILTER_CSP_NONCE_VALUE = re.compile(r\"\"\"^[a-zA-Z0-9+/_-]+={0,2}$\"\"\", re.U)\n\ndef escape_html_helper(value):\n  value = str(value)\n  return _MATCHER_FOR_ESCAPE_HTML.sub(\n      _replacer_for_escape_html__and__normalize_html__and__escape_html_nospace__and__normalize_html_nospace, value)\n\n\ndef normalize_html_helper(value):\n  value = str(value)\n  return _MATCHER_FOR_NORMALIZE_HTML.sub(\n      _replacer_for_escape_html__and__normalize_html__and__escape_html_nospace__and__normalize_html_nospace, value)\n\n\ndef escape_html_nospace_helper(value):\n  value = str(value)\n  return _MATCHER_FOR_ESCAPE_HTML_NOSPACE.sub(\n      _replacer_for_escape_html__and__normalize_html__and__escape_html_nospace__and__normalize_html_nospace, value)\n\n\ndef normalize_html_nospace_helper(value):\n  value = str(value)\n  return _MATCHER_FOR_NORMALIZE_HTML_NOSPACE.sub(\n      _replacer_for_escape_html__and__normalize_html__and__escape_html_nospace__and__normalize_html_nospace, value)\n\n\ndef escape_js_string_helper(value):\n  value = str(value)\n  return _MATCHER_FOR_ESCAPE_JS_STRING.sub(\n      _replacer_for_escape_js_string__and__escape_js_regex, value)\n\n\ndef escape_js_regex_helper(value):\n  value = str(value)\n  return _MATCHER_FOR_ESCAPE_JS_REGEX.sub(\n      _replacer_for_escape_js_string__and__escape_js_regex, value)\n\n\ndef escape_css_string_helper(value):\n  value = str(value)\n  return _MATCHER_FOR_ESCAPE_CSS_STRING.sub(\n      _replacer_for_escape_css_string, value)\n\n\ndef filter_css_value_helper(value):\n  value = str(value)\n  if not _FILTER_FOR_FILTER_CSS_VALUE.search(value):\n    return 'zSoyz'\n\n  return value\n\n\ndef normalize_uri_helper(value):\n  value = str(value)\n  return _MATCHER_FOR_NORMALIZE_URI__AND__FILTER_NORMALIZE_URI__AND__FILTER_NORMALIZE_MEDIA_URI.sub(\n      _replacer_for_normalize_uri__and__filter_normalize_uri__and__filter_normalize_media_uri, value)\n\n\ndef filter_normalize_uri_helper(value):\n  value = str(value)\n  if not _FILTER_FOR_FILTER_NORMALIZE_URI.search(value):\n    return 'about:invalid#zSoyz'\n\n  return _MATCHER_FOR_NORMALIZE_URI__AND__FILTER_NORMALIZE_URI__AND__FILTER_NORMALIZE_MEDIA_URI.sub(\n      _replacer_for_normalize_uri__and__filter_normalize_uri__and__filter_normalize_media_uri, value)\n\n\ndef filter_normalize_media_uri_helper(value):\n  value = str(value)\n  if not _FILTER_FOR_FILTER_NORMALIZE_MEDIA_URI.search(value):\n    return 'about:invalid#zSoyz'\n\n  return _MATCHER_FOR_NORMALIZE_URI__AND__FILTER_NORMALIZE_URI__AND__FILTER_NORMALIZE_MEDIA_URI.sub(\n      _replacer_for_normalize_uri__and__filter_normalize_uri__and__filter_normalize_media_uri, value)\n\n\ndef filter_image_data_uri_helper(value):\n  value = str(value)\n  if not _FILTER_FOR_FILTER_IMAGE_DATA_URI.search(value):\n    return 'data:image/gif;base64,zSoyz'\n\n  return value\n\n\ndef filter_sip_uri_helper(value):\n  value = str(value)\n  if not _FILTER_FOR_FILTER_SIP_URI.search(value):\n    return 'about:invalid#zSoyz'\n\n  return value\n\n\ndef filter_sms_uri_helper(value):\n  value = str(value)\n  if not _FILTER_FOR_FILTER_SMS_URI.search(value):\n    return 'about:invalid#zSoyz'\n\n  return value\n\n\ndef filter_tel_uri_helper(value):\n  value = str(value)\n  if not _FILTER_FOR_FILTER_TEL_URI.search(value):\n    return 'about:invalid#zSoyz'\n\n  return value\n\n\ndef filter_html_attributes_helper(value):\n  value = str(value)\n  if not _FILTER_FOR_FILTER_HTML_ATTRIBUTES.search(value):\n    return 'zSoyz'\n\n  return value\n\n\ndef filter_html_element_name_helper(value):\n  value = str(value)\n  if not _FILTER_FOR_FILTER_HTML_ELEMENT_NAME.search(value):\n    return 'zSoyz'\n\n  return value\n\n\ndef filter_csp_nonce_value_helper(value):\n  value = str(value)\n  if not _FILTER_FOR_FILTER_CSP_NONCE_VALUE.search(value):\n    return 'zSoyz'\n\n  return value\n\n_HTML_TAG_REGEX = re.compile(r\"\"\"<(?:!|/?([a-zA-Z][a-zA-Z0-9:\\-]*))(?:[^>'\"]|\"[^\"]*\"|'[^']*')*>\"\"\", re.U)\n\n_LT_REGEX = re.compile('<')\n\n_SAFE_TAG_WHITELIST = ('b', 'br', 'em', 'i', 's', 'strong', 'sub', 'sup', 'u')\n\n\n# END GENERATED CODE\n", "s are used for efficiency instead of multiple 'if's and 'elif's.\n\n\nclass DictionaryOfValues:\n    def __init__(self):\n        # For One Time Pad cipher\n        # Uses (X + Y) % 26 to generate letters\n        self.letters = (\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\",\n                        \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\",\n                        \"u\", \"v\", \"w\", \"x\", \"y\", \"z\")\n        self.values = (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,\n                       15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25)\n        self.lettervalues = dict(zip(self.letters, self.values))\n        self.letterassociation = dict(zip(self.values, self.letters))\n\n\nclass PhoneValuesSetOne:\n    def __init__(self):\n        # System one of Phone Cipher. Longer than system two's encoding.\n        # E.g. a = 2, b = 22, ..., z = 9999. Uses 0 or _ to separate letters on same buttons.\n        self.letters = (\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\",\n                        \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\",\n                        \"u\", \"v\", \"w\", \"x\", \"y\", \"z\")\n        self.values = (2, 22, 222, 3, 33, 333, 4, 44, 444, 5, 55, 555, 6, 66, 666,\n                       7, 77, 777, 7777, 8, 88, 888, 9, 99, 999, 9999)\n        self.lettervalues = dict(zip(self.letters, self.values))\n        self.letterassociation = dict(zip(self.values, self.letters))\n\n\nclass PhoneValuesSetTwo:\n    def __init__(self):\n        # System two of Phone Cipher. Shorter than system one's encoding.\n        # E.g. a = 21, b = 22, ..., z = 94. Does not need letter separators.\n        self.letters = (\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\",\n                        \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\",\n                        \"u\", \"v\", \"w\", \"x\", \"y\", \"z\")\n        self.values = (21, 22, 23, 31, 32, 33, 41, 42, 43, 51, 52, 53, 61, 62, 63,\n                       71, 72, 73, 74, 81, 82, 83, 91, 92, 93, 94)\n        self.lettervalues = dict(zip(self.letters, self.values))\n        self.letterassociation = dict(zip(self.values, self.letters))\n\n\nclass PolybiusSquare:\n    def __init__(self, system):\n        # The Polybius Square cipher's base.\n        if system == 1:\n            self.letters = (\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\",\n                            \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\",\n                            \"u\", \"v\", \"w\", \"x\", \"y\", \"z\")\n            self.values = (11, 12, 13, 14, 15, 21, 22, 23, 24, 24, 25, 31, 32, 33, 34,\n                           35, 41, 42, 43, 44, 45, 51, 52, 53, 54, 55)\n            self.lettervalues = dict(zip(self.letters, self.values))\n            self.letterassociation = dict(zip(self.values, self.letters))\n        elif system == 2:\n            self.letters = (\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\",\n                            \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\",\n                            \"u\", \"v\", \"w\", \"x\", \"y\", \"z\")\n            self.values = (11, 12, 13, 14, 15, 21, 22, 23, 24, 25, 13, 31, 32, 33, 34,\n                           35, 41, 42, 43, 44, 45, 51, 52, 53, 54, 55)\n            self.lettervalues = dict(zip(self.letters, self.values))\n            self.letterassociation = dict(zip(self.values, self.letters))\n", "function\n\nimport numpy as np\n\nfrom ray.rllib.agents.trainer import Trainer, with_common_config\nfrom ray.rllib.utils.annotations import override\n\n\n# yapf: disable\n# __sphinx_doc_begin__\nclass RandomAgent(Trainer):\n    \"\"\"Policy that takes random actions and never learns.\"\"\"\n\n    _name = \"RandomAgent\"\n    _default_config = with_common_config({\n        \"rollouts_per_iteration\": 10,\n    })\n\n    @override(Trainer)\n    def _init(self, config, env_creator):\n        self.env = env_creator(config[\"env_config\"])\n\n    @override(Trainer)\n    def _train(self):\n        rewards = []\n        steps = 0\n        for _ in range(self.config[\"rollouts_per_iteration\"]):\n            obs = self.env.reset()\n            done = False\n            reward = 0.0\n            while not done:\n                action = self.env.action_space.sample()\n                obs, r, done, info = self.env.step(action)\n                reward += r\n                steps += 1\n            rewards.append(reward)\n        return {\n            \"episode_reward_mean\": np.mean(rewards),\n            \"timesteps_this_iter\": steps,\n        }\n# __sphinx_doc_end__\n# don't enable yapf after, it's buggy here\n\n\nif __name__ == \"__main__\":\n    trainer = RandomAgent(\n        env=\"CartPole-v0\", config={\"rollouts_per_iteration\": 10})\n    result = trainer.train()\n    assert result[\"episode_reward_mean\"] > 10, result\n    print(\"Test: OK\")\n", "= {}\n\n    if task == 'pos':\n        tag_position = 1\n    elif task == 'chunk':\n        tag_position = 2\n    else:\n        tag_position = -1\n\n    for type in [\"train\", \"test\", \"valid\"] :\n        with open(os.path.join(path, type + \".txt\")) as f:\n            for line in f:\n                if len(line.strip()) is 0 :\n                    continue\n                lin = line.strip().split()\n                word = lin[0]\n                tag = lin[tag_position]\n                word = word.lower()\n                if word not in dicts['words2idx']:\n                    dicts['words2idx'][word] = len(dicts['words2idx'])\n                if tag not in dicts['labels2idx']:\n                    dicts['labels2idx'][tag] = len(dicts['labels2idx'])\n\n\n    out = {}\n    for type in [\"train\", \"test\", \"valid\"] :\n        w = []\n        l = []\n        ww = []\n        ll = []\n        with open(os.path.join(path, type + \".txt\")) as f:\n            for line in f:\n                if len(line.strip()) is 0 :\n                    if len(w) > 0:\n                        ww.append(w)\n                        ll.append(l)\n                        w = []\n                        l = []\n                    continue\n                lin = line.strip().split()\n                word = lin[0].lower()\n                tag = lin[tag_position]\n                w.append(dicts['words2idx'][word])\n                l.append(dicts['labels2idx'][tag])\n\n        out[type] = (ww, ll)\n    return out[\"train\"], out[\"valid\"], out[\"test\"], dicts\n", "y reluctantly using setuptools, so avoid this\n#__import__('pkg_resources').declare_namespace(__name__)\n\n# A Python \"namespace package\" http://www.python.org/dev/peps/pep-0382/\n# This always goes inside of a namespace package's __init__.py\n\nfrom pkgutil import extend_path\n__path__ = extend_path(__path__, __name__)\n", "mport datetime\nimport itertools\nimport collections\nimport logging\nfrom django_auth_ldap.backend import LDAPBackend\nfrom Directories.forms import dbForm, editForm, get_dynamic_form, selectForm, selectForm2, get_fields_dynamic, loginForm#, listForm\nfrom Directories.models import Department, Attributes, Employees, Instructors, Katefth, KatefthKykloi, Kykloi, KykloiExamina, ModuleKykloi, Modules, ModulesTutors, PubInstr, PubTypes, Publications, Ranks, Service, Users, Works\nfrom django import forms\nfrom django.core import serializers\nfrom django.db.models import get_models, get_app, get_model\nfrom django.http import HttpResponse, HttpResponseRedirect\nfrom django.views.generic import TemplateView,ListView\nfrom django.core.urlresolvers import reverse_lazy, reverse\nfrom django.template import loader, RequestContext\nfrom django.db import models\nfrom django.shortcuts import render, get_object_or_404, redirect, render_to_response\nfrom django.template import Template\nfrom django.core.paginator import Paginator, EmptyPage, PageNotAnInteger\nfrom django.http import Http404\nfrom django.forms.formsets import formset_factory\nfrom django.forms.models import modelformset_factory\nfrom django.forms import Textarea\nfrom django.contrib import messages\nfrom django.contrib.auth import authenticate, login, logout\nfrom django.contrib.auth.decorators import login_required\nfrom haystack.forms import ModelSearchForm\nfrom haystack.query import SearchQuerySet\nfrom haystack.views import SearchView\nfrom Directories.tables import get_table\n\n\nmodel_classes = []\nfield_choices = []\nfields_dict = []\nfield_list = []\nfield_models = []\nfield_values = {}\ndata = []\nupdate_list = []\n\t\ndef user_login(request):\n\tif request.user.is_authenticated():\n\t\treturn HttpResponseRedirect(reverse('Directories:index'))\n\tform = loginForm()\n\tif request.method == \"POST\":\n\t\tform = loginForm(request.POST)\n\t\tif form.is_valid():\n\t\t\tusername = form.cleaned_data['user']\n\t\t\tpassword = form.cleaned_data['password']\n\t\t\tprint \"user is:\", username, \"with password\", password\n\t\t\t# authenticate user\n\t\t\tuser = authenticate(username=username, password=password)\n\t\t\tif user is not None:\n\t\t\t\t# the password verified for the user\n\t\t\t\tif user.is_active:\n\t\t\t\t\tlogin(request, user)\n\t\t\t\t\tprint(\"User is valid, active and authenticated\")\n\t\t\t\t\treturn HttpResponseRedirect(reverse('Directories:index'))\n\t\t\t\t\t#return HttpResponse('Successful Authentication!')\n\t\t\t\telse:\n\t\t\t\t\tprint(\"The password is valid, but the account has been disabled!\")\n\t\t\t\t\treturn HttpResponse('Account disabled.')\n\t\t\telse:\n\t\t\t\t# the authentication system was unable to verify the username and password\n\t\t\t\tmessages.error(request, 'username and/or password were incorrect.')\n\t\t\t\treturn HttpResponseRedirect(reverse('Directories:login'))\n\t\t\t\t#return HttpResponse('The username and password were incorrect.')\n\t\t\tprint \"user is:\", user, \"with password\", password\n\t\telse:\n\t\t\treturn HttpResponse('ERROR in GET -- Return to form submission')\n\telse:\n\t\tform = loginForm()\n\t\tprint \"no POST - form: \", form.errors\n\t\tprint \"unbound form\"\n\treturn render(request, 'Directories/login.html', {'form':form})\n\t\n@login_required(login_url='Directories:login')\ndef index(request): #for two submit buttons:\n\tprint \"index\"\n\tclean(request) # clean all data that may have been previously added\n\tform = dbForm()\n\t# check if form has been submitted since user may have returned back to form page\n\tif request.method == 'GET':\t\n\t\tprint \"get request\"\n\t\tif \"_change\" in request.GET:\n\t\t\tform = dbForm(request.GET) \n\t\t\tprint \"i am in _change submit button\"\n\t\t\tif form.is_valid(): # All validation rules pass\n\t\t\t\tprint \"bound form, get data\"\n\t\t\t\tmodel_classes_field = form.cleaned_data['model_classes_field']\n\t\t\t\trequest.session['model_table'] = model_classes_field\t\n\t\t\t\tprint \"success\"\n\t\t\t\treturn HttpResponseRedirect(reverse('Directories:list_models'))\t\t\n\t\t\telse:\n\t\t\t\t#will return form errors\n\t\t\t\treturn render(request, 'Directories/index.html', {'form':form})\n\t\telif '_add' in request.GET:\n\t\t\tform = dbForm(request.GET)\n\t\t\tprint \"i am in _add submit button\"\n\t\t\tif form.is_valid(): # All validation rules pass\n\t\t\t\tprint \"bound form, get data\"\n\t\t\t\tmodel_classes_field = form.cleaned_data['model_classes_field']\n\t\t\t\trequest.session['model_table'] = model_classes_field\t\n\t\t\t\treturn HttpResponseRedirect(reverse('Directories:update_directories'))\t\n\t\t\telse:\n\t\t\t\t#will return form errors\n\t\t\t\treturn render(request, 'Directories/index.html', {'form':form})\n\t\t\n\t\telse:\n\t\t\tprint \"submit button not pressed.\"\n\telse:\n\t\tform = dbForm()\n\t\tprint \"no POST - form: \", form.errors\n\t\tprint \"unbound form\"\n \treturn render(request, 'Directories/index.html', {'form':form})\n\n'''\n\tView that displays a list of all model objects\n'''\n@login_required(login_url='Directories:login')\ndef dlist(request):\n\tprint \"list page\"\n\tprint \"field_list\", field_list\n\tm_tb_name = request.session['model_table']#request.GET['model_classes_field'] # get the model table name\n\tmodel_class = get_model('Directories', m_tb_name) #request.session['model_table']) #################################\n\tmodel_name = model_class._meta.db_table\n\tmodel_list = list(model_class.objects.all()) \n\tfields = get_model_fields(model_class)\n\tfield_names = model_class._meta.get_all_field_names()\n\tprint \"choices\", field_choices \t\t            \n\tif not field_list:\n\t\tcreate_field_list(model_class)\n\t\trequest.session['field_list'] = field_list\n\t\tprint \"field_list\", field_list\n\tm_values = model_class.objects.values_list(field_list[1], flat=True) \n\tif not field_choices:\n\t\tfor val in m_values:\n\t\t\tfield_choices.append( (val, val), )\n\ttable_class = get_table(m_tb_name)\n\ttable = table_class(model_class.objects.all())\n\ttable.paginate(page=request.GET.get('page', 1), per_page=25)\n\t#form = selectForm2(my_choices = field_choices)     \n\t#z = remove_field_list(model_class) ###################################!!NOTE: removes field_list. you may need \n\tif request.method == 'POST':\n\t\tpks = request.POST.getlist(\"selection\")\n\t\tprint \"pks ======\", pks\n\t\tselected_objects = model_class.objects.filter(pk__in=pks)\n\t\tprint \"selected_objects are: \", selected_objects\n\t\tif \"_delete\" in request.POST: \n\t\t\tprint \"_delete pressed\"\n\t\t\tselected_objects.delete()\n\t\t\tmessages.success(request, 'Selected fields deleted')\n\t\t\treturn HttpResponseRedirect(reverse('Directories:index'))\n\t\telif \"_edit\" in request.POST:\n\t\t\tprint \"You pressed update fields button in list template\"\n\t\t\t#form = selectForm2(request.POST, my_choices = field_choices)\n\t\t\t#if form.is_valid(): # All validation rules pass\n\t\t\t#\tprint \"selectForm VALID!\"\n\t\t\t#\tedit_items = form.cleaned_data['select_fields']\n\t\t\t#\titems = [value.encode(\"utf8\") for value in edit_items]\n\t\t\t#\tfor item in items:\n\t\t\t#\t\tprint \"item: \", item\n\t\t\t#\t\tinstance = model_class.objects.get(**{field_list[1]:item}) # default returns the model's id if called i.e. \"print instance\"\n\t\t\t#\t\tupdate_list.append(item)\n\t\t\tupdate_list.append(pks)\n\t\t\trequest.session['u_list'] = update_list\n\t\t\t#\trequest.session['u_list'] = update_list\n\t\t\t\t# insert fields and their values into a dictionary\n\t\t\t#\tfor count in reversed(range(1, len(field_list))):\n\t\t\t#\t\tf_val = getattr(instance, field_list[count])\n\t\t\t#\t\tfield_values[field_list[count]] = f_val\n\t\t\t#\t\tprint \"sooooo we have field_values: \", field_values\n\t\t\t#\t\tprint count\t\t\n\t\t\treturn HttpResponseRedirect(reverse('Directories:edit_models'))\n\t\t\t#else:\n\t\t\t\t#will probably return the form with errors -- not actually tested\n\t\t\t#\treturn render(request, 'Directories/list.html', {'model_class':model_class, 'model_name': model_name, \n\t#'model_list':model_list, 'fields':fields, 'field_names':field_names, 'field_list':field_list, \n\t#'model_name':model_name, 'form':form})\n\t#else:\n\t\t#form = selectForm2(my_choices = field_choices)\n\t#\tprint \"no POST - form: \", form.errors\n\t#\tprint \"unbound form\"\n\treturn render(request, 'Directories/list.html', {'model_class':model_class, 'model_name': model_name, \n\t'model_list':model_list, 'fields':fields, 'field_names':field_names, 'field_list':field_list, \n\t'model_name':model_name, 'table':table})\n\n\t\n'''\n\tView that updates a selected field. The idea is to create a queryset that will as an argument the field value that was\n\tposted in the list template. Then use a form to gather user input for the new values and after the form is valid assign \n\tthese to the corresponding queryset fields. In the end, the newly queryset has been made and you can save it in the DB.\n'''\n@login_required(login_url='Directories:login')\ndef modelEdit(request):\n\tprint \"Edit page\"\n\t#create a global list with the instances to be updated. This will be called in the modelEdit view\n\tmvar = request.session['model_table']\n\tmodel_class = get_model('Directories', mvar)\n\t# create querysets using the field chosen in the list template\n\tupdate_items = request.session['u_list']\n\tfield_names = request.session['field_list']\n\t# Get the field values list from dlist as shown below and then show the values in the edit template, inside the textboxes\n\tf_val_list = sorted(field_values.items()) #sort puts them in alphabetical order but you may need to change this for other models\n\tform_class = get_dynamic_form(mvar)\n\tif \"_alter\" in request.POST:\n\t\t#print \"instance: \", t\n\t\tprint \"alter presseedddd!\"\n\t\tform = form_class(request.POST)#, instance=t)\n\t\tif form.is_valid(): # All validation rules pass\n\t\t\t#print \"instance: \", t\n\t\t\tprint \"form valid! now start updating fields!\" \n\t\t\t# use the form data to change the queryset 't' field values\n\t\t\tprint 'start forloop', len(field_names)\n\t\t\t# iterate over update_list to get the pks from sent from the list template form\n\t\t\tfor count in range(0, len(update_items)):\n\t\t\t\tfor item in update_items:\n\t\t\t\t\tt = model_class.objects.get(pk=item[count])\n\t\t\t\t\tprint \"instance: \", t\n\t\t\t# iterate over field_names to get the user form input for each field in field_names\n\t\t\tfor count in range(1, len(field_names)):\n\t\t\t\tform_data = form.cleaned_data[field_names[count]]\n\t\t\t\tdata = form_data.encode(\"utf8\")\n\t\t\t\tprint \"FORM DATA:\", data\n\t\t\t\tprint \"field_name: \", field_names[count]\n\t\t\t\tsetattr(t, field_names[count], form_data)\n\t\t\t\tprint \"data\", getattr(t, field_names[count]).encode(\"utf8\")\n\t\t\t#print 'end forloop'\n\t\t\t##### alles epiloges gia to t:\n\t\t\t##### Foo.objects.get(pk=????).update(**data) opou data = {'field1': 'value1', 'field5': 'value5', 'field7': 'value7'}\n\t\t\t##### i kanw Foo.objects.get(pk=????).update(**{field_list[2]:item})\n\t\t\t# after changing the queryset data you can now save to database\n\t\t\tt.save()\n\t\t\t#send a message to inform users that form submission was a success\n\t\t\tmessages.success(request, 'Selected fields updated') # kalw ama kanei duplicate la8os na epistrefei validate_unique()\n\t\t\treturn HttpResponseRedirect(reverse('Directories:index')) \n\t\telse:\n\t\t\t# will return the form with an error message\n\t\t\treturn render(request, 'Directories/edit.html', {'form':form, 'field_list':field_list, 'f_val_list':f_val_list})\n\telse:\n\t\tfor item in update_list:\n\t\t\tt = model_class.objects.get(**{field_list[0]:item[0] for item in update_list})\n\t\tform = form_class(instance=t)\n\t\tprint \"something not working or submit button not pressed\"\n\treturn render(request, 'Directories/edit.html', {'form':form, 'field_list':field_list, 'f_val_list':f_val_list})\n\t\n'''\n\tView for creating a new row to a model dynamically.\n'''\n@login_required(login_url='Directories:login')\ndef modelUpdate(request):\t\n\tprint \"Creation page\"\n\tm_tb_name = request.session['model_table']\n\tprint 'model_classes_field', m_tb_name\n\tmodel_class = get_model('Directories', m_tb_name)\n\tmodel_name = model_class.__name__\n\tfield_names = model_class._meta.get_all_field_names()\t\t\t\n\tif request.method == 'POST': \n\t\tprint \"m_tb_name is: \", m_tb_name\n\t\tform_class = get_dynamic_form(m_tb_name)\n\t\tform = form_class(request.POST)\t\t\t\n\t\tif form.is_valid(): # All validation rules pass\n\t\t\tprint \"form is valid!\" \n\t\t\trow = form.save() \n\t\t\tform = dbForm()\n\t\t\t#send a message to inform users that form submission was a success\n\t\t\tmessages.success(request, 'New model data created!')\n\t\t\treturn HttpResponseRedirect(reverse('Directories:index'))\n\t\telse:\n\t\t\t#will probably return the form with errors\n\t\t\treturn render(request, 'Directories/create.html', {'form':form, 'model_name':model_name, 'field_names':field_names})\n\t\t\n\telse:\n\t\tform_class = get_dynamic_form(m_tb_name)\n\t\tform = form_class() # An unbound form\n\t\tprint \"no form submission: \", form.errors\n\treturn render(request, 'Directories/create.html', {'form':form, 'model_name':model_name, 'field_names':field_names})\n\n'''\n\tLogs out a user\n'''\ndef user_logout(request):\n\tlogout(request)\n\tmessages.success(request, 'Succesfully logged out') \n\treturn HttpResponseRedirect(reverse('Directories:login'))\n\t\ndef search(request):\n\treturn SearchView(template='list.html')(request)\n\t\n#returns a list of field names\ndef create_field_list(model):\n\tfor f_name in model._meta.get_all_field_names():\n\t\tfield_list.append(f_name)\n\treturn field_list\n\ndef remove_field_list(model):\t\n\tfor f_name in model._meta.get_all_field_names():\n\t\tfield_list.remove(f_name)\n\treturn field_list\n\t\ndef get_model_fields(model):\n\treturn model._meta.fields\n  \ndef get_field_data(model, field):\n\treturn model.objects.values_list(field, flat=True)\n\t\ndef clean(request):\n\t#check that model table name is not stored in the session before initiating it\n\tif 'model_table' in request.session:\n\t\tprint \"session model_table is:\", request.session['model_table']\n\t\tdel request.session['model_table']\t\n\telse:\n\t\tprint \"session model_table is null\"\n\t#clean field_list and field_choices to start from the beginning\n\tif field_list:\n\t\tdel field_list[:]\n\t\tprint 'field_list', field_list\n\tif field_choices:\n\t\tdel field_choices[:]\n\t\tprint 'field_list', field_choices\n", "rt gym\nimport numpy as np\nimport os\n\nimport ray.utils\n\nfrom ray.rllib.models.preprocessors import get_preprocessor\nfrom ray.rllib.evaluation.sample_batch_builder import SampleBatchBuilder\nfrom ray.rllib.offline.json_writer import JsonWriter\n\nif __name__ == \"__main__\":\n    batch_builder = SampleBatchBuilder()  # or MultiAgentSampleBatchBuilder\n    writer = JsonWriter(\n        os.path.join(ray.utils.get_user_temp_dir(), \"demo-out\"))\n\n    # You normally wouldn't want to manually create sample batches if a\n    # simulator is available, but let's do it anyways for example purposes:\n    env = gym.make(\"CartPole-v0\")\n\n    # RLlib uses preprocessors to implement transforms such as one-hot encoding\n    # and flattening of tuple and dict observations. For CartPole a no-op\n    # preprocessor is used, but this may be relevant for more complex envs.\n    prep = get_preprocessor(env.observation_space)(env.observation_space)\n    print(\"The preprocessor is\", prep)\n\n    for eps_id in range(100):\n        obs = env.reset()\n        prev_action = np.zeros_like(env.action_space.sample())\n        prev_reward = 0\n        done = False\n        t = 0\n        while not done:\n            action = env.action_space.sample()\n            new_obs, rew, done, info = env.step(action)\n            batch_builder.add_values(\n                t=t,\n                eps_id=eps_id,\n                agent_index=0,\n                obs=prep.transform(obs),\n                actions=action,\n                action_prob=1.0,  # put the true action probability here\n                rewards=rew,\n                prev_actions=prev_action,\n                prev_rewards=prev_reward,\n                dones=done,\n                infos=info,\n                new_obs=prep.transform(new_obs))\n            obs = new_obs\n            prev_action = action\n            prev_reward = rew\n            t += 1\n        writer.write(batch_builder.build_and_reset())\n# __sphinx_doc_end__\n", "Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n\nimport mock\nimport netaddr\nfrom neutron_lib.api.definitions import portbindings\nfrom neutron_lib import constants\nfrom oslo_utils import uuidutils\nimport webob.exc\n\nfrom neutron.db import db_base_plugin_v2\nfrom neutron.db import ipam_backend_mixin\nfrom neutron.db import portbindings_db\nfrom neutron.objects import subnet as subnet_obj\nfrom neutron.tests import base\nfrom neutron.tests.unit.db import test_db_base_plugin_v2\n\n\nclass TestIpamBackendMixin(base.BaseTestCase):\n\n    def setUp(self):\n        super(TestIpamBackendMixin, self).setUp()\n        self.mixin = ipam_backend_mixin.IpamBackendMixin()\n        self.ctx = mock.Mock()\n        self.default_new_ips = (('id-1', '192.168.1.1'),\n                                ('id-2', '192.168.1.2'))\n        self.default_original_ips = (('id-1', '192.168.1.1'),\n                                     ('id-5', '172.20.16.5'))\n        self.owner_non_router = constants.DEVICE_OWNER_DHCP\n        self.owner_router = constants.DEVICE_OWNER_ROUTER_INTF\n\n    def _prepare_ips(self, ips):\n        results = []\n        for ip in ips:\n            ip_dict = {'ip_address': ip[1],\n                       'subnet_id': ip[0]}\n            if len(ip) > 2:\n                ip_dict['delete_subnet'] = ip[2]\n            results.append(ip_dict)\n        return results\n\n    def _mock_slaac_subnet_on(self):\n        slaac_subnet_obj = subnet_obj.Subnet(\n            self.ctx,\n            ipv6_address_mode=constants.IPV6_SLAAC,\n            ipv6_ra_mode=constants.IPV6_SLAAC)\n        self.mixin._get_subnet_object = mock.Mock(\n            return_value=slaac_subnet_obj)\n\n    def _mock_slaac_subnet_off(self):\n        non_slaac_subnet_obj = subnet_obj.Subnet(\n            self.ctx,\n            ipv6_address_mode=None,\n            ipv6_ra_mode=None)\n        self.mixin._get_subnet_object = mock.Mock(\n            return_value=non_slaac_subnet_obj)\n\n    def _mock_slaac_for_subnet_ids(self, subnet_ids):\n        \"\"\"Mock incoming subnets as autoaddressed.\"\"\"\n        def _get_subnet_object(context, subnet_id):\n            if subnet_id in subnet_ids:\n                return subnet_obj.Subnet(\n                    self.ctx,\n                    ipv6_address_mode=constants.IPV6_SLAAC,\n                    ipv6_ra_mode=constants.IPV6_SLAAC)\n            else:\n                return subnet_obj.Subnet(\n                    self.ctx,\n                    ipv6_address_mode=None,\n                    ipv6_ra_mode=None)\n\n        self.mixin._get_subnet_object = mock.Mock(\n            side_effect=_get_subnet_object)\n\n    def _test_get_changed_ips_for_port(self, expected, original_ips,\n                                       new_ips, owner):\n        change = self.mixin._get_changed_ips_for_port(self.ctx,\n                                                      original_ips,\n                                                      new_ips,\n                                                      owner)\n\n        self.assertItemsEqual(expected.add, change.add)\n        self.assertItemsEqual(expected.original, change.original)\n        self.assertItemsEqual(expected.remove, change.remove)\n\n    def test__get_changed_ips_for_port(self):\n        new_ips = self._prepare_ips(self.default_new_ips)\n        original_ips = self._prepare_ips(self.default_original_ips)\n\n        expected_change = self.mixin.Changes(add=[new_ips[1]],\n                                             original=[original_ips[0]],\n                                             remove=[original_ips[1]])\n        self._test_get_changed_ips_for_port(expected_change, original_ips,\n                                            new_ips, self.owner_router)\n\n    def test__get_changed_ips_for_port_autoaddress(self):\n        new_ips = self._prepare_ips(self.default_new_ips)\n\n        original = (('id-1', '192.168.1.1'),\n                    ('id-5', '2000:1234:5678::12FF:FE34:5678'))\n        original_ips = self._prepare_ips(original)\n\n        self._mock_slaac_subnet_on()\n\n        expected_change = self.mixin.Changes(add=[new_ips[1]],\n                                             original=original_ips,\n                                             remove=[])\n        self._test_get_changed_ips_for_port(expected_change, original_ips,\n                                            new_ips, self.owner_non_router)\n\n    def test__get_changed_ips_for_port_remove_autoaddress(self):\n        new = (('id-5', '2000:1234:5678::12FF:FE34:5678', True),\n               ('id-1', '192.168.1.1'))\n        new_ips = self._prepare_ips(new)\n        reference_ips = [ip for ip in new_ips\n                         if ip['subnet_id'] == 'id-1']\n\n        original = (('id-5', '2000:1234:5678::12FF:FE34:5678'),)\n        original_ips = self._prepare_ips(original)\n\n        # mock ipv6 subnet as auto addressed and leave ipv4 as regular\n        self._mock_slaac_for_subnet_ids([new[0][0]])\n        # Autoaddressed ip allocation has to be removed\n        # if it has 'delete_subnet' flag set to True\n        expected_change = self.mixin.Changes(add=reference_ips,\n                                             original=[],\n                                             remove=original_ips)\n        self._test_get_changed_ips_for_port(expected_change, original_ips,\n                                            new_ips, self.owner_non_router)\n\n    def test__get_changed_ips_for_port_autoaddress_ipv6_pd_enabled(self):\n        owner_not_router = constants.DEVICE_OWNER_DHCP\n        new_ips = self._prepare_ips(self.default_new_ips)\n\n        original = (('id-1', '192.168.1.1'),\n                    ('id-5', '2000:1234:5678::12FF:FE34:5678'))\n        original_ips = self._prepare_ips(original)\n\n        # mock to test auto address part\n        pd_subnet_obj = subnet_obj.Subnet(\n            self.ctx,\n            id=uuidutils.generate_uuid(),\n            subnetpool_id=constants.IPV6_PD_POOL_ID,\n            ipv6_address_mode=constants.IPV6_SLAAC,\n            ipv6_ra_mode=constants.IPV6_SLAAC)\n        self.mixin._get_subnet_object = mock.Mock(return_value=pd_subnet_obj)\n\n        # make a copy of original_ips\n        # since it is changed by _get_changed_ips_for_port\n        expected_change = self.mixin.Changes(add=[new_ips[1]],\n                                             original=[original_ips[0]],\n                                             remove=[original_ips[1]])\n\n        self._test_get_changed_ips_for_port(expected_change, original_ips,\n                                            new_ips, owner_not_router)\n\n    def _test_get_changed_ips_for_port_no_ip_address(self):\n        # IP address should be added if only subnet_id is provided,\n        # independently from auto_address status for subnet\n        new_ips = [{'subnet_id': 'id-3'}]\n        original_ips = []\n\n        expected_change = self.mixin.Changes(add=[new_ips[0]],\n                                             original=[],\n                                             remove=[])\n        self._test_get_changed_ips_for_port(expected_change, original_ips,\n                                            new_ips, self.owner_non_router)\n\n    def test__get_changed_ips_for_port_no_ip_address_no_slaac(self):\n        self._mock_slaac_subnet_off()\n        self._test_get_changed_ips_for_port_no_ip_address()\n\n    def test__get_changed_ips_for_port_no_ip_address_slaac(self):\n        self._mock_slaac_subnet_on()\n        self._test_get_changed_ips_for_port_no_ip_address()\n\n    def test__get_changed_ips_for_port_subnet_id_no_ip(self):\n        # If a subnet is specified without an IP address only allocate a new\n        # address if one doesn't exist\n        self._mock_slaac_subnet_off()\n        new_ips = [{'subnet_id': 'id-3'}]\n        original_ips = [{'subnet_id': 'id-3', 'ip_address': '4.3.2.1'}]\n\n        expected_change = self.mixin.Changes(\n            add=[],\n            original=[{'subnet_id': 'id-3', 'ip_address': '4.3.2.1'}],\n            remove=[])\n        self._test_get_changed_ips_for_port(expected_change, original_ips,\n                                            new_ips, self.owner_non_router)\n\n    def test__get_changed_ips_for_port_multiple_ips_one_subnet_add_third(self):\n        # If a subnet is specified without an IP address only allocate a new\n        # address if one doesn't exist\n        self._mock_slaac_subnet_off()\n        new_ips = [{'subnet_id': 'id-3', 'ip_address': '4.3.2.1'},\n                   {'subnet_id': 'id-3'},\n                   {'subnet_id': 'id-3', 'ip_address': '4.3.2.10'}]\n        original_ips = [{'subnet_id': 'id-3', 'ip_address': '4.3.2.1'},\n                        {'subnet_id': 'id-3', 'ip_address': '4.3.2.10'}]\n\n        expected_change = self.mixin.Changes(\n            add=[{'subnet_id': 'id-3'}],\n            original=[{'subnet_id': 'id-3', 'ip_address': '4.3.2.1'},\n                      {'subnet_id': 'id-3', 'ip_address': '4.3.2.10'}],\n            remove=[])\n        self._test_get_changed_ips_for_port(expected_change, original_ips,\n                                            new_ips, self.owner_non_router)\n\n    def test__get_changed_ips_for_port_multiple_ips_one_subnet_noip(self):\n        # If a subnet is specified without an IP address only allocate a new\n        # address if one doesn't exist\n        self._mock_slaac_subnet_off()\n        new_ips = [{'subnet_id': 'id-3'},\n                   {'subnet_id': 'id-3'}]\n        original_ips = [{'subnet_id': 'id-3', 'ip_address': '4.3.2.1'},\n                        {'subnet_id': 'id-3', 'ip_address': '4.3.2.10'}]\n\n        expected_change = self.mixin.Changes(\n            add=[],\n            original=[{'subnet_id': 'id-3', 'ip_address': '4.3.2.1'},\n                      {'subnet_id': 'id-3', 'ip_address': '4.3.2.10'}],\n            remove=[])\n        self._test_get_changed_ips_for_port(expected_change, original_ips,\n                                            new_ips, self.owner_non_router)\n\n    def test__get_changed_ips_for_port_subnet_id_no_ip_ipv6(self):\n        # If a subnet is specified without an IP address only allocate a new\n        # address if one doesn't exist\n        self._mock_slaac_subnet_off()\n        new_ips = [{'subnet_id': 'id-3'}]\n        original_ips = [{'subnet_id': 'id-3', 'ip_address': '2001:db8::8'}]\n\n        expected_change = self.mixin.Changes(\n            add=[],\n            original=[{'subnet_id': 'id-3', 'ip_address': '2001:db8::8'}],\n            remove=[])\n        self._test_get_changed_ips_for_port(expected_change, original_ips,\n                                            new_ips, self.owner_non_router)\n\n    def test__get_changed_ips_for_port_subnet_id_no_ip_eui64(self):\n        # If a subnet is specified without an IP address allocate a new address\n        # if the address is eui-64. This supports changing prefix when prefix\n        # delegation is in use.\n        self._mock_slaac_subnet_off()\n        new_ips = [{'subnet_id': 'id-3'}]\n        original_ips = [{'subnet_id': 'id-3',\n                         'ip_address': '2001::eeb1:d7ff:fe2c:9c5f'}]\n\n        expected_change = self.mixin.Changes(\n            add=[{'subnet_id': 'id-3'}],\n            original=[],\n            remove=[{'subnet_id': 'id-3',\n                     'ip_address': '2001::eeb1:d7ff:fe2c:9c5f'}])\n        self._test_get_changed_ips_for_port(expected_change, original_ips,\n                                            new_ips, self.owner_non_router)\n\n    def test__is_ip_required_by_subnet_for_router_port(self):\n        # Owner -> router:\n        # _get_subnet_object should not be called,\n        # expected True\n        self._mock_slaac_subnet_off()\n\n        result = self.mixin._is_ip_required_by_subnet(self.ctx, 'id',\n                                                      self.owner_router)\n        self.assertTrue(result)\n        self.assertFalse(self.mixin._get_subnet_object.called)\n\n    def test__is_ip_required_by_subnet_for_non_router_port(self):\n        # Owner -> not router:\n        # _get_subnet_object should be called,\n        # expected True, because subnet is not slaac\n        self._mock_slaac_subnet_off()\n\n        result = self.mixin._is_ip_required_by_subnet(self.ctx, 'id',\n                                                      self.owner_non_router)\n        self.assertTrue(result)\n        self.assertTrue(self.mixin._get_subnet_object.called)\n\n    def test__is_ip_required_by_subnet_for_non_router_port_and_slaac(self):\n        # Owner -> not router:\n        # _get_subnet_object should be called,\n        # expected False, because subnet is slaac\n        self._mock_slaac_subnet_on()\n\n        result = self.mixin._is_ip_required_by_subnet(self.ctx, 'id',\n                                                      self.owner_non_router)\n        self.assertFalse(result)\n        self.assertTrue(self.mixin._get_subnet_object.called)\n\n\nclass TestPlugin(db_base_plugin_v2.NeutronDbPluginV2,\n                 portbindings_db.PortBindingMixin):\n    __native_pagination_support = True\n    __native_sorting_support = True\n\n    supported_extension_aliases = [\"binding\"]\n\n    def get_plugin_description(self):\n        return \"Test Plugin\"\n\n    @classmethod\n    def get_plugin_type(cls):\n        return \"test_plugin\"\n\n    def create_port(self, context, port):\n        port_dict = super(TestPlugin, self).create_port(context, port)\n        self._process_portbindings_create_and_update(\n            context, port['port'], port_dict)\n        return port_dict\n\n\nclass TestPortUpdateIpam(test_db_base_plugin_v2.NeutronDbPluginV2TestCase):\n    def setUp(self, plugin=None):\n        if not plugin:\n            plugin = 'neutron.tests.unit.db.test_ipam_backend_mixin.TestPlugin'\n        super(TestPortUpdateIpam, self).setUp(plugin=plugin)\n\n    def test_port_update_allocate_from_net_subnet(self):\n        \"\"\"Tests that a port can get address by updating fixed_ips\"\"\"\n        with self.network() as network:\n            pass\n\n        # Create a bound port with no IP address (since there is not subnet)\n        response = self._create_port(self.fmt,\n                                     net_id=network['network']['id'],\n                                     tenant_id=network['network']['tenant_id'],\n                                     arg_list=(portbindings.HOST_ID,),\n                                     **{portbindings.HOST_ID: 'fakehost'})\n        port = self.deserialize(self.fmt, response)\n\n        # Create the subnet and try to update the port to get an IP\n        with self.subnet(network=network) as subnet:\n            data = {'port': {\n                'fixed_ips': [{'subnet_id': subnet['subnet']['id']}]}}\n            port_id = port['port']['id']\n            port_req = self.new_update_request('ports', data, port_id)\n            response = port_req.get_response(self.api)\n            res = self.deserialize(self.fmt, response)\n\n        self.assertEqual(webob.exc.HTTPOk.code, response.status_int)\n        self.assertEqual(1, len(res['port']['fixed_ips']))\n        ip = res['port']['fixed_ips'][0]['ip_address']\n        ip_net = netaddr.IPNetwork(subnet['subnet']['cidr'])\n        self.assertIn(ip, ip_net)\n\n\nclass TestPortUpdateIpamML2(TestPortUpdateIpam):\n    def setUp(self):\n        super(TestPortUpdateIpamML2, self).setUp(plugin='ml2')\n", "cense, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n\n\"\"\"\nManage hosts in the current zone.\n\"\"\"\n\nimport collections\nimport functools\nimport time\ntry:\n    from collections import UserDict as IterableUserDict   # Python 3\nexcept ImportError:\n    from UserDict import IterableUserDict                  # Python 2\n\n\nimport iso8601\nfrom oslo_log import log as logging\nfrom oslo_utils import timeutils\nimport six\n\nimport nova.conf\nfrom nova import context as context_module\nfrom nova import exception\nfrom nova.i18n import _LI, _LW\nfrom nova import objects\nfrom nova.pci import stats as pci_stats\nfrom nova.scheduler import filters\nfrom nova.scheduler import weights\nfrom nova import utils\nfrom nova.virt import hardware\n\n\nCONF = nova.conf.CONF\n\nLOG = logging.getLogger(__name__)\nHOST_INSTANCE_SEMAPHORE = \"host_instance\"\n\n\nclass ReadOnlyDict(IterableUserDict):\n    \"\"\"A read-only dict.\"\"\"\n    def __init__(self, source=None):\n        self.data = {}\n        if source:\n            self.data.update(source)\n\n    def __setitem__(self, key, item):\n        raise TypeError()\n\n    def __delitem__(self, key):\n        raise TypeError()\n\n    def clear(self):\n        raise TypeError()\n\n    def pop(self, key, *args):\n        raise TypeError()\n\n    def popitem(self):\n        raise TypeError()\n\n    def update(self):\n        raise TypeError()\n\n\n@utils.expects_func_args('self', 'spec_obj')\ndef set_update_time_on_success(function):\n    \"\"\"Set updated time of HostState when consuming succeed.\"\"\"\n\n    @functools.wraps(function)\n    def decorated_function(self, spec_obj):\n        return_value = None\n        try:\n            return_value = function(self, spec_obj)\n        except Exception as e:\n            # Ignores exception raised from consume_from_request() so that\n            # booting instance would fail in the resource claim of compute\n            # node, other suitable node may be chosen during scheduling retry.\n            LOG.warning(_LW(\"Selected host: %(host)s failed to consume from \"\n                            \"instance. Error: %(error)s\"),\n                        {'host': self.host, 'error': e})\n        else:\n            now = timeutils.utcnow()\n            # NOTE(sbauza): Objects are UTC tz-aware by default\n            self.updated = now.replace(tzinfo=iso8601.iso8601.Utc())\n        return return_value\n\n    return decorated_function\n\n\nclass HostState(object):\n    \"\"\"Mutable and immutable information tracked for a host.\n    This is an attempt to remove the ad-hoc data structures\n    previously used and lock down access.\n    \"\"\"\n\n    def __init__(self, host, node, compute=None):\n        self.host = host\n        self.nodename = node\n\n        # Mutable available resources.\n        # These will change as resources are virtually \"consumed\".\n        self.total_usable_ram_mb = 0\n        self.total_usable_disk_gb = 0\n        self.disk_mb_used = 0\n        self.free_ram_mb = 0\n        self.free_disk_mb = 0\n        self.vcpus_total = 0\n        self.vcpus_used = 0\n        self.pci_stats = None\n        self.numa_topology = None\n\n        # Additional host information from the compute node stats:\n        self.num_instances = 0\n        self.num_io_ops = 0\n\n        # Other information\n        self.host_ip = None\n        self.hypervisor_type = None\n        self.hypervisor_version = None\n        self.hypervisor_hostname = None\n        self.cpu_info = None\n        self.supported_instances = None\n\n        # Resource oversubscription values for the compute host:\n        self.limits = {}\n\n        # Generic metrics from compute nodes\n        self.metrics = None\n\n        # List of aggregates the host belongs to\n        self.aggregates = []\n\n        # Instances on this host\n        self.instances = {}\n\n        # Allocation ratios for this host\n        self.ram_allocation_ratio = None\n        self.cpu_allocation_ratio = None\n\n        self.updated = None\n        if compute:\n            self.update_from_compute_node(compute)\n\n    def update_service(self, service):\n        self.service = ReadOnlyDict(service)\n\n    def update_from_compute_node(self, compute):\n        \"\"\"Update information about a host from a ComputeNode object.\"\"\"\n        if (self.updated and compute.updated_at\n                and self.updated > compute.updated_at):\n            return\n        all_ram_mb = compute.memory_mb\n\n        # Assume virtual size is all consumed by instances if use qcow2 disk.\n        free_gb = compute.free_disk_gb\n        least_gb = compute.disk_available_least\n        if least_gb is not None:\n            if least_gb > free_gb:\n                # can occur when an instance in database is not on host\n                LOG.warning(_LW(\"Host %(hostname)s has more disk space than \"\n                                \"database expected \"\n                                \"(%(physical)s GB > %(database)s GB)\"),\n                            {'physical': least_gb, 'database': free_gb,\n                             'hostname': compute.hypervisor_hostname})\n            free_gb = min(least_gb, free_gb)\n        free_disk_mb = free_gb * 1024\n\n        self.disk_mb_used = compute.local_gb_used * 1024\n\n        # NOTE(jogo) free_ram_mb can be negative\n        self.free_ram_mb = compute.free_ram_mb\n        self.total_usable_ram_mb = all_ram_mb\n        self.total_usable_disk_gb = compute.local_gb\n        self.free_disk_mb = free_disk_mb\n        self.vcpus_total = compute.vcpus\n        self.vcpus_used = compute.vcpus_used\n        self.updated = compute.updated_at\n        self.numa_topology = compute.numa_topology\n        self.pci_stats = pci_stats.PciDeviceStats(\n            compute.pci_device_pools)\n\n        # All virt drivers report host_ip\n        self.host_ip = compute.host_ip\n        self.hypervisor_type = compute.hypervisor_type\n        self.hypervisor_version = compute.hypervisor_version\n        self.hypervisor_hostname = compute.hypervisor_hostname\n        self.cpu_info = compute.cpu_info\n        if compute.supported_hv_specs:\n            self.supported_instances = [spec.to_list() for spec\n                                        in compute.supported_hv_specs]\n        else:\n            self.supported_instances = []\n\n        # Don't store stats directly in host_state to make sure these don't\n        # overwrite any values, or get overwritten themselves. Store in self so\n        # filters can schedule with them.\n        self.stats = compute.stats or {}\n\n        # Track number of instances on host\n        self.num_instances = int(self.stats.get('num_instances', 0))\n\n        self.num_io_ops = int(self.stats.get('io_workload', 0))\n\n        # update metrics\n        self.metrics = objects.MonitorMetricList.from_json(compute.metrics)\n\n        # update allocation ratios given by the ComputeNode object\n        self.cpu_allocation_ratio = compute.cpu_allocation_ratio\n        self.ram_allocation_ratio = compute.ram_allocation_ratio\n\n    @set_update_time_on_success\n    def consume_from_request(self, spec_obj):\n        \"\"\"Incrementally update host state from an RequestSpec object.\"\"\"\n        disk_mb = (spec_obj.root_gb +\n                   spec_obj.ephemeral_gb) * 1024\n        ram_mb = spec_obj.memory_mb\n        vcpus = spec_obj.vcpus\n        self.free_ram_mb -= ram_mb\n        self.free_disk_mb -= disk_mb\n        self.vcpus_used += vcpus\n\n        # Track number of instances on host\n        self.num_instances += 1\n\n        pci_requests = spec_obj.pci_requests\n        if pci_requests and self.pci_stats:\n            pci_requests = pci_requests.requests\n        else:\n            pci_requests = None\n\n        # Calculate the numa usage\n        host_numa_topology, _fmt = hardware.host_topology_and_format_from_host(\n                                self)\n        instance_numa_topology = spec_obj.numa_topology\n\n        spec_obj.numa_topology = hardware.numa_fit_instance_to_host(\n            host_numa_topology, instance_numa_topology,\n            limits=self.limits.get('numa_topology'),\n            pci_requests=pci_requests, pci_stats=self.pci_stats)\n        if pci_requests:\n            instance_cells = None\n            if spec_obj.numa_topology:\n                instance_cells = spec_obj.numa_topology.cells\n            self.pci_stats.apply_requests(pci_requests, instance_cells)\n\n        # NOTE(sbauza): Yeah, that's crap. We should get rid of all of those\n        # NUMA helpers because now we're 100% sure that spec_obj.numa_topology\n        # is an InstanceNUMATopology object. Unfortunately, since\n        # HostState.host_numa_topology is still limbo between an NUMATopology\n        # object (when updated by consume_from_request), a ComputeNode object\n        # (when updated by update_from_compute_node), we need to keep the call\n        # to get_host_numa_usage_from_instance until it's fixed (and use a\n        # temporary orphaned Instance object as a proxy)\n        instance = objects.Instance(numa_topology=spec_obj.numa_topology)\n\n        self.numa_topology = hardware.get_host_numa_usage_from_instance(\n                self, instance)\n\n        # NOTE(sbauza): By considering all cases when the scheduler is called\n        # and when consume_from_request() is run, we can safely say that there\n        # is always an IO operation because we want to move the instance\n        self.num_io_ops += 1\n\n    def __repr__(self):\n        return (\"(%s, %s) ram:%s disk:%s io_ops:%s instances:%s\" %\n                (self.host, self.nodename, self.free_ram_mb, self.free_disk_mb,\n                 self.num_io_ops, self.num_instances))\n\n\nclass HostManager(object):\n    \"\"\"Base HostManager class.\"\"\"\n\n    # Can be overridden in a subclass\n    def host_state_cls(self, host, node, **kwargs):\n        return HostState(host, node, **kwargs)\n\n    def __init__(self):\n        self.host_state_map = {}\n        self.filter_handler = filters.HostFilterHandler()\n        filter_classes = self.filter_handler.get_matching_classes(\n                CONF.scheduler_available_filters)\n        self.filter_cls_map = {cls.__name__: cls for cls in filter_classes}\n        self.filter_obj_map = {}\n        self.default_filters = self._choose_host_filters(self._load_filters())\n        self.weight_handler = weights.HostWeightHandler()\n        weigher_classes = self.weight_handler.get_matching_classes(\n                CONF.scheduler_weight_classes)\n        self.weighers = [cls() for cls in weigher_classes]\n        # Dict of aggregates keyed by their ID\n        self.aggs_by_id = {}\n        # Dict of set of aggregate IDs keyed by the name of the host belonging\n        # to those aggregates\n        self.host_aggregates_map = collections.defaultdict(set)\n        self._init_aggregates()\n        self.tracks_instance_changes = CONF.scheduler_tracks_instance_changes\n        # Dict of instances and status, keyed by host\n        self._instance_info = {}\n        if self.tracks_instance_changes:\n            self._init_instance_info()\n\n    def _load_filters(self):\n        return CONF.scheduler_default_filters\n\n    def _init_aggregates(self):\n        elevated = context_module.get_admin_context()\n        aggs = objects.AggregateList.get_all(elevated)\n        for agg in aggs:\n            self.aggs_by_id[agg.id] = agg\n            for host in agg.hosts:\n                self.host_aggregates_map[host].add(agg.id)\n\n    def update_aggregates(self, aggregates):\n        \"\"\"Updates internal HostManager information about aggregates.\"\"\"\n        if isinstance(aggregates, (list, objects.AggregateList)):\n            for agg in aggregates:\n                self._update_aggregate(agg)\n        else:\n            self._update_aggregate(aggregates)\n\n    def _update_aggregate(self, aggregate):\n        self.aggs_by_id[aggregate.id] = aggregate\n        for host in aggregate.hosts:\n            self.host_aggregates_map[host].add(aggregate.id)\n        # Refreshing the mapping dict to remove all hosts that are no longer\n        # part of the aggregate\n        for host in self.host_aggregates_map:\n            if (aggregate.id in self.host_aggregates_map[host]\n                    and host not in aggregate.hosts):\n                self.host_aggregates_map[host].remove(aggregate.id)\n\n    def delete_aggregate(self, aggregate):\n        \"\"\"Deletes internal HostManager information about a specific aggregate.\n        \"\"\"\n        if aggregate.id in self.aggs_by_id:\n            del self.aggs_by_id[aggregate.id]\n        for host in aggregate.hosts:\n            if aggregate.id in self.host_aggregates_map[host]:\n                self.host_aggregates_map[host].remove(aggregate.id)\n\n    def _init_instance_info(self):\n        \"\"\"Creates the initial view of instances for all hosts.\n\n        As this initial population of instance information may take some time,\n        we don't wish to block the scheduler's startup while this completes.\n        The async method allows us to simply mock out the _init_instance_info()\n        method in tests.\n        \"\"\"\n\n        def _async_init_instance_info():\n            context = context_module.get_admin_context()\n            LOG.debug(\"START:_async_init_instance_info\")\n            self._instance_info = {}\n            compute_nodes = objects.ComputeNodeList.get_all(context).objects\n            LOG.debug(\"Total number of compute nodes: %s\", len(compute_nodes))\n            # Break the queries into batches of 10 to reduce the total number\n            # of calls to the DB.\n            batch_size = 10\n            start_node = 0\n            end_node = batch_size\n            while start_node <= len(compute_nodes):\n                curr_nodes = compute_nodes[start_node:end_node]\n                start_node += batch_size\n                end_node += batch_size\n                filters = {\"host\": [curr_node.host\n                                    for curr_node in curr_nodes]}\n                result = objects.InstanceList.get_by_filters(context,\n                                                             filters)\n                instances = result.objects\n                LOG.debug(\"Adding %s instances for hosts %s-%s\",\n                          len(instances), start_node, end_node)\n                for instance in instances:\n                    host = instance.host\n                    if host not in self._instance_info:\n                        self._instance_info[host] = {\"instances\": {},\n                                                     \"updated\": False}\n                    inst_dict = self._instance_info[host]\n                    inst_dict[\"instances\"][instance.uuid] = instance\n                # Call sleep() to cooperatively yield\n                time.sleep(0)\n            LOG.debug(\"END:_async_init_instance_info\")\n\n        # Run this async so that we don't block the scheduler start-up\n        utils.spawn_n(_async_init_instance_info)\n\n    def _choose_host_filters(self, filter_cls_names):\n        \"\"\"Since the caller may specify which filters to use we need\n        to have an authoritative list of what is permissible. This\n        function checks the filter names against a predefined set\n        of acceptable filters.\n        \"\"\"\n        if not isinstance(filter_cls_names, (list, tuple)):\n            filter_cls_names = [filter_cls_names]\n\n        good_filters = []\n        bad_filters = []\n        for filter_name in filter_cls_names:\n            if filter_name not in self.filter_obj_map:\n                if filter_name not in self.filter_cls_map:\n                    bad_filters.append(filter_name)\n                    continue\n                filter_cls = self.filter_cls_map[filter_name]\n                self.filter_obj_map[filter_name] = filter_cls()\n            good_filters.append(self.filter_obj_map[filter_name])\n        if bad_filters:\n            msg = \", \".join(bad_filters)\n            raise exception.SchedulerHostFilterNotFound(filter_name=msg)\n        return good_filters\n\n    def get_filtered_hosts(self, hosts, spec_obj,\n            filter_class_names=None, index=0):\n        \"\"\"Filter hosts and return only ones passing all filters.\"\"\"\n\n        def _strip_ignore_hosts(host_map, hosts_to_ignore):\n            ignored_hosts = []\n            for host in hosts_to_ignore:\n                for (hostname, nodename) in list(host_map.keys()):\n                    if host == hostname:\n                        del host_map[(hostname, nodename)]\n                        ignored_hosts.append(host)\n            ignored_hosts_str = ', '.join(ignored_hosts)\n            LOG.info(_LI('Host filter ignoring hosts: %s'), ignored_hosts_str)\n\n        def _match_forced_hosts(host_map, hosts_to_force):\n            forced_hosts = []\n            for (hostname, nodename) in list(host_map.keys()):\n                if hostname not in hosts_to_force:\n                    del host_map[(hostname, nodename)]\n                else:\n                    forced_hosts.append(hostname)\n            if host_map:\n                forced_hosts_str = ', '.join(forced_hosts)\n                msg = _LI('Host filter forcing available hosts to %s')\n            else:\n                forced_hosts_str = ', '.join(hosts_to_force)\n                msg = _LI(\"No hosts matched due to not matching \"\n                          \"'force_hosts' value of '%s'\")\n            LOG.info(msg % forced_hosts_str)\n\n        def _match_forced_nodes(host_map, nodes_to_force):\n            forced_nodes = []\n            for (hostname, nodename) in list(host_map.keys()):\n                if nodename not in nodes_to_force:\n                    del host_map[(hostname, nodename)]\n                else:\n                    forced_nodes.append(nodename)\n            if host_map:\n                forced_nodes_str = ', '.join(forced_nodes)\n                msg = _LI('Host filter forcing available nodes to %s')\n            else:\n                forced_nodes_str = ', '.join(nodes_to_force)\n                msg = _LI(\"No nodes matched due to not matching \"\n                          \"'force_nodes' value of '%s'\")\n            LOG.info(msg % forced_nodes_str)\n\n        if filter_class_names is None:\n            filters = self.default_filters\n        else:\n            filters = self._choose_host_filters(filter_class_names)\n        ignore_hosts = spec_obj.ignore_hosts or []\n        force_hosts = spec_obj.force_hosts or []\n        force_nodes = spec_obj.force_nodes or []\n\n        if ignore_hosts or force_hosts or force_nodes:\n            # NOTE(deva): we can't assume \"host\" is unique because\n            #             one host may have many nodes.\n            name_to_cls_map = {(x.host, x.nodename): x for x in hosts}\n            if ignore_hosts:\n                _strip_ignore_hosts(name_to_cls_map, ignore_hosts)\n                if not name_to_cls_map:\n                    return []\n            # NOTE(deva): allow force_hosts and force_nodes independently\n            if force_hosts:\n                _match_forced_hosts(name_to_cls_map, force_hosts)\n            if force_nodes:\n                _match_forced_nodes(name_to_cls_map, force_nodes)\n            if force_hosts or force_nodes:\n                # NOTE(deva): Skip filters when forcing host or node\n                if name_to_cls_map:\n                    return name_to_cls_map.values()\n            hosts = six.itervalues(name_to_cls_map)\n\n        return self.filter_handler.get_filtered_objects(filters,\n                hosts, spec_obj, index)\n\n    def get_weighed_hosts(self, hosts, spec_obj):\n        \"\"\"Weigh the hosts.\"\"\"\n        return self.weight_handler.get_weighed_objects(self.weighers,\n                hosts, spec_obj)\n\n    def get_all_host_states(self, context):\n        \"\"\"Returns a list of HostStates that represents all the hosts\n        the HostManager knows about. Also, each of the consumable resources\n        in HostState are pre-populated and adjusted based on data in the db.\n        \"\"\"\n\n        service_refs = {service.host: service\n                        for service in objects.ServiceList.get_by_binary(\n                            context, 'nova-compute')}\n        # Get resource usage across the available compute nodes:\n        compute_nodes = objects.ComputeNodeList.get_all(context)\n        seen_nodes = set()\n        for compute in compute_nodes:\n            service = service_refs.get(compute.host)\n\n            if not service:\n                LOG.warning(_LW(\n                    \"No compute service record found for host %(host)s\"),\n                    {'host': compute.host})\n                continue\n            host = compute.host\n            node = compute.hypervisor_hostname\n            state_key = (host, node)\n            host_state = self.host_state_map.get(state_key)\n            if host_state:\n                host_state.update_from_compute_node(compute)\n            else:\n                host_state = self.host_state_cls(host, node, compute=compute)\n                self.host_state_map[state_key] = host_state\n            # We force to update the aggregates info each time a new request\n            # comes in, because some changes on the aggregates could have been\n            # happening after setting this field for the first time\n            host_state.aggregates = [self.aggs_by_id[agg_id] for agg_id in\n                                     self.host_aggregates_map[\n                                         host_state.host]]\n            host_state.update_service(dict(service))\n            self._add_instance_info(context, compute, host_state)\n            seen_nodes.add(state_key)\n\n        # remove compute nodes from host_state_map if they are not active\n        dead_nodes = set(self.host_state_map.keys()) - seen_nodes\n        for state_key in dead_nodes:\n            host, node = state_key\n            LOG.info(_LI(\"Removing dead compute node %(host)s:%(node)s \"\n                         \"from scheduler\"), {'host': host, 'node': node})\n            del self.host_state_map[state_key]\n\n        return six.itervalues(self.host_state_map)\n\n    def _add_instance_info(self, context, compute, host_state):\n        \"\"\"Adds the host instance info to the host_state object.\n\n        Some older compute nodes may not be sending instance change updates to\n        the Scheduler; other sites may disable this feature for performance\n        reasons. In either of these cases, there will either be no information\n        for the host, or the 'updated' value for that host dict will be False.\n        In those cases, we need to grab the current InstanceList instead of\n        relying on the version in _instance_info.\n        \"\"\"\n        host_name = compute.host\n        host_info = self._instance_info.get(host_name)\n        if host_info and host_info.get(\"updated\"):\n            inst_dict = host_info[\"instances\"]\n        else:\n            # Host is running old version, or updates aren't flowing.\n            inst_list = objects.InstanceList.get_by_host(context, host_name)\n            inst_dict = {instance.uuid: instance\n                         for instance in inst_list.objects}\n        host_state.instances = inst_dict\n\n    def _recreate_instance_info(self, context, host_name):\n        \"\"\"Get the InstanceList for the specified host, and store it in the\n        _instance_info dict.\n        \"\"\"\n        instances = objects.InstanceList.get_by_host(context, host_name)\n        inst_dict = {instance.uuid: instance for instance in instances}\n        host_info = self._instance_info[host_name] = {}\n        host_info[\"instances\"] = inst_dict\n        host_info[\"updated\"] = False\n\n    @utils.synchronized(HOST_INSTANCE_SEMAPHORE)\n    def update_instance_info(self, context, host_name, instance_info):\n        \"\"\"Receives an InstanceList object from a compute node.\n\n        This method receives information from a compute node when it starts up,\n        or when its instances have changed, and updates its view of hosts and\n        instances with it.\n        \"\"\"\n        host_info = self._instance_info.get(host_name)\n        if host_info:\n            inst_dict = host_info.get(\"instances\")\n            for instance in instance_info.objects:\n                # Overwrite the entry (if any) with the new info.\n                inst_dict[instance.uuid] = instance\n            host_info[\"updated\"] = True\n        else:\n            instances = instance_info.objects\n            if len(instances) > 1:\n                # This is a host sending its full instance list, so use it.\n                host_info = self._instance_info[host_name] = {}\n                host_info[\"instances\"] = {instance.uuid: instance\n                                          for instance in instances}\n                host_info[\"updated\"] = True\n            else:\n                self._recreate_instance_info(context, host_name)\n                LOG.info(_LI(\"Received an update from an unknown host '%s'. \"\n                             \"Re-created its InstanceList.\"), host_name)\n\n    @utils.synchronized(HOST_INSTANCE_SEMAPHORE)\n    def delete_instance_info(self, context, host_name, instance_uuid):\n        \"\"\"Receives the UUID from a compute node when one of its instances is\n        terminated.\n\n        The instance in the local view of the host's instances is removed.\n        \"\"\"\n        host_info = self._instance_info.get(host_name)\n        if host_info:\n            inst_dict = host_info[\"instances\"]\n            # Remove the existing Instance object, if any\n            inst_dict.pop(instance_uuid, None)\n            host_info[\"updated\"] = True\n        else:\n            self._recreate_instance_info(context, host_name)\n            LOG.info(_LI(\"Received a delete update from an unknown host '%s'. \"\n                         \"Re-created its InstanceList.\"), host_name)\n\n    @utils.synchronized(HOST_INSTANCE_SEMAPHORE)\n    def sync_instance_info(self, context, host_name, instance_uuids):\n        \"\"\"Receives the uuids of the instances on a host.\n\n        This method is periodically called by the compute nodes, which send a\n        list of all the UUID values for the instances on that node. This is\n        used by the scheduler's HostManager to detect when its view of the\n        compute node's instances is out of sync.\n        \"\"\"\n        host_info = self._instance_info.get(host_name)\n        if host_info:\n            local_set = set(host_info[\"instances\"].keys())\n            compute_set = set(instance_uuids)\n            if not local_set == compute_set:\n                self._recreate_instance_info(context, host_name)\n                LOG.info(_LI(\"The instance sync for host '%s' did not match. \"\n                             \"Re-created its InstanceList.\"), host_name)\n                return\n            host_info[\"updated\"] = True\n            LOG.info(_LI(\"Successfully synced instances from host '%s'.\"),\n                     host_name)\n        else:\n            self._recreate_instance_info(context, host_name)\n            LOG.info(_LI(\"Received a sync request from an unknown host '%s'. \"\n                         \"Re-created its InstanceList.\"), host_name)\n", "y\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n#\n\nimport netaddr\nfrom neutron_lib import context as nctx\nfrom neutron_lib.db import api as db_api\nfrom neutron_lib.plugins import constants\nfrom neutron_lib.plugins import directory\nfrom oslo_config import cfg\nfrom oslo_db import exception as db_exc\nfrom oslo_utils import uuidutils\nfrom sqlalchemy.orm import session as se\nfrom webob import exc\n\nfrom neutron.db import models_v2\nfrom neutron.objects import ports as port_obj\nfrom neutron.tests.unit.plugins.ml2 import test_plugin\n\n\nclass TestRevisionPlugin(test_plugin.Ml2PluginV2TestCase):\n\n    l3_plugin = ('neutron.tests.unit.extensions.test_extraroute.'\n                 'TestExtraRouteL3NatServicePlugin')\n\n    _extension_drivers = ['qos']\n\n    def get_additional_service_plugins(self):\n        p = super(TestRevisionPlugin, self).get_additional_service_plugins()\n        p.update({'revision_plugin_name': 'revisions',\n                  'qos_plugin_name': 'qos',\n                  'tag_name': 'tag'})\n        return p\n\n    def setUp(self):\n        cfg.CONF.set_override('extension_drivers',\n                              self._extension_drivers,\n                              group='ml2')\n        super(TestRevisionPlugin, self).setUp()\n        self.cp = directory.get_plugin()\n        self.l3p = directory.get_plugin(constants.L3)\n        self._ctx = nctx.get_admin_context()\n\n    @property\n    def ctx(self):\n        # TODO(kevinbenton): return ctx without expire_all after switch to\n        # enginefacade complete. We expire_all here because the switch to\n        # the new engine facade is resulting in changes being spread over\n        # other sessions so we can end up getting stale reads in the parent\n        # session if objects remain in the identity map.\n        if not self._ctx.session.is_active:\n            self._ctx.session.expire_all()\n        return self._ctx\n\n    def test_handle_expired_object(self):\n        rp = directory.get_plugin('revision_plugin')\n        with self.port():\n            with self.ctx.session.begin():\n                ipal_objs = port_obj.IPAllocation.get_objects(self.ctx)\n                if not ipal_objs:\n                    raise Exception(\"No IP allocations available.\")\n                ipal_obj = ipal_objs[0]\n                # load port into our session\n                port = self.ctx.session.query(models_v2.Port).one()\n                # simulate concurrent delete in another session\n                other_ctx = nctx.get_admin_context()\n                other_ctx.session.delete(\n                    other_ctx.session.query(models_v2.Port).first()\n                )\n                # expire the port so the revision bumping code will trigger a\n                # lookup on its attributes and encounter an ObjectDeletedError\n                self.ctx.session.expire(port)\n                rp._bump_related_revisions(self.ctx.session, ipal_obj)\n\n    def test_port_name_update_revises(self):\n        with self.port() as port:\n            rev = port['port']['revision_number']\n            new = {'port': {'name': 'seaweed'}}\n            response = self._update('ports', port['port']['id'], new)\n            new_rev = response['port']['revision_number']\n            self.assertGreater(new_rev, rev)\n\n    def test_constrained_port_update(self):\n        with self.port() as port:\n            rev = port['port']['revision_number']\n            new = {'port': {'name': 'nigiri'}}\n            for val in (rev - 1, rev + 1):\n                # make sure off-by ones are rejected\n                self._update('ports', port['port']['id'], new,\n                             headers={'If-Match': 'revision_number=%s' % val},\n                             expected_code=exc.HTTPPreconditionFailed.code)\n            after_attempt = self._show('ports', port['port']['id'])\n            self.assertEqual(rev, after_attempt['port']['revision_number'])\n            self.assertEqual(port['port']['name'],\n                             after_attempt['port']['name'])\n            # correct revision should work\n            self._update('ports', port['port']['id'], new,\n                         headers={'If-Match': 'revision_number=%s' % rev})\n\n    def test_constrained_port_delete(self):\n        with self.port() as port:\n            rev = port['port']['revision_number']\n            for val in (rev - 1, rev + 1):\n                # make sure off-by ones are rejected\n                self._delete('ports', port['port']['id'],\n                             headers={'If-Match': 'revision_number=%s' % val},\n                             expected_code=exc.HTTPPreconditionFailed.code)\n            # correct revision should work\n            self._delete('ports', port['port']['id'],\n                         headers={'If-Match': 'revision_number=%s' % rev})\n\n    def test_constrained_port_update_handles_db_retries(self):\n        # here we ensure all of the constraint handling logic persists\n        # on retriable failures to commit caused by races with another\n        # update\n        with self.port() as port:\n            rev = port['port']['revision_number']\n            new = {'port': {'name': 'nigiri'}}\n\n            def concurrent_increment(s):\n                db_api.sqla_remove(se.Session, 'before_commit',\n                                   concurrent_increment)\n                # slip in a concurrent update that will bump the revision\n                plugin = directory.get_plugin()\n                plugin.update_port(nctx.get_admin_context(),\n                                   port['port']['id'], new)\n                raise db_exc.DBDeadlock()\n            db_api.sqla_listen(se.Session, 'before_commit',\n                               concurrent_increment)\n            self._update('ports', port['port']['id'], new,\n                         headers={'If-Match': 'revision_number=%s' % rev},\n                         expected_code=exc.HTTPPreconditionFailed.code)\n\n    def test_port_ip_update_revises(self):\n        with self.port() as port:\n            rev = port['port']['revision_number']\n            new = {'port': {'fixed_ips': port['port']['fixed_ips']}}\n            # ensure adding an IP allocation updates the port\n            next_ip = str(netaddr.IPAddress(\n                  new['port']['fixed_ips'][0]['ip_address']) + 1)\n            new['port']['fixed_ips'].append({'ip_address': next_ip})\n            response = self._update('ports', port['port']['id'], new)\n            self.assertEqual(2, len(response['port']['fixed_ips']))\n            new_rev = response['port']['revision_number']\n            self.assertGreater(new_rev, rev)\n            # ensure deleting an IP allocation updates the port\n            rev = new_rev\n            new['port']['fixed_ips'].pop()\n            response = self._update('ports', port['port']['id'], new)\n            self.assertEqual(1, len(response['port']['fixed_ips']))\n            new_rev = response['port']['revision_number']\n            self.assertGreater(new_rev, rev)\n\n    def test_security_group_rule_ops_bump_security_group(self):\n        s = {'security_group': {'tenant_id': 'some_tenant', 'name': '',\n                                'description': 's'}}\n        sg = self.cp.create_security_group(self.ctx, s)\n        s['security_group']['name'] = 'hello'\n        updated = self.cp.update_security_group(self.ctx, sg['id'], s)\n        self.assertGreater(updated['revision_number'], sg['revision_number'])\n        # ensure rule changes bump parent SG\n        r = {'security_group_rule': {'tenant_id': 'some_tenant',\n                                     'port_range_min': 80, 'protocol': 6,\n                                     'port_range_max': 90,\n                                     'remote_ip_prefix': '0.0.0.0/0',\n                                     'ethertype': 'IPv4',\n                                     'remote_group_id': None,\n                                     'direction': 'ingress',\n                                     'security_group_id': sg['id']}}\n        rule = self.cp.create_security_group_rule(self.ctx, r)\n        sg = updated\n        updated = self.cp.get_security_group(self.ctx, sg['id'])\n        self.assertGreater(updated['revision_number'], sg['revision_number'])\n        self.cp.delete_security_group_rule(self.ctx, rule['id'])\n        sg = updated\n        updated = self.cp.get_security_group(self.ctx, sg['id'])\n        self.assertGreater(updated['revision_number'], sg['revision_number'])\n\n    def test_router_interface_ops_bump_router(self):\n        r = {'router': {'name': 'myrouter', 'tenant_id': 'some_tenant',\n                        'admin_state_up': True}}\n        router = self.l3p.create_router(self.ctx, r)\n        r['router']['name'] = 'yourrouter'\n        updated = self.l3p.update_router(self.ctx, router['id'], r)\n        self.assertGreater(updated['revision_number'],\n                           router['revision_number'])\n        # add an intf and make sure it bumps rev\n        with self.subnet(tenant_id='some_tenant', cidr='10.0.1.0/24') as s:\n            interface_info = {'subnet_id': s['subnet']['id']}\n        self.l3p.add_router_interface(self.ctx, router['id'],\n                                      interface_info)\n        router = updated\n        updated = self.l3p.get_router(self.ctx, router['id'])\n        self.assertGreater(updated['revision_number'],\n                           router['revision_number'])\n        # Add a route and make sure it bumps revision number\n        router = updated\n        body = {'router': {'routes': [{'destination': '192.168.2.0/24',\n                                       'nexthop': '10.0.1.3'}]}}\n        self.l3p.update_router(self.ctx, router['id'], body)\n        updated = self.l3p.get_router(self.ctx, router['id'])\n        self.assertGreater(updated['revision_number'],\n                           router['revision_number'])\n        router = updated\n        body['router']['routes'] = []\n        self.l3p.update_router(self.ctx, router['id'], body)\n        updated = self.l3p.get_router(self.ctx, router['id'])\n        self.assertGreater(updated['revision_number'],\n                           router['revision_number'])\n        self.l3p.remove_router_interface(self.ctx, router['id'],\n                                         interface_info)\n        router = updated\n        updated = self.l3p.get_router(self.ctx, router['id'])\n        self.assertGreater(updated['revision_number'],\n                           router['revision_number'])\n\n    def test_qos_policy_bump_port_revision(self):\n        with self.port() as port:\n            rev = port['port']['revision_number']\n            qos_plugin = directory.get_plugin('QOS')\n            qos_policy = {'policy': {'id': uuidutils.generate_uuid(),\n                                     'name': \"policy1\",\n                                     'project_id': uuidutils.generate_uuid()}}\n            qos_obj = qos_plugin.create_policy(self.ctx, qos_policy)\n            data = {'port': {'qos_policy_id': qos_obj['id']}}\n            response = self._update('ports', port['port']['id'], data)\n            new_rev = response['port']['revision_number']\n            self.assertGreater(new_rev, rev)\n\n    def test_qos_policy_bump_network_revision(self):\n        with self.network() as network:\n            rev = network['network']['revision_number']\n            qos_plugin = directory.get_plugin('QOS')\n            qos_policy = {'policy': {'id': uuidutils.generate_uuid(),\n                                     'name': \"policy1\",\n                                     'project_id': uuidutils.generate_uuid()}}\n            qos_obj = qos_plugin.create_policy(self.ctx, qos_policy)\n            data = {'network': {'qos_policy_id': qos_obj['id']}}\n            response = self._update('networks', network['network']['id'], data)\n            new_rev = response['network']['revision_number']\n            self.assertGreater(new_rev, rev)\n\n    def test_net_tag_bumps_net_revision(self):\n        with self.network() as network:\n            rev = network['network']['revision_number']\n            tag_plugin = directory.get_plugin('TAG')\n            tag_plugin.update_tag(self.ctx, 'networks',\n                                  network['network']['id'], 'mytag')\n            updated = directory.get_plugin().get_network(\n                self.ctx, network['network']['id'])\n            self.assertGreater(updated['revision_number'], rev)\n            tag_plugin.delete_tag(self.ctx, 'networks',\n                                  network['network']['id'], 'mytag')\n            rev = updated['revision_number']\n            updated = directory.get_plugin().get_network(\n                self.ctx, network['network']['id'])\n            self.assertGreater(updated['revision_number'], rev)\n", "tions\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('store', '0005_auto_20150815_1745'),\n    ]\n\n    operations = [\n        migrations.RemoveField(\n            model_name='item',\n            name='main_sub_category',\n        ),\n        migrations.AddField(\n            model_name='item',\n            name='main_sub_category',\n            field=models.ManyToManyField(to='store.MainSubCategory', related_name='main_sub_cat_item'),\n        ),\n    ]\n", "cense\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n#\n\n\"\"\"ml2_type_driver_refactor_dynamic_segments\n\nRevision ID: 236b90af57ab\nRevises: 58fe87a01143\nCreate Date: 2014-08-14 16:22:14.293788\n\n\"\"\"\n\n# revision identifiers, used by Alembic.\nrevision = '236b90af57ab'\ndown_revision = '58fe87a01143'\n\nfrom alembic import op\nimport sqlalchemy as sa\n\n\ndef upgrade(active_plugins=None, options=None):\n\n    op.add_column('ml2_network_segments',\n                  sa.Column('is_dynamic', sa.Boolean(), nullable=False,\n                            server_default=sa.sql.false()))\n\n\ndef downgrade(active_plugins=None, options=None):\n\n    op.drop_column('ml2_network_segments', 'is_dynamic')\n", "_QUEUE = 'backache'\nDEFAULT_QUARANTINE_QUEUE = 'backache-quarantine'\nLOGGER = logging.getLogger(__name__)\n\n\nclass ProcessingRetryException(Exception):\n    \"\"\"Raised by an operation task, it tells backache to retry\n    the processing task later.\n    \"\"\"\n    def __init__(self, countdown=None, **kwargs):\n        \"\"\"\n        :param int countdown:\n          Time in seconds to delay the retry\n        :param dict op_kwargs:\n          Arguments given to the next processing task\n        \"\"\"\n        super(ProcessingRetryException, self).__init__()\n        self.countdown = countdown\n        self.op_kwargs = kwargs\n\n\nclass ProcessingInQuarantineException(Exception):\n    \"\"\"Raised by an operation task, it tells backache to move this failing\n    task in a dedicated task used for quarantine\n    \"\"\"\n    def __init__(self, op_kwargs=None, **kwargs):\n        \"\"\"\n        :param dict op_kwargs:\n          `kwargs` arguments given the task that raised this exception.\n\n        :param dict kwargs:\n          optional `dict` given to the quarantine task. It may provide\n          additional information, useful to investigate the issue like\n          status_code, errors, ...\n        \"\"\"\n        super(ProcessingInQuarantineException, self).__init__()\n        self.op_kwargs = op_kwargs\n        self.kwargs = kwargs\n\n\nclass CeleryCache(Backache):\n    def __init__(self, **kwargs):\n        self.__super = super(CeleryCache, self)\n        self.__super.__init__(**kwargs)\n        self._config.celery = kwargs.get('celery', {})\n        self._tasks = nameddict(kwargs.get('celery_tasks'))\n\n    def get_or_delegate(self, operation, uri, *cb_args):\n        \"\"\" Retrieve result of the specified operation for the given URI\n        or trigger the processing asynchronously.\n\n        :return:\n          cached value if available, a Celery `TaskResult` otherwise.\n        \"\"\"\n        cached_doc = self.__super.get_or_delegate(operation, uri, *cb_args)\n        if cached_doc is not None:\n            return cached_doc\n        else:\n            return self._delegate_async(operation, uri)\n\n    def bulk_get_or_delegate(self, commands, cache_hits_cb):\n        \"\"\" Behavior and signature are the same than\n        `Backache:bulk_get_or_delegate`\n\n        :return:\n          task identifiers, asynchronously fired.\n        :rtype:\n          list of task identifier\n        \"\"\"\n        misses, errors = self.__super.bulk_get_or_delegate(\n            commands, cache_hits_cb\n        )\n        tasks = []\n        for operation, uri, _ in misses:\n            tasks.append(self._delegate_async(operation, uri))\n        for task_error in errors:\n            if isinstance(task_error.exc, ProcessingRetryException):\n                LOGGER.debug(u'async spawn of failed mitigated task: %s/%s',\n                             task_error.operation, task_error.uri)\n                tasks.append(self._delegate_async(\n                    task_error.operation,\n                    task_error.uri,\n                    countdown=task_error.exc.countdown,\n                    op_kwargs=task_error.exc.op_kwargs\n                ))\n            else:\n                self.move_in_quarantine(\n                    task_error.operation,\n                    task_error.uri,\n                    task_error.exc\n                )\n        return tasks\n\n    def _delegate_async(self, operation, uri, countdown=None, op_kwargs=None):\n        \"\"\" Asynchronously process the operation and provide the result to\n        the operation callback.\n        \"\"\"\n        from celery import chain\n        return chain(\n            self._tasks.consume.subtask(\n                args=(operation, uri),\n                kwargs={'op_kwargs': op_kwargs},\n                queue=self._celery_queue('consume', operation),\n                countdown=countdown\n            ),\n            self._processing_callback(operation, uri)\n        )()\n\n    def _processing_callback(self, operation, uri):\n        \"\"\" Internally provide the proper callback.\n        \"\"\"\n        from celery.local import Proxy\n        from celery.canvas import Signature\n        callback = self._operation_callback(operation)\n        if isinstance(callback, Proxy):\n            return callback.s()\n        elif isinstance(callback, Signature):\n            return callback\n        else:\n            return self._tasks.callback.subtask(\n                (operation, uri),\n                queue=self._celery_queue('callback', operation)\n            )\n\n    def _celery_queue(self, task_name, operation):\n        tasks_config = self._config.celery.get('tasks', {})\n        task_config = tasks_config.get(task_name, {})\n        default_queue = self._config.celery.get('default_queue', DEFAULT_QUEUE)\n        queue = task_config.get('queue', default_queue)\n        return queue.format(operation=operation)\n\n    def fire_callback(self, operation, cached_doc, cb_args, **kwargs):\n        \"\"\" Override parent member method\n        The callback method is NOT directly called here. This code is\n        executed in a Celery chain, and the next task is the callback,\n        that expects the 2 arguments returned by this method.\n\n        :returns: 2 first arguments of the Celery `callback` task\n        \"\"\"\n        del kwargs  # unused\n        return cached_doc, cb_args\n\n    def move_in_quarantine(self, operation, uri, exc):\n        LOGGER.debug(u\"move in quarantine: %s/%s\", operation, uri)\n        cb_args = self._config.resource.pop(operation, uri)\n        if self._tasks.quarantine is not None:\n            queue = self._config.celery.get(\n                'quarantine_queue',\n                DEFAULT_QUARANTINE_QUEUE\n            )\n            return self._tasks.quarantine.apply_async(\n                args=(operation, uri, cb_args, exc),\n                queue=queue\n            )\n\n    def _context(self, **kwargs):\n        return AsyncOperationContext(**kwargs)\n\n\nclass AsyncOperationContext(OperationContext):\n    \"\"\"operation context given to processing task in Celery context.\n    Tasks have the ability:\n    - to retry themselves later in the future\n    - set the task status to failure, and the task in moved in a quarantine\n    queue.\n    \"\"\"\n    def retry(self, **kwargs):\n        \"\"\"Ask the task to be retried later in the future.\n\n        :param dict kwargs:\n          If present, the `countdown` attribute specifies the number of\n          seconds the task should execute in the future. Defaults to\n          immediate execution.\n          Other attributes will be given to the retried task as `kwargs`\n          argument.\n\n        :raises ProcessingRetryException:\n          That must not be catched by the processing task\n        \"\"\"\n        raise ProcessingRetryException(**kwargs)\n\n    def quarantine(self, **kwargs):\n        \"\"\"Cancel this processing task and move it to a quarantine queue.\n\n        :param dict kwargs:\n          optional `dict` given to the quarantine task. It may provide\n          additional information, useful to investigate the issue like\n          status_code, errors, ...\n\n        :raises ProcessingInQuarantineException:\n          That must be catched by the processing task\n        \"\"\"\n        raise ProcessingInQuarantineException(self._op_kwargs, **kwargs)\n\n\ndef celerize(celery_app, **config):\n    \"\"\"Build a `Backache` instanced using Celery to perform\n    asynchronous processing.\n    \"\"\"\n    backache = None\n\n    @celery_app.task(name='backache.consume', bind=True,\n                     max_retries=None, default_retry_delay=60)\n    def backache_consume(task, operation, uri, op_kwargs=None):\n        result, cb_args = None, None\n        try:\n            result, cb_args = backache.consume(\n                operation, uri, op_kwargs=op_kwargs\n            )\n        except ProcessingRetryException as e:\n            raise task.retry(  # pragma NOCOVER\n                countdown=e.countdown,\n                kwargs={'op_kwargs': e.op_kwargs}\n            )\n        except ProcessingInQuarantineException as e:  # pragma: no cover\n            backache.move_in_quarantine(operation, uri, e)\n        if cb_args is None or not any(cb_args):  # pragma: no cover\n            # do not call the callback\n            if backache._config.celery.skip_callback_when_no_payload:\n                LOGGER.debug('skip callback because no \"cb_args\": %s/%s',\n                             operation, uri)\n\n                task.request.callbacks = None\n        return result, cb_args\n\n    @celery_app.task(name='backache.callback',\n                     max_retries=None, default_retry_delay=60)\n    def backache_callback(args, operation, uri):\n        cached_doc, cb_args = args\n        _super = super(CeleryCache, backache)\n        return _super.fire_callback(operation, cached_doc, cb_args, True)\n\n    config['celery_tasks'] = {\n        'consume': backache_consume,\n        'callback': backache_callback,\n        'quarantine': config['celery'].get('quarantine_task'),\n    }\n    config['celery'].setdefault('skip_callback_when_no_payload', True)\n\n    backache = CeleryCache(**config)\n    return backache\n", "go.utils import timezone\nimport mendeley\n\nfrom addons.mendeley import models\nfrom tests.base import OsfTestCase\nfrom addons.mendeley.api import APISession\n\npytestmark = pytest.mark.django_db\n\nclass MendeleyApiTestCase(OsfTestCase):\n\n    def setUp(self):\n        super(MendeleyApiTestCase, self).setUp()\n        self.provider = models.Mendeley()\n        self.mock_partial = mendeley.Mendeley(\n            client_id='1234567890',\n            client_secret='1a2s3d4f5g',\n            redirect_uri='/api/v1/some/fake/url/mendeley'\n        )\n        self.mock_credentials = {\n            'access_token': '1234567890987654321',\n            'refresh_token': 'asdfghjklkjhgfdsa',\n            'expires_at': time.mktime((timezone.now() + datetime.timedelta(days=10)).timetuple()),\n            'token_type': 'bearer',\n        }\n\n    @mock.patch('addons.mendeley.api.MendeleySession.request')\n    def test_request_params(self, mock_request):\n        # All GET requests to Mendeley should have the param \"view=all\"\n        client = APISession(self.mock_partial, self.mock_credentials)\n        client.request()\n        args, kwargs = mock_request.call_args\n        assert_equal(kwargs['params'], {'view': 'all', 'limit': '500'})\n", "\u6210\u529f\n    def login(self,username,password):\n         pass\n    #\u5224\u65ad\u662f\u5426\u6709\u9a8c\u8bc1\u7801\n    def hasCaptcha(self):\n         pass\n    #\u5e26\u9a8c\u8bc1\u7801\u767b\u5f55\n    #\u8fd4\u56de\u662f\u5426\u6210\u529f\n    def loginWithCaptcha(self,username,password,c_code):\n         pass\n\n    #\u4f7f\u7528cookie\u767b\u5f55\n    def loginWithCookie(self,c_value):\n         pass\n    #\u53d1\u9001\u4e00\u6761\u5fae\u535a\n    #\u8fd4\u56de\u662f\u5426\u6210\u529f\n    def publish(self,content):     \n         pass\n        \n\n    \n", "under the Apache License, Version 2.0 (the \"License\"); you may\n# not use this file except in compliance with the License. You may obtain\n# a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations\n# under the License.\n\"\"\"\nBase classes for storage engines\n\"\"\"\n\nimport abc\n\nfrom oslo_config import cfg\nfrom oslo_db import api as db_api\nimport six\n\n_BACKEND_MAPPING = {'sqlalchemy': 'iotronic.db.sqlalchemy.api'}\nIMPL = db_api.DBAPI.from_config(cfg.CONF,\n                                backend_mapping=_BACKEND_MAPPING,\n                                lazy=True)\n\n\ndef get_instance():\n    \"\"\"Return a DB API instance.\"\"\"\n    return IMPL\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass Connection(object):\n    \"\"\"Base class for storage system connections.\"\"\"\n\n    @abc.abstractmethod\n    def __init__(self):\n        \"\"\"Constructor.\"\"\"\n\n    @abc.abstractmethod\n    def get_boardinfo_list(self, columns=None, filters=None, limit=None,\n                           marker=None, sort_key=None, sort_dir=None):\n        \"\"\"Get specific columns for matching boards.\n\n        Return a list of the specified columns for all boards that match the\n        specified filters.\n\n        :param columns: List of column names to return.\n                        Defaults to 'id' column when columns == None.\n        :param filters: Filters to apply. Defaults to None.\n\n                        :associated: True | False\n                        :reserved: True | False\n                        :maintenance: True | False\n                        :provision_state: provision state of board\n                        :provisioned_before:\n                            boards with provision_updated_at field before this\n                            interval in seconds\n        :param limit: Maximum number of boards to return.\n        :param marker: the last item of the previous page; we return the next\n                       result set.\n        :param sort_key: Attribute by which results should be sorted.\n        :param sort_dir: direction in which results should be sorted.\n                         (asc, desc)\n        :returns: A list of tuples of the specified columns.\n        \"\"\"\n\n    @abc.abstractmethod\n    def get_board_list(self, filters=None, limit=None, marker=None,\n                       sort_key=None, sort_dir=None):\n        \"\"\"Return a list of boards.\n\n        :param filters: Filters to apply. Defaults to None.\n\n                        :associated: True | False\n                        :reserved: True | False\n                        :maintenance: True | False\n                        :provision_state: provision state of board\n                        :provisioned_before:\n                            boards with provision_updated_at field before this\n                            interval in seconds\n        :param limit: Maximum number of boards to return.\n        :param marker: the last item of the previous page; we return the next\n                       result set.\n        :param sort_key: Attribute by which results should be sorted.\n        :param sort_dir: direction in which results should be sorted.\n                         (asc, desc)\n        \"\"\"\n\n    @abc.abstractmethod\n    def create_board(self, values):\n        \"\"\"Create a new board.\n\n        :param values: A dict containing several items used to identify\n                       and track the board, and several dicts which are passed\n                       into the Drivers when managing this board. For example:\n\n                       ::\n\n                        {\n                         'uuid': uuidutils.generate_uuid(),\n                         'instance_uuid': None,\n                         'power_state': states.POWER_OFF,\n                         'provision_state': states.AVAILABLE,\n                         'properties': { ... },\n                         'extra': { ... },\n                        }\n        :returns: A board.\n        \"\"\"\n\n    @abc.abstractmethod\n    def get_board_by_id(self, board_id):\n        \"\"\"Return a board.\n\n        :param board_id: The id of a board.\n        :returns: A board.\n        \"\"\"\n\n    @abc.abstractmethod\n    def get_board_by_uuid(self, board_uuid):\n        \"\"\"Return a board.\n\n        :param board_uuid: The uuid of a board.\n        :returns: A board.\n        \"\"\"\n\n    @abc.abstractmethod\n    def get_board_id_by_uuid(self, board_uuid):\n        \"\"\"Return a board id.\n\n        :param board_uuid: The uuid of a board.\n        # :returns: A board.id.\n        \"\"\"\n\n    @abc.abstractmethod\n    def get_board_by_name(self, board_name):\n        \"\"\"Return a board.\n\n        :param board_name: The logical name of a board.\n        :returns: A board.\n        \"\"\"\n\n    @abc.abstractmethod\n    def get_board_by_code(self, instance):\n        \"\"\"Return a board.\n\n        :param instance: The instance code or uuid to search for.\n        :returns: A board.\n        \"\"\"\n\n    @abc.abstractmethod\n    def destroy_board(self, board_id):\n        \"\"\"Destroy a board and all associated interfaces.\n\n        :param board_id: The id or uuid of a board.\n        \"\"\"\n\n    @abc.abstractmethod\n    def update_board(self, board_id, values):\n        \"\"\"Update properties of a board.\n\n        :param board_id: The id or uuid of a board.\n        :param values: Dict of values to update.\n        :returns: A board.\n        :raises: BoardAssociated\n        :raises: BoardNotFound\n        \"\"\"\n\n    @abc.abstractmethod\n    def get_conductor(self, hostname):\n        \"\"\"Retrieve a conductor's service record from the database.\n\n        :param hostname: The hostname of the conductor service.\n        :returns: A conductor.\n        :raises: ConductorNotFound\n        \"\"\"\n\n    @abc.abstractmethod\n    def unregister_conductor(self, hostname):\n        \"\"\"Remove this conductor from the service registry immediately.\n\n        :param hostname: The hostname of this conductor service.\n        :raises: ConductorNotFound\n        \"\"\"\n\n    @abc.abstractmethod\n    def touch_conductor(self, hostname):\n        \"\"\"Mark a conductor as active by updating its 'updated_at' property.\n\n        :param hostname: The hostname of this conductor service.\n        :raises: ConductorNotFound\n        \"\"\"\n\n    @abc.abstractmethod\n    def create_session(self, values):\n        \"\"\"Create a new location.\n\n        :param values: session_id.\n        \"\"\"\n\n    @abc.abstractmethod\n    def update_session(self, session_id, values):\n        \"\"\"Update properties of an session.\n\n        :param session_id: The id of a session.\n        :param values: Dict of values to update.\n        :returns: A session.\n        \"\"\"\n\n    @abc.abstractmethod\n    def get_session_by_board_uuid(self, board_uuid, valid):\n        \"\"\"Return a Wamp session of a Board\n\n        :param board_uuid: Filters to apply. Defaults to None.\n        :param valid: is valid\n        :returns: A session.\n        \"\"\"\n\n    @abc.abstractmethod\n    def get_session_by_id(self, session_id):\n        \"\"\"Return a Wamp session\n\n        :param session_id: The id of a session.\n         :returns: A session.\n        \"\"\"\n\n    @abc.abstractmethod\n    def create_location(self, values):\n        \"\"\"Create a new location.\n\n        :param values: Dict of values.\n        \"\"\"\n\n    @abc.abstractmethod\n    def update_location(self, location_id, values):\n        \"\"\"Update properties of an location.\n\n        :param location_id: The id of a location.\n        :param values: Dict of values to update.\n        :returns: A location.\n        \"\"\"\n\n    @abc.abstractmethod\n    def destroy_location(self, location_id):\n        \"\"\"Destroy an location.\n\n        :param location_id: The id or MAC of a location.\n        \"\"\"\n\n    @abc.abstractmethod\n    def get_locations_by_board_id(self, board_id, limit=None, marker=None,\n                                  sort_key=None, sort_dir=None):\n        \"\"\"List all the locations for a given board.\n\n        :param board_id: The integer board ID.\n        :param limit: Maximum number of locations to return.\n        :param marker: the last item of the previous page; we return the next\n                       result set.\n        :param sort_key: Attribute by which results should be sorted\n        :param sort_dir: direction in which results should be sorted\n                         (asc, desc)\n        :returns: A list of locations.\n        \"\"\"\n\n    @abc.abstractmethod\n    def get_valid_wpsessions_list(self):\n        \"\"\"Return a list of wpsession.\"\"\"\n\n    @abc.abstractmethod\n    def get_wampagent(self, hostname):\n        \"\"\"Retrieve a wampagent's service record from the database.\n\n        :param hostname: The hostname of the wampagent service.\n        :returns: A wampagent.\n        :raises: WampAgentNotFound\n        \"\"\"\n\n    @abc.abstractmethod\n    def get_registration_wampagent(self):\n        \"\"\"Retrieve the registration wampagent record from the database.\n\n        :returns: A wampagent.\n        :raises: WampAgentNotFound\n        \"\"\"\n\n    @abc.abstractmethod\n    def unregister_wampagent(self, hostname):\n        \"\"\"Remove this wampagent from the service registry immediately.\n\n        :param hostname: The hostname of this wampagent service.\n        :raises: WampAgentNotFound\n        \"\"\"\n\n    @abc.abstractmethod\n    def touch_wampagent(self, hostname):\n        \"\"\"Mark a wampagent as active by updating its 'updated_at' property.\n\n        :param hostname: The hostname of this wampagent service.\n        :raises: WampAgentNotFound\n        \"\"\"\n\n    @abc.abstractmethod\n    def get_wampagent_list(self, filters=None, limit=None, marker=None,\n                           sort_key=None, sort_dir=None):\n        \"\"\"Return a list of wampagents.\n\n        :param filters: Filters to apply. Defaults to None.\n        :param limit: Maximum number of wampagents to return.\n        :param marker: the last item of the previous page; we return the next\n                       result set.\n        :param sort_key: Attribute by which results should be sorted.\n        :param sort_dir: direction in which results should be sorted.\n                         (asc, desc)\n        \"\"\"\n\n    @abc.abstractmethod\n    def get_plugin_by_id(self, plugin_id):\n        \"\"\"Return a plugin.\n\n        :param plugin_id: The id of a plugin.\n        :returns: A plugin.\n        \"\"\"\n\n    @abc.abstractmethod\n    def get_plugin_by_uuid(self, plugin_uuid):\n        \"\"\"Return a plugin.\n\n        :param plugin_uuid: The uuid of a plugin.\n        :returns: A plugin.\n        \"\"\"\n\n    @abc.abstractmethod\n    def get_plugin_by_name(self, plugin_name):\n        \"\"\"Return a plugin.\n\n        :param plugin_name: The logical name of a plugin.\n        :returns: A plugin.\n        \"\"\"\n\n    @abc.abstractmethod\n    def create_plugin(self, values):\n        \"\"\"Create a new plugin.\n\n        :param values: A dict containing several items used to identify\n                       and track the plugin\n        :returns: A plugin.\n        \"\"\"\n\n    @abc.abstractmethod\n    def destroy_plugin(self, plugin_id):\n        \"\"\"Destroy a plugin and all associated interfaces.\n\n        :param plugin_id: The id or uuid of a plugin.\n        \"\"\"\n\n    @abc.abstractmethod\n    def update_plugin(self, plugin_id, values):\n        \"\"\"Update properties of a plugin.\n\n        :param plugin_id: The id or uuid of a plugin.\n        :param values: Dict of values to update.\n        :returns: A plugin.\n        :raises: PluginAssociated\n        :raises: PluginNotFound\n        \"\"\"\n\n    @abc.abstractmethod\n    def get_injection_plugin_by_board_uuid(self, board_uuid):\n        \"\"\"get an injection of a plugin using a board_uuid\n\n        :param board_uuid: The id or uuid of a board.\n        :returns: An injection_plugin.\n\n        \"\"\"\n\n    @abc.abstractmethod\n    def get_injection_plugin_by_uuids(self, board_uuid, plugin_uuid):\n        \"\"\"get an injection of a plugin using a board_uuid and plugin_uuid\n\n        :param board_uuid: The id or uuid of a board.\n        :param plugin_uuid: The id or uuid of a plugin.\n        :returns: An injection_plugin.\n\n        \"\"\"\n\n    @abc.abstractmethod\n    def create_injection_plugin(self, values):\n        \"\"\"Create a new injection_plugin.\n\n        :param values: A dict containing several items used to identify\n                       and track the plugin\n        :returns: An injection plugin.\n        \"\"\"\n\n    @abc.abstractmethod\n    def destroy_injection_plugin(self, injection_plugin_id):\n        \"\"\"Destroy an injection plugin and all associated interfaces.\n\n        :param injection_plugin_id: The id or uuid of a plugin.\n        \"\"\"\n\n    @abc.abstractmethod\n    def update_injection_plugin(self, plugin_injection_id, values):\n        \"\"\"Update properties of a plugin.\n\n        :param plugin_id: The id or uuid of a plugin.\n        :param values: Dict of values to update.\n        :returns: A plugin.\n        :raises: PluginAssociated\n        :raises: PluginNotFound\n        \"\"\"\n\n    @abc.abstractmethod\n    def get_injection_plugin_list(self, board_uuid):\n        \"\"\"Return a list of injection_plugins.\n\n        :param board_uuid: The id or uuid of a plugin.\n        :returns: A list of InjectionPlugins on the board.\n\n        \"\"\"\n", "ime\nimport logging\n\nfrom aiohomekit.exceptions import (\n    AccessoryDisconnectedError,\n    AccessoryNotFoundError,\n    EncryptionError,\n)\nfrom aiohomekit.model import Accessories\nfrom aiohomekit.model.characteristics import CharacteristicsTypes\nfrom aiohomekit.model.services import ServicesTypes\n\nfrom homeassistant.core import callback\nfrom homeassistant.helpers.event import async_track_time_interval\n\nfrom .const import CONTROLLER, DOMAIN, ENTITY_MAP, HOMEKIT_ACCESSORY_DISPATCH\n\nDEFAULT_SCAN_INTERVAL = datetime.timedelta(seconds=60)\nRETRY_INTERVAL = 60  # seconds\n\n_LOGGER = logging.getLogger(__name__)\n\n\ndef get_accessory_information(accessory):\n    \"\"\"Obtain the accessory information service of a HomeKit device.\"\"\"\n    result = {}\n    for service in accessory[\"services\"]:\n        stype = service[\"type\"].upper()\n        if ServicesTypes.get_short(stype) != \"accessory-information\":\n            continue\n        for characteristic in service[\"characteristics\"]:\n            ctype = CharacteristicsTypes.get_short(characteristic[\"type\"])\n            if \"value\" in characteristic:\n                result[ctype] = characteristic[\"value\"]\n    return result\n\n\ndef get_bridge_information(accessories):\n    \"\"\"Return the accessory info for the bridge.\"\"\"\n    for accessory in accessories:\n        if accessory[\"aid\"] == 1:\n            return get_accessory_information(accessory)\n    return get_accessory_information(accessories[0])\n\n\ndef get_accessory_name(accessory_info):\n    \"\"\"Return the name field of an accessory.\"\"\"\n    for field in (\"name\", \"model\", \"manufacturer\"):\n        if field in accessory_info:\n            return accessory_info[field]\n    return None\n\n\nclass HKDevice:\n    \"\"\"HomeKit device.\"\"\"\n\n    def __init__(self, hass, config_entry, pairing_data):\n        \"\"\"Initialise a generic HomeKit device.\"\"\"\n\n        self.hass = hass\n        self.config_entry = config_entry\n\n        # We copy pairing_data because homekit_python may mutate it, but we\n        # don't want to mutate a dict owned by a config entry.\n        self.pairing_data = pairing_data.copy()\n\n        self.pairing = hass.data[CONTROLLER].load_pairing(\n            self.pairing_data[\"AccessoryPairingID\"], self.pairing_data\n        )\n\n        self.accessories = None\n        self.config_num = 0\n\n        self.entity_map = Accessories()\n\n        # A list of callbacks that turn HK service metadata into entities\n        self.listeners = []\n\n        # The platorms we have forwarded the config entry so far. If a new\n        # accessory is added to a bridge we may have to load additional\n        # platforms. We don't want to load all platforms up front if its just\n        # a lightbulb. And we don't want to forward a config entry twice\n        # (triggers a Config entry already set up error)\n        self.platforms = set()\n\n        # This just tracks aid/iid pairs so we know if a HK service has been\n        # mapped to a HA entity.\n        self.entities = []\n\n        self.available = True\n\n        self.signal_state_updated = \"_\".join((DOMAIN, self.unique_id, \"state_updated\"))\n\n        # Current values of all characteristics homekit_controller is tracking.\n        # Key is a (accessory_id, characteristic_id) tuple.\n        self.current_state = {}\n\n        self.pollable_characteristics = []\n\n        # If this is set polling is active and can be disabled by calling\n        # this method.\n        self._polling_interval_remover = None\n\n        # Never allow concurrent polling of the same accessory or bridge\n        self._polling_lock = asyncio.Lock()\n        self._polling_lock_warned = False\n\n        self.watchable_characteristics = []\n\n        self.pairing.dispatcher_connect(self.process_new_events)\n\n    def add_pollable_characteristics(self, characteristics):\n        \"\"\"Add (aid, iid) pairs that we need to poll.\"\"\"\n        self.pollable_characteristics.extend(characteristics)\n\n    def remove_pollable_characteristics(self, accessory_id):\n        \"\"\"Remove all pollable characteristics by accessory id.\"\"\"\n        self.pollable_characteristics = [\n            char for char in self.pollable_characteristics if char[0] != accessory_id\n        ]\n\n    def add_watchable_characteristics(self, characteristics):\n        \"\"\"Add (aid, iid) pairs that we need to poll.\"\"\"\n        self.watchable_characteristics.extend(characteristics)\n        self.hass.async_create_task(self.pairing.subscribe(characteristics))\n\n    def remove_watchable_characteristics(self, accessory_id):\n        \"\"\"Remove all pollable characteristics by accessory id.\"\"\"\n        self.watchable_characteristics = [\n            char for char in self.watchable_characteristics if char[0] != accessory_id\n        ]\n\n    @callback\n    def async_set_unavailable(self):\n        \"\"\"Mark state of all entities on this connection as unavailable.\"\"\"\n        self.available = False\n        self.hass.helpers.dispatcher.async_dispatcher_send(self.signal_state_updated)\n\n    async def async_setup(self):\n        \"\"\"Prepare to use a paired HomeKit device in Home Assistant.\"\"\"\n        cache = self.hass.data[ENTITY_MAP].get_map(self.unique_id)\n        if not cache:\n            if await self.async_refresh_entity_map(self.config_num):\n                self._polling_interval_remover = async_track_time_interval(\n                    self.hass, self.async_update, DEFAULT_SCAN_INTERVAL\n                )\n                return True\n            return False\n\n        self.accessories = cache[\"accessories\"]\n        self.config_num = cache[\"config_num\"]\n\n        self.entity_map = Accessories.from_list(self.accessories)\n\n        self._polling_interval_remover = async_track_time_interval(\n            self.hass, self.async_update, DEFAULT_SCAN_INTERVAL\n        )\n\n        self.hass.async_create_task(self.async_process_entity_map())\n\n        return True\n\n    async def async_process_entity_map(self):\n        \"\"\"\n        Process the entity map and load any platforms or entities that need adding.\n\n        This is idempotent and will be called at startup and when we detect metadata changes\n        via the c# counter on the zeroconf record.\n        \"\"\"\n        # Ensure the Pairing object has access to the latest version of the entity map. This\n        # is especially important for BLE, as the Pairing instance relies on the entity map\n        # to map aid/iid to GATT characteristics. So push it to there as well.\n\n        self.pairing.pairing_data[\"accessories\"] = self.accessories\n\n        await self.async_load_platforms()\n\n        self.add_entities()\n\n        if self.watchable_characteristics:\n            await self.pairing.subscribe(self.watchable_characteristics)\n\n        await self.async_update()\n\n        return True\n\n    async def async_unload(self):\n        \"\"\"Stop interacting with device and prepare for removal from hass.\"\"\"\n        if self._polling_interval_remover:\n            self._polling_interval_remover()\n\n        await self.pairing.unsubscribe(self.watchable_characteristics)\n\n        unloads = []\n        for platform in self.platforms:\n            unloads.append(\n                self.hass.config_entries.async_forward_entry_unload(\n                    self.config_entry, platform\n                )\n            )\n\n        results = await asyncio.gather(*unloads)\n\n        return False not in results\n\n    async def async_refresh_entity_map(self, config_num):\n        \"\"\"Handle setup of a HomeKit accessory.\"\"\"\n        try:\n            self.accessories = await self.pairing.list_accessories_and_characteristics()\n        except AccessoryDisconnectedError:\n            # If we fail to refresh this data then we will naturally retry\n            # later when Bonjour spots c# is still not up to date.\n            return False\n\n        self.entity_map = Accessories.from_list(self.accessories)\n\n        self.hass.data[ENTITY_MAP].async_create_or_update_map(\n            self.unique_id, config_num, self.accessories\n        )\n\n        self.config_num = config_num\n        self.hass.async_create_task(self.async_process_entity_map())\n\n        return True\n\n    def add_listener(self, add_entities_cb):\n        \"\"\"Add a callback to run when discovering new entities.\"\"\"\n        self.listeners.append(add_entities_cb)\n        self._add_new_entities([add_entities_cb])\n\n    def add_entities(self):\n        \"\"\"Process the entity map and create HA entities.\"\"\"\n        self._add_new_entities(self.listeners)\n\n    def _add_new_entities(self, callbacks):\n        for accessory in self.accessories:\n            aid = accessory[\"aid\"]\n            for service in accessory[\"services\"]:\n                iid = service[\"iid\"]\n                stype = ServicesTypes.get_short(service[\"type\"].upper())\n                service[\"stype\"] = stype\n\n                if (aid, iid) in self.entities:\n                    # Don't add the same entity again\n                    continue\n\n                for listener in callbacks:\n                    if listener(aid, service):\n                        self.entities.append((aid, iid))\n                        break\n\n    async def async_load_platforms(self):\n        \"\"\"Load any platforms needed by this HomeKit device.\"\"\"\n        for accessory in self.accessories:\n            for service in accessory[\"services\"]:\n                stype = ServicesTypes.get_short(service[\"type\"].upper())\n                if stype not in HOMEKIT_ACCESSORY_DISPATCH:\n                    continue\n\n                platform = HOMEKIT_ACCESSORY_DISPATCH[stype]\n                if platform in self.platforms:\n                    continue\n\n                self.platforms.add(platform)\n                try:\n                    await self.hass.config_entries.async_forward_entry_setup(\n                        self.config_entry, platform\n                    )\n                except Exception:\n                    self.platforms.remove(platform)\n                    raise\n\n    async def async_update(self, now=None):\n        \"\"\"Poll state of all entities attached to this bridge/accessory.\"\"\"\n        if not self.pollable_characteristics:\n            _LOGGER.debug(\"HomeKit connection not polling any characteristics.\")\n            return\n\n        if self._polling_lock.locked():\n            if not self._polling_lock_warned:\n                _LOGGER.warning(\n                    \"HomeKit controller update skipped as previous poll still in flight\"\n                )\n                self._polling_lock_warned = True\n            return\n\n        if self._polling_lock_warned:\n            _LOGGER.info(\n                \"HomeKit controller no longer detecting back pressure - not skipping poll\"\n            )\n            self._polling_lock_warned = False\n\n        async with self._polling_lock:\n            _LOGGER.debug(\"Starting HomeKit controller update\")\n\n            try:\n                new_values_dict = await self.get_characteristics(\n                    self.pollable_characteristics\n                )\n            except AccessoryNotFoundError:\n                # Not only did the connection fail, but also the accessory is not\n                # visible on the network.\n                self.async_set_unavailable()\n                return\n            except (AccessoryDisconnectedError, EncryptionError):\n                # Temporary connection failure. Device is still available but our\n                # connection was dropped.\n                return\n\n            self.process_new_events(new_values_dict)\n\n            _LOGGER.debug(\"Finished HomeKit controller update\")\n\n    def process_new_events(self, new_values_dict):\n        \"\"\"Process events from accessory into HA state.\"\"\"\n        self.available = True\n\n        for (aid, cid), value in new_values_dict.items():\n            accessory = self.current_state.setdefault(aid, {})\n            accessory[cid] = value\n\n        # self.current_state will be replaced by entity_map in a future PR\n        # For now we update both\n        self.entity_map.process_changes(new_values_dict)\n\n        self.hass.helpers.dispatcher.async_dispatcher_send(self.signal_state_updated)\n\n    async def get_characteristics(self, *args, **kwargs):\n        \"\"\"Read latest state from homekit accessory.\"\"\"\n        return await self.pairing.get_characteristics(*args, **kwargs)\n\n    async def put_characteristics(self, characteristics):\n        \"\"\"Control a HomeKit device state from Home Assistant.\"\"\"\n        results = await self.pairing.put_characteristics(characteristics)\n\n        # Feed characteristics back into HA and update the current state\n        # results will only contain failures, so anythin in characteristics\n        # but not in results was applied successfully - we can just have HA\n        # reflect the change immediately.\n\n        new_entity_state = {}\n        for aid, iid, value in characteristics:\n            key = (aid, iid)\n\n            # If the key was returned by put_characteristics() then the\n            # change didn't work\n            if key in results:\n                continue\n\n            # Otherwise it was accepted and we can apply the change to\n            # our state\n            new_entity_state[key] = {\"value\": value}\n\n        self.process_new_events(new_entity_state)\n\n    @property\n    def unique_id(self):\n        \"\"\"\n        Return a unique id for this accessory or bridge.\n\n        This id is random and will change if a device undergoes a hard reset.\n        \"\"\"\n        return self.pairing_data[\"AccessoryPairingID\"]\n\n    @property\n    def connection_info(self):\n        \"\"\"Return accessory information for the main accessory.\"\"\"\n        return get_bridge_information(self.accessories)\n\n    @property\n    def name(self):\n        \"\"\"Name of the bridge accessory.\"\"\"\n        return get_accessory_name(self.connection_info) or self.unique_id\n", "mport absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport contextlib\nimport copy\nimport linecache\nimport re\nimport sys\nimport threading\nimport weakref\n\nimport tensorflow.python.platform\n\nimport six\nfrom tensorflow.core.framework import attr_value_pb2\nfrom tensorflow.core.framework import graph_pb2\nfrom tensorflow.python.framework import device as pydev\nfrom tensorflow.python.framework import registry\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.framework import types\n\n\ndef _convert_stack(stack):\n  \"\"\"Converts a stack extracted using _extract_stack() to a traceback stack.\n\n  Args:\n    stack: A list of n 4-tuples, (filename, lineno, name, frame_globals).\n\n  Returns:\n    A list of n 4-tuples (filename, lineno, name, code), where the code tuple\n    element is calculated from the corresponding elements of the input tuple.\n  \"\"\"\n  ret = []\n  for filename, lineno, name, frame_globals in stack:\n    linecache.checkcache(filename)\n    line = linecache.getline(filename, lineno, frame_globals)\n    if line:\n      line = line.strip()\n    else:\n      line = None\n    ret.append((filename, lineno, name, line))\n  return ret\n\n\n# pylint: disable=line-too-long\ndef _extract_stack():\n  \"\"\"A lightweight re-implementation of traceback.extract_stack.\n\n  NOTE(mrry): traceback.extract_stack eagerly retrieves the line of code for\n    each stack frame using linecache, which results in an abundance of stat()\n    calls. This implementation does not retrieve the code, and any consumer\n    should apply _convert_stack to the result to obtain a traceback that can\n    be formatted etc. using traceback methods.\n\n  Returns:\n    A list of 4-tuples (filename, lineno, name, frame_globals) corresponding to\n    the call stack of the current thread.\n  \"\"\"\n  # pylint: enable=line-too-long\n  try:\n    raise ZeroDivisionError\n  except ZeroDivisionError:\n    f = sys.exc_info()[2].tb_frame.f_back\n  ret = []\n  while f is not None:\n    lineno = f.f_lineno\n    co = f.f_code\n    filename = co.co_filename\n    name = co.co_name\n    frame_globals = f.f_globals\n    ret.append((filename, lineno, name, frame_globals))\n    f = f.f_back\n  ret.reverse()\n  return ret\n\n\nclass Tensor(object):\n  \"\"\"Represents a value produced by an `Operation`.\n\n  A `Tensor` is a symbolic handle to one of the outputs of an\n  `Operation`. It does not hold the values of that operation's output,\n  but instead provides a means of computing those values in a\n  TensorFlow [`Session`](../../api_docs/python/client.md#Session).\n\n  This class has two primary purposes:\n\n  1. A `Tensor` can be passed as an input to another `Operation`.\n     This builds a dataflow connection between operations, which\n     enables TensorFlow to execute an entire `Graph` that represents a\n     large, multi-step computation.\n\n  2. After the graph has been launched in a session, the value of the\n     `Tensor` can be computed by passing it to\n     [`Session.run()`](../../api_docs/python/client.md#Session.run).\n     `t.eval()` is a shortcut for calling\n     `tf.get_default_session().run(t)`.\n\n  In the following example, `c`, `d`, and `e` are symbolic `Tensor`\n  objects, whereas `result` is a numpy array that stores a concrete\n  value:\n\n  ```python\n  # Build a dataflow graph.\n  c = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n  d = tf.constant([[1.0, 1.0], [0.0, 1.0]])\n  e = tf.matmul(c, d)\n\n  # Construct a `Session` to execut the graph.\n  sess = tf.Session()\n\n  # Execute the graph and store the value that `e` represents in `result`.\n  result = sess.run(e)\n  ```\n\n  @@dtype\n  @@name\n  @@value_index\n  @@graph\n  @@op\n  @@consumers\n\n  @@eval\n\n  @@get_shape\n  @@set_shape\n\n  \"\"\"\n\n  # List of Python operators that we allow to override.\n  OVERLOADABLE_OPERATORS = {\n      # Binary.\n      \"__add__\",\n      \"__radd__\",\n      \"__sub__\",\n      \"__rsub__\",\n      \"__mul__\",\n      \"__rmul__\",\n      \"__div__\",\n      \"__rdiv__\",\n      \"__truediv__\",\n      \"__rtruediv__\",\n      \"__floordiv__\",\n      \"__rfloordiv__\",\n      \"__mod__\",\n      \"__rmod__\",\n      \"__lt__\",\n      \"__le__\",\n      \"__gt__\",\n      \"__ge__\",\n      \"__and__\",\n      \"__rand__\",\n      \"__or__\",\n      \"__ror__\",\n      \"__xor__\",\n      \"__rxor__\",\n      \"__getitem__\",\n      # Unary.\n      \"__invert__\",\n      \"__neg__\",\n      \"__abs__\"\n  }\n\n  def __init__(self, op, value_index, dtype):\n    \"\"\"Creates a new `Tensor`.\n\n    Args:\n      op: An `Operation`. `Operation` that computes this tensor.\n      value_index: An `int`. Index of the operation's endpoint that produces\n        this tensor.\n      dtype: A `types.DType`. Type of data stored in this tensor.\n\n    Raises:\n      TypeError: If the op is not an `Operation`.\n    \"\"\"\n    if not isinstance(op, Operation):\n      raise TypeError(\"op needs to be an Operation: %s\" % op)\n    self._op = op\n    self._value_index = value_index\n    self._dtype = types.as_dtype(dtype)\n    self._shape = tensor_shape.unknown_shape()\n    # List of operations that use this Tensor as input.  We maintain this list\n    # to easily navigate a computation graph.\n    self._consumers = []\n\n  @property\n  def op(self):\n    \"\"\"The `Operation` that produces this tensor as an output.\"\"\"\n    return self._op\n\n  @property\n  def dtype(self):\n    \"\"\"The `DType` of elements in this tensor.\"\"\"\n    return self._dtype\n\n  @property\n  def graph(self):\n    \"\"\"The `Graph` that contains this tensor.\"\"\"\n    return self._op.graph\n\n  @property\n  def name(self):\n    \"\"\"The string name of this tensor.\"\"\"\n    if not self._op.name:\n      raise ValueError(\"Operation was not named: %s\" % self._op)\n    return \"%s:%d\" % (self._op.name, self._value_index)\n\n  @property\n  def device(self):\n    \"\"\"The name of the device on which this tensor will be produced, or None.\"\"\"\n    return self._op.device\n\n  def _shape_as_list(self):\n    if self._shape.ndims is not None:\n      return [dim.value for dim in self._shape.dims]\n    else:\n      return None\n\n  def get_shape(self):\n    \"\"\"Returns the `TensorShape` that represents the shape of this tensor.\n\n    The shape is computed using shape inference functions that are\n    registered for each `Operation` type using `tf.RegisterShape`.\n    See [`TensorShape`](../../api_docs/python/framework.md#TensorShape) for more\n    details of what a shape represents.\n\n    The inferred shape of a tensor is used to provide shape\n    information without having to launch the graph in a session. This\n    can be used for debugging, and providing early error messages. For\n    example:\n\n    ```python\n    c = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n\n    print c.get_shape()\n    ==> TensorShape([Dimension(2), Dimension(3)])\n\n    d = tf.constant([[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]])\n\n    print d.get_shape()\n    ==> TensorShape([Dimension(4), Dimension(2)])\n\n    # Raises a ValueError, because `c` and `d` do not have compatible\n    # inner dimensions.\n    e = tf.matmul(c, d)\n\n    f = tf.matmul(c, d, transpose_a=True, transpose_b=True)\n\n    print f.get_shape()\n    ==> TensorShape([Dimension(3), Dimension(4)])\n    ```\n\n    In some cases, the inferred shape may have unknown dimensions. If\n    the caller has additional information about the values of these\n    dimensions, `Tensor.set_shape()` can be used to augment the\n    inferred shape.\n\n    Returns:\n      A `TensorShape` representing the shape of this tensor.\n    \"\"\"\n    return self._shape\n\n  def set_shape(self, shape):\n    \"\"\"Updates the shape of this tensor.\n\n    This method can be called multiple times, and will merge the given\n    `shape` with the current shape of this tensor. It can be used to\n    provide additional information about the shape of this tensor that\n    cannot be inferred from the graph alone. For example, this can be used\n    to provide additional information about the shapes of images:\n\n    ```python\n    _, image_data = tf.TFRecordReader(...).read(...)\n    image = tf.image.decode_png(image_data, channels=3)\n\n    # The height and width dimensions of `image` are data dependent, and\n    # cannot be computed without executing the op.\n    print image.get_shape()\n    ==> TensorShape([Dimension(None), Dimension(None), Dimension(3)])\n\n    # We know that each image in this dataset is 28 x 28 pixels.\n    image.set_shape([28, 28, 3])\n    print image.get_shape()\n    ==> TensorShape([Dimension(28), Dimension(28), Dimension(3)])\n    ```\n\n    Args:\n      shape: A `TensorShape` representing the shape of this tensor.\n\n    Raises:\n      ValueError: If `shape` is not compatible with the current shape of\n        this tensor.\n    \"\"\"\n    self._shape = self._shape.merge_with(shape)\n\n  @property\n  def value_index(self):\n    \"\"\"The index of this tensor in the outputs of its `Operation`.\"\"\"\n    return self._value_index\n\n  def consumers(self):\n    \"\"\"Returns a list of `Operation`s that consume this tensor.\n\n    Returns:\n      A list of `Operation`s.\n    \"\"\"\n    return self._consumers\n\n  def _add_consumer(self, consumer):\n    \"\"\"Add a consumer to this tensor.\n\n    Args:\n      consumer: an Operation.\n\n    Raises:\n      TypeError: if the consumer is not an Operation.\n    \"\"\"\n    if not isinstance(consumer, Operation):\n      raise TypeError(\"Consumer must be an Operation: %s\" % consumer)\n    self._consumers.append(consumer)\n\n  def _as_node_def_input(self):\n    \"\"\"Return a value to use for the NodeDef \"input\" attribute.\n\n    The returned string can be used in a NodeDef \"input\" attribute\n    to indicate that the NodeDef uses this Tensor as input.\n\n    Raises:\n      ValueError: if this Tensor's Operation does not have a name.\n\n    Returns:\n      a string.\n    \"\"\"\n    if not self._op.name:\n      raise ValueError(\"Operation was not named: %s\" % self._op)\n    if self._value_index == 0:\n      return self._op.name\n    else:\n      return \"%s:%d\" % (self._op.name, self._value_index)\n\n  def __str__(self):\n    return \"Tensor(\\\"%s\\\"%s%s%s)\" % (\n        self.name,\n        (\", shape=%s\" % self.get_shape())\n        if self.get_shape().ndims is not None else \"\",\n        (\", dtype=%s\" % self._dtype.name) if self._dtype else \"\",\n        (\", device=%s\" % self.device) if self.device else \"\")\n\n  def __hash__(self):\n    # Necessary to support Python's collection membership operators\n    return id(self)\n\n  def __eq__(self, other):\n    # Necessary to support Python's collection membership operators\n    return id(self) == id(other)\n\n  # NOTE(mrry): This enables the Tensor's overloaded \"right\" binary\n  # operators to run when the left operand is an ndarray, because it\n  # accords the Tensor class higher priority than an ndarray, or a\n  # numpy matrix.\n  # TODO(mrry): Convert this to using numpy's __numpy_ufunc__\n  # mechanism, which allows more control over how Tensors interact\n  # with ndarrays.\n  __array_priority__ = 100\n\n  @staticmethod\n  def _override_operator(operator, func):\n    \"\"\"Overrides (string) operator on Tensors to call func.\n\n    Args:\n      operator: the string name of the operator to override.\n      func: the function that replaces the overriden operator.\n\n    Raises:\n      ValueError: If operator has already been overwritten,\n        or if operator is not allowed to be overwritten.\n    \"\"\"\n    if getattr(Tensor, operator, None) is not None:\n      # check to see if this is a default method-wrapper which will be true\n      # for the comparison operators.\n      if not isinstance(getattr(Tensor, operator, None), type(all.__call__)):\n        raise ValueError(\"operator %s cannot be overwritten again.\" % operator)\n    if operator not in Tensor.OVERLOADABLE_OPERATORS:\n      raise ValueError(\"Overriding %s is disallowed\" % operator)\n    setattr(Tensor, operator, func)\n\n  def __iter__(self):\n    \"\"\"Dummy method to prevent iteration. Do not call.\n\n    NOTE(mrry): If we register __getitem__ as an overloaded operator,\n    Python will valiantly attempt to iterate over the Tensor from 0 to\n    infinity.  Declaring this method prevents this unintended\n    behavior.\n\n    Raises:\n      TypeError: when invoked.\n    \"\"\"\n    raise TypeError(\"'Tensor' object is not iterable\")\n\n  def eval(self, feed_dict=None, session=None):\n    \"\"\"Evaluates this tensor in a `Session`.\n\n    Calling this method will execute all preceding operations that\n    produce the inputs needed for the operation that produces this\n    tensor.\n\n    *N.B.* Before invoking `Tensor.eval()`, its graph must have been\n    launched in a session, and either a default session must be\n    available, or `session` must be specified explicitly.\n\n    Args:\n      feed_dict: A dictionary that maps `Tensor` objects to feed values.\n        See [`Session.run()`](../../api_docs/python/client.md#Session.run) for a\n        description of the valid feed values.\n      session: (Optional.) The `Session` to be used to evaluate this tensor. If\n        none, the default session will be used.\n\n    Returns:\n      A numpy array corresponding to the value of this tensor.\n\n    \"\"\"\n    return _eval_using_default_session(self, feed_dict, self.graph, session)\n\n\ndef _TensorTensorConversionFunction(t, dtype=None, name=None):\n  _ = name\n  if dtype and not dtype.is_compatible_with(t.dtype):\n    raise ValueError(\n        \"Tensor conversion requested dtype %s for Tensor with dtype %s: %r\"\n        % (dtype.name, t.dtype.name, str(t)))\n  return t\n\n\n_tensor_conversion_func_registry = {\n    0: [(Tensor, _TensorTensorConversionFunction)]}\n\n\ndef convert_to_tensor(value, dtype=None, name=None):\n  \"\"\"Converts the given `value` to a `Tensor`.\n\n  This function converts Python objects of various types to `Tensor`\n  objects. It accepts `Tensor` objects, numpy arrays, Python lists,\n  and Python scalars. For example:\n\n  ```python\n  import numpy as np\n  array = np.random.rand((32, 100, 100))\n\n  def my_func(arg):\n    arg = tf.convert_to_tensor(arg, dtype=tf.float32)\n    return tf.matmul(arg, arg) + arg\n\n  # The following calls are equivalent.\n  value_1 = my_func(tf.constant([[1.0, 2.0], [3.0, 4.0]]))\n  value_2 = my_func([[1.0, 2.0], [3.0, 4.0]])\n  value_3 = my_func(np.array([[1.0, 2.0], [3.0, 4.0]], dtype=np.float32))\n  ```\n\n  This function can be useful when composing a new operation in Python\n  (such as `my_func` in the example above). All standard Python op\n  constructors apply this function to each of their Tensor-valued\n  inputs, which allows those ops to accept numpy arrays, Python lists,\n  and scalars in addition to `Tensor` objects.\n\n  Args:\n    value: An object whose type has a registered `Tensor` conversion function.\n    dtype: Optional element type for the returned tensor. If missing, the\n      type is inferred from the type of `value`.\n    name: Optional name to use if a new `Tensor` is created.\n\n  Returns:\n    A `Tensor` based on `value`.\n\n  Raises:\n    TypeError: If no conversion function is registered for `value`.\n    RuntimeError: If a registered conversion function returns an invalid value.\n\n  \"\"\"\n  error_prefix = \"\" if name is None else \"%s: \" % name\n  if dtype is not None:\n    dtype = types.as_dtype(dtype)\n  for _, funcs_at_priority in sorted(_tensor_conversion_func_registry.items()):\n    for base_type, conversion_func in funcs_at_priority:\n      if isinstance(value, base_type):\n        ret = conversion_func(value, dtype=dtype, name=name)\n        if not isinstance(ret, Tensor):\n          raise RuntimeError(\n              \"%sConversion function %r for type %s returned non-Tensor: %r\"\n              % (error_prefix, conversion_func, base_type, ret))\n        if dtype and not dtype.is_compatible_with(ret.dtype):\n          raise RuntimeError(\n              \"%sConversion function %r for type %s returned incompatible \"\n              \"dtype: requested = %s, actual = %s\"\n              % (error_prefix, conversion_func, base_type,\n                 dtype.name, ret.dtype.name))\n        return ret\n  raise TypeError(\"%sCannot convert %r with type %s to Tensor: \"\n                  \"no conversion function registered.\"\n                  % (error_prefix, value, type(value)))\n\n\ndef convert_to_tensor_or_indexed_slices(value, dtype=None, name=None):\n  \"\"\"Converts the given object to a `Tensor` or an `IndexedSlices`.\n\n  If `value` is an `IndexedSlices` it is returned\n  unmodified. Otherwise, it is converted to a `Tensor` using\n  `convert_to_tensor()`.\n\n  Args:\n    value: An `IndexedSlices` or an object that can be consumed by\n      `convert_to_tensor()`.\n    dtype: (Optional.) The required `DType` of the returned `Tensor` or\n      `IndexedSlices`.\n    name: (Optional.) A name to use if a new `Tensor` is created.\n\n  Returns:\n    An `Tensor` or an `IndexedSlices` based on `value`.\n\n  Raises:\n    ValueError: If `dtype` does not match the element type of `value`.\n  \"\"\"\n  if isinstance(value, IndexedSlices):\n    if dtype and not types.AsDType(dtype).is_compatible_with(value.dtype):\n      raise ValueError(\n          \"Tensor conversion requested dtype %s for Tensor with dtype %s: %r\"\n          % (types.AsDType(dtype).name, value.dtype.name, str(value)))\n    return value\n  else:\n    return convert_to_tensor(value, dtype, name)\n\n\ndef convert_n_to_tensor_or_indexed_slices(values, dtype=None, name=None):\n  \"\"\"Converts `values` to a list of `Tensor` or `IndexedSlices` objects.\n\n  Args:\n    values: A list of `None`, `IndexedSlices`, or objects that can be consumed\n      by `convert_to_tensor()`.\n    dtype: (Optional.) The required `DType` of the returned `Tensor`\n      `IndexedSlices`.\n\n    name: (Optional.) A name prefix to used when a new `Tensor` is\n      created, in which case element `i` will be given the name `name\n      + '_' + i`.\n\n  Returns:\n    A list of `Tensor` and/or `IndexedSlices` objects.\n\n  Raises:\n    TypeError: If no conversion function is registered for an element in\n      `values`.\n    RuntimeError: If a registered conversion function returns an invalid\n      value.\n  \"\"\"\n  if not isinstance(values, collections.Sequence):\n    raise TypeError(\"values must be a list.\")\n  ret = []\n  for i, value in enumerate(values):\n    if value is None:\n      ret.append(value)\n    else:\n      n = None if name is None else \"%s_%d\" % (name, i)\n      ret.append(\n          convert_to_tensor_or_indexed_slices(value, dtype=dtype, name=n))\n  return ret\n\n\ndef register_tensor_conversion_function(base_type, conversion_func,\n                                        priority=100):\n  \"\"\"Registers a function for converting objects of base_type to Tensor.\n\n  The conversion function must have the following signature:\n\n      def conversion_func(value, dtype=None, name=None):\n        # ...\n\n  It must return a Tensor with the given dtype if specified. If the\n  conversion function creates a new Tensor, it should use the given\n  name if specified. All exceptions will be propagated to the caller.\n\n  NOTE: The conversion functions will execute in order of priority,\n    followed by order of registration. To ensure that a conversion\n    function F runs before another conversion function G, ensure that\n    F is registered with a smaller priority than G.\n\n  Args:\n    base_type: The base type or tuple of base types for all objects that\n      `conversion_func` accepts.\n    conversion_func: A function that converts instances of base_type to Tensor.\n    priority: Optional integer that indicates the priority for applying this\n      conversion function. Conversion functions with smaller priority values\n      run earlier than conversion functions with larger priority values.\n      Defaults to 100.\n\n  Raises:\n    TypeError: If the arguments do not have the appropriate type.\n\n  \"\"\"\n  if not (isinstance(base_type, type) or\n          (isinstance(base_type, tuple)\n           and all(isinstance(x, type) for x in base_type))):\n    raise TypeError(\"base_type must be a type or a tuple of types.\")\n  if not callable(conversion_func):\n    raise TypeError(\"conversion_func must be callable.\")\n\n  try:\n    funcs_at_priority = _tensor_conversion_func_registry[priority]\n  except KeyError:\n    funcs_at_priority = []\n    _tensor_conversion_func_registry[priority] = funcs_at_priority\n  funcs_at_priority.append((base_type, conversion_func))\n\n\nclass IndexedSlices(object):\n  \"\"\"A sparse representation of a set of tensor slices at given indices.\n\n  This class is a simple wrapper for a pair of `Tensor` objects:\n\n  * `values`: A `Tensor` of any dtype with shape `[D0, D1, ..., Dn]`.\n  * `indices`: A 1-D integer `Tensor` with shape `[D0]`.\n\n  An `IndexedSlices` is typically used to represent a subset of a larger\n  tensor `dense` of shape `[LARGE0, D1, .. , DN]` where `LARGE0 >> D0`.\n  The values in `indices` are the indices in the first dimension of\n  the slices that have been extracted from the larger tensor.\n\n  The dense tensor `dense` represented by an `IndexedSlices` `slices` has\n\n  ```python\n  dense[slices.indices[i], :, :, :, ...] = slices.values[i, :, :, :, ...]\n  ```\n\n  The `IndexedSlices` class is used principally in the definition of\n  gradients for operations that have sparse gradients\n  (e.g. [`tf.gather`](../../api_docs/python/array_ops.md#gather)).\n\n  Contrast this representation with\n  [`SparseTensor`](../../api_docs/python/sparse_ops.md#SparseTensor),\n  which uses multi-dimensional indices and scalar values.\n\n  @@__init__\n\n  @@values\n  @@indices\n  @@dense_shape\n\n  @@name\n  @@dtype\n  @@device\n  @@op\n  \"\"\"\n\n  def __init__(self, values, indices, dense_shape=None):\n    \"\"\"Creates an `IndexedSlices`.\"\"\"\n    self._values = values\n    self._indices = indices\n    self._dense_shape = dense_shape\n\n  @property\n  def values(self):\n    \"\"\"A `Tensor` containing the values of the slices.\"\"\"\n    return self._values\n\n  @property\n  def indices(self):\n    \"\"\"A 1-D `Tensor` containing the indices of the slices.\"\"\"\n    return self._indices\n\n  @property\n  def dense_shape(self):\n    \"\"\"A 1-D `Tensor` containing the shape of the corresponding dense tensor.\"\"\"\n    return self._dense_shape\n\n  @property\n  def name(self):\n    \"\"\"The name of this `IndexedSlices`.\"\"\"\n    return self.values.name\n\n  @property\n  def device(self):\n    \"\"\"The name of the device on which `values` will be produced, or `None`.\"\"\"\n    return self.values.device\n\n  @property\n  def op(self):\n    \"\"\"The `Operation` that produces `values` as an output.\"\"\"\n    return self.values.op\n\n  @property\n  def dtype(self):\n    \"\"\"The `DType` of elements in this tensor.\"\"\"\n    return self.values.dtype\n\n  def __str__(self):\n    return \"IndexedSlices(indices=%s, values=%s)\" % (\n        self._indices, self._values)\n\n\ndef assert_same_graph(items, expected_graph=None):\n  \"\"\"Asserts all items are from the same graph.\n\n  Args:\n    items: List of graph items (e.g., Variable, Tensor, SparseTensor,\n        Operation, or IndexedSlices).\n    expected_graph: Expected graph. If not specified, assert all tensors are\n        from the same graph.\n  Returns:\n    items, for chaining.\n  Raises:\n    ValueError: If any graphs do not match.\n  \"\"\"\n  for item in items:\n    if not expected_graph:\n      expected_graph = item.graph\n    elif expected_graph != item.graph:\n      raise ValueError(\"Items must be from the same graph.\")\n  return items\n\n\nclass SparseTensor(object):\n  \"\"\"Represents a sparse tensor.\n\n  Tensorflow represents a sparse tensor as three separate dense tensors:\n  `indices`, `values`, and `dense_shape`.  In Python, the three tensors are\n  collected into a `SparseTensor` class for ease of use.  If you have separate\n  `indices`, `values`, and `dense_shape` tensors, wrap them in a `SparseTensor`\n  object before passing to the Ops below.\n\n  Concretely, the sparse tensor `SparseTensor(values, indices, dense_shape)` is\n\n  * `indices`: A 2-D int64 tensor of shape `[N, ndims]`.\n  * `values`: A 1-D tensor of any type and shape `[N]`.\n  * `dense_shape`: A 1-D int64 tensor of shape `[ndims]`.\n\n  where `N` and `ndims` are the number of values, and number of dimensions in\n  the `SparseTensor` respectively.\n\n  The corresponding dense tensor satisfies\n\n  ```python\n  dense.shape = dense_shape\n  dense[tuple(indices[i])] = values[i]\n  ```\n\n  By convention, `indices` should be sorted in row-major order (or equivalently\n  lexigraphic order on the tuples `indices[i]`).  This is not enforced when\n  `SparseTensor` objects are constructed, but most Ops assume correct ordering.\n  If the ordering is wrong, it can be fixed by calling `sparse_reorder` on the\n  misordered `SparseTensor`.\n\n  Example: The sparse tensor\n\n  ```python\n    SparseTensor(values=[1, 2], indices=[[0, 0], [1, 2]], shape=[3, 4])\n  ```\n\n  represents the dense tensor\n\n  ```python\n    [[1, 0, 0, 0]\n     [0, 0, 2, 0]\n     [0, 0, 0, 0]]\n  ```\n\n  @@__init__\n  @@indices\n  @@values\n  @@dtype\n  @@shape\n  @@graph\n  \"\"\"\n\n  def __init__(self, indices, values, shape):\n    \"\"\"Creates a `SparseTensor`.\n\n    Args:\n      indices: A 2-D int64 tensor of shape `[N, ndims]`.\n      values: A 1-D tensor of any type and shape `[N]`.\n     dense_shape: A 1-D int64 tensor of shape `[ndims]`.\n\n    Returns:\n      A `SparseTensor`\n    \"\"\"\n    with op_scope([indices, values, shape], None, \"SparseTensor\"):\n      indices = convert_to_tensor(indices, name=\"indices\")\n      values = convert_to_tensor(values, name=\"values\")\n      shape = convert_to_tensor(shape, name=\"shape\")\n    self._indices = indices\n    self._values = values\n    self._shape = shape\n\n    indices_shape = indices.get_shape().with_rank(2)\n    values_shape = values.get_shape().with_rank(1)\n    shape_shape = shape.get_shape().with_rank(1)\n\n    # Assert number of rows in indices match the number of elements in values.\n    indices_shape[0].merge_with(values_shape[0])\n    # Assert number of columns in indices matches the number of elements in\n    # shape.\n    indices_shape[1].merge_with(shape_shape[0])\n\n  @property\n  def indices(self):\n    \"\"\"The indices of non-zero values in the represented dense tensor.\n\n    Returns:\n      A 2-D Tensor of int64 with shape `[N, ndims]`, where `N` is the\n        number of non-zero values in the tensor, and `ndims` is the rank.\n    \"\"\"\n    return self._indices\n\n  @property\n  def values(self):\n    \"\"\"The non-zero values in the represented dense tensor.\n\n    Returns:\n      A 1-D Tensor of any data type.\n    \"\"\"\n    return self._values\n\n  @property\n  def dtype(self):\n    \"\"\"The `DType` of elements in this tensor.\"\"\"\n    return self._values.dtype\n\n  @property\n  def shape(self):\n    \"\"\"A 1-D Tensor of int64 representing the shape of the dense tensor.\"\"\"\n    return self._shape\n\n  @property\n  def graph(self):\n    \"\"\"The `Graph` that contains the index, value, and shape tensors.\"\"\"\n    return self._indices.graph\n\n  def __str__(self):\n    return \"SparseTensor(indices=%s, values=%s, shape=%s)\" % (\n        self._indices, self._values, self._shape)\n\n\nSparseTensorValue = collections.namedtuple(\"SparseTensorValue\",\n                                           [\"indices\", \"values\", \"shape\"])\n\n\ndef _device_string(dev_spec):\n  if isinstance(dev_spec, pydev.Device):\n    return dev_spec.to_string()\n  else:\n    return dev_spec\n\n\ndef _NodeDef(op_type, name, device=None, attrs=None):\n  \"\"\"Create a NodeDef proto.\n\n  Args:\n    op_type: Value for the \"op\" attribute of the NodeDef proto.\n    name: Value for the \"name\" attribute of the NodeDef proto.\n    device: string, device, or function from NodeDef to string.\n      Value for the \"device\" attribute of the NodeDef proto.\n    attrs: optional list for the \"attr\" attribute of the NodeDef proto.\n\n  Returns:\n    A graph_pb2.NodeDef protocol buffer.\n  \"\"\"\n  node_def = graph_pb2.NodeDef()\n  node_def.op = str(op_type)\n  node_def.name = str(name)\n  if attrs is not None:\n    for k, v in six.iteritems(attrs):\n      node_def.attr[k].CopyFrom(v)\n  if device is not None:\n    if callable(device):\n      node_def.device = device(node_def)\n    else:\n      node_def.device = _device_string(device)\n  return node_def\n\n\n# Copied from core/framework/node_def_util.cc\n# TODO(mrry,josh11b): Consolidate this validation in C++ code.\n_VALID_OP_NAME_REGEX = re.compile(\"[A-Za-z0-9.][A-Za-z0-9_.\\\\-/]*\")\n\n\nclass Operation(object):\n  \"\"\"Represents a graph node that performs computation on tensors.\n\n  An `Operation` is a node in a TensorFlow `Graph` that takes zero or\n  more `Tensor` objects as input, and produces zero or more `Tensor`\n  objects as output. Objects of type `Operation` are created by\n  calling a Python op constructor (such as\n  [`tf.matmul()`](../../api_docs/python/math_ops.md#matmul))\n  or [`Graph.create_op()`](../../api_docs/python/framework.md#Graph.create_op).\n\n  For example `c = tf.matmul(a, b)` creates an `Operation` of type\n  \"MatMul\" that takes tensors `a` and `b` as input, and produces `c`\n  as output.\n\n  After the graph has been launched in a session, an `Operation` can\n  be executed by passing it to\n  [`Session.run()`](../../api_docs/python/client.md#Session.run).\n  `op.run()` is a shortcut for calling `tf.get_default_session().run(op)`.\n\n  @@name\n  @@type\n  @@inputs\n  @@control_inputs\n  @@outputs\n  @@device\n  @@graph\n\n  @@run\n\n  @@get_attr\n  @@traceback\n  \"\"\"\n\n  def __init__(self, node_def, g, inputs=None, output_types=None,\n               control_inputs=None, input_types=None, original_op=None,\n               op_def=None):\n    \"\"\"Creates an `Operation`.\n\n    NOTE: This constructor validates the name of the Operation (passed\n    as \"node_def.name\"). Valid Operation names match the following\n    regular expression:\n\n      [A-Za-z0-9.][A-Za-z0-9_.\\\\-/]*\n\n    Args:\n      node_def: graph_pb2.NodeDef.  NodeDef for the Operation.\n        Used for attributes of graph_pb2.NodeDef, typically \"name\",\n        \"op\", and \"device\".  The \"input\" attribute is irrelevant here\n        as it will be computed when generating the model.\n      g: Graph. The parent graph.\n      inputs: list of Tensor objects. The inputs to this Operation.\n      output_types: list of types_pb2.DataType.  List of the types of the\n        Tensors computed by this operation.  The length of this list indicates\n        the number of output endpoints of the Operation.\n      control_inputs: list of operations or tensors from which to have a\n        control dependency.\n      input_types: List of types_pb2.DataType representing the\n        types of the Tensors accepted by the Operation.  By default\n        uses [x.dtype.base_dtype for x in inputs].  Operations that expect\n        reference-typed inputs must specify these explicitly.\n      original_op: Optional. Used to associate the new Operation with an\n        existing Operation (for example, a replica with the op that was\n        replicated).\n      op_def: Optional. The op_def_pb2.OpDef proto that describes the\n        op type that this Operation represents.\n\n    Raises:\n      TypeError: if control inputs are not Operations or Tensors,\n        or if node_def is not a NodeDef,\n        or if g is not a Graph,\n        or if inputs are not Tensors,\n        or if inputs and input_types are incompatible.\n      ValueError: if the node_def name is not valid.\n    \"\"\"\n    if not isinstance(node_def, graph_pb2.NodeDef):\n      raise TypeError(\"node_def needs to be a NodeDef: %s\" % node_def)\n    if node_def.ByteSize() >= (1 << 31) or node_def.ByteSize() < 0:\n      raise ValueError(\n          \"Cannot create an Operation with a NodeDef larger than 2GB.\")\n    if not _VALID_OP_NAME_REGEX.match(node_def.name):\n      raise ValueError(\"'%s' is not a valid node name\" % node_def.name)\n    if not isinstance(g, Graph):\n      raise TypeError(\"g needs to be a Graph: %s\" % g)\n    self._node_def = copy.deepcopy(node_def)\n    self._graph = g\n    if inputs is None:\n      inputs = []\n    self._inputs = inputs\n    for a in self._inputs:\n      if not isinstance(a, Tensor):\n        raise TypeError(\"input needs to be a Tensor: %s\" % a)\n      # Mark that we consume the inputs.\n      a._add_consumer(self)  # pylint: disable=protected-access\n    if output_types is None:\n      output_types = []\n    self._output_types = output_types\n    self._outputs = [Tensor(self, i, output_type)\n                     for i, output_type in enumerate(output_types)]\n    if input_types is None:\n      input_types = [i.dtype.base_dtype for i in self._inputs]\n    else:\n      if not all(x.is_compatible_with(i.dtype)\n                 for i, x in zip(self._inputs, input_types)):\n        raise TypeError(\"Inputs are not compatible with input types\")\n    self._input_types = input_types\n\n    # Build the list of control inputs.\n    self._control_inputs = []\n    if control_inputs:\n      for c in control_inputs:\n        c_op = None\n        if isinstance(c, Operation):\n          c_op = c\n        elif isinstance(c, (Tensor, IndexedSlices)):\n          c_op = c.op\n        else:\n          raise TypeError(\"Control input must be an Operation, \"\n                          \"a Tensor, or IndexedSlices: %s\" % c)\n        self._control_inputs.append(c_op)\n\n    self._original_op = original_op\n    self._op_def = op_def\n    self._traceback = _extract_stack()\n    # Add this op to the current control flow context:\n    self._control_flow_context = g._get_control_flow_context()\n    if g._get_control_flow_context() is not None:\n      g._get_control_flow_context().AddOp(self)\n    # NOTE(keveman): Control flow context's AddOp could be creating new ops and\n    # setting op.inputs[index] = new_op. Thus the new ops' id could be larger\n    # than this op's id even though this op depend on them. Therefore, delaying\n    # assigning id to this op until all ops this could be dependent on are\n    # created.\n    self._id_value = self._graph._next_id()  # pylint: disable=protected-access\n    self._recompute_node_def()\n\n  def values(self):\n    \"\"\"DEPRECATED: Use outputs.\"\"\"\n    return tuple(self.outputs)\n\n  def _get_control_flow_context(self):\n    \"\"\"Returns the current control flow context.\n\n    Returns:\n      A context object.\n    \"\"\"\n    return self._control_flow_context\n\n  @property\n  def name(self):\n    \"\"\"The full name of this operation.\"\"\"\n    return self._node_def.name\n\n  @property\n  def _id(self):\n    \"\"\"The unique integer id of this operation.\"\"\"\n    return self._id_value\n\n  @property\n  def device(self):\n    \"\"\"The name of the device to which this op has been assigned, if any.\n\n    Returns:\n      The string name of the device to which this op has been\n      assigned, or None if it has not been assigned to a device.\n    \"\"\"\n    dev = self._node_def.device\n    return None if not dev else dev\n\n  def _set_device(self, device):\n    \"\"\"Set the device of this operation.\n\n    Args:\n      device: string or device..  The device to set.\n    \"\"\"\n    self._node_def.device = _device_string(device)\n\n  def _add_input(self, tensor, dtype=None):\n    \"\"\"Add a new input to this operation.\n\n    Args:\n      tensor: the Tensor to add as an input.\n      dtype: types.DType: type of the input; defaults to\n        the tensor's dtype.\n\n    Raises:\n      TypeError: if tensor is not a Tensor,\n        or if input tensor type is not convertible to dtype.\n      ValueError: if the Tensor is from a different graph.\n    \"\"\"\n    if not isinstance(tensor, Tensor):\n      raise TypeError(\"tensor must be a Tensor: %s\" % tensor)\n    assert_same_graph([self, tensor])\n    if dtype is None:\n      dtype = tensor.dtype\n    else:\n      dtype = types.as_dtype(dtype)\n      if not dtype.is_compatible_with(tensor.dtype):\n        raise TypeError(\n            \"Cannot convert a tensor of type %s to an input of type %s\"\n            % (tensor.dtype.name, dtype.name))\n    self._inputs.append(tensor)\n    self._input_types.append(dtype)\n    tensor._add_consumer(self)  # pylint: disable=protected-access\n    self._recompute_node_def()\n\n  def _update_input(self, index, tensor, dtype=None):\n    \"\"\"Update the input to this operation at the given index.\n\n    NOTE: This is for TF internal use only. Please don't use it.\n\n    Args:\n      index: the index of the input to update.\n      tensor: the Tensor to be used as the input at the given index.\n      dtype: types.DType: type of the input; defaults to\n        the tensor's dtype.\n\n    Raises:\n      TypeError: if tensor is not a Tensor,\n        or if input tensor type is not convertible to dtype.\n      ValueError: if the Tensor is from a different graph.\n    \"\"\"\n    if not isinstance(tensor, Tensor):\n      raise TypeError(\"tensor must be a Tensor: %s\" % tensor)\n    assert_same_graph([self, tensor])\n    if dtype is None:\n      dtype = tensor.dtype\n    else:\n      dtype = types.as_dtype(dtype)\n      if not dtype.is_compatible_with(tensor.dtype):\n        raise TypeError(\n            \"Cannot convert a tensor of type %s to an input of type %s\"\n            % (tensor.dtype.name, dtype.name))\n\n    self._inputs[index].consumers().remove(self)\n    self._inputs[index] = tensor\n    self._input_types[index] = dtype\n    tensor._add_consumer(self)  # pylint: disable=protected-access\n    self._recompute_node_def()\n\n  def _add_control_input(self, op):\n    \"\"\"Add a new control input to this operation.\n\n    Args:\n      op: the Operation to add as control input.\n\n    Raises:\n      TypeError: if op is not an Operation.\n      ValueError: if op is from a different graph.\n    \"\"\"\n    if not isinstance(op, Operation):\n      raise TypeError(\"op must be an Operation: %s\" % op)\n    assert_same_graph([self, op])\n    self._control_inputs.append(op)\n    self._recompute_node_def()\n\n  # Methods below are used when building the NodeDef and Graph proto.\n  def _recompute_node_def(self):\n    del self._node_def.input[:]\n    self._node_def.input.extend([t._as_node_def_input() for t in self._inputs])\n    if self._control_inputs:\n      self._node_def.input.extend([\"^%s\" % op.name for op in\n                                   self._control_inputs])\n\n  def __str__(self):\n    return str(self._node_def)\n\n  @property\n  def outputs(self):\n    \"\"\"The list of `Tensor` objects representing the outputs of this op.\"\"\"\n    return self._outputs\n\n# pylint: disable=protected-access\n  class _InputList(object):\n    \"\"\"Immutable input list wrapper.\"\"\"\n\n    def __init__(self, op):\n      self._op = op\n\n    def __iter__(self):\n      return iter(self._op._inputs)\n\n    def __len__(self):\n      return len(self._op._inputs)\n\n    def __bool__(self):\n      return bool(self._op._inputs)\n\n    # Python 3 wants __bool__, Python 2.7 wants __nonzero__\n    __nonzero__ = __bool__\n\n    def __getitem__(self, i):\n      return self._op._inputs[i]\n# pylint: enable=protected-access\n\n  @property\n  def inputs(self):\n    \"\"\"The list of `Tensor` objects representing the data inputs of this op.\"\"\"\n    return Operation._InputList(self)\n\n  @property\n  def _input_dtypes(self):\n    return self._input_types\n\n  @property\n  def control_inputs(self):\n    \"\"\"The `Operation` objects on which this op has a control dependency.\n\n    Before this op is executed, TensorFlow will ensure that the\n    operations in `self.control_inputs` have finished executing. This\n    mechanism can be used to run ops sequentially for performance\n    reasons, or to ensure that the side effects of an op are observed\n    in the correct order.\n\n    Returns:\n      A list of `Operation` objects.\n\n    \"\"\"\n    return self._control_inputs\n\n  @property\n  def type(self):\n    \"\"\"The type of the op (e.g. `\"MatMul\"`).\"\"\"\n    return self._node_def.op\n\n  @property\n  def graph(self):\n    \"\"\"The `Graph` that contains this operation.\"\"\"\n    return self._graph\n\n  @property\n  def node_def(self):\n    \"\"\"Returns a serialized `NodeDef` representation of this operation.\n\n    Returns:\n      A\n      [`NodeDef`](https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/core/framework/graph.proto)\n      protocol buffer.\n    \"\"\"\n    return self._node_def\n\n  @property\n  def op_def(self):\n    \"\"\"Returns the `OpDef` proto that represents the type of this op.\n\n    Returns:\n      An\n      [`OpDef`](https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/core/framework/op_def.proto)\n      protocol buffer.\n    \"\"\"\n    return self._op_def\n\n  @property\n  def traceback(self):\n    \"\"\"Returns the call stack from when this operation was constructed.\"\"\"\n    return _convert_stack(self._traceback)\n\n  def get_attr(self, name):\n    \"\"\"Returns the value of the attr of this op with the given `name`.\n\n    Args:\n      name: The name of the attr to fetch.\n\n    Returns:\n      The value of the attr, as a Python object.\n\n    Raises:\n      ValueError: If this op does not have an attr with the given `name`.\n    \"\"\"\n    fields = [\"s\", \"i\", \"f\", \"b\", \"type\", \"shape\", \"tensor\"]\n    if name not in self._node_def.attr:\n      raise ValueError(\"No attr named '\" + name + \"' in \" +\n                       str(self._node_def))\n    x = self._node_def.attr[name]\n    # Treat an empty oneof value as an empty list.\n    if not x.WhichOneof(\"value\"):\n      return []\n    if x.HasField(\"list\"):\n      for f in fields:\n        if getattr(x.list, f):\n          return list(getattr(x.list, f))\n      return []\n    else:\n      for f in fields:\n        if x.HasField(f):\n          return getattr(x, f)\n      assert False, \"Unsupported field type in \" + str(x)\n\n  def run(self, feed_dict=None, session=None):\n    \"\"\"Runs this operation in a `Session`.\n\n    Calling this method will execute all preceding operations that\n    produce the inputs needed for this operation.\n\n    *N.B.* Before invoking `Operation.run()`, its graph must have been\n    launched in a session, and either a default session must be\n    available, or `session` must be specified explicitly.\n\n    Args:\n      feed_dict: A dictionary that maps `Tensor` objects to feed values.\n        See [`Session.run()`](../../api_docs/python/client.md#Session.run)\n        for a description of the valid feed values.\n      session: (Optional.) The `Session` to be used to run to this operation. If\n        none, the default session will be used.\n    \"\"\"\n    _run_using_default_session(self, feed_dict, self.graph, session)\n\n\n_gradient_registry = registry.Registry(\"gradient\")\n\n\nclass RegisterGradient(object):\n  \"\"\"A decorator for registering the gradient function for an op type.\n\n  This decorator is only used when defining a new op type. For an op\n  with `m` inputs and `n` inputs, the gradient function is a function\n  that takes the original `Operation` and `n` `Tensor` objects\n  (representing the gradients with respect to each output of the op),\n  and returns `m` `Tensor` objects (representing the partial gradients\n  with respect to each input of the op).\n\n  For example, assuming that operations of type `\"Sub\"` take two\n  inputs `x` and `y`, and return a single output `x - y`, the\n  following gradient function would be registered:\n\n  ```python\n  @tf.RegisterGradient(\"Sub\")\n  def _sub_grad(unused_op, grad):\n    return grad, tf.Neg(grad)\n  ```\n\n  The decorator argument `op_type` is the string type of an\n  operation. This corresponds to the `OpDef.name` field for the proto\n  that defines the operation.\n\n  @@__init__\n  \"\"\"\n\n  def __init__(self, op_type):\n    \"\"\"Creates a new decorator with `op_type` as the Operation type.\n\n    Args:\n      op_type: The string type of an operation. This corresponds to the\n        `OpDef.name` field for the proto that defines the operation.\n    \"\"\"\n    if not isinstance(op_type, six.string_types):\n      raise TypeError(\"op_type must be a string\")\n    self._op_type = op_type\n\n  def __call__(self, f):\n    \"\"\"Registers the function `f` as gradient function for `op_type`.\"\"\"\n    _gradient_registry.register(f, self._op_type)\n    return f\n\n\ndef NoGradient(op_type):\n  \"\"\"Specifies that ops of type `op_type` do not have a defined gradient.\n\n  This function is only used when defining a new op type. It may be\n  used for ops such as `tf.size()` that are not differentiable.  For\n  example:\n\n  ```python\n  tf.NoGradient(\"Size\")\n  ```\n\n  Args:\n    op_type: The string type of an operation. This corresponds to the\n      `OpDef.name` field for the proto that defines the operation.\n\n  Raises:\n    TypeError: If `op_type` is not a string.\n\n  \"\"\"\n  if not isinstance(op_type, six.string_types):\n    raise TypeError(\"op_type must be a string\")\n  _gradient_registry.register(None, op_type)\n\n\ndef get_gradient_function(op):\n  \"\"\"Returns the function that computes gradients for \"op\".\"\"\"\n  if not op.inputs: return None\n  try:\n    op_type = op.get_attr(\"_gradient_op_type\")\n  except ValueError:\n    op_type = op.type\n  return _gradient_registry.lookup(op_type)\n\n\n_shape_registry = registry.Registry(\"shape functions\")\n_default_shape_function_registry = registry.Registry(\"default shape functions\")\n\nclass RegisterShape(object):\n  \"\"\"A decorator for registering the shape function for an op type.\n\n  This decorator is only used when defining a new op type. A shape\n  function is a function from an `Operation` object to a list of\n  `TensorShape` objects, with one `TensorShape` for each output of the\n  operation.\n\n  For example, assuming that operations of type `\"Sub\"` take two\n  inputs `x` and `y`, and return a single output `x - y`, all with the\n  same shape, the following shape function would be registered:\n\n  ```python\n  @tf.RegisterShape(\"Sub\")\n  def _sub_shape(op):\n    return [op.inputs[0].get_shape().merge_with(op.inputs[1].get_shape())]\n  ```\n\n  The decorator argument `op_type` is the string type of an\n  operation. This corresponds to the `OpDef.name` field for the proto\n  that defines the operation.\n\n  \"\"\"\n\n  def __init__(self, op_type):\n    \"\"\"Saves the \"op_type\" as the Operation type.\"\"\"\n    if not isinstance(op_type, six.string_types):\n      raise TypeError(\"op_type must be a string\")\n    self._op_type = op_type\n\n  def __call__(self, f):\n    \"\"\"Registers \"f\" as the shape function for \"op_type\".\"\"\"\n    if f is None:\n      # None is a special \"weak\" value that provides a default shape function,\n      # and can be overridden by a non-None registration.\n      try:\n        _default_shape_function_registry.register(_no_shape_function,\n                                                  self._op_type)\n      except KeyError:\n        # Ignore duplicate registrations of the weak value. This can\n        # occur if the op library input to wrapper generation\n        # inadvertently links in one or more of the standard op\n        # libraries.\n        pass\n    else:\n      _shape_registry.register(f, self._op_type)\n    return f\n\n\ndef _no_shape_function(op):\n  return [tensor_shape.unknown_shape() for _ in op.outputs]\n\n\ndef set_shapes_for_outputs(op):\n  \"\"\"Uses the registered shape functions to set the shapes for op's outputs.\"\"\"\n  try:\n    shape_func = _shape_registry.lookup(op.type)\n  except LookupError:\n    try:\n      shape_func = _default_shape_function_registry.lookup(op.type)\n    except LookupError:\n      raise RuntimeError(\"No shape function registered for standard op: %s\"\n                         % op.type)\n  shapes = shape_func(op)\n  if len(op.outputs) != len(shapes):\n    raise RuntimeError(\n        \"Shape function for op %s returned %g shapes but expecting %g\" %\n        (op, len(op.outputs), len(shapes)))\n  for output, s in zip(op.outputs, shapes):\n    output.set_shape(s)\n\n\nclass Graph(object):\n  \"\"\"A TensorFlow computation, represented as a dataflow graph.\n\n  A `Graph` contains a set of\n  [`Operation`](../../api_docs/python/framework.md#Operation) objects,\n  which represent units of computation; and\n  [`Tensor`](../../api_docs/python/framework.md#Tensor) objects, which represent\n  the units of data that flow between operations.\n\n  A default `Graph` is always registered, and accessible by calling\n  [`tf.get_default_graph()`](../../api_docs/python/framework.md#get_default_graph).\n  To add an operation to the default graph, simply call one of the functions\n  that defines a new `Operation`:\n\n  ```\n  c = tf.constant(4.0)\n  assert c.graph is tf.get_default_graph()\n  ```\n\n  Another typical usage involves the\n  [`Graph.as_default()`](../../api_docs/python/framework.md#Graph.as_default)\n  context manager, which overrides the current default graph for the\n  lifetime of the context:\n\n  ```python\n  g = tf.Graph()\n  with g.as_default():\n    # Define operations and tensors in `g`.\n    c = tf.constant(30.0)\n    assert c.graph is g\n  ```\n\n  Important note: This class *is not* thread-safe for graph construction. All\n  operations should be created from a single thread, or external\n  synchronization must be provided. Unless otherwise specified, all methods\n  are not thread-safe.\n\n  @@__init__\n  @@as_default\n  @@as_graph_def\n  @@finalize\n  @@finalized\n\n  @@control_dependencies\n  @@device\n  @@name_scope\n\n  A `Graph` instance supports an arbitrary number of \"collections\"\n  that are identified by name. For convenience when building a large\n  graph, collections can store groups of related objects: for\n  example, the `tf.Variable` uses a collection (named\n  [`tf.GraphKeys.VARIABLES`](../../api_docs/python/framework.md#GraphKeys)) for\n  all variables that are created during the construction of a graph. The caller\n  may define additional collections by specifying a new name.\n\n  @@add_to_collection\n  @@get_collection\n\n  @@as_graph_element\n  @@get_operation_by_name\n  @@get_tensor_by_name\n  @@get_operations\n\n  @@get_default_device\n  @@seed\n  @@unique_name\n  @@version\n\n  @@create_op\n  @@gradient_override_map\n  \"\"\"\n\n  def __init__(self):\n    \"\"\"Creates a new, empty Graph.\"\"\"\n    self._nodes_by_id = dict()\n    self._next_node_id = [dict()]\n    self._next_id_counter = 0\n    self._nodes_by_name = dict()\n    # Current name stack: a pair of uniquified names and plain names.\n    self._name_stack = (\"\", \"\")\n    # Maps a name used in the graph to the next id to use for that name.\n    self._names_in_use = {}\n    # Default device applied to new ops.\n    self._default_device = None\n    # Functions that will be applied to choose a device if none is specified.\n    self._device_function_stack = []\n    # Default original_op applied to new ops.\n    self._default_original_op = None\n    # Current control flow context. It could be either CondContext or\n    # WhileContext defined in ops/control_flow_ops.py\n    self._control_flow_context = None\n    # A new node will depend of the union of all of the nodes in the stack.\n    self._control_dependencies_stack = []\n    # Arbritrary collections of objects.\n    self._collections = {}\n    # The graph-level random seed\n    self._seed = None\n    # A map from op type to the kernel label that should be used.\n    self._op_to_kernel_label_map = {}\n    # A map from op type to an alternative op type that should be used when\n    # computing gradients.\n    self._gradient_override_map = {}\n    # True if the graph is considered \"finalized\".  In that case no\n    # new operations can be added.\n    self._finalized = False\n    # Functions defined in the graph\n    self._functions = collections.OrderedDict()\n\n  def _check_not_finalized(self):\n    \"\"\"Check if the graph is finalized.\n\n    Raises:\n      RuntimeError: If the graph finalized.\n    \"\"\"\n    if self._finalized:\n      raise RuntimeError(\"Graph is finalized and cannot be modified.\")\n\n  def _add_op(self, op):\n    \"\"\"Adds 'op' to the graph.\n\n    Args:\n      op: the Operator or Tensor to add.\n\n    Raises:\n      TypeError: if op is not an Operation or Tensor.\n      ValueError: if the op.name or op._id are already used.\n    \"\"\"\n    self._check_not_finalized()\n    if not isinstance(op, (Tensor, Operation)):\n      raise TypeError(\"op must be a Tensor or Operation: %s\" % op)\n\n    if op._id in self._nodes_by_id:\n      raise ValueError(\"cannot add an op with id %d as it already \"\n                       \"exists in the graph\" % op._id)\n    if op.name in self._nodes_by_name:\n      raise ValueError(\"cannot add op with name %s as that name \"\n                       \"is already used\" % op.name)\n    self._nodes_by_id[op._id] = op\n    self._nodes_by_name[op.name] = op\n\n  @property\n  def version(self):\n    \"\"\"Returns a version number that increases as ops are added to the graph.\"\"\"\n    return self._next_id_counter\n\n  @property\n  def seed(self):\n    return self._seed\n\n  @seed.setter\n  def seed(self, seed):\n    self._seed = seed\n\n  @property\n  def finalized(self):\n    \"\"\"True if this graph has been finalized.\"\"\"\n    return self._finalized\n\n  def finalize(self):\n    \"\"\"Finalizes this graph, making it read-only.\n\n    After calling `g.finalize()`, no new operations can be added to\n    `g`.  This method is used to ensure that no operations are added\n    to a graph when it is shared between multiple threads, for example\n    when using a [`QueueRunner`](../../api_docs/python/train.md#QueueRunner).\n    \"\"\"\n    self._finalized = True\n\n  def _get_control_flow_context(self):\n    \"\"\"Returns the current control flow context.\n\n    Returns:\n      A context object.\n    \"\"\"\n    return self._control_flow_context\n\n  def _set_control_flow_context(self, context):\n    \"\"\"Sets the current control flow context.\n\n    Args:\n      context: a context object.\n    \"\"\"\n    self._control_flow_context = context\n\n  def as_graph_def(self, from_version=None):\n    \"\"\"Returns a serialized `GraphDef` representation of this graph.\n\n    The serialized `GraphDef` can be imported into another `Graph`\n    (using [`import_graph_def()`](#import_graph_def)) or used with the\n    [C++ Session API](../../api_docs/cc/index.md).\n\n    This method is thread-safe.\n\n    Args:\n      from_version: Optional.  If this is set, returns a `GraphDef`\n        containing only the nodes that were added to this graph since\n        its `version` property had the given value.\n\n    Returns:\n      A [`GraphDef`](https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/core/framework/graph.proto)\n      protocol buffer.\n\n    Raises:\n      ValueError: If the graph_def would be too large.\n    \"\"\"\n    graph = graph_pb2.GraphDef()\n    bytesize = 0\n    for op_id in sorted(self._nodes_by_id):\n      op = self._nodes_by_id[op_id]\n      if from_version is None or op_id > from_version:\n        graph.node.extend([op.node_def])\n        bytesize += op.node_def.ByteSize()\n        if bytesize >= (1 << 31) or bytesize < 0:\n          raise ValueError(\"GraphDef cannot be larger than 2GB.\")\n    if self._functions:\n      for f in self._functions.values():\n        bytesize += f.ByteSize()\n        if bytesize >= (1 << 31) or bytesize < 0:\n          raise ValueError(\"GraphDef cannot be larger than 2GB.\")\n      graph.library.function.extend(self._functions.values())\n    return graph\n\n  def _add_function(self, function_def):\n    \"\"\"Adds a function to the graph.\n\n    The function is specified as a [`FunctionDef`]\n    (https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/core/framework/function.proto)\n    protocol buffer.\n\n    After the function has been added, you can call to the function by\n    passing the function name in place of an op name to\n    `Graph.create_op()`.\n\n    Args:\n      function_def: A `FunctionDef` protocol buffer.\n    \"\"\"\n    previous_def = self._functions.get(function_def.signature.name, None)\n    if previous_def:\n      if previous_def != function_def:\n        raise ValueError(\"Another function is already defined with that name\")\n      else:\n        # No need to add again.\n        return\n    self._functions[function_def.signature.name] = function_def\n\n  # Helper functions to create operations.\n  def create_op(self, op_type, inputs, dtypes,\n                input_types=None, name=None, attrs=None, op_def=None,\n                compute_shapes=True):\n    \"\"\"Creates an `Operation` in this graph.\n\n    This is a low-level interface for creating an `Operation`. Most\n    programs will not call this method directly, and instead use the\n    Python op constructors, such as `tf.constant()`, which add ops to\n    the default graph.\n\n    Args:\n      op_type: The `Operation` type to create. This corresponds to the\n        `OpDef.name` field for the proto that defines the operation.\n      inputs: A list of `Tensor` objects that will be inputs to the `Operation`.\n      dtypes: A list of `DType` objects that will be the types of the tensors\n        that the operation produces.\n      input_types: (Optional.) A list of `DType`s that will be the types of\n        the tensors that the operation consumes. By default, uses the base\n        `DType` of each input in `inputs`. Operations that expect\n        reference-typed inputs must specify `input_types` explicitly.\n      name: (Optional.) A string name for the operation. If not specified, a\n        name is generated based on `op_type`.\n      attrs: (Optional.) A list of `AttrValue` protos for the `attr` field of\n        the `NodeDef` proto that will represent the operation.\n      op_def: (Optional.) The `OpDef` proto that describes the `op_type` that\n        the operation will have.\n      compute_shapes: (Optional.) If True, shape inference will be performed\n        to compute the shapes of the outputs.\n\n    Raises:\n      TypeError: if any of the inputs is not a `Tensor`.\n\n    Returns:\n      An `Operation` object.\n\n    \"\"\"\n    self._check_not_finalized()\n    for idx, a in enumerate(inputs):\n      if not isinstance(a, Tensor):\n        raise TypeError(\"Input #%d is not a tensor: %s\" % (idx, a))\n    if name is None:\n      name = op_type\n    # If a names ends with a '/' it is a \"name scope\" and we use it as-is,\n    # after removing the trailing '/'.\n    if name and name[-1] == \"/\":\n      name = name[:-1]\n    else:\n      name = self.unique_name(name)\n\n    node_def = _NodeDef(\n        op_type, name, device=self._default_device or None, attrs=attrs)\n\n    # Apply a kernel label if one has been specified for this op_type.\n    try:\n      kernel_label = self._op_to_kernel_label_map[op_type]\n      node_def.attr[\"_kernel\"].CopyFrom(\n          attr_value_pb2.AttrValue(s=kernel_label))\n    except KeyError:\n      pass\n\n    # Apply the overriding op_type for gradients if one has been\n    # specified for this op_type.\n    try:\n      mapped_op_type = self._gradient_override_map[op_type]\n      node_def.attr[\"_gradient_op_type\"].CopyFrom(\n          attr_value_pb2.AttrValue(s=mapped_op_type))\n    except KeyError:\n      pass\n\n    control_inputs = self._control_dependencies_for_inputs(inputs)\n    ret = Operation(node_def, self, inputs=inputs, output_types=dtypes,\n                    control_inputs=control_inputs, input_types=input_types,\n                    original_op=self._default_original_op, op_def=op_def)\n    if compute_shapes:\n      set_shapes_for_outputs(ret)\n    self._add_op(ret)\n    self._record_op_seen_by_control_dependencies(ret)\n    # Apply any device functions in reverse order, so that the most recently\n    # pushed function has the first chance to apply a device to the op.\n    # We apply here because the result can depend on the Operation's\n    # signature, which is computed in the Operation constructor.\n    for device_function in reversed(self._device_function_stack):\n      ret._set_device(device_function(ret))\n    return ret\n\n  def as_graph_element(self, obj, allow_tensor=True, allow_operation=True):\n    \"\"\"Returns the object referred to by `obj`, as an `Operation` or `Tensor`.\n\n    This function validates that `obj` represents an element of this\n    graph, and gives an informative error message if it is not.\n\n    This function is the canonical way to get/validate an object of\n    one of the allowed types from an external argument reference in the\n    Session API.\n\n    This method may be called concurrently from multiple threads.\n\n    Args:\n      obj: A `Tensor`, an `Operation`, or the name of a tensor or operation.\n        Can also be any object with an `_as_graph_element()` method that returns\n        a value of one of these types.\n      allow_tensor: If true, `obj` may refer to a `Tensor`.\n      allow_operation: If true, `obj` may refer to an `Operation`.\n\n    Returns:\n      The `Tensor` or `Operation` in the Graph corresponding to `obj`.\n\n    Raises:\n      TypeError: If `obj` is not a type we support attempting to convert\n        to types.\n      ValueError: If `obj` is of an appropriate type but invalid. For\n        example, an invalid string.\n      KeyError: If `obj` is not an object in the graph.\n    \"\"\"\n\n    # The vast majority of this function is figuring\n    # out what an API user might be doing wrong, so\n    # that we can give helpful error messages.\n    #\n    # Ideally, it would be nice to split it up, but we\n    # need context to generate nice error messages.\n\n    if allow_tensor and allow_operation:\n      types_str = \"Tensor or Operation\"\n    elif allow_tensor:\n      types_str = \"Tensor\"\n    elif allow_operation:\n      types_str = \"Operation\"\n    else:\n      raise ValueError(\"allow_tensor and allow_operation can't both be False.\")\n\n    conv_fn = getattr(obj, \"_as_graph_element\", None)\n    if conv_fn and callable(conv_fn):\n      obj = conv_fn()\n\n    # If obj appears to be a name...\n    if isinstance(obj, six.string_types):\n      name = obj\n\n      if \":\" in name and allow_tensor:\n        # Looks like a Tensor name and can be a Tensor.\n        try:\n          op_name, out_n = name.split(\":\")\n          out_n = int(out_n)\n        except:\n          raise ValueError(\"The name %s looks a like a Tensor name, but is \"\n                           \"not a valid one. Tensor names must be of the \"\n                           \"form \\\"<op_name>:<output_index>\\\".\" % repr(name))\n        if op_name in self._nodes_by_name:\n          op = self._nodes_by_name[op_name]\n        else:\n          raise KeyError(\"The name %s refers to a Tensor which does not \"\n                         \"exist. The operation, %s, does not exist in the \"\n                         \"graph.\" % (repr(name), repr(op_name)))\n        try:\n          return op.outputs[out_n]\n        except:\n          raise KeyError(\"The name %s refers to a Tensor which does not \"\n                         \"exist. The operation, %s, exists but only has \"\n                         \"%s outputs.\"\n                         % (repr(name), repr(op_name), len(op.outputs)))\n\n      elif \":\" in name and not allow_tensor:\n        # Looks like a Tensor name but can't be a Tensor.\n        raise ValueError(\"Name %s appears to refer to a Tensor, not a %s.\"\n                         % (repr(name), types_str))\n\n      elif \":\" not in name and allow_operation:\n        # Looks like an Operation name and can be an Operation.\n        if name not in self._nodes_by_name:\n          raise KeyError(\"The name %s refers to an Operation not in the \"\n                         \"graph.\" % repr(name))\n        return self._nodes_by_name[name]\n\n      elif \":\" not in name and not allow_operation:\n        # Looks like an Operation name but can't be an Operation.\n        if name in self._nodes_by_name:\n          # Yep, it's an Operation name\n          err_msg = (\"The name %s refers to an Operation, not a %s.\"\n                     % (repr(name), types_str))\n        else:\n          err_msg = (\"The name %s looks like an (invalid) Operation name, \"\n                     \"not a %s.\" % (repr(name), types_str))\n        err_msg += (\" Tensor names must be of the form \"\n                    \"\\\"<op_name>:<output_index>\\\".\")\n        raise ValueError(err_msg)\n\n    elif isinstance(obj, Tensor) and allow_tensor:\n      # Actually obj is just the object it's referring to.\n      if obj.graph is not self:\n        raise ValueError(\"Tensor %s is not an element of this graph.\" % obj)\n      return obj\n    elif isinstance(obj, Operation) and allow_operation:\n      # Actually obj is just the object it's referring to.\n      if obj.graph is not self:\n        raise ValueError(\"Operation %s is not an element of this graph.\" % obj)\n      return obj\n    else:\n      # We give up!\n      raise TypeError(\"Can not convert a %s into a %s.\"\n                      % (type(obj).__name__, types_str))\n\n  def get_operations(self):\n    \"\"\"Return the list of operations in the graph.\n\n    You can modify the operations in place, but modifications\n    to the list such as inserts/delete have no effect on the\n    list of operations known to the graph.\n\n    This method may be called concurrently from multiple threads.\n\n    Returns:\n      A list of Operations.\n    \"\"\"\n    return list(self._nodes_by_id.values())\n  def get_operation_by_name(self, name):\n    \"\"\"Returns the `Operation` with the given `name`.\n\n    This method may be called concurrently from multiple threads.\n\n    Args:\n      name: The name of the `Operation` to return.\n\n    Returns:\n      The `Operation` with the given `name`.\n\n    Raises:\n      TypeError: If `name` is not a string.\n      KeyError: If `name` does not correspond to an operation in this graph.\n    \"\"\"\n\n    if not isinstance(name, six.string_types):\n      raise TypeError(\"Operation names are strings (or similar), not %s.\"\n                      % type(name).__name__)\n    return self.as_graph_element(name, allow_tensor=False, allow_operation=True)\n\n  def get_tensor_by_name(self, name):\n    \"\"\"Returns the `Tensor` with the given `name`.\n\n    This method may be called concurrently from multiple threads.\n\n    Args:\n      name: The name of the `Tensor` to return.\n\n    Returns:\n      The `Tensor` with the given `name`.\n\n    Raises:\n      TypeError: If `name` is not a string.\n      KeyError: If `name` does not correspond to a tensor in this graph.\n    \"\"\"\n    # Names should be strings.\n    if not isinstance(name, six.string_types):\n      raise TypeError(\"Tensor names are strings (or similar), not %s.\"\n                      % type(name).__name__)\n    return self.as_graph_element(name, allow_tensor=True, allow_operation=False)\n\n  def _next_id(self):\n    \"\"\"Id for next Operation instance. Also increments the internal id.\"\"\"\n    self._check_not_finalized()\n    self._next_id_counter += 1\n    return self._next_id_counter\n\n  @property\n  def _last_id(self):\n    return self._next_id_counter\n\n  def as_default(self):\n    \"\"\"Returns a context manager that makes this `Graph` the default graph.\n\n    This method should be used if you want to create multiple graphs\n    in the same process. For convenience, a global default graph is\n    provided, and all ops will be added to this graph if you do not\n    create a new graph explicitly. Use this method the `with` keyword\n    to specify that ops created within the scope of a block should be\n    added to this graph.\n\n    The default graph is a property of the current thread. If you\n    create a new thread, and wish to use the default graph in that\n    thread, you must explicitly add a `with g.as_default():` in that\n    thread's function.\n\n    The following code examples are equivalent:\n\n    ```python\n    # 1. Using Graph.as_default():\n    g = tf.Graph()\n    with g.as_default():\n      c = tf.constant(5.0)\n      assert c.graph is g\n\n    # 2. Constructing and making default:\n    with tf.Graph().as_default() as g:\n      c = tf.constant(5.0)\n      assert c.graph is g\n    ```\n\n    Returns:\n      A context manager for using this graph as the default graph.\n    \"\"\"\n    return _default_graph_stack.get_controller(self)\n\n  def add_to_collection(self, name, value):\n    \"\"\"Stores `value` in the collection with the given `name`.\n\n    Args:\n      name: The key for the collection. For example, the `GraphKeys` class\n        contains many standard names for collections.\n      value: The value to add to the collection.\n    \"\"\"\n    self._check_not_finalized()\n    if name not in self._collections:\n      self._collections[name] = [value]\n    else:\n      self._collections[name].append(value)\n\n  def get_collection(self, name, scope=None):\n    \"\"\"Returns a list of values in the collection with the given `name`.\n\n    Args:\n      key: The key for the collection. For example, the `GraphKeys` class\n        contains many standard names for collections.\n      scope: (Optional.) If supplied, the resulting list is filtered to include\n        only items whose name begins with this string.\n\n    Returns:\n      The list of values in the collection with the given `name`, or\n      an empty list if no value has been added to that collection. The\n      list contains the values in the order under which they were\n      collected.\n    \"\"\"\n    if scope is None:\n      return self._collections.get(name, list())\n    else:\n      c = []\n      for item in self._collections.get(name, list()):\n        if hasattr(item, 'name') and item.name.startswith(scope):\n          c.append(item)\n      return c\n\n  @contextlib.contextmanager\n  def _original_op(self, op):\n    \"\"\"Python 'with' handler to help annotate ops with their originator.\n\n    An op may have an 'original_op' property that indicates the op on which\n    it was based. For example a replica op is based on the op that was\n    replicated and a gradient op is based on the op that was differentiated.\n\n    All ops created in the scope of this 'with' handler will have\n    the given 'op' as their original op.\n\n    Args:\n      op: The Operation that all ops created in this scope will have as their\n        original op.\n\n    Yields:\n      Nothing.\n    \"\"\"\n    old_original_op = self._default_original_op\n    try:\n      self._default_original_op = op\n      yield\n    finally:\n      self._default_original_op = old_original_op\n\n  # pylint: disable=g-doc-return-or-yield\n  @contextlib.contextmanager\n  def name_scope(self, name):\n    \"\"\"Returns a context manager that creates hierarchical names for operations.\n\n    A graph maintains a stack of name scopes. A `with name_scope(...):`\n    statement pushes a new name onto the stack for the lifetime of the context.\n\n    The `name` argument will be interpreted as follows:\n\n    * A string (not ending with '/') will create a new name scope, in which\n      `name` is appended to the prefix of all operations created in the\n      context. If `name` has been used before, it will be made unique by\n      calling `self.unique_name(name)`.\n    * A scope previously captured from a `with g.name_scope(...) as\n      scope:` statement will be treated as an \"absolute\" name scope, which\n      makes it possible to re-enter existing scopes.\n    * A value of `None` or the empty string will reset the current name scope\n      to the top-level (empty) name scope.\n\n    For example:\n\n    ```python\n    with tf.Graph().as_default() as g:\n      c = tf.constant(5.0, name=\"c\")\n      assert c_1.name == \"c\"\n      c_1 = tf.constant(6.0, name=\"c\")\n      assert c_1.name == \"c_1\"\n\n      # Creates a scope called \"nested\"\n      with g.name_scope(\"nested\") as scope:\n        nested_c = tf.constant(10.0, name=\"c\")\n        assert nested_c.name == \"nested/c\"\n\n        # Creates a nested scope called \"inner\".\n        with g.name_scope(\"inner\"):\n          nested_inner_c = tf.constant(20.0, name=\"c\")\n          assert nested_inner_c.name == \"nested/inner/c\"\n\n        # Create a nested scope called \"inner_1\".\n        with g.name_scope(\"inner\"):\n          nested_inner_1_c = tf.constant(30.0, name=\"c\")\n          assert nested_inner_1_c.name == \"nested/inner_1/c\"\n\n          # Treats `scope` as an absolute name scope, and\n          # switches to the \"nested/\" scope.\n          with g.name_scope(scope):\n            nested_d = tf.constant(40.0, name=\"d\")\n            assert nested_d.name == \"nested/d\"\n\n            with g.name_scope(\"\"):\n              e = tf.constant(50.0, name=\"e\")\n              assert e.name == \"e\"\n    ```\n\n    The name of the scope itself can be captured by `with\n    g.name_scope(...) as scope:`, which stores the name of the scope\n    in the variable `scope`. This value can be used to name an\n    operation that represents the overall result of executing the ops\n    in a scope. For example:\n\n    ```python\n    inputs = tf.constant(...)\n    with g.name_scope('my_layer') as scope:\n      weights = tf.Variable(..., name=\"weights\")\n      biases = tf.Variable(..., name=\"biases\")\n      affine = tf.matmul(inputs, weights) + biases\n      output = tf.nn.relu(affine, name=scope)\n    ```\n\n\n    Args:\n      name: A name for the scope.\n\n    Returns:\n      A context manager that installs `name` as a new name scope.\n    \"\"\"\n    try:\n      old_stack = self._name_stack\n      if not name:  # Both for name=None nad name=\"\" we re-set to empty scope.\n        new_stack = (None, None)\n      elif name and name[-1] == \"/\":\n        new_stack = (name[:-1], name[:-1])\n      else:\n        new_stack = (self.unique_name(name), self._plain_name(name))\n      self._name_stack = new_stack\n      yield \"\" if new_stack[0] is None else new_stack[0] + \"/\"\n    finally:\n      self._name_stack = old_stack\n  # pylint: enable=g-doc-return-or-yield\n\n  def unique_name(self, name):\n    \"\"\"Return a unique Operation name for \"name\".\n\n    Note: You rarely need to call unique_name() directly.  Most of the time you\n    just need to create \"with g.name_scope()\" blocks to generate structured\n    names.\n\n    `unique_name` is used to generate structured names, separated by \"/\",\n    to help identify Operations when debugging a Graph.  Operation names\n    are displayed in error messages reported by the TensorFlow runtime,\n    and in various visualization tools such as TensorBoard.\n\n    Args:\n      name: The name for an `Operation`.\n\n    Returns:\n      A string to be passed to `create_op()` that will be used\n      to name the operation being created.\n    \"\"\"\n    if self._name_stack[0]:\n      name = self._name_stack[0] + \"/\" + name\n    i = self._names_in_use.get(name, 0)\n    # Increment the number for \"name\".\n    self._names_in_use[name] = i + 1\n    if i > 0:\n      base_name = name\n      # Make sure the composed name is not already used.\n      while name in self._names_in_use:\n        name = \"%s_%d\" % (base_name, i)\n        i += 1\n      # Mark the composed name as used in case someone wants\n      # to call unique_name(\"name_1\").\n      self._names_in_use[name] = 1\n    return name\n\n  # TODO(touts): remove\n  def _plain_name(self, name):\n    \"\"\"Return the fully scoped 'name'.\n\n    Args:\n      name: a string.\n\n    Returns:\n      'name' scoped in the current name stack, without any uniquified\n      elements.\n    \"\"\"\n    if self._name_stack[1]:\n      return self._name_stack[1] + \"/\" + name\n    else:\n      return name\n\n  def _set_default_device(self, dev):\n    \"\"\"Set the default device properties.\n\n    Args:\n      dev: string or Device.\n    \"\"\"\n    self._default_device = _device_string(dev)\n\n  def get_default_device(self):\n    \"\"\"Returns the default device.\n\n    Returns:\n      A string.\n    \"\"\"\n    return self._default_device\n\n  def _push_default_device_function(self, device_function):\n    \"\"\"Pushes the given function onto the stack of device functions.\n\n    See Graph.device for more details.\n\n    Args:\n      device_function: The function to be pushed onto the stack of device\n        functions.\n    \"\"\"\n    self._device_function_stack.append(device_function)\n\n  def _pop_default_device_function(self, device_function):\n    \"\"\"Pops the given function from the stack of device functions.\n\n    See Graph.device for more details.\n\n    Args:\n      device_function: The function to be popped from the stack of device\n        functions.\n\n    Raises:\n      ValueError: if the device_function to be popped is not top of the stack,\n        or if the stack is empty.\n    \"\"\"\n    if not self._device_function_stack:\n      raise ValueError(\"Tried to pop, but the device function stack is empty\")\n    if self._device_function_stack[-1] is not device_function:\n      raise ValueError(\"Tried to pop device function, but it was not on top \"\n                       \"of the stack\")\n\n    self._device_function_stack.pop()\n\n  @contextlib.contextmanager\n  def device(self, device_name_or_function):\n    \"\"\"Returns a context manager that specifies the default device to use.\n\n    The `device_name_or_function` argument may either be a device name\n    string, a device function, or None:\n\n    * If it is a device name string, all operations constructed in\n      this context will be assigned to the device with that name.\n    * If it is a function, it will be treated as function from\n      Operation objects to device name strings, and invoked each time\n      a new Operation is created. The Operation will be assigned to\n      the device with the returned name.\n    * If it is None, the default device will be cleared.\n\n    For example:\n\n    ```python\n    with g.device('/gpu:0'):\n      # All operations constructed in this context will be placed\n      # on GPU 0.\n      with g.device(None):\n        # All operations constructed in this context will have no\n        # assigned device.\n\n    # Defines a function from `Operation` to device string.\n    def matmul_on_gpu(n):\n      if n.type == \"MatMul\":\n        return \"/gpu:0\"\n      else:\n        return \"/cpu:0\"\n\n    with g.device(matmul_on_gpu):\n      # All operations of type \"MatMul\" constructed in this context\n      # will be placed on GPU 0; all other operations will be placed\n      # on CPU 0.\n    ```\n\n    Args:\n      device_name_or_function: The device name or function to use in\n        the context.\n\n    Returns:\n      A context manager that specifies the default device to use for newly\n      created ops.\n    \"\"\"\n    if callable(device_name_or_function):\n      try:\n        self._push_default_device_function(device_name_or_function)\n        yield\n      finally:\n        self._pop_default_device_function(device_name_or_function)\n    else:\n      try:\n        old_dev = self.get_default_device()\n        self._set_default_device(_device_string(device_name_or_function))\n        yield\n      finally:\n        self._set_default_device(old_dev)\n\n  class _ControlDependenciesController(object):\n    \"\"\"Context manager for `control_dependencies()`.\"\"\"\n\n    def __init__(self, graph, control_inputs):\n      self._graph = graph\n      self._control_inputs = control_inputs\n      self._seen_nodes = set()\n\n# pylint: disable=protected-access\n    def __enter__(self):\n      self._graph._push_control_dependencies_controller(self)\n\n    def __exit__(self, unused_type, unused_value, unused_traceback):\n      self._graph._pop_control_dependencies_controller(self)\n# pylint: enable=protected-access\n\n    @property\n    def control_inputs(self):\n      return self._control_inputs\n\n    def add_op(self, op):\n      self._seen_nodes.add(op)\n\n    def op_in_group(self, op):\n      return op in self._seen_nodes\n\n  def _push_control_dependencies_controller(self, controller):\n    self._control_dependencies_stack.append(controller)\n\n  def _pop_control_dependencies_controller(self, controller):\n    assert self._control_dependencies_stack[-1] is controller\n    self._control_dependencies_stack.pop()\n\n  def _current_control_dependencies(self):\n    ret = set()\n    for controller in self._control_dependencies_stack:\n      for op in controller.control_inputs:\n        ret.add(op)\n    return ret\n\n  def _control_dependencies_for_inputs(self, input_tensors):\n    \"\"\"For an op that takes `input_tensors` as inputs, compute control inputs.\n\n    The returned control dependencies should yield an execution that\n    is equivalent to adding all control inputs in\n    self._control_dependencies_stack to a newly created op. However,\n    this function attempts to prune the returned control dependencies\n    by observing that nodes created within the same `with\n    control_dependencies(...):` block may have data dependencies that make\n    the explicit approach redundant.\n\n    Args:\n      input_tensors: The direct data dependencies for an op to be created.\n\n    Returns:\n      A list of control inputs for the op to be created.\n    \"\"\"\n    ret = []\n    input_ops = set([t.op for t in input_tensors])\n    for controller in self._control_dependencies_stack:\n      # If any of the input_ops already depends on the inputs from controller,\n      # we say that the new op is dominated (by that input), and we therefore\n      # do not need to add control dependences for this controller's inputs.\n      dominated = False\n      for op in input_ops:\n        if controller.op_in_group(op):\n          dominated = True\n          break\n      if not dominated:\n        # Don't add a control input if we already have a data dependency on i.\n        # NOTE(mrry): We do not currently track transitive data dependencies,\n        #   so we may add redundant control inputs.\n        ret.extend([c for c in controller.control_inputs if c not in input_ops])\n    return ret\n\n  def _record_op_seen_by_control_dependencies(self, op):\n    \"\"\"Record that the given op depends on all registered control dependencies.\n\n    Args:\n      op: An Operation.\n    \"\"\"\n    for controller in self._control_dependencies_stack:\n      controller.add_op(op)\n\n  def control_dependencies(self, control_inputs):\n    \"\"\"Returns a context manager that specifies control dependencies.\n\n    Use with the `with` keyword to specify that all operations constructed\n    within the context should have control dependencies on\n    `control_inputs`. For example:\n\n    ```python\n    with g.control_dependencies([a, b, c]):\n      # `d` and `e` will only run after `a`, `b`, and `c` have executed.\n      d = ...\n      e = ...\n    ```\n\n    Multiple calls to `control_dependencies()` can be nested, and in\n    that case a new `Operation` will have control dependencies on the union\n    of `control_inputs` from all active contexts.\n\n    ```python\n    with g.control_dependencies([a, b]):\n      # Ops declared here run after `a` and `b`.\n      with g.control_dependencies([c, d]):\n        # Ops declared here run after `a`, `b`, `c`, and `d`.\n    ```\n\n    *N.B.* The control dependencies context applies *only* to ops that\n    are constructed within the context. Merely using an op or tensor\n    in the context does not add a control dependency. The following\n    example illustrates this point:\n\n    ```python\n    # WRONG\n    def my_func(pred, tensor):\n      t = tf.matmul(tensor, tensor)\n      with tf.control_dependencies([pred]):\n        # The matmul op is created outside the context, so no control\n        # dependency will be added.\n        return t\n\n    # RIGHT\n    def my_func(pred, tensor):\n      with tf.control_dependencies([pred]):\n        # The matmul op is created in the context, so a control dependency\n        # will be added.\n        return tf.matmul(tensor, tensor)\n    ```\n\n    Args:\n      control_inputs: A list of `Operation` or `Tensor` objects, which\n        must be executed or computed before running the operations\n        defined in the context.\n\n    Returns:\n     A context manager that specifies control dependencies for all\n     operations constructed within the context.\n\n    Raises:\n      TypeError: If `control_inputs` is not a list of `Operation` or\n        `Tensor` objects.\n    \"\"\"\n    # First convert the inputs to ops, and deduplicate them.\n    # NOTE(mrry): Other than deduplication, we do not currently track direct\n    #   or indirect dependencies between control_inputs, which may result in\n    #   redundant control inputs.\n    control_ops = []\n    current = self._current_control_dependencies()\n    for c in control_inputs:\n      c = self.as_graph_element(c)\n      if isinstance(c, Tensor):\n        c = c.op\n      elif not isinstance(c, Operation):\n        raise TypeError(\"Control input must be Operation or Tensor: %s\" % c)\n      if c not in current:\n        control_ops.append(c)\n        current.add(c)\n    return self._ControlDependenciesController(self, control_ops)\n\n  # pylint: disable=g-doc-return-or-yield\n  @contextlib.contextmanager\n  def _kernel_label_map(self, op_to_kernel_label_map):\n    \"\"\"EXPERIMENTAL: A context manager for setting kernel labels.\n\n    This context manager can be used to select particular\n    implementations of kernels within the scope of the context.\n\n    For example:\n\n        with ops.Graph().as_default() as g:\n          f_1 = Foo()  # Uses the default registered kernel for the Foo op.\n          with g.kernel_label_map({\"Foo\": \"v_2\"}):\n            f_2 = Foo()  # Uses the registered kernel with label \"v_2\"\n                         # for the Foo op.\n            with g.kernel_label_map({\"Foo\": \"v_3\"}):\n              f_3 = Foo()  # Uses the registered kernel with label \"v_3\"\n                           # for the Foo op.\n              with g.kernel_label_map({\"Foo\": \"\"}):\n                f_4 = Foo()  # Uses the default registered kernel\n                             # for the Foo op.\n\n    Args:\n      op_to_kernel_label_map: A dictionary mapping op type strings to\n        kernel label strings.\n\n    Returns:\n      A context manager that sets the kernel label to be used for one or more\n      ops created in that context.\n\n    Raises:\n      TypeError: If op_to_kernel_label_map is not a dictionary mapping\n        strings to strings.\n    \"\"\"\n    if not isinstance(op_to_kernel_label_map, dict):\n      raise TypeError(\"op_to_kernel_label_map must be a dictionary mapping \"\n                      \"strings to strings\")\n    # The saved_labels dictionary stores any currently-set labels that\n    # will be overridden by this context manager.\n    saved_labels = {}\n    # Install the given label\n    for op_type, label in op_to_kernel_label_map.items():\n      if not (isinstance(op_type, six.string_types)\n              and isinstance(label, six.string_types)):\n        raise TypeError(\"op_to_kernel_label_map must be a dictionary mapping \"\n                        \"strings to strings\")\n      try:\n        saved_labels[op_type] = self._op_to_kernel_label_map[op_type]\n      except KeyError:\n        pass\n      self._op_to_kernel_label_map[op_type] = label\n    try:\n      yield  # The code within the context runs here.\n    finally:\n      # Remove the labels set for this context, and restore any saved labels.\n      for op_type, label in op_to_kernel_label_map.items():\n        try:\n          self._op_to_kernel_label_map[op_type] = saved_labels[op_type]\n        except KeyError:\n          del self._op_to_kernel_label_map[op_type]\n  # pylint: enable=g-doc-return-or-yield\n\n  # pylint: disable=g-doc-return-or-yield\n  @contextlib.contextmanager\n  def gradient_override_map(self, op_type_map):\n    \"\"\"EXPERIMENTAL: A context manager for overriding gradient functions.\n\n    This context manager can be used to override the gradient function\n    that will be used for ops within the scope of the context.\n\n    For example:\n\n    ```python\n    @tf.RegisterGradient(\"CustomSquare\")\n    def _custom_square_grad(op, inputs):\n      # ...\n\n    with tf.Graph().as_default() as g:\n      c = tf.constant(5.0)\n      s_1 = tf.square(c)  # Uses the default gradient for tf.square.\n      with g.gradient_override_map({\"Square\": \"CustomSquare\"}):\n        s_2 = tf.square(s_2)  # Uses _custom_square_grad to compute the\n                              # gradient of s_2.\n    ```\n\n    Args:\n      op_type_map: A dictionary mapping op type strings to alternative op\n        type strings.\n\n    Returns:\n      A context manager that sets the alternative op type to be used for one\n      or more ops created in that context.\n\n    Raises:\n      TypeError: If `op_type_map` is not a dictionary mapping strings to\n        strings.\n    \"\"\"\n    if not isinstance(op_type_map, dict):\n      raise TypeError(\"op_type_map must be a dictionary mapping \"\n                      \"strings to strings\")\n    # The saved_mappings dictionary stores any currently-set mappings that\n    # will be overridden by this context manager.\n    saved_mappings = {}\n    # Install the given label\n    for op_type, mapped_op_type in op_type_map.items():\n      if not (isinstance(op_type, six.string_types)\n              and isinstance(mapped_op_type, six.string_types)):\n        raise TypeError(\"op_type_map must be a dictionary mapping \"\n                        \"strings to strings\")\n      try:\n        saved_mappings[op_type] = self._gradient_override_map[op_type]\n      except KeyError:\n        pass\n      self._gradient_override_map[op_type] = mapped_op_type\n    try:\n      yield  # The code within the context runs here.\n    finally:\n      # Remove the labels set for this context, and restore any saved labels.\n      for op_type, mapped_op_type in op_type_map.items():\n        try:\n          self._gradient_override_map[op_type] = saved_mappings[op_type]\n        except KeyError:\n          del self._gradient_override_map[op_type]\n  # pylint: enable=g-doc-return-or-yield\n\n\ndef device(dev):\n  \"\"\"Wrapper for `Graph.device()` using the default graph.\n\n  See\n  [`Graph.name_scope()`](../../api_docs/python/framework.md#Graph.name_scope)\n  for more details.\n\n  Args:\n    device_name_or_function: The device name or function to use in\n      the context.\n\n  Returns:\n    A context manager that specifies the default device to use for newly\n    created ops.\n  \"\"\"\n  return get_default_graph().device(dev)\n\n\ndef name_scope(name):\n  \"\"\"Wrapper for `Graph.name_scope()` using the default graph.\n\n  See\n  [`Graph.name_scope()`](../../api_docs/python/framework.md#Graph.name_scope)\n  for more details.\n\n  Args:\n    name: A name for the scope.\n\n  Returns:\n    A context manager that installs `name` as a new name scope in the\n    default graph.\n  \"\"\"\n  return get_default_graph().name_scope(name)\n\n\ndef control_dependencies(control_inputs):\n  \"\"\"Wrapper for `Graph.control_dependencies()` using the default graph.\n\n  See [`Graph.control_dependencies()`](../../api_docs/python/framework.md#Graph.control_dependencies)\n  for more details.\n\n  Args:\n    control_inputs: A list of `Operation` or `Tensor` objects, which\n      must be executed or computed before running the operations\n      defined in the context.\n\n  Returns:\n   A context manager that specifies control dependencies for all\n   operations constructed within the context.\n  \"\"\"\n  return get_default_graph().control_dependencies(control_inputs)\n\n\nclass _DefaultStack(threading.local):\n  \"\"\"A thread-local stack of objects for providing implicit defaults.\"\"\"\n\n  def __init__(self):\n    super(_DefaultStack, self).__init__()\n    self.stack = []\n\n  def get_default(self):\n    return self.stack[-1] if len(self.stack) >= 1 else None\n\n  def reset(self):\n    self.stack = []\n\n  @contextlib.contextmanager\n  def get_controller(self, default):\n    \"\"\"A context manager for manipulating a default stack.\"\"\"\n    try:\n      self.stack.append(default)\n      yield default\n    finally:\n      assert self.stack[-1] is default\n      self.stack.pop()\n\n\n_default_session_stack = _DefaultStack()\n\n\ndef default_session(session):\n  \"\"\"Python \"with\" handler for defining a default session.\n\n  This function provides a means of registering a session for handling\n  Tensor.eval() and Operation.run() calls. It is primarily intended for use\n  by session.Session, but can be used with any object that implements\n  the Session.run() interface.\n\n  Use with the \"with\" keyword to specify that Tensor.eval() and Operation.run()\n  invocations within the scope of a block should be executed by a particular\n  session.\n\n  The default session applies to the current thread only, so it is always\n  possible to inspect the call stack and determine the scope of a default\n  session. If you create a new thread, and wish to use the default session\n  in that thread, you must explicitly add a \"with ops.default_session(sess):\"\n  block in that thread's function.\n\n  Example:\n    The following code examples are equivalent:\n\n    # 1. Using the Session object directly:\n    sess = ...\n    c = tf.constant(5.0)\n    sess.run(c)\n\n    # 2. Using default_session():\n    sess = ...\n    with ops.default_session(sess):\n      c = tf.constant(5.0)\n      result = c.eval()\n\n    # 3. Overriding default_session():\n    sess = ...\n    with ops.default_session(sess):\n      c = tf.constant(5.0)\n      with ops.default_session(...):\n        c.eval(session=sess)\n\n  Args:\n    session: The session to be installed as the default session.\n\n  Returns:\n    A context manager for the default session.\n  \"\"\"\n  return _default_session_stack.get_controller(weakref.ref(session))\n\n\ndef get_default_session():\n  \"\"\"Returns the default session for the current thread.\n\n  The returned `Session` will be the innermost session on which a\n  `Session` or `Session.as_default()` context has been entered.\n\n  *N.B.* The default session is a property of the current thread. If you\n  create a new thread, and wish to use the default session in that\n  thread, you must explicitly add a `with sess.as_default():` in that\n  thread's function.\n\n  Returns:\n    The default `Session` being used in the current thread.\n  \"\"\"\n  ref = _default_session_stack.get_default()\n  if ref is None:\n    # No default session has been registered.\n    return None\n  else:\n    # De-reference ref.\n    ret = ref()\n    if ret is None:\n      # This should never happen with the current session implementations.\n      raise RuntimeError(\"Default session has been garbage collected.\")\n  return ret\n\n\ndef _eval_using_default_session(tensors, feed_dict, graph, session=None):\n  \"\"\"Uses the default session to evaluate one or more tensors.\n\n  Args:\n    tensors: A single Tensor, or a list of Tensor objects.\n    feed_dict: A dictionary that maps Tensor objects (or tensor names) to lists,\n      numpy ndarrays, TensorProtos, or strings.\n    graph: The graph in which the tensors are defined.\n    session: (Optional) A different session to use to evaluate \"tensors\".\n\n  Returns:\n    Either a single numpy ndarray if \"tensors\" is a single tensor; or a list\n    of numpy ndarrays that each correspond to the respective element in\n    \"tensors\".\n\n  Raises:\n    ValueError: If no default session is available; the default session\n      does not have \"graph\" as its graph; or if \"session\" is specified,\n      and it does not have \"graph\" as its graph.\n  \"\"\"\n  if session is None:\n    session = get_default_session()\n    if session is None:\n      raise ValueError(\"Cannot evaluate tensor using eval(): No default \"\n                       \"session is registered. Use 'with \"\n                       \"DefaultSession(sess)' or pass an explicit session to \"\n                       \"eval(session=sess)\")\n    if session.graph is not graph:\n      raise ValueError(\"Cannot use the default session to evaluate tensor: \"\n                       \"the tensor's graph is different from the session's \"\n                       \"graph. Pass an explicit session to \"\n                       \"eval(session=sess).\")\n  else:\n    if session.graph is not graph:\n      raise ValueError(\"Cannot use the given session to evaluate tensor: \"\n                       \"the tensor's graph is different from the session's \"\n                       \"graph.\")\n  return session.run(tensors, feed_dict)\n\n\ndef _run_using_default_session(operation, feed_dict, graph, session=None):\n  \"\"\"Uses the default session to run \"operation\".\n\n  Args:\n    operation: The Operation to be run.\n    feed_dict: A dictionary that maps Tensor objects (or tensor names) to lists,\n      numpy ndarrays, TensorProtos, or strings.\n    graph: The graph in which \"operation\" is defined.\n    session: (Optional) A different session to use to run \"operation\".\n\n  Raises:\n    ValueError: If no default session is available; the default session\n      does not have \"graph\" as its graph; or if \"session\" is specified,\n      and it does not have \"graph\" as its graph.\n  \"\"\"\n  if session is None:\n    session = get_default_session()\n    if session is None:\n      raise ValueError(\"Cannot execute operation using Run(): No default \"\n                       \"session is registered. Use 'with \"\n                       \"default_session(sess)' or pass an explicit session to \"\n                       \"Run(session=sess)\")\n    if session.graph is not graph:\n      raise ValueError(\"Cannot use the default session to execute operation: \"\n                       \"the operation's graph is different from the \"\n                       \"session's graph. Pass an explicit session to \"\n                       \"Run(session=sess).\")\n  else:\n    if session.graph is not graph:\n      raise ValueError(\"Cannot use the given session to execute operation: \"\n                       \"the operation's graph is different from the session's \"\n                       \"graph.\")\n  session.run(operation, feed_dict)\n\n\nclass _DefaultGraphStack(_DefaultStack):\n  \"\"\"A thread-local stack of objects for providing an implicit default graph.\"\"\"\n\n  def __init__(self):\n    super(_DefaultGraphStack, self).__init__()\n    self._global_default_graph = None\n\n  def get_default(self):\n    \"\"\"Override that returns a global default if the stack is empty.\"\"\"\n    ret = super(_DefaultGraphStack, self).get_default()\n    if ret is None:\n      ret = self._GetGlobalDefaultGraph()\n    return ret\n\n  def _GetGlobalDefaultGraph(self):\n    if self._global_default_graph is None:\n      # TODO(mrry): Perhaps log that the default graph is being used, or set\n      #   provide some other feedback to prevent confusion when a mixture of\n      #   the global default graph and an explicit graph are combined in the\n      #   same process.\n      self._global_default_graph = Graph()\n    return self._global_default_graph\n\n  def reset(self):\n    super(_DefaultGraphStack, self).reset()\n    self._global_default_graph = None\n\n_default_graph_stack = _DefaultGraphStack()\n\n\ndef reset_default_graph():\n  \"\"\"Clears the default graph stack and resets the global default graph.\n\n  *N.B.* The default graph is a property of the current thread. This\n   function applies only to the current thread.\n  \"\"\"\n  _default_graph_stack.reset()\n\n\ndef get_default_graph():\n  \"\"\"Returns the default graph for the current thread.\n\n  The returned graph will be the innermost graph on which a\n  `Graph.as_default()` context has been entered, or a global default\n  graph if none has been explicitly created.\n\n  *N.B.* The default graph is a property of the current thread. If you\n  create a new thread, and wish to use the default graph in that\n  thread, you must explicitly add a `with g.as_default():` in that\n  thread's function.\n\n  Returns:\n    The default `Graph` being used in the current thread.\n  \"\"\"\n  return _default_graph_stack.get_default()\n\n\ndef _get_graph_from_inputs(op_input_list, graph=None):\n  \"\"\"Returns the appropriate graph to use for the given inputs.\n\n  This library method provides a consistent algorithm for choosing the graph\n  in which an Operation should be constructed:\n\n  1. If the \"graph\" is specified explicitly, we validate that all of the inputs\n     in \"op_input_list\" are compatible with that graph.\n  2. Otherwise, we attempt to select a graph from the first Operation-\n     or Tensor-valued input in \"op_input_list\", and validate that all other\n     such inputs are in the same graph.\n  3. If the graph was not specified and it could not be inferred from\n     \"op_input_list\", we attempt to use the default graph.\n\n  Args:\n    op_input_list: A list of inputs to an operation, which may include Tensor\n      and Operation objects.\n    graph: (Optional) The explicit graph to use.\n\n  Raises:\n    TypeError: If op_input_list is not a list or tuple, or if graph is not a\n      Graph.\n    ValueError: If a graph is explicitly passed and not all inputs are from it,\n      or if the inputs are from multiple graphs, or we could not find a graph\n      and there was no default graph.\n\n  Returns:\n    The appropriate graph to use for the given inputs.\n  \"\"\"\n  if not isinstance(op_input_list, (list, tuple)):\n    raise TypeError(\"The op_input_list must be a list or tuple\")\n\n  # 1. If the graph is specified explicitly, we validate that all of the inputs\n  #    are compatible with that graph.\n  if graph is not None:\n    if not isinstance(graph, Graph):\n      raise TypeError(\"Input graph needs to be a Graph: %s\" % graph)\n    for op_input in op_input_list:\n      if isinstance(op_input, Operation):\n        if op_input.graph is not graph:\n          raise ValueError(\"Operation %s is not from the passed-in graph\"\n                           % op_input)\n      elif isinstance(op_input, Tensor):\n        if op_input.graph is not graph:\n          raise ValueError(\"Tensor %s is not from the passed-in graph\"\n                           % op_input)\n    return graph\n\n  # 2. Otherwise, we attempt to select a graph from one of the Operation-\n  #    or Tensor-valued inputs.\n  original_input = None\n  for op_input in op_input_list:\n    if isinstance(op_input, (Operation, Tensor)):\n      if original_input is None:\n        original_input = op_input\n      else:\n        assert_same_graph([original_input, op_input])\n  if original_input is not None:\n    return original_input.graph\n\n  # 3. If all else fails, we use the default graph, which is always there.\n  return get_default_graph()\n\n\nclass GraphKeys(object):\n  \"\"\"Standard names to use for graph collections.\n\n  The standard library uses various well-known names to collect and\n  retrieve values associated with a graph. For example, the\n  `tf.Optimizer` subclasses default to optimizing the variables\n  collected under `tf.GraphKeys.TRAINABLE_VARIABLES` if none is\n  specified, but it is also possible to pass an explicit list of\n  variables.\n\n  The following standard keys are defined:\n\n  * `VARIABLES`: the `Variable` objects that comprise a model, and\n    must be saved and restored together. See\n    [`tf.all_variables()`](../../api_docs/python/state_ops.md#all_variables)\n    for more details.\n  * `TRAINABLE_VARIABLES`: the subset of `Variable` objects that will\n    be trained by an optimizer. See\n    [`tf.trainable_variables()`](../../api_docs/python/state_ops.md#trainable_variables)\n    for more details.\n  * `SUMMARIES`: the summary `Tensor` objects that have been created in the\n    graph. See\n    [`tf.merge_all_summaries()`](../../api_docs/python/train.md#merge_all_summaries)\n    for more details.\n  * `QUEUE_RUNNERS`: the `QueueRunner` objects that are used to\n    produce input for a computation. See\n    [`tf.start_queue_runners()`](../../api_docs/python/train.md#start_queue_runners)\n    for more details.\n  \"\"\"\n\n  # Key to collect variables.Variable objects that must be saved and restored\n  # by the model.\n  VARIABLES = \"variables\"\n  # Key to collect variables.Variable objects that will be trained by the\n  # optimizers.\n  TRAINABLE_VARIABLES = \"trainable_variables\"\n  # Key to collect summaries.\n  SUMMARIES = \"summaries\"\n  # Key to collect QueueRunners.\n  QUEUE_RUNNERS = \"queue_runners\"\n  # Key to collect table initializers.\n  TABLE_INITIALIZERS = \"table_initializer\"\n\n\ndef add_to_collection(name, value):\n  \"\"\"Wrapper for `Graph.add_to_collection()` using the default graph.\n\n  See [`Graph.add_to_collection()`](../../api_docs/python/framework.md#Graph.add_to_collection)\n  for more details.\n\n  Args:\n    name: The key for the collection. For example, the `GraphKeys` class\n      contains many standard names for collections.\n    value: The value to add to the collection.\n  \"\"\"\n  get_default_graph().add_to_collection(name, value)\n\n\ndef get_collection(key, scope=None):\n  \"\"\"Wrapper for `Graph.get_collection()` using the default graph.\n\n  See [`Graph.get_collection()`](../../api_docs/python/framework.md#Graph.get_collection)\n  for more details.\n\n  Args:\n    key: The key for the collection. For example, the `GraphKeys` class\n      contains many standard names for collections.\n    scope: (Optional.) If supplied, the resulting list is filtered to include\n      only items whose name begins with this string.\n\n  Returns:\n    The list of values in the collection with the given `name`, or\n    an empty list if no value has been added to that collection. The\n    list contains the values in the order under which they were\n    collected.\n  \"\"\"\n  return get_default_graph().get_collection(key, scope)\n\n\n# pylint: disable=g-doc-return-or-yield\n@contextlib.contextmanager\ndef op_scope(values, name, default_name):\n  \"\"\"Returns a context manager for use when defining a Python op.\n\n  This context manager validates that the given `values` are from the\n  same graph, ensures that that graph is the default graph, and pushes a\n  name scope.\n\n  For example, to define a new Python op called `my_op`:\n\n  ```python\n  def my_op(a, b, c, name=None):\n    with tf.op_scope([a, b, c], name, \"MyOp\") as scope:\n      a = tf.convert_to_tensor(a, name=\"a\")\n      b = tf.convert_to_tensor(b, name=\"b\")\n      c = tf.convert_to_tensor(c, name=\"c\")\n      # Define some computation that uses `a`, `b`, and `c`.\n      return foo_op(..., name=scope)\n  ```\n\n  Args:\n    values: The list of `Tensor` arguments that are passed to the op function.\n    name: The name argument that is passed to the op function.\n    default_name: The default name to use if the `name` argument is `None`.\n\n  Returns:\n    A context manager for use in defining a Python op.\n  \"\"\"\n  g = _get_graph_from_inputs(values)\n  n = default_name if name is None else name\n  with g.as_default(), g.name_scope(n) as scope:\n    yield scope\n# pylint: enable=g-doc-return-or-yield\n", "ight (c) 2012 Mitch Garnaat http://garnaat.org/\n# Copyright (c) 2012 Amazon.com, Inc. or its affiliates.  All Rights Reserved\n#\n# Permission is hereby granted, free of charge, to any person obtaining a\n# copy of this software and associated documentation files (the\n# \"Software\"), to deal in the Software without restriction, including\n# without limitation the rights to use, copy, modify, merge, publish, dis-\n# tribute, sublicense, and/or sell copies of the Software, and to permit\n# persons to whom the Software is furnished to do so, subject to the fol-\n# lowing conditions:\n#\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\n# OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABIL-\n# ITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT\n# SHALL THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\n# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n# IN THE SOFTWARE.\n\n\"\"\"\nThis module provides an interface to the Elastic Compute Cloud (EC2)\nAuto Scaling service.\n\"\"\"\n\nimport base64\n\nimport boto\nfrom boto.connection import AWSQueryConnection\nfrom boto.regioninfo import RegionInfo, get_regions, load_regions\nfrom boto.ec2.autoscale.request import Request\nfrom boto.ec2.autoscale.launchconfig import LaunchConfiguration\nfrom boto.ec2.autoscale.group import AutoScalingGroup\nfrom boto.ec2.autoscale.group import ProcessType\nfrom boto.ec2.autoscale.activity import Activity\nfrom boto.ec2.autoscale.policy import AdjustmentType\nfrom boto.ec2.autoscale.policy import MetricCollectionTypes\nfrom boto.ec2.autoscale.policy import ScalingPolicy\nfrom boto.ec2.autoscale.policy import TerminationPolicies\nfrom boto.ec2.autoscale.instance import Instance\nfrom boto.ec2.autoscale.scheduled import ScheduledUpdateGroupAction\nfrom boto.ec2.autoscale.tag import Tag\nfrom boto.ec2.autoscale.limits import AccountLimits\nfrom boto.compat import six\n\nRegionData = load_regions().get('autoscaling', {})\n\n\ndef regions():\n    \"\"\"\n    Get all available regions for the Auto Scaling service.\n\n    :rtype: list\n    :return: A list of :class:`boto.RegionInfo` instances\n    \"\"\"\n    return get_regions('autoscaling', connection_cls=AutoScaleConnection)\n\n\ndef connect_to_region(region_name, **kw_params):\n    \"\"\"\n    Given a valid region name, return a\n    :class:`boto.ec2.autoscale.AutoScaleConnection`.\n\n    :param str region_name: The name of the region to connect to.\n\n    :rtype: :class:`boto.ec2.AutoScaleConnection` or ``None``\n    :return: A connection to the given region, or None if an invalid region\n        name is given\n    \"\"\"\n    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    return None\n\n\nclass AutoScaleConnection(AWSQueryConnection):\n    APIVersion = boto.config.get('Boto', 'autoscale_version', '2011-01-01')\n    DefaultRegionEndpoint = boto.config.get('Boto', 'autoscale_endpoint',\n                                            'autoscaling.us-east-1.amazonaws.com')\n    DefaultRegionName = boto.config.get('Boto', 'autoscale_region_name',\n                                        'us-east-1')\n\n    def __init__(self, aws_access_key_id=None, aws_secret_access_key=None,\n                 is_secure=True, port=None, proxy=None, proxy_port=None,\n                 proxy_user=None, proxy_pass=None, debug=0,\n                 https_connection_factory=None, region=None, path='/',\n                 security_token=None, validate_certs=True, profile_name=None,\n                 use_block_device_types=False):\n        \"\"\"\n        Init method to create a new connection to the AutoScaling service.\n\n        B{Note:} The host argument is overridden by the host specified in the\n                 boto configuration file.\n\n\n        \"\"\"\n        if not region:\n            region = RegionInfo(self, self.DefaultRegionName,\n                                self.DefaultRegionEndpoint,\n                                AutoScaleConnection)\n        self.region = region\n        self.use_block_device_types = use_block_device_types\n        super(AutoScaleConnection, self).__init__(aws_access_key_id,\n                                                  aws_secret_access_key,\n                                                  is_secure, port, proxy, proxy_port,\n                                                  proxy_user, proxy_pass,\n                                                  self.region.endpoint, debug,\n                                                  https_connection_factory, path=path,\n                                                  security_token=security_token,\n                                                  validate_certs=validate_certs,\n                                                  profile_name=profile_name)\n\n    def _required_auth_capability(self):\n        return ['hmac-v4']\n\n    def build_list_params(self, params, items, label):\n        \"\"\"\n        Items is a list of dictionaries or strings::\n\n            [\n                {\n                    'Protocol' : 'HTTP',\n                    'LoadBalancerPort' : '80',\n                    'InstancePort' : '80'\n                },\n                ..\n            ] etc.\n\n        or::\n\n            ['us-east-1b',...]\n        \"\"\"\n        # different from EC2 list params\n        for i in range(1, len(items) + 1):\n            if isinstance(items[i - 1], dict):\n                for k, v in six.iteritems(items[i - 1]):\n                    if isinstance(v, dict):\n                        for kk, vv in six.iteritems(v):\n                            params['%s.member.%d.%s.%s' % (label, i, k, kk)] = vv\n                    else:\n                        params['%s.member.%d.%s' % (label, i, k)] = v\n            elif isinstance(items[i - 1], six.string_types):\n                params['%s.member.%d' % (label, i)] = items[i - 1]\n\n    def _update_group(self, op, as_group):\n        params = {'AutoScalingGroupName': as_group.name,\n                  'LaunchConfigurationName': as_group.launch_config_name,\n                  'MinSize': as_group.min_size,\n                  'MaxSize': as_group.max_size}\n        # get availability zone information (required param)\n        zones = as_group.availability_zones\n        self.build_list_params(params, zones, 'AvailabilityZones')\n        if as_group.desired_capacity is not None:\n            params['DesiredCapacity'] = as_group.desired_capacity\n        if as_group.vpc_zone_identifier:\n            params['VPCZoneIdentifier'] = as_group.vpc_zone_identifier\n        if as_group.health_check_period:\n            params['HealthCheckGracePeriod'] = as_group.health_check_period\n        if as_group.health_check_type:\n            params['HealthCheckType'] = as_group.health_check_type\n        if as_group.default_cooldown:\n            params['DefaultCooldown'] = as_group.default_cooldown\n        if as_group.placement_group:\n            params['PlacementGroup'] = as_group.placement_group\n        if as_group.instance_id:\n            params['InstanceId'] = as_group.instance_id\n        if as_group.termination_policies:\n            self.build_list_params(params, as_group.termination_policies,\n                                   'TerminationPolicies')\n        if op.startswith('Create'):\n            # you can only associate load balancers with an autoscale\n            # group at creation time\n            if as_group.load_balancers:\n                self.build_list_params(params, as_group.load_balancers,\n                                       'LoadBalancerNames')\n            if as_group.tags:\n                for i, tag in enumerate(as_group.tags):\n                    tag.build_params(params, i + 1)\n        return self.get_object(op, params, Request)\n\n    def attach_instances(self, name, instance_ids):\n        \"\"\"\n        Attach instances to an autoscaling group.\n        \"\"\"\n        params = {\n            'AutoScalingGroupName': name,\n        }\n        self.build_list_params(params, instance_ids, 'InstanceIds')\n        return self.get_status('AttachInstances', params)\n\n    def detach_instances(self, name, instance_ids, decrement_capacity=True):\n        \"\"\"\n        Detach instances from an Auto Scaling group.\n\n        :type name: str\n        :param name: The name of the Auto Scaling group from which to detach instances.\n\n        :type instance_ids: list\n        :param instance_ids: Instance ids to be detached from the Auto Scaling group.\n\n        :type decrement_capacity: bool\n        :param decrement_capacity: Whether to decrement the size of the\n            Auto Scaling group or not.\n        \"\"\"\n\n        params = {'AutoScalingGroupName': name}\n        params['ShouldDecrementDesiredCapacity'] = 'true' if decrement_capacity else 'false'\n\n        self.build_list_params(params, instance_ids, 'InstanceIds')\n        return self.get_status('DetachInstances', params)\n\n    def create_auto_scaling_group(self, as_group):\n        \"\"\"\n        Create auto scaling group.\n        \"\"\"\n        return self._update_group('CreateAutoScalingGroup', as_group)\n\n    def delete_auto_scaling_group(self, name, force_delete=False):\n        \"\"\"\n        Deletes the specified auto scaling group if the group has no instances\n        and no scaling activities in progress.\n        \"\"\"\n        if(force_delete):\n            params = {'AutoScalingGroupName': name, 'ForceDelete': 'true'}\n        else:\n            params = {'AutoScalingGroupName': name}\n        return self.get_object('DeleteAutoScalingGroup', params, Request)\n\n    def create_launch_configuration(self, launch_config):\n        \"\"\"\n        Creates a new Launch Configuration.\n\n        :type launch_config: :class:`boto.ec2.autoscale.launchconfig.LaunchConfiguration`\n        :param launch_config: LaunchConfiguration object.\n        \"\"\"\n        params = {'ImageId': launch_config.image_id,\n                  'LaunchConfigurationName': launch_config.name,\n                  'InstanceType': launch_config.instance_type}\n        if launch_config.key_name:\n            params['KeyName'] = launch_config.key_name\n        if launch_config.user_data:\n            user_data = launch_config.user_data\n            if isinstance(user_data, six.text_type):\n                user_data = user_data.encode('utf-8')\n            params['UserData'] = base64.b64encode(user_data).decode('utf-8')\n        if launch_config.kernel_id:\n            params['KernelId'] = launch_config.kernel_id\n        if launch_config.ramdisk_id:\n            params['RamdiskId'] = launch_config.ramdisk_id\n        if launch_config.block_device_mappings:\n            [x.autoscale_build_list_params(params) for x in launch_config.block_device_mappings]\n        if launch_config.security_groups:\n            self.build_list_params(params, launch_config.security_groups,\n                                   'SecurityGroups')\n        if launch_config.instance_monitoring:\n            params['InstanceMonitoring.Enabled'] = 'true'\n        else:\n            params['InstanceMonitoring.Enabled'] = 'false'\n        if launch_config.spot_price is not None:\n            params['SpotPrice'] = str(launch_config.spot_price)\n        if launch_config.instance_profile_name is not None:\n            params['IamInstanceProfile'] = launch_config.instance_profile_name\n        if launch_config.ebs_optimized:\n            params['EbsOptimized'] = 'true'\n        else:\n            params['EbsOptimized'] = 'false'\n        if launch_config.associate_public_ip_address is True:\n            params['AssociatePublicIpAddress'] = 'true'\n        elif launch_config.associate_public_ip_address is False:\n            params['AssociatePublicIpAddress'] = 'false'\n        if launch_config.volume_type:\n            params['VolumeType'] = launch_config.volume_type\n        if launch_config.delete_on_termination:\n            params['DeleteOnTermination'] = 'true'\n        else:\n            params['DeleteOnTermination'] = 'false'\n        if launch_config.iops:\n            params['Iops'] = launch_config.iops\n        if launch_config.classic_link_vpc_id:\n            params['ClassicLinkVPCId'] = launch_config.classic_link_vpc_id\n        if launch_config.classic_link_vpc_security_groups:\n            self.build_list_params(\n                params,\n                launch_config.classic_link_vpc_security_groups,\n                'ClassicLinkVPCSecurityGroups'\n            )\n        return self.get_object('CreateLaunchConfiguration', params,\n                               Request, verb='POST')\n\n    def get_account_limits(self):\n        \"\"\"\n        Returns the limits for the Auto Scaling resources currently granted for\n        your AWS account.\n        \"\"\"\n        params = {}\n        return self.get_object('DescribeAccountLimits', params, AccountLimits)\n\n    def create_scaling_policy(self, scaling_policy):\n        \"\"\"\n        Creates a new Scaling Policy.\n\n        :type scaling_policy: :class:`boto.ec2.autoscale.policy.ScalingPolicy`\n        :param scaling_policy: ScalingPolicy object.\n        \"\"\"\n        params = {'AdjustmentType': scaling_policy.adjustment_type,\n                  'AutoScalingGroupName': scaling_policy.as_name,\n                  'PolicyName': scaling_policy.name,\n                  'ScalingAdjustment': scaling_policy.scaling_adjustment}\n\n        if scaling_policy.adjustment_type == \"PercentChangeInCapacity\" and \\\n           scaling_policy.min_adjustment_step is not None:\n            params['MinAdjustmentStep'] = scaling_policy.min_adjustment_step\n\n        if scaling_policy.cooldown is not None:\n            params['Cooldown'] = scaling_policy.cooldown\n\n        return self.get_object('PutScalingPolicy', params, Request)\n\n    def delete_launch_configuration(self, launch_config_name):\n        \"\"\"\n        Deletes the specified LaunchConfiguration.\n\n        The specified launch configuration must not be attached to an Auto\n        Scaling group. Once this call completes, the launch configuration is no\n        longer available for use.\n        \"\"\"\n        params = {'LaunchConfigurationName': launch_config_name}\n        return self.get_object('DeleteLaunchConfiguration', params, Request)\n\n    def get_all_groups(self, names=None, max_records=None, next_token=None):\n        \"\"\"\n        Returns a full description of each Auto Scaling group in the given\n        list. This includes all Amazon EC2 instances that are members of the\n        group. If a list of names is not provided, the service returns the full\n        details of all Auto Scaling groups.\n\n        This action supports pagination by returning a token if there are more\n        pages to retrieve. To get the next page, call this action again with\n        the returned token as the NextToken parameter.\n\n        :type names: list\n        :param names: List of group names which should be searched for.\n\n        :type max_records: int\n        :param max_records: Maximum amount of groups to return.\n\n        :rtype: list\n        :returns: List of :class:`boto.ec2.autoscale.group.AutoScalingGroup`\n            instances.\n        \"\"\"\n        params = {}\n        if max_records:\n            params['MaxRecords'] = max_records\n        if next_token:\n            params['NextToken'] = next_token\n        if names:\n            self.build_list_params(params, names, 'AutoScalingGroupNames')\n        return self.get_list('DescribeAutoScalingGroups', params,\n                             [('member', AutoScalingGroup)])\n\n    def get_all_launch_configurations(self, **kwargs):\n        \"\"\"\n        Returns a full description of the launch configurations given the\n        specified names.\n\n        If no names are specified, then the full details of all launch\n        configurations are returned.\n\n        :type names: list\n        :param names: List of configuration names which should be searched for.\n\n        :type max_records: int\n        :param max_records: Maximum amount of configurations to return.\n\n        :type next_token: str\n        :param next_token: If you have more results than can be returned\n            at once, pass in this  parameter to page through all results.\n\n        :rtype: list\n        :returns: List of\n            :class:`boto.ec2.autoscale.launchconfig.LaunchConfiguration`\n            instances.\n        \"\"\"\n        params = {}\n        max_records = kwargs.get('max_records', None)\n        names = kwargs.get('names', None)\n        if max_records is not None:\n            params['MaxRecords'] = max_records\n        if names:\n            self.build_list_params(params, names, 'LaunchConfigurationNames')\n        next_token = kwargs.get('next_token')\n        if next_token:\n            params['NextToken'] = next_token\n        return self.get_list('DescribeLaunchConfigurations', params,\n                             [('member', LaunchConfiguration)])\n\n    def get_all_activities(self, autoscale_group, activity_ids=None,\n                           max_records=None, next_token=None):\n        \"\"\"\n        Get all activities for the given autoscaling group.\n\n        This action supports pagination by returning a token if there are more\n        pages to retrieve. To get the next page, call this action again with\n        the returned token as the NextToken parameter\n\n        :type autoscale_group: str or\n            :class:`boto.ec2.autoscale.group.AutoScalingGroup` object\n        :param autoscale_group: The auto scaling group to get activities on.\n\n        :type max_records: int\n        :param max_records: Maximum amount of activities to return.\n\n        :rtype: list\n        :returns: List of\n            :class:`boto.ec2.autoscale.activity.Activity` instances.\n        \"\"\"\n        name = autoscale_group\n        if isinstance(autoscale_group, AutoScalingGroup):\n            name = autoscale_group.name\n        params = {'AutoScalingGroupName': name}\n        if max_records:\n            params['MaxRecords'] = max_records\n        if next_token:\n            params['NextToken'] = next_token\n        if activity_ids:\n            self.build_list_params(params, activity_ids, 'ActivityIds')\n        return self.get_list('DescribeScalingActivities',\n                             params, [('member', Activity)])\n\n    def get_termination_policies(self):\n        \"\"\"Gets all valid termination policies.\n\n        These values can then be used as the termination_policies arg\n        when creating and updating autoscale groups.\n        \"\"\"\n        return self.get_object('DescribeTerminationPolicyTypes',\n                               {}, TerminationPolicies)\n\n    def delete_scheduled_action(self, scheduled_action_name,\n                                autoscale_group=None):\n        \"\"\"\n        Deletes a previously scheduled action.\n\n        :type scheduled_action_name: str\n        :param scheduled_action_name: The name of the action you want\n            to delete.\n\n        :type autoscale_group: str\n        :param autoscale_group: The name of the autoscale group.\n        \"\"\"\n        params = {'ScheduledActionName': scheduled_action_name}\n        if autoscale_group:\n            params['AutoScalingGroupName'] = autoscale_group\n        return self.get_status('DeleteScheduledAction', params)\n\n    def terminate_instance(self, instance_id, decrement_capacity=True):\n        \"\"\"\n        Terminates the specified instance. The desired group size can\n        also be adjusted, if desired.\n\n        :type instance_id: str\n        :param instance_id: The ID of the instance to be terminated.\n\n        :type decrement_capability: bool\n        :param decrement_capacity: Whether to decrement the size of the\n            autoscaling group or not.\n        \"\"\"\n        params = {'InstanceId': instance_id}\n        if decrement_capacity:\n            params['ShouldDecrementDesiredCapacity'] = 'true'\n        else:\n            params['ShouldDecrementDesiredCapacity'] = 'false'\n        return self.get_object('TerminateInstanceInAutoScalingGroup', params,\n                               Activity)\n\n    def delete_policy(self, policy_name, autoscale_group=None):\n        \"\"\"\n        Delete a policy.\n\n        :type policy_name: str\n        :param policy_name: The name or ARN of the policy to delete.\n\n        :type autoscale_group: str\n        :param autoscale_group: The name of the autoscale group.\n        \"\"\"\n        params = {'PolicyName': policy_name}\n        if autoscale_group:\n            params['AutoScalingGroupName'] = autoscale_group\n        return self.get_status('DeletePolicy', params)\n\n    def get_all_adjustment_types(self):\n        return self.get_list('DescribeAdjustmentTypes', {},\n                             [('member', AdjustmentType)])\n\n    def get_all_autoscaling_instances(self, instance_ids=None,\n                                      max_records=None, next_token=None):\n        \"\"\"\n        Returns a description of each Auto Scaling instance in the instance_ids\n        list. If a list is not provided, the service returns the full details\n        of all instances up to a maximum of fifty.\n\n        This action supports pagination by returning a token if there are more\n        pages to retrieve. To get the next page, call this action again with\n        the returned token as the NextToken parameter.\n\n        :type instance_ids: list\n        :param instance_ids: List of Autoscaling Instance IDs which should be\n            searched for.\n\n        :type max_records: int\n        :param max_records: Maximum number of results to return.\n\n        :rtype: list\n        :returns: List of\n            :class:`boto.ec2.autoscale.instance.Instance` objects.\n        \"\"\"\n        params = {}\n        if instance_ids:\n            self.build_list_params(params, instance_ids, 'InstanceIds')\n        if max_records:\n            params['MaxRecords'] = max_records\n        if next_token:\n            params['NextToken'] = next_token\n        return self.get_list('DescribeAutoScalingInstances',\n                             params, [('member', Instance)])\n\n    def get_all_metric_collection_types(self):\n        \"\"\"\n        Returns a list of metrics and a corresponding list of granularities\n        for each metric.\n        \"\"\"\n        return self.get_object('DescribeMetricCollectionTypes',\n                               {}, MetricCollectionTypes)\n\n    def get_all_policies(self, as_group=None, policy_names=None,\n                         max_records=None, next_token=None):\n        \"\"\"\n        Returns descriptions of what each policy does. This action supports\n        pagination. If the response includes a token, there are more records\n        available. To get the additional records, repeat the request with the\n        response token as the NextToken parameter.\n\n        If no group name or list of policy names are provided, all\n        available policies are returned.\n\n        :type as_group: str\n        :param as_group: The name of the\n            :class:`boto.ec2.autoscale.group.AutoScalingGroup` to filter for.\n\n        :type policy_names: list\n        :param policy_names: List of policy names which should be searched for.\n\n        :type max_records: int\n        :param max_records: Maximum amount of groups to return.\n\n        :type next_token: str\n        :param next_token: If you have more results than can be returned\n            at once, pass in this  parameter to page through all results.\n        \"\"\"\n        params = {}\n        if as_group:\n            params['AutoScalingGroupName'] = as_group\n        if policy_names:\n            self.build_list_params(params, policy_names, 'PolicyNames')\n        if max_records:\n            params['MaxRecords'] = max_records\n        if next_token:\n            params['NextToken'] = next_token\n        return self.get_list('DescribePolicies', params,\n                             [('member', ScalingPolicy)])\n\n    def get_all_scaling_process_types(self):\n        \"\"\"\n        Returns scaling process types for use in the ResumeProcesses and\n        SuspendProcesses actions.\n        \"\"\"\n        return self.get_list('DescribeScalingProcessTypes', {},\n                             [('member', ProcessType)])\n\n    def suspend_processes(self, as_group, scaling_processes=None):\n        \"\"\"\n        Suspends Auto Scaling processes for an Auto Scaling group.\n\n        :type as_group: string\n        :param as_group: The auto scaling group to suspend processes on.\n\n        :type scaling_processes: list\n        :param scaling_processes: Processes you want to suspend. If omitted,\n            all processes will be suspended.\n        \"\"\"\n        params = {'AutoScalingGroupName': as_group}\n        if scaling_processes:\n            self.build_list_params(params, scaling_processes,\n                                   'ScalingProcesses')\n        return self.get_status('SuspendProcesses', params)\n\n    def resume_processes(self, as_group, scaling_processes=None):\n        \"\"\"\n        Resumes Auto Scaling processes for an Auto Scaling group.\n\n        :type as_group: string\n        :param as_group: The auto scaling group to resume processes on.\n\n        :type scaling_processes: list\n        :param scaling_processes: Processes you want to resume. If omitted, all\n            processes will be resumed.\n        \"\"\"\n        params = {'AutoScalingGroupName': as_group}\n\n        if scaling_processes:\n            self.build_list_params(params, scaling_processes,\n                                   'ScalingProcesses')\n        return self.get_status('ResumeProcesses', params)\n\n    def create_scheduled_group_action(self, as_group, name, time=None,\n                                      desired_capacity=None,\n                                      min_size=None, max_size=None,\n                                      start_time=None, end_time=None,\n                                      recurrence=None):\n        \"\"\"\n        Creates a scheduled scaling action for a Auto Scaling group. If you\n        leave a parameter unspecified, the corresponding value remains\n        unchanged in the affected Auto Scaling group.\n\n        :type as_group: string\n        :param as_group: The auto scaling group to get activities on.\n\n        :type name: string\n        :param name: Scheduled action name.\n\n        :type time: datetime.datetime\n        :param time: The time for this action to start. (Depracated)\n\n        :type desired_capacity: int\n        :param desired_capacity: The number of EC2 instances that should\n            be running in this group.\n\n        :type min_size: int\n        :param min_size: The minimum size for the new auto scaling group.\n\n        :type max_size: int\n        :param max_size: The minimum size for the new auto scaling group.\n\n        :type start_time: datetime.datetime\n        :param start_time: The time for this action to start. When StartTime and EndTime are specified with Recurrence, they form the boundaries of when the recurring action will start and stop.\n\n        :type end_time: datetime.datetime\n        :param end_time: The time for this action to end. When StartTime and EndTime are specified with Recurrence, they form the boundaries of when the recurring action will start and stop.\n\n        :type recurrence: string\n        :param recurrence: The time when recurring future actions will start. Start time is specified by the user following the Unix cron syntax format. EXAMPLE: '0 10 * * *'\n        \"\"\"\n        params = {'AutoScalingGroupName': as_group,\n                  'ScheduledActionName': name}\n        if start_time is not None:\n            params['StartTime'] = start_time.isoformat()\n        if end_time is not None:\n            params['EndTime'] = end_time.isoformat()\n        if recurrence is not None:\n            params['Recurrence'] = recurrence\n        if time:\n            params['Time'] = time.isoformat()\n        if desired_capacity is not None:\n            params['DesiredCapacity'] = desired_capacity\n        if min_size is not None:\n            params['MinSize'] = min_size\n        if max_size is not None:\n            params['MaxSize'] = max_size\n        return self.get_status('PutScheduledUpdateGroupAction', params)\n\n    def get_all_scheduled_actions(self, as_group=None, start_time=None,\n                                  end_time=None, scheduled_actions=None,\n                                  max_records=None, next_token=None):\n        params = {}\n        if as_group:\n            params['AutoScalingGroupName'] = as_group\n        if scheduled_actions:\n            self.build_list_params(params, scheduled_actions,\n                                   'ScheduledActionNames')\n        if max_records:\n            params['MaxRecords'] = max_records\n        if next_token:\n            params['NextToken'] = next_token\n        return self.get_list('DescribeScheduledActions', params,\n                             [('member', ScheduledUpdateGroupAction)])\n\n    def disable_metrics_collection(self, as_group, metrics=None):\n        \"\"\"\n        Disables monitoring of group metrics for the Auto Scaling group\n        specified in AutoScalingGroupName. You can specify the list of affected\n        metrics with the Metrics parameter.\n        \"\"\"\n        params = {'AutoScalingGroupName': as_group}\n\n        if metrics:\n            self.build_list_params(params, metrics, 'Metrics')\n        return self.get_status('DisableMetricsCollection', params)\n\n    def enable_metrics_collection(self, as_group, granularity, metrics=None):\n        \"\"\"\n        Enables monitoring of group metrics for the Auto Scaling group\n        specified in AutoScalingGroupName. You can specify the list of enabled\n        metrics with the Metrics parameter.\n\n        Auto scaling metrics collection can be turned on only if the\n        InstanceMonitoring.Enabled flag, in the Auto Scaling group's launch\n        configuration, is set to true.\n\n        :type autoscale_group: string\n        :param autoscale_group: The auto scaling group to get activities on.\n\n        :type granularity: string\n        :param granularity: The granularity to associate with the metrics to\n            collect. Currently, the only legal granularity is \"1Minute\".\n\n        :type metrics: string list\n        :param metrics: The list of metrics to collect. If no metrics are\n                        specified, all metrics are enabled.\n        \"\"\"\n        params = {'AutoScalingGroupName': as_group,\n                  'Granularity': granularity}\n        if metrics:\n            self.build_list_params(params, metrics, 'Metrics')\n        return self.get_status('EnableMetricsCollection', params)\n\n    def execute_policy(self, policy_name, as_group=None, honor_cooldown=None):\n        params = {'PolicyName': policy_name}\n        if as_group:\n            params['AutoScalingGroupName'] = as_group\n        if honor_cooldown:\n            params['HonorCooldown'] = honor_cooldown\n        return self.get_status('ExecutePolicy', params)\n\n    def put_notification_configuration(self, autoscale_group, topic, notification_types):\n        \"\"\"\n        Configures an Auto Scaling group to send notifications when\n        specified events take place.\n\n        :type autoscale_group: str or\n            :class:`boto.ec2.autoscale.group.AutoScalingGroup` object\n        :param autoscale_group: The Auto Scaling group to put notification\n            configuration on.\n\n        :type topic: str\n        :param topic: The Amazon Resource Name (ARN) of the Amazon Simple\n            Notification Service (SNS) topic.\n\n        :type notification_types: list\n        :param notification_types: The type of events that will trigger\n            the notification. Valid types are:\n            'autoscaling:EC2_INSTANCE_LAUNCH',\n            'autoscaling:EC2_INSTANCE_LAUNCH_ERROR',\n            'autoscaling:EC2_INSTANCE_TERMINATE',\n            'autoscaling:EC2_INSTANCE_TERMINATE_ERROR',\n            'autoscaling:TEST_NOTIFICATION'\n        \"\"\"\n\n        name = autoscale_group\n        if isinstance(autoscale_group, AutoScalingGroup):\n            name = autoscale_group.name\n\n        params = {'AutoScalingGroupName': name,\n                  'TopicARN': topic}\n        self.build_list_params(params, notification_types, 'NotificationTypes')\n        return self.get_status('PutNotificationConfiguration', params)\n\n    def delete_notification_configuration(self, autoscale_group, topic):\n        \"\"\"\n        Deletes notifications created by put_notification_configuration.\n\n        :type autoscale_group: str or\n            :class:`boto.ec2.autoscale.group.AutoScalingGroup` object\n        :param autoscale_group: The Auto Scaling group to put notification\n            configuration on.\n\n        :type topic: str\n        :param topic: The Amazon Resource Name (ARN) of the Amazon Simple\n            Notification Service (SNS) topic.\n        \"\"\"\n\n        name = autoscale_group\n        if isinstance(autoscale_group, AutoScalingGroup):\n            name = autoscale_group.name\n\n        params = {'AutoScalingGroupName': name,\n                  'TopicARN': topic}\n\n        return self.get_status('DeleteNotificationConfiguration', params)\n\n    def set_instance_health(self, instance_id, health_status,\n                            should_respect_grace_period=True):\n        \"\"\"\n        Explicitly set the health status of an instance.\n\n        :type instance_id: str\n        :param instance_id: The identifier of the EC2 instance.\n\n        :type health_status: str\n        :param health_status: The health status of the instance.\n            \"Healthy\" means that the instance is healthy and should remain\n            in service. \"Unhealthy\" means that the instance is unhealthy.\n            Auto Scaling should terminate and replace it.\n\n        :type should_respect_grace_period: bool\n        :param should_respect_grace_period: If True, this call should\n            respect the grace period associated with the group.\n        \"\"\"\n        params = {'InstanceId': instance_id,\n                  'HealthStatus': health_status}\n        if should_respect_grace_period:\n            params['ShouldRespectGracePeriod'] = 'true'\n        else:\n            params['ShouldRespectGracePeriod'] = 'false'\n        return self.get_status('SetInstanceHealth', params)\n\n    def set_desired_capacity(self, group_name, desired_capacity, honor_cooldown=False):\n        \"\"\"\n        Adjusts the desired size of the AutoScalingGroup by initiating scaling\n        activities. When reducing the size of the group, it is not possible to define\n        which Amazon EC2 instances will be terminated. This applies to any Auto Scaling\n        decisions that might result in terminating instances.\n\n        :type group_name: string\n        :param group_name: name of the auto scaling group\n\n        :type desired_capacity: integer\n        :param desired_capacity: new capacity setting for auto scaling group\n\n        :type honor_cooldown: boolean\n        :param honor_cooldown: by default, overrides any cooldown period\n        \"\"\"\n        params = {'AutoScalingGroupName': group_name,\n                  'DesiredCapacity': desired_capacity}\n        if honor_cooldown:\n            params['HonorCooldown'] = 'true'\n\n        return self.get_status('SetDesiredCapacity', params)\n\n    # Tag methods\n\n    def get_all_tags(self, filters=None, max_records=None, next_token=None):\n        \"\"\"\n        Lists the Auto Scaling group tags.\n\n        This action supports pagination by returning a token if there\n        are more pages to retrieve. To get the next page, call this\n        action again with the returned token as the NextToken\n        parameter.\n\n        :type filters: dict\n        :param filters: The value of the filter type used to identify\n            the tags to be returned.  NOT IMPLEMENTED YET.\n\n        :type max_records: int\n        :param max_records: Maximum number of tags to return.\n\n        :rtype: list\n        :returns: List of :class:`boto.ec2.autoscale.tag.Tag`\n            instances.\n        \"\"\"\n        params = {}\n        if max_records:\n            params['MaxRecords'] = max_records\n        if next_token:\n            params['NextToken'] = next_token\n        return self.get_list('DescribeTags', params,\n                             [('member', Tag)])\n\n    def create_or_update_tags(self, tags):\n        \"\"\"\n        Creates new tags or updates existing tags for an Auto Scaling group.\n\n        :type tags: List of :class:`boto.ec2.autoscale.tag.Tag`\n        :param tags: The new or updated tags.\n        \"\"\"\n        params = {}\n        for i, tag in enumerate(tags):\n            tag.build_params(params, i + 1)\n        return self.get_status('CreateOrUpdateTags', params, verb='POST')\n\n    def delete_tags(self, tags):\n        \"\"\"\n        Deletes existing tags for an Auto Scaling group.\n\n        :type tags: List of :class:`boto.ec2.autoscale.tag.Tag`\n        :param tags: The new or updated tags.\n        \"\"\"\n        params = {}\n        for i, tag in enumerate(tags):\n            tag.build_params(params, i + 1)\n        return self.get_status('DeleteTags', params, verb='POST')\n", "os.path import dirname, join, pardir, abspath, exists\nimport subprocess\n\nimport nose\n\ndef fetch_es_repo():\n    # user is manually setting YAML dir, don't tamper with it\n    if 'TEST_ES_YAML_DIR' in environ:\n        return\n\n    repo_path = environ.get(\n        'TEST_ES_REPO',\n        abspath(join(dirname(__file__), pardir, pardir, 'elasticsearch'))\n    )\n\n    # no repo\n    if not exists(repo_path) or not exists(join(repo_path, '.git')):\n        print('No elasticsearch repo found...')\n        # set YAML DIR to empty to skip yaml tests\n        environ['TEST_ES_YAML_DIR'] = ''\n        return\n\n    # set YAML test dir\n    environ['TEST_ES_YAML_DIR'] = join(repo_path, 'rest-api-spec', 'test')\n\n    # fetching of yaml tests disabled, we'll run with what's there\n    if environ.get('TEST_ES_NOFETCH', False):\n        return\n\n    from test_elasticsearch.test_server import get_client\n    from test_elasticsearch.test_cases import SkipTest\n\n    # find out the sha of the running es\n    try:\n        es = get_client()\n        sha = es.info()['version']['build_hash']\n    except (SkipTest, KeyError):\n        print('No running elasticsearch >1.X server...')\n        return\n\n    # fetch new commits to be sure...\n    print('Fetching elasticsearch repo...')\n    subprocess.check_call('cd %s && git fetch https://github.com/elasticsearch/elasticsearch.git' % repo_path, shell=True)\n    # reset to the version fron info()\n    subprocess.check_call('cd %s && git reset --hard %s' % (repo_path, sha), shell=True)\n\ndef run_all(argv=None):\n    sys.exitfunc = lambda: sys.stderr.write('Shutting down....\\n')\n\n    # fetch yaml tests\n    fetch_es_repo()\n\n    # always insert coverage when running tests\n    if argv is None:\n        argv = [\n            'nosetests', '--with-xunit',\n            '--with-xcoverage', '--cover-package=elasticsearch', '--cover-erase',\n            '--logging-filter=elasticsearch', '--logging-level=DEBUG',\n            '--verbose',\n        ]\n\n    nose.run_exit(\n        argv=argv,\n        defaultTest=abspath(dirname(__file__))\n    )\n\nif __name__ == '__main__':\n    run_all(sys.argv)\n\n", "ng Python2.7 but to get it working with Python2.6 and\n# before (as well as Python 3.0) require that you number the placeholders in the format method().\n# This way wherever the {} is used, number it starting from 0. e.g., {0}.nova.hypervisor\n\n# #RED\nfrom argparse import ArgumentParser\nimport socket\nimport time\nimport os\n\nfrom novaclient.client import Client\n\nDEFAULT_SCHEME = '{}.nova.hypervisors'.format(socket.gethostname())\n\nMETRIC_KEYS = (\n    'current_workload',\n    'disk_available_least',\n    'local_gb',\n    'local_gb_used',\n    'memory_mb',\n    'memory_mb_used',\n    'running_vms',\n    'vcpus',\n    'vcpus_used',\n)\n\ndef output_metric(name, value):\n    print '{}\\t{}\\t{}'.format(name, value, int(time.time()))\n\ndef main():\n    parser = ArgumentParser()\n    parser.add_argument('-u', '--user', default=os.environ['OS_USERNAME'])\n    parser.add_argument('-p', '--password', default=os.environ['OS_PASSWORD'])\n    parser.add_argument('-t', '--tenant', default=os.environ['OS_TENANT_NAME'])\n    parser.add_argument('-a', '--auth-url', default=os.environ['OS_AUTH_URL'])\n    parser.add_argument('-S', '--service-type', default='compute')\n    parser.add_argument('-H', '--host')\n    parser.add_argument('-s', '--scheme', default=DEFAULT_SCHEME)\n    args = parser.parse_args()\n\n    args.user\n\n    client = Client(version=2, username=args.user, api_key=args.password,\n                    project_id=args.tenant, auth_url=args.auth_url,\n                    service_type=args.service_type)\n\n    if args.host:\n        hypervisors = client.hypervisors.search(args.host)\n    else:\n        hypervisors = client.hypervisors.list()\n\n    for hv in hypervisors:\n        hostname = hv.hypervisor_hostname.split('.')[0]\n        for key, value in hv.to_dict().iteritems():\n            if key in METRIC_KEYS:\n                output_metric('{}.{}.{}'.format(args.scheme, hostname, key), value)\n\nif __name__ == '__main__':\n    main()\n", "def nsp(super_client, sim_context):\n    nsp = create_agent_instance_nsp(super_client, sim_context)\n    create_and_activate(super_client, 'networkService',\n                        networkServiceProviderId=nsp.id,\n                        networkId=nsp.networkId)\n\n    return nsp\n\n\ndef random_str():\n    return 'random{0}'.format(random_num())\n\n\ndef create_env_and_svc(super_client, admin_client, nsp):\n    env = admin_client.create_environment(name=random_str())\n    env = admin_client.wait_success(env)\n    assert env.state == \"active\"\n\n    image_uuid = sim_context['imageUuid']\n    launch_config = {\"imageUuid\": image_uuid}\n\n    service = super_client.create_service(name=random_str(),\n                                          environmentId=env.id,\n                                          networkId=nsp.networkId,\n                                          launchConfig=launch_config)\n    service = super_client.wait_success(service)\n    assert service.state == \"inactive\"\n    return service, env\n\n\ndef test_activate_single_service(super_client, admin_client, sim_context, nsp):\n    env = admin_client.create_environment(name=random_str())\n    env = admin_client.wait_success(env)\n    assert env.state == \"active\"\n\n    image_uuid = sim_context['imageUuid']\n    host = sim_context['host']\n    container1 = admin_client.create_container(imageUuid=image_uuid,\n                                               startOnCreate=False)\n    container1 = admin_client.wait_success(container1)\n\n    container2 = admin_client.create_container(imageUuid=image_uuid,\n                                               startOnCreate=False)\n    container2 = admin_client.wait_success(container2)\n\n    caps = [\"SYS_MODULE\"]\n\n    restart_policy = {\"maximumRetryCount\": 2, \"name\": \"on-failure\"}\n\n    dns = ['8.8.8.8', '1.2.3.4']\n\n    launch_config = {\"imageUuid\": image_uuid}\n\n    consumed_service = super_client.create_service(name=random_str(),\n                                                   environmentId=env.id,\n                                                   networkId=nsp.networkId,\n                                                   launchConfig=launch_config)\n    consumed_service = super_client.wait_success(consumed_service)\n\n    reg_cred = _create_registry_credential(admin_client)\n\n    launch_config = {\"imageUuid\": image_uuid,\n                     \"command\": ['sleep', '42'],\n                     \"environment\": {'TEST_FILE': \"/etc/testpath.conf\"},\n                     \"ports\": ['8081', '8082/tcp'],\n                     \"dataVolumes\": ['/foo'],\n                     \"dataVolumesFrom\": [container1.id],\n                     \"capAdd\": caps,\n                     \"capDrop\": caps,\n                     \"dnsSearch\": dns,\n                     \"dns\": dns,\n                     \"privileged\": True,\n                     \"domainName\": \"rancher.io\",\n                     \"memory\": 8000000,\n                     \"stdinOpen\": True,\n                     \"tty\": True,\n                     \"entryPoint\": [\"/bin/sh\", \"-c\"],\n                     \"cpuShares\": 400,\n                     \"cpuSet\": \"2\",\n                     \"restartPolicy\": restart_policy,\n                     \"directory\": \"/\",\n                     \"hostname\": \"test\",\n                     \"user\": \"test\",\n                     \"instanceLinks\": {\n                         'container2_link':\n                             container2.id},\n                     \"registryCredentialId\": reg_cred.id,\n                     \"requestedHostId\": host.id}\n\n    service = super_client.create_service(name=random_str(),\n                                          environmentId=env.id,\n                                          networkId=nsp.networkId,\n                                          launchConfig=launch_config)\n    service = super_client.wait_success(service)\n\n    # validate that parameters were set for service\n    assert service.state == \"inactive\"\n    assert service.launchConfig.imageUuid == image_uuid\n    assert service.launchConfig.command == ['sleep', '42']\n    assert len(service.launchConfig.environment) == 1\n    assert len(service.launchConfig.ports) == 2\n    assert len(service.launchConfig.dataVolumes) == 1\n    # assert set(service.launchConfig.dataVolumesFrom) == set([container1.id])\n    assert service.launchConfig.capAdd == caps\n    assert service.launchConfig.capDrop == caps\n    assert service.launchConfig.dns == dns\n    assert service.launchConfig.dnsSearch == dns\n    assert service.launchConfig.privileged is True\n    assert service.launchConfig.domainName == \"rancher.io\"\n    assert service.launchConfig.memory == 8000000\n    assert service.launchConfig.stdinOpen is True\n    assert service.launchConfig.tty is True\n    assert service.launchConfig.entryPoint == [\"/bin/sh\", \"-c\"]\n    assert service.launchConfig.cpuShares == 400\n    assert service.launchConfig.restartPolicy == restart_policy\n    assert service.launchConfig.directory == \"/\"\n    assert service.launchConfig.hostname == \"test\"\n    assert service.launchConfig.user == \"test\"\n    assert len(service.launchConfig.instanceLinks) == 1\n    assert service.kind == \"service\"\n    # assert service.launchConfig.registryCredentialId == reg_cred.id\n\n    # activate the service and validate that parameters were set for instance\n    service = wait_success(super_client, service.activate(), 120)\n    assert service.state == \"active\"\n    instance_service_map = super_client. \\\n        list_serviceExposeMap(serviceId=service.id)\n\n    assert len(instance_service_map) == 1\n    wait_for_condition(\n        super_client, instance_service_map[0], _resource_is_active,\n        lambda x: 'State is: ' + x.state)\n\n    instances = super_client. \\\n        list_container(name=env.name + \"_\" + service.name + \"_\" + \"1\")\n    assert len(instances) == 1\n    container = instances[0]\n    assert container.imageUuid == image_uuid\n    assert container.command == ['sleep', '42']\n    assert len(container.instanceLinks()) == 1\n    assert len(container.environment) == 1\n    assert len(container.ports()) == 2\n    assert len(container.dataVolumes) == 1\n    assert set(container.dataVolumesFrom) == set([container1.id])\n    assert container.capAdd == caps\n    assert container.capDrop == caps\n    assert container.dns == dns\n    assert container.dnsSearch == dns\n    assert container.privileged is True\n    assert container.domainName == \"rancher.io\"\n    assert container.memory == 8000000\n    assert container.stdinOpen is True\n    assert container.tty is True\n    assert container.entryPoint == [\"/bin/sh\", \"-c\"]\n    assert container.cpuShares == 400\n    assert container.restartPolicy == restart_policy\n    assert container.directory == \"/\"\n    assert container.hostname == \"test\"\n    assert container.user == \"test\"\n    assert container.state == \"running\"\n    assert container.registryCredentialId == reg_cred.id\n    assert container.cpuSet == \"2\"\n    assert container.requestedHostId == host.id\n\n\ndef test_activate_services(super_client, admin_client, sim_context, nsp):\n    env = admin_client.create_environment(name=random_str())\n    env = admin_client.wait_success(env)\n    assert env.state == \"active\"\n\n    image_uuid = sim_context['imageUuid']\n    launch_config = {\"imageUuid\": image_uuid}\n\n    service1 = super_client.create_service(name=random_str(),\n                                           environmentId=env.id,\n                                           networkId=nsp.networkId,\n                                           launchConfig=launch_config)\n    service1 = super_client.wait_success(service1)\n    assert service1.state == \"inactive\"\n\n    service2 = super_client.create_service(name=random_str(),\n                                           environmentId=env.id,\n                                           networkId=nsp.networkId,\n                                           launchConfig=launch_config)\n    service2 = super_client.wait_success(service2)\n    assert service2.state == \"inactive\"\n\n    env = env.activateservices()\n    service1 = super_client.wait_success(service1, 120)\n    service2 = super_client.wait_success(service2, 120)\n    assert service1.state == \"active\"\n    assert service2.state == \"active\"\n\n\ndef _validate_instance_stopped(service, super_client, env):\n    instances = super_client. \\\n        list_container(name=env.name + \"_\" + service.name + \"_\" + \"1\")\n    assert len(instances) == 1\n    instance = instances[0]\n    wait_for_condition(\n        super_client, instance, _resource_is_stopped,\n        lambda x: 'State is: ' + x.state)\n\n\ndef _validate_compose_instance_removed(super_client, service, env, number=\"1\"):\n    instances = super_client. \\\n        list_container(name=env.name + \"_\" + service.name + \"_\" + number)\n    assert len(instances) == 1\n    instance = instances[0]\n    wait_for_condition(\n        super_client, instance, _resource_is_removed,\n        lambda x: 'State is: ' + x.state)\n\n\ndef _validate_instance_removed(super_client, service, name):\n    instances = super_client. \\\n        list_container(name=name)\n    assert len(instances) == 1\n    instance = instances[0]\n    wait_for_condition(\n        super_client, instance, _resource_is_removed,\n        lambda x: 'State is: ' + x.state)\n\n\ndef test_deactivate_remove_service(super_client, admin_client,\n                                   sim_context, nsp):\n    env = admin_client.create_environment(name=random_str())\n    env = admin_client.wait_success(env)\n    assert env.state == \"active\"\n\n    image_uuid = sim_context['imageUuid']\n    launch_config = {\"imageUuid\": image_uuid}\n\n    service = super_client.create_service(name=random_str(),\n                                          environmentId=env.id,\n                                          networkId=nsp.networkId,\n                                          launchConfig=launch_config)\n    service = super_client.wait_success(service)\n    assert service.state == \"inactive\"\n    service = wait_success(super_client, service.activate(), 120)\n    assert service.state == \"active\"\n    instance_service_map = super_client. \\\n        list_serviceExposeMap(serviceId=service.id)\n\n    assert len(instance_service_map) == 1\n    wait_for_condition(\n        super_client, instance_service_map[0], _resource_is_active,\n        lambda x: 'State is: ' + x.state)\n\n    _validate_compose_instance_start(super_client, service, env, \"1\")\n\n    # deactivate service\n    service = wait_success(super_client, service.deactivate())\n    assert service.state == \"inactive\"\n    _validate_instance_stopped(service, super_client, env)\n\n    # remove service\n    service = wait_success(super_client, service.remove())\n    _validate_compose_instance_removed(super_client, service, env)\n\n\ndef test_env_deactivate_services(super_client, admin_client, sim_context, nsp):\n    env = admin_client.create_environment(name=random_str())\n    env = admin_client.wait_success(env)\n    assert env.state == \"active\"\n\n    image_uuid = sim_context['imageUuid']\n    launch_config = {\"imageUuid\": image_uuid}\n\n    service1 = super_client.create_service(name=random_str(),\n                                           environmentId=env.id,\n                                           networkId=nsp.networkId,\n                                           launchConfig=launch_config)\n    service1 = super_client.wait_success(service1)\n    assert service1.state == \"inactive\"\n\n    service2 = super_client.create_service(name=random_str(),\n                                           environmentId=env.id,\n                                           networkId=nsp.networkId,\n                                           launchConfig=launch_config)\n    service2 = super_client.wait_success(service2)\n    assert service2.state == \"inactive\"\n\n    # activate services\n    env = env.activateservices()\n    service1 = super_client.wait_success(service1, 120)\n    service2 = super_client.wait_success(service2, 120)\n    assert service1.state == \"active\"\n    assert service2.state == \"active\"\n\n    # deactivate services\n    env.deactivateservices()\n    service1 = super_client.wait_success(service1)\n    service2 = super_client.wait_success(service2)\n    assert service1.state == \"inactive\"\n    assert service2.state == \"inactive\"\n    _validate_instance_stopped(service1, super_client, env)\n    _validate_instance_stopped(service2, super_client, env)\n\n\ndef test_remove_inactive_service(super_client, admin_client, sim_context, nsp):\n    env = admin_client.create_environment(name=random_str())\n    env = admin_client.wait_success(env)\n    assert env.state == \"active\"\n\n    image_uuid = sim_context['imageUuid']\n    launch_config = {\"imageUuid\": image_uuid}\n\n    service = super_client.create_service(name=random_str(),\n                                          environmentId=env.id,\n                                          networkId=nsp.networkId,\n                                          launchConfig=launch_config)\n    service = super_client.wait_success(service)\n    assert service.state == \"inactive\"\n\n    # activate service\n    service = wait_success(super_client, service.activate(), 120)\n    assert service.state == \"active\"\n    instance_service_map = super_client. \\\n        list_serviceExposeMap(serviceId=service.id)\n\n    assert len(instance_service_map) == 1\n    wait_for_condition(\n        super_client, instance_service_map[0], _resource_is_active,\n        lambda x: 'State is: ' + x.state)\n\n    _validate_compose_instance_start(super_client, service, env, \"1\")\n\n    # deactivate service\n    service = wait_success(super_client, service.deactivate())\n    assert service.state == \"inactive\"\n\n    # remove service\n    service = wait_success(super_client, service.remove())\n    assert service.state == \"removed\"\n    _validate_compose_instance_removed(super_client, service, env)\n\n\ndef test_remove_environment(super_client, admin_client, sim_context, nsp):\n    env = admin_client.create_environment(name=random_str())\n    env = admin_client.wait_success(env)\n    assert env.state == \"active\"\n\n    image_uuid = sim_context['imageUuid']\n    launch_config = {\"imageUuid\": image_uuid}\n\n    service = super_client.create_service(name=random_str(),\n                                          environmentId=env.id,\n                                          networkId=nsp.networkId,\n                                          launchConfig=launch_config)\n    service = super_client.wait_success(service)\n    assert service.state == \"inactive\"\n\n    # activate services\n    env = env.activateservices()\n    service = super_client.wait_success(service, 120)\n    assert service.state == \"active\"\n    instance_service_map = super_client. \\\n        list_serviceExposeMap(serviceId=service.id)\n\n    assert len(instance_service_map) == 1\n    wait_for_condition(\n        super_client, instance_service_map[0], _resource_is_active,\n        lambda x: 'State is: ' + x.state)\n\n    _validate_compose_instance_start(super_client, service, env, \"1\")\n\n    # deactivate services\n    env = env.deactivateservices()\n    service = super_client.wait_success(service)\n    assert service.state == \"inactive\"\n\n    # remove environment\n    env = wait_success(admin_client, env.remove())\n    assert env.state == \"removed\"\n    wait_for_condition(\n        super_client, service, _resource_is_removed,\n        lambda x: 'State is: ' + x.state)\n\n\ndef test_create_duplicated_services(super_client, admin_client,\n                                    sim_context, nsp):\n    env = admin_client.create_environment(name=random_str())\n    env = admin_client.wait_success(env)\n    assert env.state == \"active\"\n\n    image_uuid = sim_context['imageUuid']\n    launch_config = {\"imageUuid\": image_uuid}\n    service_name = random_str()\n    service1 = super_client.create_service(name=service_name,\n                                           environmentId=env.id,\n                                           networkId=nsp.networkId,\n                                           launchConfig=launch_config)\n    super_client.wait_success(service1)\n\n    with pytest.raises(ApiError) as e:\n        super_client.create_service(name=service_name,\n                                    environmentId=env.id,\n                                    networkId=nsp.networkId,\n                                    launchConfig=launch_config)\n    assert e.value.error.status == 422\n    assert e.value.error.code == 'NotUnique'\n    assert e.value.error.fieldName == 'name'\n\n\ndef test_service_add_remove_service_link(super_client, admin_client,\n                                         sim_context, nsp):\n    env = admin_client.create_environment(name=random_str())\n    env = admin_client.wait_success(env)\n    assert env.state == \"active\"\n\n    image_uuid = sim_context['imageUuid']\n    launch_config = {\"imageUuid\": image_uuid}\n\n    service1 = super_client.create_service(name=random_str(),\n                                           environmentId=env.id,\n                                           networkId=nsp.networkId,\n                                           launchConfig=launch_config)\n    service1 = super_client.wait_success(service1)\n\n    service2 = super_client.create_service(name=random_str(),\n                                           environmentId=env.id,\n                                           networkId=nsp.networkId,\n                                           launchConfig=launch_config)\n    service2 = super_client.wait_success(service2)\n\n    # link service2 to service1\n    service1 = service1.addservicelink(serviceId=service2.id)\n    _validate_add_service_link(service1, service2, super_client)\n\n    # remove service link\n    service1 = service1.removeservicelink(serviceId=service2.id)\n    _validate_remove_service_link(service1, service2, super_client)\n\n\ndef test_link_service_twice(super_client, admin_client,\n                            sim_context, nsp):\n    env = admin_client.create_environment(name=random_str())\n    env = admin_client.wait_success(env)\n    assert env.state == \"active\"\n\n    image_uuid = sim_context['imageUuid']\n    launch_config = {\"imageUuid\": image_uuid}\n\n    service1 = super_client.create_service(name=random_str(),\n                                           environmentId=env.id,\n                                           networkId=nsp.networkId,\n                                           launchConfig=launch_config)\n    service1 = super_client.wait_success(service1)\n\n    service2 = super_client.create_service(name=random_str(),\n                                           environmentId=env.id,\n                                           networkId=nsp.networkId,\n                                           launchConfig=launch_config)\n    service2 = super_client.wait_success(service2)\n\n    # link servic2 to service1\n    service1 = service1.addservicelink(serviceId=service2.id)\n    _validate_add_service_link(service1, service2, super_client)\n\n    # try to link again\n    with pytest.raises(ApiError) as e:\n        service1.addservicelink(serviceId=service2.id)\n\n    assert e.value.error.status == 422\n    assert e.value.error.code == 'NotUnique'\n    assert e.value.error.fieldName == 'serviceId'\n\n\ndef test_links_after_service_remove(super_client, admin_client,\n                                    sim_context, nsp):\n    env = admin_client.create_environment(name=random_str())\n    env = admin_client.wait_success(env)\n    assert env.state == \"active\"\n\n    image_uuid = sim_context['imageUuid']\n    launch_config = {\"imageUuid\": image_uuid}\n    service1 = super_client.create_service(name=random_str(),\n                                           environmentId=env.id,\n                                           networkId=nsp.networkId,\n                                           launchConfig=launch_config)\n    service1 = super_client.wait_success(service1)\n\n    service2 = super_client.create_service(name=random_str(),\n                                           environmentId=env.id,\n                                           networkId=nsp.networkId,\n                                           launchConfig=launch_config)\n    service2 = super_client.wait_success(service2)\n\n    # link servic2 to service1\n    service1 = service1.addservicelink(serviceId=service2.id)\n    _validate_add_service_link(service1, service2, super_client)\n\n    # link service1 to service2\n    service2 = service2.addservicelink(serviceId=service1.id)\n    _validate_add_service_link(service2, service1, super_client)\n\n    # remove service1\n    service1 = wait_success(super_client, service1.remove())\n\n    _validate_remove_service_link(service1, service2, super_client)\n\n    _validate_remove_service_link(service2, service1, super_client)\n\n\ndef test_link_volumes(super_client, admin_client,\n                      sim_context, nsp):\n    env = admin_client.create_environment(name=random_str())\n    env = admin_client.wait_success(env)\n    assert env.state == \"active\"\n\n    image_uuid = sim_context['imageUuid']\n    launch_config = {\"imageUuid\": image_uuid,\n                     \"labels\": {'io.rancher.service.sidekick': \"random\"}}\n\n    service1 = super_client.create_service(name=random_str(),\n                                           environmentId=env.id,\n                                           networkId=nsp.networkId,\n                                           launchConfig=launch_config)\n    service1 = super_client.wait_success(service1)\n    service1 = wait_success(super_client, service1.activate(), 120)\n    container1 = _validate_compose_instance_start(super_client,\n                                                  service1, env, \"1\")\n\n    external_container = super_client.create_container(\n        imageUuid=image_uuid,\n        requestedHostId=container1.hosts()[0].id)\n    external_container = super_client.wait_success(external_container)\n\n    launch_config = {\"imageUuid\": image_uuid,\n                     \"dataVolumesFrom\": [external_container.id],\n                     \"labels\": {'io.rancher.service.sidekick': \"random\"}}\n\n    service2 = super_client. \\\n        create_service(name=random_str(),\n                       environmentId=env.id,\n                       networkId=nsp.networkId,\n                       launchConfig=launch_config,\n                       dataVolumesFromService=[service1.id])\n\n    service2 = super_client.wait_success(service2)\n    service2 = wait_success(super_client, service2.activate(), 120)\n    container2 = _validate_compose_instance_start(super_client,\n                                                  service2, env, \"1\")\n\n    # verify that the instance started in service2,\n    # got volume of instance of service1\n    assert len(container2.dataVolumesFrom) == 2\n    assert set(container2.dataVolumesFrom) == set([external_container.id,\n                                                   container1.id])\n\n\ndef test_volumes_service_links_scale_one(super_client, admin_client,\n                                         sim_context, nsp):\n    env = admin_client.create_environment(name=random_str())\n    env = admin_client.wait_success(env)\n    assert env.state == \"active\"\n\n    image_uuid = sim_context['imageUuid']\n    launch_config = {\"imageUuid\": image_uuid,\n                     \"labels\": {'io.rancher.service.sidekick': \"random\"}}\n    service1 = super_client.create_service(name=random_str(),\n                                           environmentId=env.id,\n                                           networkId=nsp.networkId,\n                                           launchConfig=launch_config)\n    service1 = super_client.wait_success(service1)\n\n    launch_config = {\"imageUuid\": image_uuid,\n                     \"labels\": {'io.rancher.service.sidekick': \"random\"}}\n    service2 = super_client. \\\n        create_service(name=random_str(),\n                       environmentId=env.id,\n                       networkId=nsp.networkId,\n                       launchConfig=launch_config,\n                       dataVolumesFromService=[service1.id])\n    service2 = super_client.wait_success(service2)\n\n    service3 = super_client. \\\n        create_service(name=random_str(),\n                       environmentId=env.id,\n                       networkId=nsp.networkId,\n                       launchConfig=launch_config,\n                       dataVolumesFromService=[service1.id, service2.id])\n    service3 = super_client.wait_success(service3)\n\n    service1 = wait_success(super_client, service1.activate(), 120)\n    service2 = super_client.wait_success(service2, 120)\n    service3 = super_client.wait_success(service3, 120)\n\n    assert service1.state == \"active\"\n    assert service3.state == \"active\"\n    assert service2.state == \"active\"\n\n    # 2. validate instances\n    s1_container = _validate_compose_instance_start(super_client,\n                                                    service1, env, \"1\")\n    s2_container = _validate_compose_instance_start(super_client,\n                                                    service2, env, \"1\")\n    s3_container = _validate_compose_instance_start(super_client,\n                                                    service3, env, \"1\")\n\n    assert len(s2_container.dataVolumesFrom) == 1\n    assert set(s2_container.dataVolumesFrom) == set([s1_container.id])\n\n    assert len(s3_container.dataVolumesFrom) == 2\n    assert set(s3_container.dataVolumesFrom) == set([s1_container.id,\n                                                     s2_container.id])\n\n\ndef test_volumes_service_links_scale_two(super_client, admin_client,\n                                         sim_context, nsp):\n    env = admin_client.create_environment(name=random_str())\n    env = admin_client.wait_success(env)\n    assert env.state == \"active\"\n\n    image_uuid = sim_context['imageUuid']\n    launch_config = {\"imageUuid\": image_uuid,\n                     \"labels\": {'io.rancher.service.sidekick': \"random\"}}\n    service1 = super_client.create_service(name=random_str(),\n                                           environmentId=env.id,\n                                           networkId=nsp.networkId,\n                                           launchConfig=launch_config,\n                                           scale=2)\n    service1 = super_client.wait_success(service1)\n\n    launch_config = {\"imageUuid\": image_uuid,\n                     \"labels\": {'io.rancher.service.sidekick': \"random\"}}\n    service2 = super_client. \\\n        create_service(name=random_str(),\n                       environmentId=env.id,\n                       networkId=nsp.networkId,\n                       launchConfig=launch_config,\n                       dataVolumesFromService=[service1.id],\n                       scale=2)\n    service2 = super_client.wait_success(service2)\n\n    service1 = wait_success(super_client, service1.activate(), 120)\n    service2 = super_client.wait_success(service2, 120)\n\n    assert service1.state == \"active\"\n    assert service2.state == \"active\"\n\n    # 2. validate instances\n    _validate_compose_instance_start(super_client,\n                                     service1, env, \"1\")\n    _validate_compose_instance_start(super_client,\n                                     service1, env, \"2\")\n    s21_container = _validate_compose_instance_start(super_client,\n                                                     service2, env, \"1\")\n    s22_container = _validate_compose_instance_start(super_client,\n                                                     service2, env, \"2\")\n\n    assert len(s22_container.dataVolumesFrom) == 1\n    assert len(s21_container.dataVolumesFrom) == 1\n\n\ndef test_remove_active_service(super_client, admin_client, sim_context, nsp):\n    env = admin_client.create_environment(name=random_str())\n    env = admin_client.wait_success(env)\n    assert env.state == \"active\"\n\n    image_uuid = sim_context['imageUuid']\n    launch_config = {\"imageUuid\": image_uuid}\n\n    service = super_client.create_service(name=random_str(),\n                                          environmentId=env.id,\n                                          networkId=nsp.networkId,\n                                          launchConfig=launch_config)\n    service = super_client.wait_success(service)\n    assert service.state == \"inactive\"\n\n    # activate service\n    service = wait_success(super_client, service.activate(), 120)\n    assert service.state == \"active\"\n    instance_service_map = super_client. \\\n        list_serviceExposeMap(serviceId=service.id)\n\n    assert len(instance_service_map) == 1\n    wait_for_condition(\n        super_client, instance_service_map[0], _resource_is_active,\n        lambda x: 'State is: ' + x.state)\n\n    _validate_compose_instance_start(super_client, service, env, \"1\")\n\n    # remove service\n    service = wait_success(super_client, service.remove(), 120)\n    assert service.state == \"removed\"\n    _validate_compose_instance_removed(super_client, service, env)\n\n\ndef _wait_until_active_map_count(service, count, super_client, timeout=30):\n    # need this function because agent state changes\n    # active->deactivating->removed\n    start = time.time()\n    instance_service_map = super_client. \\\n        list_serviceExposeMap(serviceId=service.id, state=\"active\")\n    while len(instance_service_map) != count:\n        time.sleep(.5)\n        instance_service_map = super_client. \\\n            list_serviceExposeMap(serviceId=service.id, state=\"active\")\n        if time.time() - start > timeout:\n            assert 'Timeout waiting for map to be removed.'\n\n    return\n\n\ndef test_remove_environment_w_active_svcs(super_client,\n                                          admin_client, sim_context, nsp):\n    env = admin_client.create_environment(name=random_str())\n    env = admin_client.wait_success(env)\n    assert env.state == \"active\"\n\n    image_uuid = sim_context['imageUuid']\n    launch_config = {\"imageUuid\": image_uuid}\n\n    service = super_client.create_service(name=random_str(),\n                                          environmentId=env.id,\n                                          networkId=nsp.networkId,\n                                          launchConfig=launch_config)\n    service = super_client.wait_success(service)\n    assert service.state == \"inactive\"\n\n    # activate services\n    env = env.activateservices()\n    service = super_client.wait_success(service, 120)\n    assert service.state == \"active\"\n    instance_service_map = super_client. \\\n        list_serviceExposeMap(serviceId=service.id)\n    assert len(instance_service_map) == 1\n    wait_for_condition(\n        super_client, instance_service_map[0], _resource_is_active,\n        lambda x: 'State is: ' + x.state)\n\n    _validate_compose_instance_start(super_client, service, env, \"1\")\n\n    # remove environment\n    env = wait_success(admin_client, env.remove())\n    assert env.state == \"removed\"\n    service = super_client.wait_success(service)\n    _validate_compose_instance_removed(super_client, service, env)\n\n\ndef _validate_compose_instance_start(super_client, service, env, number):\n    instances = super_client. \\\n        list_container(name=env.name + \"_\" + service.name + \"_\" + number,\n                       state=\"running\")\n    assert len(instances) == 1\n    return instances[0]\n\n\ndef _validate_instance_start(service, super_client, name):\n    instances = super_client. \\\n        list_container(name=name)\n    assert len(instances) == 1\n    return instances[0]\n\n\ndef test_validate_service_scaleup_scaledown(super_client,\n                                            admin_client, sim_context, nsp):\n    env = admin_client.create_environment(name=random_str())\n    env = admin_client.wait_success(env)\n    assert env.state == \"active\"\n\n    image_uuid = sim_context['imageUuid']\n    launch_config = {\"imageUuid\": image_uuid}\n\n    service = super_client.create_service(name=random_str(),\n                                          environmentId=env.id,\n                                          networkId=nsp.networkId,\n                                          launchConfig=launch_config,\n                                          scale=2)\n    service = super_client.wait_success(service)\n    assert service.state == \"inactive\"\n\n    # scale up the inactive service\n    service = super_client.update(service, scale=3, name=service.name)\n    service = super_client.wait_success(service, 120)\n    assert service.state == \"inactive\"\n    assert service.scale == 3\n\n    # activate services\n    env.activateservices()\n    service = super_client.wait_success(service, 120)\n    assert service.state == \"active\"\n\n    _validate_compose_instance_start(super_client, service, env, \"1\")\n    instance2 = _validate_compose_instance_start(super_client, service,\n                                                 env, \"2\")\n    instance3 = _validate_compose_instance_start(super_client, service,\n                                                 env, \"3\")\n\n    # stop the instance2\n    instance2 = wait_success(super_client, instance2)\n    instance2 = wait_success(super_client, instance2.stop())\n    assert instance2.state == 'stopped'\n\n    # rename the instance 3\n    instance3 = super_client.update(instance3, name='newName')\n\n    # scale up the service\n    # instance 2 should get started; env_service_3 name should be utilized\n    service = super_client.update(service, scale=4, name=service.name)\n    service = super_client.wait_success(service, 120)\n    assert service.state == \"active\"\n    assert service.scale == 4\n\n    _validate_compose_instance_start(super_client, service, env, \"1\")\n    _validate_compose_instance_start(super_client, service, env, \"2\")\n    _validate_compose_instance_start(super_client, service, env, \"3\")\n    _validate_instance_start(service, super_client, instance3.name)\n\n    # scale down the service\n    service = super_client.update(service, scale=2, name=service.name)\n    service = super_client.wait_success(service, 120)\n    assert service.state == \"active\"\n    # validate that only 2 service instance mappings exist\n    instance_service_map = super_client. \\\n        list_serviceExposeMap(serviceId=service.id, state=\"active\")\n    assert len(instance_service_map) == 2\n\n\ndef test_link_services_from_diff_env(super_client, admin_client,\n                                     sim_context, nsp):\n    env1 = admin_client.create_environment(name=random_str())\n    env1 = admin_client.wait_success(env1)\n\n    image_uuid = sim_context['imageUuid']\n    launch_config = {\"imageUuid\": image_uuid}\n\n    service1 = super_client.create_service(name=random_str(),\n                                           environmentId=env1.id,\n                                           networkId=nsp.networkId,\n                                           launchConfig=launch_config)\n    service1 = super_client.wait_success(service1)\n\n    env2 = admin_client.create_environment(name=random_str())\n    env2 = admin_client.wait_success(env2)\n    service2 = super_client.create_service(name=random_str(),\n                                           environmentId=env2.id,\n                                           networkId=nsp.networkId,\n                                           launchConfig=launch_config)\n    service2 = super_client.wait_success(service2)\n\n    # try to link\n    with pytest.raises(ApiError) as e:\n        service1.addservicelink(serviceId=service2.id)\n\n    assert e.value.error.status == 422\n    assert e.value.error.code == 'InvalidReference'\n    assert e.value.error.fieldName == 'serviceId'\n\n\ndef test_set_service_links(super_client, admin_client,\n                           sim_context, nsp):\n    env1 = admin_client.create_environment(name=random_str())\n    env1 = admin_client.wait_success(env1)\n\n    image_uuid = sim_context['imageUuid']\n    launch_config = {\"imageUuid\": image_uuid}\n\n    service1 = super_client.create_service(name=random_str(),\n                                           environmentId=env1.id,\n                                           networkId=nsp.networkId,\n                                           launchConfig=launch_config)\n    service1 = super_client.wait_success(service1)\n\n    service2 = super_client.create_service(name=random_str(),\n                                           environmentId=env1.id,\n                                           networkId=nsp.networkId,\n                                           launchConfig=launch_config)\n    service2 = super_client.wait_success(service2)\n\n    service3 = super_client.create_service(name=random_str(),\n                                           environmentId=env1.id,\n                                           networkId=nsp.networkId,\n                                           launchConfig=launch_config)\n    service3 = super_client.wait_success(service3)\n\n    # set service2, service3 links for service1\n    service1 = service1.setservicelinks(serviceIds=[service2.id, service3.id])\n    _validate_add_service_link(service1, service2, super_client)\n    _validate_add_service_link(service1, service3, super_client)\n\n    # set service2 links for service1\n    service1 = service1.setservicelinks(serviceIds=[service2.id])\n    _validate_add_service_link(service1, service2, super_client)\n    _validate_remove_service_link(service1, service3, super_client)\n\n    # set empty service link set\n    service1 = service1.setservicelinks(serviceIds=[])\n    _validate_remove_service_link(service1, service2, super_client)\n    _validate_remove_service_link(service1, service3, super_client)\n\n    # try to link to the service from diff environment\n    env2 = admin_client.create_environment(name=random_str())\n    env2 = admin_client.wait_success(env2)\n\n    service4 = super_client.create_service(name=random_str(),\n                                           environmentId=env2.id,\n                                           networkId=nsp.networkId,\n                                           launchConfig=launch_config)\n    service4 = super_client.wait_success(service4)\n\n    with pytest.raises(ApiError) as e:\n        service1.setservicelinks(serviceIds=[service4.id])\n\n    assert e.value.error.status == 422\n    assert e.value.error.code == 'InvalidReference'\n    assert e.value.error.fieldName == 'serviceId'\n\n\ndef _instance_remove(instance, super_client):\n    instance = wait_success(super_client, instance)\n    instance = wait_success(super_client, instance.stop())\n    assert instance.state == 'stopped'\n    instance = wait_success(super_client, instance.remove())\n    assert instance.state == 'removed'\n    return instance\n\n\ndef test_destroy_service_instance(super_client,\n                                  admin_client, sim_context, nsp):\n    env = admin_client.create_environment(name=random_str())\n    env = admin_client.wait_success(env)\n    assert env.state == \"active\"\n\n    image_uuid = sim_context['imageUuid']\n    launch_config = {\"imageUuid\": image_uuid}\n\n    service = super_client.create_service(name=random_str(),\n                                          environmentId=env.id,\n                                          networkId=nsp.networkId,\n                                          launchConfig=launch_config,\n                                          scale=3)\n    service = super_client.wait_success(service)\n    assert service.state == \"inactive\"\n\n    # activate service\n    service.activate()\n    service = super_client.wait_success(service, 120)\n    assert service.state == \"active\"\n\n    instance1 = _validate_compose_instance_start(super_client, service,\n                                                 env, \"1\")\n    instance2 = _validate_compose_instance_start(super_client, service,\n                                                 env, \"2\")\n    instance3 = _validate_compose_instance_start(super_client, service,\n                                                 env, \"3\")\n\n    return\n\n    # 1. stop and remove the instance2. Validate the mapping still exist\n    instance2 = _instance_remove(instance2, super_client)\n\n    instance_service_map = super_client. \\\n        list_serviceExposeMap(serviceId=service.id, instanceId=instance2.id)\n    assert len(instance_service_map) == 1\n    wait_for_condition(\n        super_client, instance_service_map[0], _resource_is_active,\n        lambda x: 'State is: ' + x.state)\n\n    # 2. deactivate the service\n    service.deactivate()\n    service = super_client.wait_success(service, 120)\n    assert service.state == \"inactive\"\n\n    # 3. activate the service. The map should be gone\n    service.activate()\n    service = super_client.wait_success(service, 120)\n    assert service.state == \"active\"\n\n    # 4. destroy instance3 and update the service's scale.\n    # Validate that instance3 map is gone\n    instance3 = _instance_remove(instance3, super_client)\n    service = super_client.update(service, scale=4, name=service.name)\n    service = super_client.wait_success(service, 120)\n\n    instance_service_map = super_client. \\\n        list_serviceExposeMap(serviceId=service.id, instanceId=instance3.id)\n    assert len(instance_service_map) == 1\n    wait_for_condition(\n        super_client, instance_service_map[0], _resource_is_removed,\n        lambda x: 'State is: ' + x.state)\n\n    # purge the instance1 w/o changing the service\n    # and validate instance1-service map is gone\n    instance1 = _instance_remove(instance1, super_client)\n    instance1 = wait_success(super_client, instance1.purge())\n    assert instance1.state == 'purged'\n    instance_service_map = super_client. \\\n        list_serviceExposeMap(serviceId=service.id, instanceId=instance1.id)\n    assert len(instance_service_map) == 1\n    wait_for_condition(\n        super_client, instance_service_map[0], _resource_is_removed,\n        lambda x: 'State is: ' + x.state)\n\n\ndef test_service_rename(super_client, admin_client, sim_context, nsp):\n    env = admin_client.create_environment(name=random_str())\n    env = admin_client.wait_success(env)\n    assert env.state == \"active\"\n\n    image_uuid = sim_context['imageUuid']\n    launch_config = {\"imageUuid\": image_uuid}\n\n    service = super_client.create_service(name=random_str(),\n                                          environmentId=env.id,\n                                          networkId=nsp.networkId,\n                                          launchConfig=launch_config,\n                                          scale=2)\n    service = super_client.wait_success(service)\n\n    # activate service\n    service.activate()\n    service = super_client.wait_success(service, 120)\n    assert service.state == \"active\"\n\n    _validate_compose_instance_start(super_client, service, env, \"1\")\n    _validate_compose_instance_start(super_client, service, env, \"2\")\n\n    # update name and validate that the service name got\n    # updated as well as its instances\n    new_name = \"newname\"\n    service = super_client.update(service, scale=3, name=new_name)\n    service = super_client.wait_success(service)\n    assert service.name == new_name\n    _validate_compose_instance_start(super_client, service, env, \"1\")\n    _validate_compose_instance_start(super_client, service, env, \"2\")\n\n\ndef test_env_rename(super_client, admin_client, sim_context, nsp):\n    env = admin_client.create_environment(name=random_str())\n    env = admin_client.wait_success(env)\n    assert env.state == \"active\"\n\n    image_uuid = sim_context['imageUuid']\n    launch_config = {\"imageUuid\": image_uuid}\n\n    service_1 = super_client.create_service(name=random_str(),\n                                            environmentId=env.id,\n                                            networkId=nsp.networkId,\n                                            launchConfig=launch_config,\n                                            scale=2)\n    service_1 = super_client.wait_success(service_1)\n\n    service_2 = super_client.create_service(name=random_str(),\n                                            environmentId=env.id,\n                                            networkId=nsp.networkId,\n                                            launchConfig=launch_config,\n                                            scale=1)\n    service_2 = super_client.wait_success(service_2)\n\n    # activate services\n    env = env.activateservices()\n    service_1 = super_client.wait_success(service_1, 120)\n    service_2 = super_client.wait_success(service_2, 120)\n    assert service_1.state == \"active\"\n    assert service_2.state == \"active\"\n\n    _validate_compose_instance_start(super_client, service_1, env, \"1\")\n    _validate_compose_instance_start(super_client, service_1, env, \"2\")\n    _validate_compose_instance_start(super_client, service_2, env, \"1\")\n\n    # update env name and validate that the\n    # env name got updated as well as all instances\n    new_name = \"newname\"\n    env = admin_client.update(env, name=new_name)\n    env = admin_client.wait_success(env)\n    assert env.name == new_name\n    _validate_compose_instance_start(super_client, service_1, env, \"1\")\n    _validate_compose_instance_start(super_client, service_1, env, \"2\")\n    _validate_compose_instance_start(super_client, service_2, env, \"1\")\n\n\ndef test_validate_scale_down_restore_state(super_client,\n                                           admin_client, sim_context, nsp):\n    env = admin_client.create_environment(name=random_str())\n    env = admin_client.wait_success(env)\n    assert env.state == \"active\"\n\n    image_uuid = sim_context['imageUuid']\n    launch_config = {\"imageUuid\": image_uuid}\n\n    service = super_client.create_service(name=random_str(),\n                                          environmentId=env.id,\n                                          networkId=nsp.networkId,\n                                          launchConfig=launch_config,\n                                          scale=3)\n    service = super_client.wait_success(service)\n    assert service.state == \"inactive\"\n\n    # activate services\n    env.activateservices()\n    service = super_client.wait_success(service, 120)\n    assert service.state == \"active\"\n\n    instance1 = _validate_compose_instance_start(super_client, service,\n                                                 env, \"1\")\n    instance2 = _validate_compose_instance_start(super_client, service,\n                                                 env, \"2\")\n    instance3 = _validate_compose_instance_start(super_client, service,\n                                                 env, \"3\")\n    # stop the instances 1, 2 and destroy instance 3\n    instance1 = wait_success(super_client, instance1.stop())\n    assert instance1.state == 'stopped'\n    instance2 = wait_success(super_client, instance2.stop())\n    assert instance2.state == 'stopped'\n    instance3 = _instance_remove(instance3, super_client)\n    assert instance3.state == 'removed'\n\n    # scale down the service and validate that:\n    # first instance is running\n    # second instance is removed\n    # third instance is removed\n    service = super_client.update(service, scale=1, name=service.name)\n    super_client.wait_success(service)\n\n    # validate that only one service instance mapping exists\n    instance_service_map = super_client. \\\n        list_serviceExposeMap(serviceId=service.id, state=\"active\")\n    assert len(instance_service_map) == 1\n\n\ndef test_validate_labels(super_client, admin_client, sim_context, nsp):\n    env = admin_client.create_environment(name=random_str())\n    env = admin_client.wait_success(env)\n    assert env.state == \"active\"\n\n    # create service1 with labels defined\n    service_name1 = random_str()\n    initial_labels1 = {'affinity': \"container==B\", '!affinity': \"container==C\"}\n    image_uuid = sim_context['imageUuid']\n    launch_config1 = {\"imageUuid\": image_uuid, \"labels\": initial_labels1}\n\n    service1 = super_client.create_service(name=service_name1,\n                                           environmentId=env.id,\n                                           networkId=nsp.networkId,\n                                           launchConfig=launch_config1)\n    service1 = super_client.wait_success(service1)\n    assert service1.state == \"inactive\"\n    assert service1.launchConfig.labels == initial_labels1\n\n    # create service2 w/o labels defined\n    service_name2 = random_str()\n    image_uuid = sim_context['imageUuid']\n    launch_config2 = {\"imageUuid\": image_uuid}\n\n    service2 = super_client.create_service(name=service_name2,\n                                           environmentId=env.id,\n                                           networkId=nsp.networkId,\n                                           launchConfig=launch_config2)\n    service2 = super_client.wait_success(service2)\n    assert service2.state == \"inactive\"\n    assert \"labels\" not in service2.launchConfig\n\n    # activate services\n    env.activateservices()\n    service1 = super_client.wait_success(service1, 120)\n    assert service1.state == \"active\"\n    service2 = super_client.wait_success(service2, 120)\n    assert service2.state == \"active\"\n\n    # check that labels defined in launch config + the internal label, are set\n    result_labels_1 = {'affinity': 'container==B', '!affinity': \"container==C\",\n                       'io.rancher.service.name': service_name1,\n                       'io.rancher.environment.name': env.name}\n    instance1 = _validate_compose_instance_start(super_client, service1,\n                                                 env, \"1\")\n    assert all(item in instance1.labels for item in result_labels_1) is True\n\n    # check that only one internal label is set\n    result_labels_2 = {'io.rancher.service.name': service_name2,\n                       'io.rancher.environment.name': env.name}\n    instance2 = _validate_compose_instance_start(super_client, service2,\n                                                 env, \"1\")\n    assert all(item in instance2.labels for item in result_labels_2) is True\n\n\ndef test_sidekick_services_activate(super_client,\n                                    admin_client, sim_context, nsp):\n    env = admin_client.create_environment(name=random_str())\n    env = admin_client.wait_success(env)\n    assert env.state == \"active\"\n\n    # create service1/service2 with the same sidekick label defined\n    # service3 with a diff sidekick label, and service4 with no label\n    image_uuid = sim_context['imageUuid']\n    launch_config = {\"imageUuid\": image_uuid,\n                     \"labels\": {'io.rancher.service.sidekick': \"random\"}}\n\n    service1 = super_client.create_service(name=random_str(),\n                                           environmentId=env.id,\n                                           networkId=nsp.networkId,\n                                           launchConfig=launch_config)\n    service1 = super_client.wait_success(service1)\n\n    service2 = super_client.create_service(name=random_str(),\n                                           environmentId=env.id,\n                                           networkId=nsp.networkId,\n                                           launchConfig=launch_config)\n    service2 = super_client.wait_success(service2)\n\n    launch_config1 = {\"imageUuid\": image_uuid}\n    service3 = super_client.create_service(name=random_str(),\n                                           environmentId=env.id,\n                                           networkId=nsp.networkId,\n                                           launchConfig=launch_config1)\n    service3 = super_client.wait_success(service3)\n\n    launch_config2 = {\"imageUuid\": image_uuid,\n                      \"labels\": {'io.rancher.service.sidekick': \"random123\"}}\n    service4 = super_client.create_service(name=random_str(),\n                                           environmentId=env.id,\n                                           networkId=nsp.networkId,\n                                           launchConfig=launch_config2)\n    service4 = super_client.wait_success(service4)\n\n    # activate service1, service 2 should be activated too\n    service1 = wait_success(super_client, service1.activate(), 120)\n    assert service1.state == \"active\"\n    service2 = super_client.wait_success(service2, 120)\n    assert service2.state == \"active\"\n\n    # service 3 and 4 should be inactive\n    service3 = super_client.wait_success(service3)\n    assert service3.state == \"inactive\"\n    service4 = super_client.wait_success(service4)\n    assert service4.state == \"inactive\"\n\n\ndef test_sidekick_restart_instances(super_client,\n                                    admin_client, sim_context, nsp):\n    env = admin_client.create_environment(name=random_str())\n    env = admin_client.wait_success(env)\n    assert env.state == \"active\"\n\n    # create service1/service2 with the same sidekick label defined\n    image_uuid = sim_context['imageUuid']\n    launch_config = {\"imageUuid\": image_uuid,\n                     \"labels\": {'io.rancher.service.sidekick': \"random\"}}\n\n    service1 = super_client.create_service(name=random_str(),\n                                           environmentId=env.id,\n                                           networkId=nsp.networkId,\n                                           launchConfig=launch_config,\n                                           scale=2)\n    service1 = super_client.wait_success(service1)\n\n    service2 = super_client.create_service(name=random_str(),\n                                           environmentId=env.id,\n                                           networkId=nsp.networkId,\n                                           launchConfig=launch_config, scale=2)\n    service2 = super_client.wait_success(service2)\n\n    # activate service1, service 2 should be activated too\n    service1 = wait_success(super_client, service1.activate(), 120)\n    assert service1.state == \"active\"\n    service2 = super_client.wait_success(service2, 120)\n    assert service2.state == \"active\"\n\n    instance11 = _validate_compose_instance_start(super_client,\n                                                  service1, env, \"1\")\n    _validate_compose_instance_start(super_client, service1, env, \"2\")\n    _validate_compose_instance_start(super_client, service2, env, \"1\")\n    instance22 = _validate_compose_instance_start(super_client,\n                                                  service2, env, \"2\")\n\n    instance_service_map1 = super_client. \\\n        list_serviceExposeMap(serviceId=service1.id, state=\"active\")\n    assert len(instance_service_map1) == 2\n\n    instance_service_map2 = super_client. \\\n        list_serviceExposeMap(serviceId=service2.id, state=\"active\")\n    assert len(instance_service_map2) == 2\n\n    # stop instance11, destroy instance12 and call update on a service1\n    # scale should be restored\n    wait_success(super_client, instance11.stop())\n    _instance_remove(instance22, super_client)\n    service1 = super_client.update(service1, scale=2, name=service1.name)\n    service1 = super_client.wait_success(service1, 120)\n\n    _validate_compose_instance_start(super_client, service1, env, \"1\")\n    _validate_compose_instance_start(super_client, service1, env, \"2\")\n    _validate_compose_instance_start(super_client, service2, env, \"1\")\n    _validate_compose_instance_start(super_client, service2, env, \"2\")\n\n    instance_service_map1 = super_client. \\\n        list_serviceExposeMap(serviceId=service1.id, state=\"active\")\n    assert len(instance_service_map1) == 2\n\n    instance_service_map2 = super_client. \\\n        list_serviceExposeMap(serviceId=service2.id, state=\"active\")\n    assert len(instance_service_map2) == 2\n\n\ndef test_sidekick_scaleup(super_client, admin_client, sim_context, nsp):\n    env = admin_client.create_environment(name=random_str())\n    env = admin_client.wait_success(env)\n    assert env.state == \"active\"\n\n    # create service1/service2 with the same sidekick label defined\n    image_uuid = sim_context['imageUuid']\n    launch_config = {\"imageUuid\": image_uuid,\n                     \"labels\": {'io.rancher.service.sidekick': \"random\"}}\n\n    service1 = super_client.create_service(name=random_str(),\n                                           environmentId=env.id,\n                                           networkId=nsp.networkId,\n                                           launchConfig=launch_config,\n                                           scale=1)\n    service1 = super_client.wait_success(service1)\n\n    service2 = super_client.create_service(name=random_str(),\n                                           environmentId=env.id,\n                                           networkId=nsp.networkId,\n                                           launchConfig=launch_config, scale=1)\n    service2 = super_client.wait_success(service2)\n\n    # activate service1, service 2 should be activated too\n    service1 = wait_success(super_client, service1.activate(), 120)\n    assert service1.state == \"active\"\n    service2 = super_client.wait_success(service2, 120)\n    assert service2.state == \"active\"\n\n    _validate_compose_instance_start(super_client, service1, env, \"1\")\n    _validate_compose_instance_start(super_client, service2, env, \"1\")\n\n    # scale up service1, verify that the service 2 was scaled up and updated\n    service1 = super_client.update(service1, scale=2, name=service1.name)\n    _wait_compose_instance_start(super_client, service1, env, \"1\")\n    _wait_compose_instance_start(super_client, service1, env, \"2\")\n    _wait_compose_instance_start(super_client, service2, env, \"1\")\n    _wait_compose_instance_start(super_client, service2, env, \"2\")\n\n    service1 = super_client.wait_success(service1, 120)\n    assert service1.state == \"active\"\n    assert service1.scale == 2\n    service2 = super_client.wait_success(service2, 120)\n    assert service2.state == \"active\"\n    assert service2.scale == 2\n\n    instance_service_map1 = super_client. \\\n        list_serviceExposeMap(serviceId=service1.id, state=\"active\")\n    assert len(instance_service_map1) == 2\n\n    instance_service_map2 = super_client. \\\n        list_serviceExposeMap(serviceId=service2.id, state=\"active\")\n    assert len(instance_service_map2) == 2\n\n\ndef test_sidekick_diff_scale(super_client, admin_client, sim_context, nsp):\n    env = admin_client.create_environment(name=random_str())\n    env = admin_client.wait_success(env)\n    assert env.state == \"active\"\n\n    # create service1/service2 with the same sidekick label defined,\n    # but diff scale - should fail\n    image_uuid = sim_context['imageUuid']\n    launch_config = {\"imageUuid\": image_uuid,\n                     \"labels\": {'io.rancher.service.sidekick': \"random\"}}\n\n    service1 = super_client.create_service(name=random_str(),\n                                           environmentId=env.id,\n                                           networkId=nsp.networkId,\n                                           launchConfig=launch_config,\n                                           scale=2)\n    service1 = super_client.wait_success(service1)\n    assert service1.scale == 2\n\n    service2 = super_client.create_service(name=random_str(),\n                                           environmentId=env.id,\n                                           networkId=nsp.networkId,\n                                           launchConfig=launch_config,\n                                           scale=3)\n    service2 = super_client.wait_success(service2)\n    assert service2.scale == 2\n\n\ndef _wait_compose_instance_start(super_client, service,\n                                 env, number, timeout=30):\n    start = time.time()\n    instances = super_client. \\\n        list_container(name=env.name + \"_\" + service.name + \"_\" + number,\n                       state=\"running\")\n    while len(instances) != 1:\n        time.sleep(.5)\n        instances = super_client. \\\n            list_container(name=env.name + \"_\" + service.name + \"_\" + number,\n                           state=\"running\")\n        if time.time() - start > timeout:\n            assert 'Timeout waiting for instance to become running.'\n\n\ndef _create_registry_credential(admin_client):\n    registry = _create_registry(admin_client)\n    reg_cred = admin_client.create_registry_credential(\n        registryId=registry.id,\n        email='test@rancher.com',\n        publicValue='wizardofmath+whisper',\n        secretValue='W0IUYDBM2VORHM4DTTEHSMKLXGCG3KD3IT081QWWTZA11R9DZS2DDPP72'\n                    '48NUTT6')\n    assert reg_cred is not None\n    assert reg_cred.email == 'test@rancher.com'\n    assert reg_cred.kind == 'registryCredential'\n    assert reg_cred.registryId == registry.id\n    assert reg_cred.publicValue == 'wizardofmath+whisper'\n    assert 'secretValue' not in reg_cred\n\n    return reg_cred\n\n\ndef _create_registry(admin_client):\n    registry = admin_client.create_registry(serverAddress='quay.io',\n                                            name='Quay')\n    assert registry.serverAddress == 'quay.io'\n    assert registry.name == 'Quay'\n\n    return registry\n\n\ndef _resource_is_stopped(resource):\n    return resource.state == 'stopped'\n\n\ndef _resource_is_running(resource):\n    return resource.state == 'running'\n\n\ndef _validate_add_service_link(service, consumedService, super_client):\n    service_maps = super_client. \\\n        list_serviceConsumeMap(serviceId=service.id,\n                               consumedServiceId=consumedService.id)\n    assert len(service_maps) == 1\n    service_map = service_maps[0]\n    wait_for_condition(\n        super_client, service_map, _resource_is_active,\n        lambda x: 'State is: ' + x.state)\n\n\ndef _validate_remove_service_link(service, consumedService, super_client):\n    service_maps = super_client. \\\n        list_serviceConsumeMap(serviceId=service.id,\n                               consumedServiceId=consumedService.id)\n    assert len(service_maps) == 1\n    service_map = service_maps[0]\n    wait_for_condition(\n        super_client, service_map, _resource_is_removed,\n        lambda x: 'State is: ' + x.state)\n\n\ndef _resource_is_active(resource):\n    return resource.state == 'active'\n\n\ndef _resource_is_removed(resource):\n    return resource.state == 'removed'\n", "://www.apache.org/licenses/LICENSE-2.0 <see LICENSE file>\n# Created By: silas@reciprocitylabs.com\n# Maintained By: silas@reciprocitylabs.com\n\nfrom datetime import datetime\nfrom os.path import abspath, dirname, join\n\nfrom mock import patch\n\nfrom ggrc import db\nfrom ggrc.converters.import_helper import handle_converter_csv_export\nfrom ggrc.converters.controls import ControlsConverter\nfrom ggrc.models.all_models import (\n    ControlCategory, Control, Policy, ObjectControl, Option, System,\n    )\nfrom tests.ggrc import TestCase\nfrom nose.plugins.skip import SkipTest\n\n\nTHIS_ABS_PATH = abspath(dirname(__file__))\nCSV_DIR = join(THIS_ABS_PATH, 'comparison_csvs/')\n\n\n@SkipTest\nclass TestExport(TestCase):\n  def setUp(self):\n    self.csv_filename = \"dummy_filename.csv\"\n    self.expected_status_code = 200\n    self.expected_headers = [\n        ('Content-Type', 'text/csv'),\n        (\n            'Content-Disposition',\n            'attachment; filename=\"{}\"'.format(self.csv_filename)\n        )\n    ]\n    super(TestExport, self).setUp()\n\n  def tearDown(self):\n    super(TestExport, self).tearDown()\n\n  @patch(\"ggrc.converters.import_helper.current_app.make_response\")\n  def test_simple(self, mock_response):\n    with open(join(CSV_DIR, \"minimal_export.csv\"), \"r\") as f:\n      expected_csv = f.read()\n    date1 = datetime(2013, 9, 25)\n    date2 = datetime(2013, 9, 26)\n    pol1 = Policy(\n      kind=\"Company Policy\",\n      title=\"Example Policy\",\n      slug=\"POL-123\",\n    )\n    cont1 = Control(\n      directive=pol1,\n      title=\"Minimal Control 1\",\n      slug=\"CTRL-1\",\n      created_at=date1,\n      updated_at=date1,\n      start_date=date1,\n      end_date=date2\n    )\n    cont2 = Control(\n      directive=pol1,\n      title=\"Minimal Control 2\",\n      slug=\"CTRL-2\",\n      created_at=date1,\n      updated_at=date1\n    )\n    db.session.add(pol1)\n    db.session.commit()\n    options = {\n        'parent_type': Policy,\n        'parent_id': pol1.id,\n        'export': True,\n    }\n    handle_converter_csv_export(\n        self.csv_filename,\n        pol1.controls,\n        ControlsConverter,\n        **options\n    )\n    # calls with one tuple of three args, so double parens\n    mock_response.assert_called_once_with((\n        expected_csv,\n        self.expected_status_code,\n        self.expected_headers,\n    ))\n\n  @patch(\"ggrc.converters.import_helper.current_app.make_response\")\n  def test_mappings(self, mock_response):\n    with open(join(CSV_DIR, \"mappings_export.csv\"), \"r\") as f:\n      expected_csv = f.read()\n    sample_day = datetime(2013, 9, 25)\n    pol1 = Policy(\n      kind=\"Company Policy\",\n      title=\"Example Policy 3\",\n      description=\"Example Description\",\n      slug=\"POL-123\",\n      url=\"http://example.com/policy/3\",\n    )\n    cont1 = Control(\n      directive=pol1,\n      title=\"Complex Control 2\",\n      slug=\"CTRL-2345\",\n      description=\"Example Complex Control\",\n      company_control=True,\n      fraud_related=\"1\",\n      key_control=\"1\",\n      notes=\"These are the notes on the example control.\",\n      url=\"http://example.com/control/3\",\n      created_at=sample_day,\n      updated_at=sample_day,\n    )\n    cat1 = ControlCategory(name=\"Governance\")\n    cat2 = ControlCategory(name=\"Authorization\")\n    sys1 = System(slug=\"ACLS\", title=\"System1\")\n    ob_cont1 = ObjectControl(\n        controllable=sys1,\n        control=cont1,\n    )\n    cont1.object_controls.append(ob_cont1)\n    db.session.add(cont1)\n    db.session.add(cat1)\n    db.session.add(cat2)\n    db.session.commit()\n    cont1.categories.append(cat1)\n    cont1.categories.append(cat2)\n    db.session.add(cont1)\n    db.session.commit()\n    options = {\n        'parent_type': Policy,\n        'parent_id': pol1.id,\n        'export': True,\n    }\n    handle_converter_csv_export(\n        self.csv_filename,\n        pol1.controls,\n        ControlsConverter,\n        **options\n    )\n    mock_response.assert_called_once_with((\n        expected_csv,\n        self.expected_status_code,\n        self.expected_headers,\n    ))\n", "w')\n    for l in fO:\n        if count >= 10000:\n            count = 0\n            number +=1\n            f.close()\n            f = open(prefix_X+'%d' %number, 'w')\n        f.write(l)\n        count+=1\n    f.close()\n\nprefix_X = './pieces/lenet/trainX/ALFloatMatrix_'\nprefix_Y = './pieces/lenet/trainY/ALFloatMatrix_'\nX = open('../data/t10k/train_x_normal.txt')\nY = open('../data/t10k/train_y.txt')\n\nsplit(X, prefix_X)\nsplit(Y, prefix_Y)\n", "icense,\u00a0Version\u00a02.0\u00a0(the\u00a0\"License\");\n#  you\u00a0may\u00a0not\u00a0use\u00a0this\u00a0file\u00a0except\u00a0in\u00a0compliance\u00a0with\u00a0the\u00a0License.\n#  You\u00a0may\u00a0obtain\u00a0a\u00a0copy\u00a0of\u00a0the\u00a0License\u00a0at\n#\n# \u00a0\u00a0\u00a0\u00a0\u00a0 http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless\u00a0required\u00a0by\u00a0applicable\u00a0law\u00a0or\u00a0agreed\u00a0to\u00a0in\u00a0writing,\u00a0software\n#  distributed\u00a0under\u00a0the\u00a0License\u00a0is\u00a0distributed\u00a0on\u00a0an\u00a0\"AS\u00a0IS\"\u00a0BASIS,\n#  WITHOUT\u00a0WARRANTIES\u00a0OR\u00a0CONDITIONS\u00a0OF\u00a0ANY\u00a0KIND,\u00a0either\u00a0express\u00a0or\u00a0implied.\n#  See\u00a0the\u00a0License\u00a0for\u00a0the\u00a0specific\u00a0language\u00a0governing\u00a0permissions\u00a0and\n#  limitations\u00a0under\u00a0the\u00a0License.\n#\n\nfrom setup import tc, rm, get_sandbox_path\n\nimport logging\nimport pytest\nlogger = logging.getLogger(__name__)\n\n\n@pytest.mark.xfail\ndef test_svm(tc):\n\n    logger.info(\"define schema\")\n    schema = [(\"data\", float),(\"label\", str)]\n\n    logger.info(\"creating the frame\")\n    data = [[-48,1],\n            [-75,1],\n            [-63,1],\n            [-57,1],\n            [73,0],\n            [-33,1],\n            [100,0],\n            [-54,1],\n            [78,0],\n            [48,0],\n            [-55,1],\n            [23,0],\n            [45,0],\n            [75,0],\n            [95,0],\n            [73,0],\n            [7,0],\n            [39,0],\n            [-60,1]]\n\n    f = tc.frame.create(data, schema=schema)\n    logger.info(f.inspect())\n\n    logger.info(\"training the model on the frame\")\n    model = tc.models.classification.svm.train(f, ['data'], 'label')\n    logger.info(\"predicting the class using the model and the frame\")\n    predicted_frame = model.predict(f)\n    assert(set(predicted_frame.column_names) == set(['data', 'label', 'predicted_label']))\n    assert(len(predicted_frame.column_names) == 3)\n    assert(len(f.column_names) == 2)\n    metrics = model.test(predicted_frame)\n    assert(metrics.accuracy == 1.0)\n    assert(metrics.f_measure == 1.0)\n    assert(metrics.precision == 1.0)\n    assert(metrics.recall == 1.0)\n", "tributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n\nimport six\nimport time\n\nfrom airflow.exceptions import AirflowException\nfrom airflow.contrib.hooks.databricks_hook import DatabricksHook\nfrom airflow.models import BaseOperator\nfrom airflow.utils.decorators import apply_defaults\n\n\nXCOM_RUN_ID_KEY = 'run_id'\nXCOM_RUN_PAGE_URL_KEY = 'run_page_url'\n\n\ndef _deep_string_coerce(content, json_path='json'):\n    \"\"\"\n    Coerces content or all values of content if it is a dict to a string. The\n    function will throw if content contains non-string or non-numeric types.\n\n    The reason why we have this function is because the ``self.json`` field must be a\n     dict with only string values. This is because ``render_template`` will fail\n    for numerical values.\n    \"\"\"\n    c = _deep_string_coerce\n    if isinstance(content, six.string_types):\n        return content\n    elif isinstance(content, six.integer_types + (float,)):\n        # Databricks can tolerate either numeric or string types in the API backend.\n        return str(content)\n    elif isinstance(content, (list, tuple)):\n        return [c(e, '{0}[{1}]'.format(json_path, i)) for i, e in enumerate(content)]\n    elif isinstance(content, dict):\n        return {k: c(v, '{0}[{1}]'.format(json_path, k))\n                for k, v in list(content.items())}\n    else:\n        param_type = type(content)\n        msg = 'Type {0} used for parameter {1} is not a number or a string' \\\n            .format(param_type, json_path)\n        raise AirflowException(msg)\n\n\ndef _handle_databricks_operator_execution(operator, hook, log, context):\n    \"\"\"\n    Handles the Airflow + Databricks lifecycle logic for a Databricks operator\n    :param operator: Databricks operator being handled\n    :param context: Airflow context\n    \"\"\"\n    if operator.do_xcom_push:\n        context['ti'].xcom_push(key=XCOM_RUN_ID_KEY, value=operator.run_id)\n    log.info('Run submitted with run_id: %s', operator.run_id)\n    run_page_url = hook.get_run_page_url(operator.run_id)\n    if operator.do_xcom_push:\n        context['ti'].xcom_push(key=XCOM_RUN_PAGE_URL_KEY, value=run_page_url)\n\n    log.info('View run status, Spark UI, and logs at %s', run_page_url)\n    while True:\n        run_state = hook.get_run_state(operator.run_id)\n        if run_state.is_terminal:\n            if run_state.is_successful:\n                log.info('%s completed successfully.', operator.task_id)\n                log.info('View run status, Spark UI, and logs at %s', run_page_url)\n                return\n            else:\n                error_message = '{t} failed with terminal state: {s}'.format(\n                    t=operator.task_id,\n                    s=run_state)\n                raise AirflowException(error_message)\n        else:\n            log.info('%s in run state: %s', operator.task_id, run_state)\n            log.info('View run status, Spark UI, and logs at %s', run_page_url)\n            log.info('Sleeping for %s seconds.', operator.polling_period_seconds)\n            time.sleep(operator.polling_period_seconds)\n\n\nclass DatabricksSubmitRunOperator(BaseOperator):\n    \"\"\"\n    Submits a Spark job run to Databricks using the\n    `api/2.0/jobs/runs/submit\n    <https://docs.databricks.com/api/latest/jobs.html#runs-submit>`_\n    API endpoint.\n\n    There are two ways to instantiate this operator.\n\n    In the first way, you can take the JSON payload that you typically use\n    to call the ``api/2.0/jobs/runs/submit`` endpoint and pass it directly\n    to our ``DatabricksSubmitRunOperator`` through the ``json`` parameter.\n    For example ::\n        json = {\n          'new_cluster': {\n            'spark_version': '2.1.0-db3-scala2.11',\n            'num_workers': 2\n          },\n          'notebook_task': {\n            'notebook_path': '/Users/airflow@example.com/PrepareData',\n          },\n        }\n        notebook_run = DatabricksSubmitRunOperator(task_id='notebook_run', json=json)\n\n    Another way to accomplish the same thing is to use the named parameters\n    of the ``DatabricksSubmitRunOperator`` directly. Note that there is exactly\n    one named parameter for each top level parameter in the ``runs/submit``\n    endpoint. In this method, your code would look like this: ::\n        new_cluster = {\n          'spark_version': '2.1.0-db3-scala2.11',\n          'num_workers': 2\n        }\n        notebook_task = {\n          'notebook_path': '/Users/airflow@example.com/PrepareData',\n        }\n        notebook_run = DatabricksSubmitRunOperator(\n            task_id='notebook_run',\n            new_cluster=new_cluster,\n            notebook_task=notebook_task)\n\n    In the case where both the json parameter **AND** the named parameters\n    are provided, they will be merged together. If there are conflicts during the merge,\n    the named parameters will take precedence and override the top level ``json`` keys.\n\n    Currently the named parameters that ``DatabricksSubmitRunOperator`` supports are\n        - ``spark_jar_task``\n        - ``notebook_task``\n        - ``new_cluster``\n        - ``existing_cluster_id``\n        - ``libraries``\n        - ``run_name``\n        - ``timeout_seconds``\n\n    :param json: A JSON object containing API parameters which will be passed\n        directly to the ``api/2.0/jobs/runs/submit`` endpoint. The other named parameters\n        (i.e. ``spark_jar_task``, ``notebook_task``..) to this operator will\n        be merged with this json dictionary if they are provided.\n        If there are conflicts during the merge, the named parameters will\n        take precedence and override the top level json keys. (templated)\n\n        .. seealso::\n            For more information about templating see :ref:`jinja-templating`.\n            https://docs.databricks.com/api/latest/jobs.html#runs-submit\n    :type json: dict\n    :param spark_jar_task: The main class and parameters for the JAR task. Note that\n        the actual JAR is specified in the ``libraries``.\n        *EITHER* ``spark_jar_task`` *OR* ``notebook_task`` should be specified.\n        This field will be templated.\n\n        .. seealso::\n            https://docs.databricks.com/api/latest/jobs.html#jobssparkjartask\n    :type spark_jar_task: dict\n    :param notebook_task: The notebook path and parameters for the notebook task.\n        *EITHER* ``spark_jar_task`` *OR* ``notebook_task`` should be specified.\n        This field will be templated.\n\n        .. seealso::\n            https://docs.databricks.com/api/latest/jobs.html#jobsnotebooktask\n    :type notebook_task: dict\n    :param new_cluster: Specs for a new cluster on which this task will be run.\n        *EITHER* ``new_cluster`` *OR* ``existing_cluster_id`` should be specified.\n        This field will be templated.\n\n        .. seealso::\n            https://docs.databricks.com/api/latest/jobs.html#jobsclusterspecnewcluster\n    :type new_cluster: dict\n    :param existing_cluster_id: ID for existing cluster on which to run this task.\n        *EITHER* ``new_cluster`` *OR* ``existing_cluster_id`` should be specified.\n        This field will be templated.\n    :type existing_cluster_id: str\n    :param libraries: Libraries which this run will use.\n        This field will be templated.\n\n        .. seealso::\n            https://docs.databricks.com/api/latest/libraries.html#managedlibrarieslibrary\n    :type libraries: list of dicts\n    :param run_name: The run name used for this task.\n        By default this will be set to the Airflow ``task_id``. This ``task_id`` is a\n        required parameter of the superclass ``BaseOperator``.\n        This field will be templated.\n    :type run_name: str\n    :param timeout_seconds: The timeout for this run. By default a value of 0 is used\n        which means to have no timeout.\n        This field will be templated.\n    :type timeout_seconds: int32\n    :param databricks_conn_id: The name of the Airflow connection to use.\n        By default and in the common case this will be ``databricks_default``. To use\n        token based authentication, provide the key ``token`` in the extra field for the\n        connection.\n    :type databricks_conn_id: str\n    :param polling_period_seconds: Controls the rate which we poll for the result of\n        this run. By default the operator will poll every 30 seconds.\n    :type polling_period_seconds: int\n    :param databricks_retry_limit: Amount of times retry if the Databricks backend is\n        unreachable. Its value must be greater than or equal to 1.\n    :type databricks_retry_limit: int\n    :param databricks_retry_delay: Number of seconds to wait between retries (it\n            might be a floating point number).\n    :type databricks_retry_delay: float\n    :param do_xcom_push: Whether we should push run_id and run_page_url to xcom.\n    :type do_xcom_push: bool\n    \"\"\"\n    # Used in airflow.models.BaseOperator\n    template_fields = ('json',)\n    # Databricks brand color (blue) under white text\n    ui_color = '#1CB1C2'\n    ui_fgcolor = '#fff'\n\n    @apply_defaults\n    def __init__(\n            self,\n            json=None,\n            spark_jar_task=None,\n            notebook_task=None,\n            new_cluster=None,\n            existing_cluster_id=None,\n            libraries=None,\n            run_name=None,\n            timeout_seconds=None,\n            databricks_conn_id='databricks_default',\n            polling_period_seconds=30,\n            databricks_retry_limit=3,\n            databricks_retry_delay=1,\n            do_xcom_push=False,\n            **kwargs):\n        \"\"\"\n        Creates a new ``DatabricksSubmitRunOperator``.\n        \"\"\"\n        super(DatabricksSubmitRunOperator, self).__init__(**kwargs)\n        self.json = json or {}\n        self.databricks_conn_id = databricks_conn_id\n        self.polling_period_seconds = polling_period_seconds\n        self.databricks_retry_limit = databricks_retry_limit\n        self.databricks_retry_delay = databricks_retry_delay\n        if spark_jar_task is not None:\n            self.json['spark_jar_task'] = spark_jar_task\n        if notebook_task is not None:\n            self.json['notebook_task'] = notebook_task\n        if new_cluster is not None:\n            self.json['new_cluster'] = new_cluster\n        if existing_cluster_id is not None:\n            self.json['existing_cluster_id'] = existing_cluster_id\n        if libraries is not None:\n            self.json['libraries'] = libraries\n        if run_name is not None:\n            self.json['run_name'] = run_name\n        if timeout_seconds is not None:\n            self.json['timeout_seconds'] = timeout_seconds\n        if 'run_name' not in self.json:\n            self.json['run_name'] = run_name or kwargs['task_id']\n\n        self.json = _deep_string_coerce(self.json)\n        # This variable will be used in case our task gets killed.\n        self.run_id = None\n        self.do_xcom_push = do_xcom_push\n\n    def get_hook(self):\n        return DatabricksHook(\n            self.databricks_conn_id,\n            retry_limit=self.databricks_retry_limit,\n            retry_delay=self.databricks_retry_delay)\n\n    def execute(self, context):\n        hook = self.get_hook()\n        self.run_id = hook.submit_run(self.json)\n        _handle_databricks_operator_execution(self, hook, self.log, context)\n\n    def on_kill(self):\n        hook = self.get_hook()\n        hook.cancel_run(self.run_id)\n        self.log.info(\n            'Task: %s with run_id: %s was requested to be cancelled.',\n            self.task_id, self.run_id\n        )\n\n\nclass DatabricksRunNowOperator(BaseOperator):\n    \"\"\"\n    Runs an existing Spark job run to Databricks using the\n    `api/2.0/jobs/run-now\n    <https://docs.databricks.com/api/latest/jobs.html#run-now>`_\n    API endpoint.\n\n    There are two ways to instantiate this operator.\n\n    In the first way, you can take the JSON payload that you typically use\n    to call the ``api/2.0/jobs/run-now`` endpoint and pass it directly\n    to our ``DatabricksRunNowOperator`` through the ``json`` parameter.\n    For example ::\n        json = {\n          \"job_id\": 42,\n          \"notebook_params\": {\n            \"dry-run\": \"true\",\n            \"oldest-time-to-consider\": \"1457570074236\"\n          }\n        }\n\n        notebook_run = DatabricksRunNowOperator(task_id='notebook_run', json=json)\n\n    Another way to accomplish the same thing is to use the named parameters\n    of the ``DatabricksRunNowOperator`` directly. Note that there is exactly\n    one named parameter for each top level parameter in the ``run-now``\n    endpoint. In this method, your code would look like this: ::\n\n        job_id=42\n\n        notebook_params = {\n            \"dry-run\": \"true\",\n            \"oldest-time-to-consider\": \"1457570074236\"\n        }\n\n        python_params = [\"douglas adams\", \"42\"]\n\n        spark_submit_params = [\"--class\", \"org.apache.spark.examples.SparkPi\"]\n\n        notebook_run = DatabricksRunNowOperator(\n            job_id=job_id,\n            notebook_params=notebook_params,\n            python_params=python_params,\n            spark_submit_params=spark_submit_params\n        )\n\n    In the case where both the json parameter **AND** the named parameters\n    are provided, they will be merged together. If there are conflicts during the merge,\n    the named parameters will take precedence and override the top level ``json`` keys.\n\n    Currently the named parameters that ``DatabricksRunNowOperator`` supports are\n        - ``job_id``\n        - ``json``\n        - ``notebook_params``\n        - ``python_params``\n        - ``spark_submit_params``\n\n\n    :param job_id: the job_id of the existing Databricks job.\n        This field will be templated.\n        .. seealso::\n            https://docs.databricks.com/api/latest/jobs.html#run-now\n    :type job_id: str\n    :param json: A JSON object containing API parameters which will be passed\n        directly to the ``api/2.0/jobs/run-now`` endpoint. The other named parameters\n        (i.e. ``notebook_params``, ``spark_submit_params``..) to this operator will\n        be merged with this json dictionary if they are provided.\n        If there are conflicts during the merge, the named parameters will\n        take precedence and override the top level json keys. (templated)\n\n        .. seealso::\n            For more information about templating see :ref:`jinja-templating`.\n            https://docs.databricks.com/api/latest/jobs.html#run-now\n    :type json: dict\n    :param notebook_params: A dict from keys to values for jobs with notebook task,\n        e.g. \"notebook_params\": {\"name\": \"john doe\", \"age\":  \"35\"}.\n        The map is passed to the notebook and will be accessible through the\n        dbutils.widgets.get function. See Widgets for more information.\n        If not specified upon run-now, the triggered run will use the\n        job\u2019s base parameters. notebook_params cannot be\n        specified in conjunction with jar_params. The json representation\n        of this field (i.e. {\"notebook_params\":{\"name\":\"john doe\",\"age\":\"35\"}})\n        cannot exceed 10,000 bytes.\n        This field will be templated.\n\n        .. seealso::\n            https://docs.databricks.com/user-guide/notebooks/widgets.html\n    :type notebook_params: dict\n    :param python_params: A list of parameters for jobs with python tasks,\n        e.g. \"python_params\": [\"john doe\", \"35\"].\n        The parameters will be passed to python file as command line parameters.\n        If specified upon run-now, it would overwrite the parameters specified in\n        job setting.\n        The json representation of this field (i.e. {\"python_params\":[\"john doe\",\"35\"]})\n        cannot exceed 10,000 bytes.\n        This field will be templated.\n\n        .. seealso::\n            https://docs.databricks.com/api/latest/jobs.html#run-now\n    :type python_params: array of strings\n    :param spark_submit_params: A list of parameters for jobs with spark submit task,\n        e.g. \"spark_submit_params\": [\"--class\", \"org.apache.spark.examples.SparkPi\"].\n        The parameters will be passed to spark-submit script as command line parameters.\n        If specified upon run-now, it would overwrite the parameters specified\n        in job setting.\n        The json representation of this field cannot exceed 10,000 bytes.\n        This field will be templated.\n        .. seealso::\n            https://docs.databricks.com/api/latest/jobs.html#run-now\n    :type spark_submit_params: array of strings\n    :param timeout_seconds: The timeout for this run. By default a value of 0 is used\n        which means to have no timeout.\n        This field will be templated.\n    :type timeout_seconds: int32\n    :param databricks_conn_id: The name of the Airflow connection to use.\n        By default and in the common case this will be ``databricks_default``. To use\n        token based authentication, provide the key ``token`` in the extra field for the\n        connection.\n    :type databricks_conn_id: str\n    :param polling_period_seconds: Controls the rate which we poll for the result of\n        this run. By default the operator will poll every 30 seconds.\n    :type polling_period_seconds: int\n    :param databricks_retry_limit: Amount of times retry if the Databricks backend is\n        unreachable. Its value must be greater than or equal to 1.\n    :type databricks_retry_limit: int\n    :param do_xcom_push: Whether we should push run_id and run_page_url to xcom.\n    :type do_xcom_push: bool\n    \"\"\"\n    # Used in airflow.models.BaseOperator\n    template_fields = ('json',)\n    # Databricks brand color (blue) under white text\n    ui_color = '#1CB1C2'\n    ui_fgcolor = '#fff'\n\n    @apply_defaults\n    def __init__(\n            self,\n            job_id,\n            json=None,\n            notebook_params=None,\n            python_params=None,\n            spark_submit_params=None,\n            databricks_conn_id='databricks_default',\n            polling_period_seconds=30,\n            databricks_retry_limit=3,\n            databricks_retry_delay=1,\n            do_xcom_push=False,\n            **kwargs):\n\n        \"\"\"\n        Creates a new ``DatabricksRunNowOperator``.\n        \"\"\"\n        super(DatabricksRunNowOperator, self).__init__(**kwargs)\n        self.json = json or {}\n        self.databricks_conn_id = databricks_conn_id\n        self.polling_period_seconds = polling_period_seconds\n        self.databricks_retry_limit = databricks_retry_limit\n        self.databricks_retry_delay = databricks_retry_delay\n\n        if job_id is not None:\n            self.json['job_id'] = job_id\n        if notebook_params is not None:\n            self.json['notebook_params'] = notebook_params\n        if python_params is not None:\n            self.json['python_params'] = python_params\n        if spark_submit_params is not None:\n            self.json['spark_submit_params'] = spark_submit_params\n\n        self.json = _deep_string_coerce(self.json)\n        # This variable will be used in case our task gets killed.\n        self.run_id = None\n        self.do_xcom_push = do_xcom_push\n\n    def get_hook(self):\n        return DatabricksHook(\n            self.databricks_conn_id,\n            retry_limit=self.databricks_retry_limit,\n            retry_delay=self.databricks_retry_delay)\n\n    def execute(self, context):\n        hook = self.get_hook()\n        self.run_id = hook.run_now(self.json)\n        _handle_databricks_operator_execution(self, hook, self.log, context)\n\n    def on_kill(self):\n        hook = self.get_hook()\n        hook.cancel_run(self.run_id)\n        self.log.info(\n            'Task: %s with run_id: %s was requested to be cancelled.',\n            self.task_id, self.run_id\n        )\n", "", "2.0 (the \"License\");\n#     you may not use this file except in compliance with the License.\n#     You may obtain a copy of the License at\n#\n#          http://www.apache.org/licenses/LICENSE-2.0\n#\n#     Unless required by applicable law or agreed to in writing, software\n#     distributed under the License is distributed on an \"AS IS\" BASIS,\n#     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#     See the License for the specific language governing permissions and\n#     limitations under the License.\n\n\ndef selection_brute(mlist, t):\n    for i, l in enumerate(mlist):\n        if t == l:\n            return i\n    return -1\n\n\ndef selection_pythonic(mlist, t):\n    return mlist.index(t)\n\n\ndef select_max_brute(mlist):\n    max = mlist[0]\n    max_index = -1\n    for i, l in enumerate(mlist):\n        if l > max:\n            max_index = i\n    return max_index\n\n\ndef select_max_pythonic(mlist):\n    return mlist.index(max(mlist))\n\n\ndef select_min_brute(mlist):\n    min = mlist[0]\n    min_index = -1\n    for i, l in enumerate(mlist):\n        if l < min:\n            min_index = i\n    return min_index\n\n\ndef select_min_pythonic(mylist):\n    return mylist.index(min(mylist))\n\n\nif __name__ == '__main__':\n    mylist = [1, 2, 3, 4, 5]\n    mylist2 = [1, 2, 3, 2, 1, 10, 9, 8, 11, -1]\n    print(selection_brute(mylist, 4))\n    print(selection_pythonic(mylist, 4))\n    print(select_max_brute(mylist2))\n    print(select_max_pythonic(mylist2))\n    print(select_min_brute(mylist2))\n    print(select_min_pythonic(mylist2))\n", "e\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n\nimport time\nimport sys\nfrom .common import Test, SkipTest, TestServer, free_tcp_port\nfrom proton.reactor import Container, Reactor, ApplicationEvent, EventInjector\nfrom proton.handlers import CHandshaker, MessagingHandler\nfrom proton import Handler, Url\n\nclass Barf(Exception):\n    pass\n\nclass BarfOnInit:\n\n    def on_reactor_init(self, event):\n        raise Barf()\n\n    def on_connection_init(self, event):\n        raise Barf()\n\n    def on_session_init(self, event):\n        raise Barf()\n\n    def on_link_init(self, event):\n        raise Barf()\n\nclass BarfOnTask:\n\n    def on_timer_task(self, event):\n        raise Barf()\n\nclass BarfOnFinal:\n    init = False\n\n    def on_reactor_init(self, event):\n        self.init = True\n\n    def on_reactor_final(self, event):\n        raise Barf()\n    \nclass BarfOnFinalDerived(CHandshaker):\n    init = False\n    \n    def on_reactor_init(self, event):\n        self.init = True\n\n    def on_reactor_final(self, event):\n        raise Barf()\n    \nclass ExceptionTest(Test):\n\n    def setUp(self):\n        self.reactor = Reactor()\n\n    def test_reactor_final(self):\n        self.reactor.global_handler = BarfOnFinal()\n        try:\n            self.reactor.run()\n            assert False, \"expected to barf\"\n        except Barf:\n            pass\n\n    def test_global_set(self):\n        self.reactor.global_handler = BarfOnInit()\n        try:\n            self.reactor.run()\n            assert False, \"expected to barf\"\n        except Barf:\n            pass\n\n    def test_global_add(self):\n        self.reactor.global_handler.add(BarfOnInit())\n        try:\n            self.reactor.run()\n            assert False, \"expected to barf\"\n        except Barf:\n            pass\n\n    def test_reactor_set(self):\n        self.reactor.handler = BarfOnInit()\n        try:\n            self.reactor.run()\n            assert False, \"expected to barf\"\n        except Barf:\n            pass\n\n    def test_reactor_add(self):\n        self.reactor.handler.add(BarfOnInit())\n        try:\n            self.reactor.run()\n            assert False, \"expected to barf\"\n        except Barf:\n            pass\n\n    def test_connection(self):\n        self.reactor.connection(BarfOnInit())\n        try:\n            self.reactor.run()\n            assert False, \"expected to barf\"\n        except Barf:\n            pass\n\n    def test_connection_set(self):\n        c = self.reactor.connection()\n        c.handler = BarfOnInit()\n        try:\n            self.reactor.run()\n            assert False, \"expected to barf\"\n        except Barf:\n            pass\n\n    def test_connection_add(self):\n        c = self.reactor.connection()\n        c.handler = object()\n        c.handler.add(BarfOnInit())\n        try:\n            self.reactor.run()\n            assert False, \"expected to barf\"\n        except Barf:\n            pass\n\n    def test_session_set(self):\n        c = self.reactor.connection()\n        s = c.session()\n        s.handler = BarfOnInit()\n        try:\n            self.reactor.run()\n            assert False, \"expected to barf\"\n        except Barf:\n            pass\n\n    def test_session_add(self):\n        c = self.reactor.connection()\n        s = c.session()\n        s.handler = object()\n        s.handler.add(BarfOnInit())\n        try:\n            self.reactor.run()\n            assert False, \"expected to barf\"\n        except Barf:\n            pass\n\n    def test_link_set(self):\n        c = self.reactor.connection()\n        s = c.session()\n        l = s.sender(\"xxx\")\n        l.handler = BarfOnInit()\n        try:\n            self.reactor.run()\n            assert False, \"expected to barf\"\n        except Barf:\n            pass\n\n    def test_link_add(self):\n        c = self.reactor.connection()\n        s = c.session()\n        l = s.sender(\"xxx\")\n        l.handler = object()\n        l.handler.add(BarfOnInit())\n        try:\n            self.reactor.run()\n            assert False, \"expected to barf\"\n        except Barf:\n            pass\n\n    def test_schedule(self):\n        self.reactor.schedule(0, BarfOnTask())\n        try:\n            self.reactor.run()\n            assert False, \"expected to barf\"\n        except Barf:\n            pass\n\n    def test_schedule_many_nothings(self):\n        class Nothing:\n            results = []\n            def on_timer_task(self, event):\n                self.results.append(None)\n        num = 12345\n        for a in range(num):\n            self.reactor.schedule(0, Nothing())\n        self.reactor.run()\n        assert len(Nothing.results) == num\n\n    def test_schedule_many_nothing_refs(self):\n        class Nothing:\n            results = []\n            def on_timer_task(self, event):\n                self.results.append(None)\n        num = 12345\n        tasks = []\n        for a in range(num):\n            tasks.append(self.reactor.schedule(0, Nothing()))\n        self.reactor.run()\n        assert len(Nothing.results) == num\n\n    def test_schedule_many_nothing_refs_cancel_before_run(self):\n        class Nothing:\n            results = []\n            def on_timer_task(self, event):\n                self.results.append(None)\n        num = 12345\n        tasks = []\n        for a in range(num):\n            tasks.append(self.reactor.schedule(0, Nothing()))\n        for task in tasks:\n            task.cancel()\n        self.reactor.run()\n        assert len(Nothing.results) == 0\n\n    def test_schedule_cancel(self):\n        barf = self.reactor.schedule(1234, BarfOnTask())\n        class CancelBarf:\n            def __init__(self, barf):\n                self.barf = barf\n            def on_timer_task(self, event):\n                self.barf.cancel()\n                pass\n        self.reactor.schedule(0, CancelBarf(barf))\n        now = self.reactor.mark()\n        try:\n            self.reactor.run()\n            elapsed = self.reactor.mark() - now\n            assert elapsed < 100, \"expected cancelled task to not delay the reactor by %s\" % elapsed\n        except Barf:\n            assert False, \"expected barf to be cancelled\"\n\n    def test_schedule_cancel_many(self):\n        num = 12345\n        barfs = set()\n        for a in range(num):\n            barf = self.reactor.schedule(10*(a+1), BarfOnTask())\n            class CancelBarf:\n                def __init__(self, barf):\n                    self.barf = barf\n                def on_timer_task(self, event):\n                    self.barf.cancel()\n                    barfs.discard(self.barf)\n                    pass\n            self.reactor.schedule(0, CancelBarf(barf))\n            barfs.add(barf)\n        now = self.reactor.mark()\n        try:\n            self.reactor.run()\n            elapsed = self.reactor.mark() - now\n            assert elapsed < num, \"expected cancelled task to not delay the reactor by %s\" % elapsed\n            assert not barfs, \"expected all barfs to be discarded\"\n        except Barf:\n            assert False, \"expected barf to be cancelled\"\n\n\nclass HandlerDerivationTest(Test):\n    def setUp(self):\n        import platform\n        if platform.python_implementation() != \"Jython\":\n          # Exception propagation does not work currently for CPython\n          raise SkipTest()\n        self.reactor = Reactor()\n\n    def wrong_exception(self):\n        import sys\n        ex = sys.exc_info()\n        assert False, \" Unexpected exception \" + str(ex[1])\n    \n    def test_reactor_final_derived(self):\n        h = BarfOnFinalDerived()\n        self.reactor.global_handler = h\n        try:\n            self.reactor.run()\n            assert False, \"expected to barf\"\n        except Barf:\n            pass\n        except:\n            self.wrong_exception()\n\n    def test_reactor_final_py_child_py(self):\n        class APoorExcuseForAHandler:\n            def __init__(self):\n                self.handlers = [BarfOnFinal()]\n        self.reactor.global_handler = APoorExcuseForAHandler()\n        try:\n            self.reactor.run()\n            assert False, \"expected to barf\"\n        except Barf:\n            pass\n        except:\n            self.wrong_exception()\n\n    def test_reactor_final_py_child_derived(self):\n        class APoorExcuseForAHandler:\n            def __init__(self):\n                self.handlers = [BarfOnFinalDerived()]\n        self.reactor.global_handler = APoorExcuseForAHandler()\n        try:\n            self.reactor.run()\n            assert False, \"expected to barf\"\n        except Barf:\n            pass\n        except:\n            self.wrong_exception()\n\n    def test_reactor_final_derived_child_derived(self):\n        class APoorExcuseForAHandler(CHandshaker):\n            def __init__(self):\n                CHandshaker.__init__(self)\n                self.handlers = [BarfOnFinalDerived()]\n        self.reactor.global_handler = APoorExcuseForAHandler()\n        try:\n            self.reactor.run()\n            assert False, \"expected to barf\"\n        except Barf:\n            pass\n        except:\n            self.wrong_exception()\n\n    def test_reactor_final_derived_child_py(self):\n        class APoorExcuseForAHandler(CHandshaker):\n            def __init__(self):\n                CHandshaker.__init__(self)\n                self.handlers = [BarfOnFinal()]\n        self.reactor.global_handler = APoorExcuseForAHandler()\n        try:\n            self.reactor.run()\n            assert False, \"expected to barf\"\n        except Barf:\n            pass\n        except:\n            self.wrong_exception()\n\n    def test_reactor_init_derived(self):\n        h = BarfOnFinalDerived()\n        self.reactor.global_handler = h\n        try:\n            self.reactor.run()\n            assert False, \"expected to barf\"\n        except:\n            assert h.init, \"excpected the init\"\n\n    def test_reactor_init_py_child_py(self):\n        h = BarfOnFinal()\n        class APoorExcuseForAHandler:\n            def __init__(self):\n                self.handlers = [h]\n        self.reactor.global_handler = APoorExcuseForAHandler()\n        try:\n            self.reactor.run()\n            assert False, \"expected to barf\"\n        except:\n            assert h.init, \"excpected the init\"\n\n    def test_reactor_init_py_child_derived(self):\n        h = BarfOnFinalDerived()\n        class APoorExcuseForAHandler:\n            def __init__(self):\n                self.handlers = [h]\n        self.reactor.global_handler = APoorExcuseForAHandler()\n        try:\n            self.reactor.run()\n            assert False, \"expected to barf\"\n        except:\n            assert h.init, \"excpected the init\"\n\n    def test_reactor_init_derived_child_derived(self):\n        h = BarfOnFinalDerived()\n        class APoorExcuseForAHandler(CHandshaker):\n            def __init__(self):\n                CHandshaker.__init__(self)\n                self.handlers = [h]\n        self.reactor.global_handler = APoorExcuseForAHandler()\n        try:\n            self.reactor.run()\n            assert False, \"expected to barf\"\n        except:\n            assert h.init, \"excpected the init\"\n\n    def test_reactor_init_derived_child_py(self):\n        h = BarfOnFinal()\n        class APoorExcuseForAHandler(CHandshaker):\n            def __init__(self):\n                CHandshaker.__init__(self)\n                self.handlers = [h]\n        self.reactor.global_handler = APoorExcuseForAHandler()\n        try:\n            self.reactor.run()\n            assert False, \"expected to barf\"\n        except:\n            assert h.init, \"excpected the init\"\n\n\nclass AuthenticationTestHandler(MessagingHandler):\n    def __init__(self):\n        super(AuthenticationTestHandler, self).__init__()\n        port = free_tcp_port()\n        self.url = \"localhost:%i\" % port\n        self.verified = False\n\n    def on_start(self, event):\n        self.listener = event.container.listen(self.url)\n\n    def on_connection_opened(self, event):\n        event.connection.close()\n\n    def on_connection_opening(self, event):\n        assert event.connection.transport.user == \"user@proton\"\n        self.verified = True\n\n    def on_connection_closed(self, event):\n        event.connection.close()\n        self.listener.close()\n\n    def on_connection_error(self, event):\n        event.connection.close()\n        self.listener.close()\n", "code_literals\n\nfrom django.conf import settings\nfrom django.db import migrations, models\nimport django.db.models.deletion\nimport django_extensions.db.fields\nimport osf.models.base\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('osf', '0216_auto_20200910_1259'),\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name='RegistrationRequestAction',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('created', django_extensions.db.fields.CreationDateTimeField(auto_now_add=True, verbose_name='created')),\n                ('modified', django_extensions.db.fields.ModificationDateTimeField(auto_now=True, verbose_name='modified')),\n                ('_id', models.CharField(db_index=True, default=osf.models.base.generate_object_id, max_length=24, unique=True)),\n                ('trigger', models.CharField(choices=[('submit', 'Submit'), ('accept', 'Accept'), ('reject', 'Reject'), ('edit_comment', 'Edit_Comment')], max_length=31)),\n                ('from_state', models.CharField(choices=[('initial', 'Initial'), ('pending', 'Pending'), ('accepted', 'Accepted'), ('rejected', 'Rejected')], max_length=31)),\n                ('to_state', models.CharField(choices=[('initial', 'Initial'), ('pending', 'Pending'), ('accepted', 'Accepted'), ('rejected', 'Rejected')], max_length=31)),\n                ('comment', models.TextField(blank=True)),\n                ('is_deleted', models.BooleanField(default=False)),\n                ('auto', models.BooleanField(default=False)),\n                ('creator', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='+', to=settings.AUTH_USER_MODEL)),\n            ],\n            options={\n                'abstract': False,\n            },\n            bases=(models.Model, osf.models.base.QuerySetExplainMixin),\n        ),\n        migrations.AddField(\n            model_name='draftregistration',\n            name='date_last_transitioned',\n            field=models.DateTimeField(blank=True, db_index=True, null=True),\n        ),\n        migrations.AddField(\n            model_name='draftregistration',\n            name='machine_state',\n            field=models.CharField(choices=[('initial', 'Initial'), ('pending', 'Pending'), ('accepted', 'Accepted'), ('rejected', 'Rejected'), ('withdrawn', 'Withdrawn'), ('pending_embargo', 'Pending_Embargo'), ('embargo', 'Embargo'), ('pending_embargo_termination', 'Pending_Embargo_Termination'), ('pending_withdraw', 'Pending_Withdraw')], db_index=True, default='initial', max_length=30),\n        ),\n        migrations.AddField(\n            model_name='registrationrequestaction',\n            name='target',\n            field=models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='registration_actions', to='osf.DraftRegistration'),\n        ),\n    ]\n", "rt logging\nimport voluptuous as vol\n\nfrom homeassistant.components.media_player import (\n    MediaPlayerDevice, PLATFORM_SCHEMA)\nfrom homeassistant.components.media_player.const import (\n    SUPPORT_NEXT_TRACK, SUPPORT_PAUSE, SUPPORT_PLAY, SUPPORT_PREVIOUS_TRACK,\n    SUPPORT_SELECT_SOURCE, SUPPORT_STOP, SUPPORT_TURN_OFF, SUPPORT_TURN_ON,\n    SUPPORT_VOLUME_MUTE, SUPPORT_VOLUME_STEP)\nfrom homeassistant.const import (\n    ATTR_COMMAND, ATTR_ENTITY_ID, CONF_DEVICE_CLASS, CONF_HOST, CONF_NAME,\n    CONF_PORT, STATE_IDLE, STATE_OFF, STATE_PAUSED, STATE_PLAYING,\n    STATE_STANDBY)\nfrom homeassistant.exceptions import PlatformNotReady\nimport homeassistant.helpers.config_validation as cv\n\nANDROIDTV_DOMAIN = 'androidtv'\n\n_LOGGER = logging.getLogger(__name__)\n\nSUPPORT_ANDROIDTV = SUPPORT_PAUSE | SUPPORT_PLAY | \\\n    SUPPORT_TURN_ON | SUPPORT_TURN_OFF | SUPPORT_PREVIOUS_TRACK | \\\n    SUPPORT_NEXT_TRACK | SUPPORT_STOP | SUPPORT_VOLUME_MUTE | \\\n    SUPPORT_VOLUME_STEP\n\nSUPPORT_FIRETV = SUPPORT_PAUSE | SUPPORT_PLAY | \\\n    SUPPORT_TURN_ON | SUPPORT_TURN_OFF | SUPPORT_PREVIOUS_TRACK | \\\n    SUPPORT_NEXT_TRACK | SUPPORT_SELECT_SOURCE | SUPPORT_STOP\n\nCONF_ADBKEY = 'adbkey'\nCONF_ADB_SERVER_IP = 'adb_server_ip'\nCONF_ADB_SERVER_PORT = 'adb_server_port'\nCONF_APPS = 'apps'\nCONF_GET_SOURCES = 'get_sources'\nCONF_TURN_ON_COMMAND = 'turn_on_command'\nCONF_TURN_OFF_COMMAND = 'turn_off_command'\n\nDEFAULT_NAME = 'Android TV'\nDEFAULT_PORT = 5555\nDEFAULT_ADB_SERVER_PORT = 5037\nDEFAULT_GET_SOURCES = True\nDEFAULT_DEVICE_CLASS = 'auto'\n\nDEVICE_ANDROIDTV = 'androidtv'\nDEVICE_FIRETV = 'firetv'\nDEVICE_CLASSES = [DEFAULT_DEVICE_CLASS, DEVICE_ANDROIDTV, DEVICE_FIRETV]\n\nSERVICE_ADB_COMMAND = 'adb_command'\n\nSERVICE_ADB_COMMAND_SCHEMA = vol.Schema({\n    vol.Required(ATTR_ENTITY_ID): cv.entity_ids,\n    vol.Required(ATTR_COMMAND): cv.string,\n})\n\n\nPLATFORM_SCHEMA = PLATFORM_SCHEMA.extend({\n    vol.Required(CONF_HOST): cv.string,\n    vol.Optional(CONF_DEVICE_CLASS, default=DEFAULT_DEVICE_CLASS):\n        vol.In(DEVICE_CLASSES),\n    vol.Optional(CONF_NAME, default=DEFAULT_NAME): cv.string,\n    vol.Optional(CONF_PORT, default=DEFAULT_PORT): cv.port,\n    vol.Optional(CONF_ADBKEY): cv.isfile,\n    vol.Optional(CONF_ADB_SERVER_IP): cv.string,\n    vol.Optional(CONF_ADB_SERVER_PORT, default=DEFAULT_ADB_SERVER_PORT):\n        cv.port,\n    vol.Optional(CONF_GET_SOURCES, default=DEFAULT_GET_SOURCES): cv.boolean,\n    vol.Optional(CONF_APPS, default=dict()):\n        vol.Schema({cv.string: cv.string}),\n    vol.Optional(CONF_TURN_ON_COMMAND): cv.string,\n    vol.Optional(CONF_TURN_OFF_COMMAND): cv.string\n})\n\n# Translate from `AndroidTV` / `FireTV` reported state to HA state.\nANDROIDTV_STATES = {'off': STATE_OFF,\n                    'idle': STATE_IDLE,\n                    'standby': STATE_STANDBY,\n                    'playing': STATE_PLAYING,\n                    'paused': STATE_PAUSED}\n\n\ndef setup_platform(hass, config, add_entities, discovery_info=None):\n    \"\"\"Set up the Android TV / Fire TV platform.\"\"\"\n    from androidtv import setup\n\n    hass.data.setdefault(ANDROIDTV_DOMAIN, {})\n\n    host = '{0}:{1}'.format(config[CONF_HOST], config[CONF_PORT])\n\n    if CONF_ADB_SERVER_IP not in config:\n        # Use \"python-adb\" (Python ADB implementation)\n        adb_log = \"using Python ADB implementation \"\n        if CONF_ADBKEY in config:\n            aftv = setup(host, config[CONF_ADBKEY],\n                         device_class=config[CONF_DEVICE_CLASS])\n            adb_log += \"with adbkey='{0}'\".format(config[CONF_ADBKEY])\n\n        else:\n            aftv = setup(host, device_class=config[CONF_DEVICE_CLASS])\n            adb_log += \"without adbkey authentication\"\n    else:\n        # Use \"pure-python-adb\" (communicate with ADB server)\n        aftv = setup(host, adb_server_ip=config[CONF_ADB_SERVER_IP],\n                     adb_server_port=config[CONF_ADB_SERVER_PORT],\n                     device_class=config[CONF_DEVICE_CLASS])\n        adb_log = \"using ADB server at {0}:{1}\".format(\n            config[CONF_ADB_SERVER_IP], config[CONF_ADB_SERVER_PORT])\n\n    if not aftv.available:\n        # Determine the name that will be used for the device in the log\n        if CONF_NAME in config:\n            device_name = config[CONF_NAME]\n        elif config[CONF_DEVICE_CLASS] == DEVICE_ANDROIDTV:\n            device_name = 'Android TV device'\n        elif config[CONF_DEVICE_CLASS] == DEVICE_FIRETV:\n            device_name = 'Fire TV device'\n        else:\n            device_name = 'Android TV / Fire TV device'\n\n        _LOGGER.warning(\"Could not connect to %s at %s %s\",\n                        device_name, host, adb_log)\n        raise PlatformNotReady\n\n    if host in hass.data[ANDROIDTV_DOMAIN]:\n        _LOGGER.warning(\"Platform already setup on %s, skipping\", host)\n    else:\n        if aftv.DEVICE_CLASS == DEVICE_ANDROIDTV:\n            device = AndroidTVDevice(aftv, config[CONF_NAME],\n                                     config[CONF_APPS],\n                                     config.get(CONF_TURN_ON_COMMAND),\n                                     config.get(CONF_TURN_OFF_COMMAND))\n            device_name = config[CONF_NAME] if CONF_NAME in config \\\n                else 'Android TV'\n        else:\n            device = FireTVDevice(aftv, config[CONF_NAME], config[CONF_APPS],\n                                  config[CONF_GET_SOURCES],\n                                  config.get(CONF_TURN_ON_COMMAND),\n                                  config.get(CONF_TURN_OFF_COMMAND))\n            device_name = config[CONF_NAME] if CONF_NAME in config \\\n                else 'Fire TV'\n\n        add_entities([device])\n        _LOGGER.debug(\"Setup %s at %s%s\", device_name, host, adb_log)\n        hass.data[ANDROIDTV_DOMAIN][host] = device\n\n    if hass.services.has_service(ANDROIDTV_DOMAIN, SERVICE_ADB_COMMAND):\n        return\n\n    def service_adb_command(service):\n        \"\"\"Dispatch service calls to target entities.\"\"\"\n        cmd = service.data.get(ATTR_COMMAND)\n        entity_id = service.data.get(ATTR_ENTITY_ID)\n        target_devices = [dev for dev in hass.data[ANDROIDTV_DOMAIN].values()\n                          if dev.entity_id in entity_id]\n\n        for target_device in target_devices:\n            output = target_device.adb_command(cmd)\n\n            # log the output, if there is any\n            if output:\n                _LOGGER.info(\"Output of command '%s' from '%s': %s\",\n                             cmd, target_device.entity_id, output)\n\n    hass.services.register(ANDROIDTV_DOMAIN, SERVICE_ADB_COMMAND,\n                           service_adb_command,\n                           schema=SERVICE_ADB_COMMAND_SCHEMA)\n\n\ndef adb_decorator(override_available=False):\n    \"\"\"Send an ADB command if the device is available and catch exceptions.\"\"\"\n    def _adb_decorator(func):\n        \"\"\"Wait if previous ADB commands haven't finished.\"\"\"\n        @functools.wraps(func)\n        def _adb_exception_catcher(self, *args, **kwargs):\n            # If the device is unavailable, don't do anything\n            if not self.available and not override_available:\n                return None\n\n            try:\n                return func(self, *args, **kwargs)\n            except self.exceptions as err:\n                _LOGGER.error(\n                    \"Failed to execute an ADB command. ADB connection re-\"\n                    \"establishing attempt in the next update. Error: %s\", err)\n                self._available = False  # pylint: disable=protected-access\n                return None\n\n        return _adb_exception_catcher\n\n    return _adb_decorator\n\n\nclass ADBDevice(MediaPlayerDevice):\n    \"\"\"Representation of an Android TV or Fire TV device.\"\"\"\n\n    def __init__(self, aftv, name, apps, turn_on_command,\n                 turn_off_command):\n        \"\"\"Initialize the Android TV / Fire TV device.\"\"\"\n        from androidtv.constants import APPS, KEYS\n\n        self.aftv = aftv\n        self._name = name\n        self._apps = APPS\n        self._apps.update(apps)\n        self._keys = KEYS\n\n        self.turn_on_command = turn_on_command\n        self.turn_off_command = turn_off_command\n\n        # ADB exceptions to catch\n        if not self.aftv.adb_server_ip:\n            # Using \"python-adb\" (Python ADB implementation)\n            from adb.adb_protocol import (InvalidChecksumError,\n                                          InvalidCommandError,\n                                          InvalidResponseError)\n            from adb.usb_exceptions import TcpTimeoutException\n\n            self.exceptions = (AttributeError, BrokenPipeError, TypeError,\n                               ValueError, InvalidChecksumError,\n                               InvalidCommandError, InvalidResponseError,\n                               TcpTimeoutException)\n        else:\n            # Using \"pure-python-adb\" (communicate with ADB server)\n            self.exceptions = (ConnectionResetError, RuntimeError)\n\n        # Property attributes\n        self._adb_response = None\n        self._available = self.aftv.available\n        self._current_app = None\n        self._state = None\n\n    @property\n    def app_id(self):\n        \"\"\"Return the current app.\"\"\"\n        return self._current_app\n\n    @property\n    def app_name(self):\n        \"\"\"Return the friendly name of the current app.\"\"\"\n        return self._apps.get(self._current_app, self._current_app)\n\n    @property\n    def available(self):\n        \"\"\"Return whether or not the ADB connection is valid.\"\"\"\n        return self._available\n\n    @property\n    def device_state_attributes(self):\n        \"\"\"Provide the last ADB command's response as an attribute.\"\"\"\n        return {'adb_response': self._adb_response}\n\n    @property\n    def name(self):\n        \"\"\"Return the device name.\"\"\"\n        return self._name\n\n    @property\n    def should_poll(self):\n        \"\"\"Device should be polled.\"\"\"\n        return True\n\n    @property\n    def state(self):\n        \"\"\"Return the state of the player.\"\"\"\n        return self._state\n\n    @adb_decorator()\n    def media_play(self):\n        \"\"\"Send play command.\"\"\"\n        self.aftv.media_play()\n\n    @adb_decorator()\n    def media_pause(self):\n        \"\"\"Send pause command.\"\"\"\n        self.aftv.media_pause()\n\n    @adb_decorator()\n    def media_play_pause(self):\n        \"\"\"Send play/pause command.\"\"\"\n        self.aftv.media_play_pause()\n\n    @adb_decorator()\n    def turn_on(self):\n        \"\"\"Turn on the device.\"\"\"\n        if self.turn_on_command:\n            self.aftv.adb_shell(self.turn_on_command)\n        else:\n            self.aftv.turn_on()\n\n    @adb_decorator()\n    def turn_off(self):\n        \"\"\"Turn off the device.\"\"\"\n        if self.turn_off_command:\n            self.aftv.adb_shell(self.turn_off_command)\n        else:\n            self.aftv.turn_off()\n\n    @adb_decorator()\n    def media_previous_track(self):\n        \"\"\"Send previous track command (results in rewind).\"\"\"\n        self.aftv.media_previous_track()\n\n    @adb_decorator()\n    def media_next_track(self):\n        \"\"\"Send next track command (results in fast-forward).\"\"\"\n        self.aftv.media_next_track()\n\n    @adb_decorator()\n    def adb_command(self, cmd):\n        \"\"\"Send an ADB command to an Android TV / Fire TV device.\"\"\"\n        key = self._keys.get(cmd)\n        if key:\n            self.aftv.adb_shell('input keyevent {}'.format(key))\n            self._adb_response = None\n            self.schedule_update_ha_state()\n            return\n\n        if cmd == 'GET_PROPERTIES':\n            self._adb_response = str(self.aftv.get_properties_dict())\n            self.schedule_update_ha_state()\n            return self._adb_response\n\n        response = self.aftv.adb_shell(cmd)\n        if isinstance(response, str) and response.strip():\n            self._adb_response = response.strip()\n        else:\n            self._adb_response = None\n\n        self.schedule_update_ha_state()\n        return self._adb_response\n\n\nclass AndroidTVDevice(ADBDevice):\n    \"\"\"Representation of an Android TV device.\"\"\"\n\n    def __init__(self, aftv, name, apps, turn_on_command,\n                 turn_off_command):\n        \"\"\"Initialize the Android TV device.\"\"\"\n        super().__init__(aftv, name, apps, turn_on_command,\n                         turn_off_command)\n\n        self._device = None\n        self._device_properties = self.aftv.device_properties\n        self._is_volume_muted = None\n        self._unique_id = self._device_properties.get('serialno')\n        self._volume_level = None\n\n    @adb_decorator(override_available=True)\n    def update(self):\n        \"\"\"Update the device state and, if necessary, re-connect.\"\"\"\n        # Check if device is disconnected.\n        if not self._available:\n            # Try to connect\n            self._available = self.aftv.connect(always_log_errors=False)\n\n            # To be safe, wait until the next update to run ADB commands.\n            return\n\n        # If the ADB connection is not intact, don't update.\n        if not self._available:\n            return\n\n        # Get the updated state and attributes.\n        state, self._current_app, self._device, self._is_volume_muted, \\\n            self._volume_level = self.aftv.update()\n\n        self._state = ANDROIDTV_STATES[state]\n\n    @property\n    def is_volume_muted(self):\n        \"\"\"Boolean if volume is currently muted.\"\"\"\n        return self._is_volume_muted\n\n    @property\n    def source(self):\n        \"\"\"Return the current playback device.\"\"\"\n        return self._device\n\n    @property\n    def supported_features(self):\n        \"\"\"Flag media player features that are supported.\"\"\"\n        return SUPPORT_ANDROIDTV\n\n    @property\n    def unique_id(self):\n        \"\"\"Return the device unique id.\"\"\"\n        return self._unique_id\n\n    @property\n    def volume_level(self):\n        \"\"\"Return the volume level.\"\"\"\n        return self._volume_level\n\n    @adb_decorator()\n    def media_stop(self):\n        \"\"\"Send stop command.\"\"\"\n        self.aftv.media_stop()\n\n    @adb_decorator()\n    def mute_volume(self, mute):\n        \"\"\"Mute the volume.\"\"\"\n        self.aftv.mute_volume()\n\n    @adb_decorator()\n    def volume_down(self):\n        \"\"\"Send volume down command.\"\"\"\n        self._volume_level = self.aftv.volume_down(self._volume_level)\n\n    @adb_decorator()\n    def volume_up(self):\n        \"\"\"Send volume up command.\"\"\"\n        self._volume_level = self.aftv.volume_up(self._volume_level)\n\n\nclass FireTVDevice(ADBDevice):\n    \"\"\"Representation of a Fire TV device.\"\"\"\n\n    def __init__(self, aftv, name, apps, get_sources,\n                 turn_on_command, turn_off_command):\n        \"\"\"Initialize the Fire TV device.\"\"\"\n        super().__init__(aftv, name, apps, turn_on_command,\n                         turn_off_command)\n\n        self._get_sources = get_sources\n        self._running_apps = None\n\n    @adb_decorator(override_available=True)\n    def update(self):\n        \"\"\"Update the device state and, if necessary, re-connect.\"\"\"\n        # Check if device is disconnected.\n        if not self._available:\n            # Try to connect\n            self._available = self.aftv.connect(always_log_errors=False)\n\n            # To be safe, wait until the next update to run ADB commands.\n            return\n\n        # If the ADB connection is not intact, don't update.\n        if not self._available:\n            return\n\n        # Get the `state`, `current_app`, and `running_apps`.\n        state, self._current_app, self._running_apps = \\\n            self.aftv.update(self._get_sources)\n\n        self._state = ANDROIDTV_STATES[state]\n\n    @property\n    def source(self):\n        \"\"\"Return the current app.\"\"\"\n        return self._current_app\n\n    @property\n    def source_list(self):\n        \"\"\"Return a list of running apps.\"\"\"\n        return self._running_apps\n\n    @property\n    def supported_features(self):\n        \"\"\"Flag media player features that are supported.\"\"\"\n        return SUPPORT_FIRETV\n\n    @adb_decorator()\n    def media_stop(self):\n        \"\"\"Send stop (back) command.\"\"\"\n        self.aftv.back()\n\n    @adb_decorator()\n    def select_source(self, source):\n        \"\"\"Select input source.\n\n        If the source starts with a '!', then it will close the app instead of\n        opening it.\n        \"\"\"\n        if isinstance(source, str):\n            if not source.startswith('!'):\n                self.aftv.launch_app(source)\n            else:\n                self.aftv.stop_app(source[1:].lstrip())\n", "\"); you may\n# not use this file except in compliance with the License. You may obtain\n# a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations\n# under the License.\n\nimport os\n\nimport mock\nimport pkg_resources\n\nfrom glance.common.artifacts import loader\nfrom glance.common import exception\nfrom glance.contrib.plugins.artifacts_sample.v1 import artifact as art1\nfrom glance.contrib.plugins.artifacts_sample.v2 import artifact as art2\nfrom glance.tests import utils\n\n\nclass MyArtifactDuplicate(art1.MyArtifact):\n    __type_version__ = '1.0.1'\n    __type_name__ = 'MyArtifact'\n\n\nclass MyArtifactOk(art1.MyArtifact):\n    __type_version__ = '1.0.2'\n    __type_name__ = 'MyArtifact'\n\n\nclass TestArtifactsLoader(utils.BaseTestCase):\n    def setUp(self):\n        self.path = 'glance.contrib.plugins.artifacts_sample'\n        self._setup_loader(['MyArtifact=%s.v1.artifact:MyArtifact' %\n                            self.path])\n        super(TestArtifactsLoader, self).setUp()\n\n    def _setup_loader(self, artifacts):\n        self.loader = None\n        mock_this = 'stevedore.extension.ExtensionManager._find_entry_points'\n        with mock.patch(mock_this) as fep:\n            fep.return_value = [\n                pkg_resources.EntryPoint.parse(art) for art in artifacts]\n            self.loader = loader.ArtifactsPluginLoader(\n                'glance.artifacts.types')\n\n    def test_load(self):\n        \"\"\"\n        Plugins can be loaded as entrypoint=sigle plugin and\n        entrypoint=[a, list, of, plugins]\n        \"\"\"\n        # single version\n        self.assertEqual(1, len(self.loader.mgr.extensions))\n        self.assertEqual(art1.MyArtifact,\n                         self.loader.get_class_by_endpoint('myartifact'))\n        # entrypoint = [a, list]\n        path = os.path.splitext(__file__)[0].replace('/', '.')\n        self._setup_loader([\n            'MyArtifact=%s:MyArtifactOk' % path,\n            'MyArtifact=%s.v2.artifact:MyArtifact' % self.path,\n            'MyArtifact=%s.v1.artifact:MyArtifact' % self.path]),\n        self.assertEqual(3, len(self.loader.mgr.extensions))\n        # returns the plugin with the latest version\n        self.assertEqual(art2.MyArtifact,\n                         self.loader.get_class_by_endpoint('myartifact'))\n        self.assertEqual(art1.MyArtifact,\n                         self.loader.get_class_by_endpoint('myartifact',\n                                                           '1.0.1'))\n\n    def test_basic_loader_func(self):\n        \"\"\"Test public methods of PluginLoader class here\"\"\"\n        # type_version 2 == 2.0 == 2.0.0\n        self._setup_loader(\n            ['MyArtifact=%s.v2.artifact:MyArtifact' % self.path])\n        self.assertEqual(art2.MyArtifact,\n                         self.loader.get_class_by_endpoint('myartifact'))\n        self.assertEqual(art2.MyArtifact,\n                         self.loader.get_class_by_endpoint('myartifact',\n                                                           '2.0'))\n        self.assertEqual(art2.MyArtifact,\n                         self.loader.get_class_by_endpoint('myartifact',\n                                                           '2.0.0'))\n        self.assertEqual(art2.MyArtifact,\n                         self.loader.get_class_by_endpoint('myartifact',\n                                                           '2'))\n        # now make sure that get_class_by_typename works as well\n        self.assertEqual(art2.MyArtifact,\n                         self.loader.get_class_by_typename('MyArtifact'))\n        self.assertEqual(art2.MyArtifact,\n                         self.loader.get_class_by_typename('MyArtifact', '2'))\n\n    def test_config_validation(self):\n        \"\"\"\n        Plugins can be loaded on certain conditions:\n            * entry point name == type_name\n            * no plugin with the same type_name and version has been already\n              loaded\n        \"\"\"\n        path = 'glance.contrib.plugins.artifacts_sample'\n        # here artifacts specific validation is checked\n        self.assertRaises(exception.ArtifactNonMatchingTypeName,\n                          self._setup_loader,\n                          ['non_matching_name=%s.v1.artifact:MyArtifact' %\n                           path])\n        # make sure this call is ok\n        self._setup_loader(['MyArtifact=%s.v1.artifact:MyArtifact' % path])\n        art_type = self.loader.get_class_by_endpoint('myartifact')\n        self.assertEqual('MyArtifact', art_type.metadata.type_name)\n        self.assertEqual('1.0.1', art_type.metadata.type_version)\n        # now try to add duplicate artifact with the same type_name and\n        # type_version as already exists\n        bad_art_path = os.path.splitext(__file__)[0].replace('/', '.')\n        self.assertEqual(art_type.metadata.type_version,\n                         MyArtifactDuplicate.metadata.type_version)\n        self.assertEqual(art_type.metadata.type_name,\n                         MyArtifactDuplicate.metadata.type_name)\n        # should raise an exception as (name, version) is not unique\n        self.assertRaises(\n            exception.ArtifactDuplicateNameTypeVersion, self._setup_loader,\n            ['MyArtifact=%s.v1.artifact:MyArtifact' % path,\n             'MyArtifact=%s:MyArtifactDuplicate' % bad_art_path])\n        # two artifacts with the same name but different versions coexist fine\n        self.assertEqual('MyArtifact', MyArtifactOk.metadata.type_name)\n        self.assertNotEqual(art_type.metadata.type_version,\n                            MyArtifactOk.metadata.type_version)\n        self._setup_loader(['MyArtifact=%s.v1.artifact:MyArtifact' % path,\n                            'MyArtifact=%s:MyArtifactOk' % bad_art_path])\n\n    def test_check_function(self):\n        \"\"\"\n        A test to show that plugin-load specific options in artifacts.conf\n        are correctly processed:\n            * no plugins can be loaded if load_enabled = False\n            * if available_plugins list is given only plugins specified can be\n              be loaded\n        \"\"\"\n        self.config(load_enabled=False)\n        self.assertRaises(exception.ArtifactLoadError,\n                          self._setup_loader,\n                          ['MyArtifact=%s.v1.artifact:MyArtifact' % self.path])\n        self.config(load_enabled=True, available_plugins=['MyArtifact-1.0.2'])\n        self.assertRaises(exception.ArtifactLoadError,\n                          self._setup_loader,\n                          ['MyArtifact=%s.v1.artifact:MyArtifact' % self.path])\n        path = os.path.splitext(__file__)[0].replace('/', '.')\n        self._setup_loader(['MyArtifact=%s:MyArtifactOk' % path])\n        # make sure that plugin_map has the expected plugin\n        self.assertEqual(MyArtifactOk,\n                         self.loader.get_class_by_endpoint('myartifact',\n                                                           '1.0.2'))\n"], "perplexity": [1.3673864603042603, 2.426147937774658, 1.4808419942855835, 1.915206789970398, 189.40858459472656, 3.438356399536133, 1.7431472539901733, 2.2749075889587402, 3.270189046859741, 2.4215190410614014, 3.4813172817230225, 10.804075241088867, 3.143385887145996, 7.2014594078063965, 2.5523743629455566, 3.7120964527130127, 3.360570192337036, 3.0290074348449707, 2.521862030029297, 3.4932336807250977, 2.7097954750061035, 1.4873496294021606, 6.510127544403076, 8.480474472045898, 2.808295965194702, 2.515925407409668, 1.7398897409439087, 2.937298536300659, 1.7228440046310425, 2.288424491882324, 6.424860000610352, 2.062394380569458, 2.557607889175415, 2.5340540409088135, 2.764343738555908, 2.090437412261963, 3.7444865703582764, 17.622766494750977, 3.2481460571289062, 3.83923077583313, 2.427686929702759, 2.4059226512908936, 2.6347615718841553, 2.285526990890503, 4.691099166870117, 3.772139549255371, 3.944850206375122, 4.288350582122803, 2.60085391998291, 2.078688383102417, 6.842905044555664, 4.353711128234863, 4.470126152038574, 4.123263835906982, 3.6783287525177, 5.633009433746338, 3.4176084995269775, 2.621926784515381, 2.628365993499756, 3.5870699882507324, 2.706195116043091, 1.984021544456482, 2.936049699783325, 2.6245856285095215, 2.741469621658325, 2.058897018432617, 1.496031641960144, 6.289626121520996, 1.9821059703826904, 2.129014492034912, 3.372206926345825, 2.404909372329712, 14.632003784179688, 4.032564640045166, 3.9169857501983643, 2.0789296627044678, 3.1157939434051514, 2.172081708908081, 1.967470645904541, 2.3036396503448486, 3.9501094818115234, 5.508388996124268, 4.263528347015381, 1.8041263818740845, 4.46992826461792, 2.3009300231933594, 1.8965725898742676, 5.585927963256836, 3.631366014480591, 1.6393883228302002, 3.500149965286255, 4.648936748504639, 3.112626552581787, 2.6497559547424316, 18.529190063476562, 1.8419567346572876, 1.9450342655181885, 1.9049243927001953, 2.271192789077759, 1.898062825202942], "avg_perplexity": 5.499172968864441}