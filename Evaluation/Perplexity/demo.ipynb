{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "\n",
    "\n",
    "def get_perplexity(text, stride):\n",
    "    encodings = TOKENIZER(text, return_tensors=\"pt\").to(DEVICE)\n",
    "    max_length = MODEL.config.n_positions\n",
    "    seq_len = encodings.input_ids.size(1)\n",
    "    nlls = []\n",
    "    prev_end_loc = 0\n",
    "    # for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "    for begin_loc in range(0, seq_len, stride):\n",
    "        end_loc = min(begin_loc + max_length, seq_len)\n",
    "        trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(DEVICE)\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[:, :-trg_len] = -100\n",
    "        with torch.no_grad():\n",
    "            outputs = MODEL(input_ids, labels=target_ids)\n",
    "            neg_log_likelihood = outputs.loss\n",
    "        nlls.append(neg_log_likelihood)\n",
    "        prev_end_loc = end_loc\n",
    "        if end_loc == seq_len:\n",
    "            break\n",
    "    ppl = torch.exp(torch.stack(nlls).mean())\n",
    "    return ppl\n",
    "\n",
    "def save_github_code_eval_subset(language, license, size=1000, shuffle=True):\n",
    "    \"\"\"\n",
    "    Saves a subset of the github code dataset to be used for evaluation.\n",
    "    :param language: Programming language of the code\n",
    "    :param license: License of the code\n",
    "    :param size: Size of the subset\n",
    "    :param shuffle: Whether to shuffle the dataset before taking the subset\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    ds = load_dataset(\"codeparrot/github-code\", languages=[language], licenses=[license], streaming=True, split=\"train\")\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=size)\n",
    "    ds = ds.take(size)\n",
    "    evaluation_dataset = []\n",
    "\n",
    "    for item in tqdm(ds, total=size):\n",
    "        evaluation_dataset.append(item['code'])\n",
    "\n",
    "    eval_data_name = f\"./data/evaluation_data_{language}_{license}_{size}.pkl\"\n",
    "\n",
    "    with open(eval_data_name, 'wb') as f:\n",
    "        pickle.dump(evaluation_dataset, f)\n",
    "\n",
    "\n",
    "def load_saved_github_code_eval_subset(language, license, size=1000):\n",
    "    \"\"\"\n",
    "    Loads a subset of the github code dataset to be used for evaluation.\n",
    "    :param language: Programming language of the code\n",
    "    :param license: License of the code\n",
    "    :param size: Size of the subset\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    eval_data_name = f\"./data/evaluation_data_{language}_{license}_{size}.pkl\"\n",
    "    if not os.path.exists(eval_data_name):\n",
    "        print(f\"Saved evaluation data not found. Saving now...\")\n",
    "        save_github_code_eval_subset(language, license, size)\n",
    "    with open(eval_data_name, 'rb') as f:\n",
    "        evaluation_dataset = pickle.load(f)\n",
    "    return evaluation_dataset\n",
    "\n",
    "\n",
    "def save_results_dict(results_dict, language, license, stride):\n",
    "    \"\"\"\n",
    "    Saves a results dictionary to be used for evaluation to a json file.\n",
    "    :param results_dict: Dictionary containing the results\n",
    "    :param language: Programming language of the code\n",
    "    :param license: License of the code\n",
    "    :param size: Size of the subset\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    results_name = f\"./results/results_{language}_{license}_{stride}.json\"\n",
    "    with open(results_name, 'w') as f:\n",
    "        json.dump(results_dict, f)\n",
    "    print(f\"Saved results to {results_name}\")\n",
    "\n",
    "    short_results_name = f\"./results/results_{language}_{license}_{stride}_short.json\"\n",
    "    # Delete text to save space\n",
    "    del results_dict['text']\n",
    "    with open(short_results_name, 'w') as f:\n",
    "        json.dump(results_dict, f)\n",
    "    print(f\"Saved short results to {short_results_name}\")\n",
    "\n",
    "\n",
    "def main(languages, licenses, size, stride, n_samples=None):\n",
    "    for language in languages:\n",
    "        for license in licenses:\n",
    "            print('='*50)\n",
    "            print(f\"Language: {language}, License: {license}\")\n",
    "\n",
    "            # Load evaluation data\n",
    "            eval_data = load_saved_github_code_eval_subset(language, license, size)\n",
    "            print(f\"Loaded {len(eval_data)} samples for evaluation.\")\n",
    "            \n",
    "\n",
    "            # Initialize results dictionary\n",
    "            results = {'text': [], 'perplexity': []}\n",
    "\n",
    "            # Set number of samples to evaluate\n",
    "            n_samples = len(eval_data) if n_samples is None else n_samples\n",
    "            ppl = 0\n",
    "            tbar = tqdm(range(n_samples),total=n_samples, unit='Sample', position=0, leave=True, desc=f'Current perplexity: {ppl:.2f}')\n",
    "            \n",
    "            # Evaluate\n",
    "            for i in tbar:\n",
    "                text = eval_data[i]\n",
    "                # Skip the first 500 characters\n",
    "                text = text[500:]\n",
    "\n",
    "                if len(text) > 5000:\n",
    "                    # Skip long samples\n",
    "                    continue\n",
    "\n",
    "\n",
    "                ppl = get_perplexity(text, stride)\n",
    "                results['text'].append(text[100:])\n",
    "                results['perplexity'].append(ppl.item())\n",
    "                tbar.set_description(f'Current perplexity: {ppl:.2f}')\n",
    "                tbar.refresh()\n",
    "                \n",
    "            # Save results\n",
    "            results['avg_perplexity'] = sum(results['perplexity'])/len(results['perplexity'])\n",
    "            save_results_dict(results, language, license, stride)\n",
    "            print(f\"Average perplexity: {results['avg_perplexity']:.2f}\")\n",
    "            print('='*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global DEVICE\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"Salesforce/codegen-350M-mono\"\n",
    "global MODEL\n",
    "MODEL = AutoModelWithLMHead.from_pretrained(model_name).to(DEVICE)\n",
    "global TOKENIZER\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = ['Python', 'Java']\n",
    "licenses = ['mit', 'apache-2.0']\n",
    "size = 10000\n",
    "stride = 1024\n",
    "n_samples = 3000\n",
    "main(languages, licenses, size, stride, n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amazon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
