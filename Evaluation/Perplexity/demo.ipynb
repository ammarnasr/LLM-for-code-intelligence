{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/content/drive/MyDrive/LLM-for-code-intelligence-Project/LLM-for-code-intelligence/Evaluation/Perplexity')\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "\n",
    "global DEVICE\n",
    "global MODEL\n",
    "global TOKENIZER\n",
    "global MAX_LENGTH\n",
    "\n",
    "def get_perplexity(text, stride):\n",
    "    encodings = TOKENIZER(text, return_tensors=\"pt\").to(DEVICE)\n",
    "    seq_len = encodings.input_ids.size(1)\n",
    "    nlls = []\n",
    "    prev_end_loc = 0\n",
    "    for begin_loc in range(0, seq_len, stride):\n",
    "        end_loc = min(begin_loc + MAX_LENGTH, seq_len)\n",
    "        trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(DEVICE)\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[:, :-trg_len] = -100\n",
    "        with torch.no_grad():\n",
    "            outputs = MODEL(input_ids, labels=target_ids)\n",
    "            neg_log_likelihood = outputs.loss\n",
    "        nlls.append(neg_log_likelihood)\n",
    "        prev_end_loc = end_loc\n",
    "        if end_loc == seq_len:\n",
    "            break\n",
    "    ppl = torch.exp(torch.stack(nlls).mean())\n",
    "    return ppl\n",
    "\n",
    "def save_github_code_eval_subset(language, license, size=1000, shuffle=True):\n",
    "    \"\"\"\n",
    "    Saves a subset of the github code dataset to be used for evaluation.\n",
    "    :param language: Programming language of the code\n",
    "    :param license: License of the code\n",
    "    :param size: Size of the subset\n",
    "    :param shuffle: Whether to shuffle the dataset before taking the subset\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    print(f\"Getting codepaarot/github-code dataset for {language} with {license} license and {size} samples from HuggingFace...\")\n",
    "    ds = load_dataset(\"codeparrot/github-code\", languages=[language], licenses=[license], streaming=True, split=\"train\")\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=size)\n",
    "    ds = ds.take(size)\n",
    "    evaluation_dataset = []\n",
    "\n",
    "    for item in tqdm(ds, total=size, desc=\"Saving evaluation data Locally\"):\n",
    "        evaluation_dataset.append(item['code'])\n",
    "\n",
    "    eval_data_name = f\"./data/evaluation_data_{language}_{license}_{size}.pkl\"\n",
    "\n",
    "    with open(eval_data_name, 'wb') as f:\n",
    "        pickle.dump(evaluation_dataset, f)\n",
    "\n",
    "\n",
    "def load_saved_github_code_eval_subset(language, license, size=1000):\n",
    "    \"\"\"\n",
    "    Loads a subset of the github code dataset to be used for evaluation.\n",
    "    :param language: Programming language of the code\n",
    "    :param license: License of the code\n",
    "    :param size: Size of the subset\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    eval_data_name = f\"./data/evaluation_data_{language}_{license}_{size}.pkl\"\n",
    "    if not os.path.exists(eval_data_name):\n",
    "        print(f\"Saved evaluation data NOT found. Saving now...\")\n",
    "        save_github_code_eval_subset(language, license, size)\n",
    "    else:\n",
    "        print(f\"FOUND saved evaluation data\")\n",
    "    with open(eval_data_name, 'rb') as f:\n",
    "        evaluation_dataset = pickle.load(f)\n",
    "    return evaluation_dataset\n",
    "\n",
    "\n",
    "def save_results_dict(results_dict, language, license, stride, res_dir):\n",
    "    \"\"\"\n",
    "    Saves a results dictionary to be used for evaluation to a json file.\n",
    "    :param results_dict: Dictionary containing the results\n",
    "    :param language: Programming language of the code\n",
    "    :param license: License of the code\n",
    "    :param size: Size of the subset\n",
    "    :param res_dir: Directory to save the results\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    #check if directory name results exists\n",
    "    if not os.path.exists('results'):\n",
    "        os.makedirs('results')\n",
    "    #check if res_dir exists in results\n",
    "    final_res_dir = f\"./results/{res_dir}\"\n",
    "    if not os.path.exists(final_res_dir):\n",
    "        os.makedirs(final_res_dir)\n",
    "    results_name = f\"{final_res_dir}/results_{language}_{license}_{stride}.json\"\n",
    "    results_name_short = f\"{final_res_dir}/results_{language}_{license}_{stride}_short.json\"\n",
    "\n",
    "    with open(results_name, 'w') as f:\n",
    "        json.dump(results_dict, f)\n",
    "    print(f\"Saved results to {results_name}\")\n",
    "\n",
    "    # Delete text to save space\n",
    "    del results_dict['text']\n",
    "    with open(results_name_short, 'w') as f:\n",
    "        json.dump(results_dict, f)\n",
    "    print(f\"Saved short results to {results_name_short}\")\n",
    "\n",
    "\n",
    "def main(languages, licenses, stride, size=10000, n_samples=None, res_dir='base'):\n",
    "    lang_tbar = tqdm(languages, total=len(languages), unit='Language', position=0, leave=True)\n",
    "    for language in lang_tbar:\n",
    "        lang_tbar.set_description(f'Current language: {language}')\n",
    "        license_tbar = tqdm(licenses, total=len(licenses), unit='License', position=1, leave=True)\n",
    "        for license in license_tbar:\n",
    "            license_tbar.set_description(f'Current license: {license}')\n",
    "\n",
    "            print(f\"Loading Evaluation Data for {language} with {license} license and {size} samples...\")\n",
    "            eval_data = load_saved_github_code_eval_subset(language, license, size)\n",
    "\n",
    "\n",
    "            n_samples = len(eval_data) if n_samples is None else n_samples\n",
    "            ppl = 0\n",
    "            current_text_len = 0\n",
    "            results = {'text': [], 'perplexity': []}\n",
    "\n",
    "            tbar = tqdm(range(n_samples),total=n_samples, unit='Sample', position=0, leave=True, desc=f'Current perplexity: {ppl:.2f}| Current text length: {current_text_len}')\n",
    "            for i in tbar:\n",
    "                text = eval_data[i]\n",
    "                current_text_len = len(text)\n",
    "                ppl = get_perplexity(text, stride)\n",
    "                results['text'].append(text[100:])\n",
    "                results['perplexity'].append(ppl.item())\n",
    "                tbar.set_description(f'Current perplexity: {ppl:.2f}| Current text length: {current_text_len}')\n",
    "                tbar.refresh()\n",
    "\n",
    "            # Save results\n",
    "            results['avg_perplexity'] = sum(results['perplexity'])/len(results['perplexity'])\n",
    "            save_results_dict(results, language, license, stride, res_dir)\n",
    "            print(f\"Average perplexity: {results['avg_perplexity']:.2f}\")\n",
    "            print('='*50)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"Salesforce/codegen-350M-mono\"\n",
    "MODEL = AutoModelWithLMHead.from_pretrained(model_name).to(DEVICE)\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(model_name)\n",
    "MAX_LENGTH = MODEL.config.n_positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = ['Python', 'Java']\n",
    "licenses = ['mit', 'apache-2.0']\n",
    "stride = 1024\n",
    "n_samples = 100\n",
    "size = 100\n",
    "main(languages, licenses, stride,size=size, n_samples=n_samples)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LoRa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftConfig, PeftModel\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"ammarnasr/codegne-finetuned-LoRa-the-stack-java-v2-checkpoint-200\"\n",
    "config = PeftConfig.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "\n",
    "ckpt =  AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, trust_remote_code=True, use_cache=False)\n",
    "ckpt.enable_input_require_grads()\n",
    "MODEL = PeftModel.from_pretrained(ckpt, model_name)\n",
    "for name, param in MODEL.named_parameters():\n",
    "  if 'lora' in name:\n",
    "    param.requires_grad = True\n",
    "MODEL = MODEL.to(DEVICE)\n",
    "MODEL.print_trainable_parameters()\n",
    "\n",
    "\n",
    "MAX_LENGTH = MODEL.config.n_positions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize The Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "# Read the folder names inside the results folder\n",
    "results_dir = './results'\n",
    "models_dirs = os.listdir(results_dir)\n",
    "# print(models_dirs)\n",
    "\n",
    "# Read the results json files that are inside each folder \n",
    "results = {}\n",
    "for model_dir in models_dirs:\n",
    "    model_dir_path = os.path.join(results_dir, model_dir)\n",
    "    results[model_dir] = {}\n",
    "    for file in os.listdir(model_dir_path):\n",
    "        if file.endswith('.json'):\n",
    "            file_path = os.path.join(model_dir_path, file)\n",
    "            with open(file_path, 'r') as f:\n",
    "                results[model_dir][file[:-5]] = json.load(f)\n",
    "\n",
    "\n",
    "df_dict = {\n",
    "    'Model': [],\n",
    "    'programming_language': [],\n",
    "    'license': [],\n",
    "    'stride': [],\n",
    "    'avg_perplexity': []\n",
    "}\n",
    "\n",
    "for model_dir in results:\n",
    "    for file in results[model_dir]:\n",
    "        if 'short' in file:\n",
    "            continue\n",
    "        df_dict['Model'].append(model_dir)\n",
    "        df_dict['programming_language'].append(file.split('_')[1])\n",
    "        df_dict['license'].append(file.split('_')[2])\n",
    "        df_dict['stride'].append(file.split('_')[3])\n",
    "        df_dict['avg_perplexity'].append(results[model_dir][file]['avg_perplexity'])\n",
    "\n",
    "df = pd.DataFrame(df_dict)\n",
    "df['avg_perplexity'] = df['avg_perplexity'].astype(float)\n",
    "df['stride'] = df['stride'].astype(int)\n",
    "df['Model'] = df['Model'].astype(str)\n",
    "df['programming_language'] = df['programming_language'].astype(str)\n",
    "df['license'] = df['license'].astype(str)\n",
    "\n",
    "#drop license column\n",
    "df = df.drop(columns=['license'])\n",
    "\n",
    "#drop stride column\n",
    "df = df.drop(columns=['stride'])\n",
    "\n",
    "#create new df_dict\n",
    "df_dict = {\n",
    "    'Model': [],\n",
    "    'programming_language': [],\n",
    "    'avg_perplexity': []\n",
    "}\n",
    "\n",
    "#group by model and programming language\n",
    "df_grouped = df.groupby(['Model', 'programming_language']).mean().reset_index()\n",
    "\n",
    "#add to df_dict\n",
    "df_dict['Model'] = df_grouped['Model']\n",
    "df_dict['programming_language'] = df_grouped['programming_language']\n",
    "df_dict['avg_perplexity'] = df_grouped['avg_perplexity']\n",
    "\n",
    "#convert to dataframe\n",
    "df = pd.DataFrame(df_dict)\n",
    "\n",
    "\n",
    "# Plot the results\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "ax = sns.barplot(x=\"Model\", y=\"avg_perplexity\", hue=\"programming_language\", data=df)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amazon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
