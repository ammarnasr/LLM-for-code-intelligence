{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import os\n",
    " \n",
    "# get current directory\n",
    "cwd = os.getcwd()\n",
    "parent = os.path.dirname(cwd)\n",
    "parent = os.path.dirname(parent)\n",
    "\n",
    "src_folder = parent + '\\\\Generation\\\\runs\\\\' \n",
    "dst_folder = './src/'\n",
    "print(src_folder)\n",
    "print(dst_folder)\n",
    "\n",
    "# Copy all files from src_folder to dst_folder\n",
    "for file_name in os.listdir(src_folder):\n",
    "    shutil.copy(src_folder + file_name, dst_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = 'codegen2_1B_humaneval_py'\n",
    "# exp_name = 'codegne_finetuned_LoRa_the_stack_java_v3_checkpoint_2000_humaneval_java'\n",
    "sf = f'src/{exp_name}.jsonl'\n",
    "td = f'tgt/{exp_name.replace(\"-\", \"\")}'\n",
    "!python convert_to_pre_eval.py --source_file $sf --target_dir $td\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "sf = f'src/{exp_name}.jsonl'\n",
    "td = f'tgt/{exp_name.replace(\"-\", \"\")}'\n",
    "eval_cmd = f\"podman run --rm --network none -v ./{td}:/{td}:rw multipl-e-eval --dir /{td} --output-dir /{td} --recursive\"\n",
    "\n",
    "\n",
    "# # subprocess.run(eval_cmd, shell=True)\n",
    "print(\"### EVALUATION COMMAND ###\")\n",
    "print('#',eval_cmd)\n",
    "print(\"### EVALUATION COMMAND ###\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EVALUATION COMMAND ###\n",
    "# podman run --rm --network none -v ./tgt/codegne_finetuned_LoRa_the_stack_java_v3_checkpoint_2000_humaneval_java:/tgt/codegne_finetuned_LoRa_the_stack_java_v3_checkpoint_2000_humaneval_java:rw multipl-e-eval --dir /tgt/codegne_finetuned_LoRa_the_stack_java_v3_checkpoint_2000_humaneval_java --output-dir /tgt/codegne_finetuned_LoRa_the_stack_java_v3_checkpoint_2000_humaneval_java --recursive\n",
    "### EVALUATION COMMAND ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = 'codegen2_1B_humaneval_py'\n",
    "sf = f'src/{exp_name}.jsonl'\n",
    "td = f'tgt/{exp_name.replace(\"-\", \"\")}'\n",
    "target_dir = f'{td}'\n",
    "output_file = f'res/{exp_name}_results.json'\n",
    "!python pass_k.py $target_dir --output $output_file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "\n",
    "\n",
    "#get all the results files in the res folder\n",
    "results_files = [f for f in os.listdir('res') if os.path.isfile(os.path.join('res', f))]\n",
    "\n",
    "#load all the results files into a dictionary\n",
    "results = {}\n",
    "for f in results_files:\n",
    "    with open(os.path.join('res', f)) as json_file:\n",
    "        results[f] = json.load(json_file)\n",
    "['Baseline', 'LoRa-V3-2000',\n",
    "       'codegne_finetuned_LoRa_the_stack_java_v5_checkpoint_1000_humaneval_java',\n",
    "       'codegne_finetuned_LoRa_the_stack_java_v5_checkpoint_1000_humaneval_py',\n",
    "       'Full-V3-1000']\n",
    "#Models Short Names\n",
    "model_names_dict = {\n",
    "    'codegen_350M_mono_humaneval_py': 'Baseline-mono',\n",
    "    'codegen_350M_mono_humaneval_java': 'Baseline-mono',\n",
    "\n",
    "    'codegen_350M_multi_humaneval_py': 'Baseline-multi',\n",
    "    'codegen_350M_multi_humaneval_java': 'Baseline-multi',\n",
    "\n",
    "    'codegen2_1B_humaneval_py': 'Baseline-multi-1B',\n",
    "    'codegen2_1B_humaneval_java': 'Baseline-multi-1B',\n",
    "\n",
    "    'codegne_finetuned_LoRa_the_stack_java_v5_checkpoint_3000_humaneval_java': 'LoRa',\n",
    "    'codegne_finetuned_LoRa_the_stack_java_v5_checkpoint_3000_humaneval_py': 'LoRa',\n",
    "\n",
    "    'codegne1B_finetuned_LoRa_the_stack_java_v5_checkpoint_800_humaneval_py': 'LoRa-1B',\n",
    "    'codegne1B_finetuned_LoRa_the_stack_java_v5_checkpoint_800_humaneval_java': 'LoRa-1B',\n",
    "\n",
    "    'codegne_finetuned_the_stack_java_v3_checkpoint_1000_humaneval_py': 'Full',\n",
    "    'codegne_finetuned_the_stack_java_v3_checkpoint_1000_humaneval_java': 'Full'\n",
    "}\n",
    "\n",
    "\n",
    "#Merge all the results into a single dataframe\n",
    "dict_df = {\n",
    "    'model': [],\n",
    "    'dataset': [],\n",
    "    'pass@10': [],\n",
    "    'pass@100': [],\n",
    "    'num_problems': [],\n",
    "    'min_completions': [],\n",
    "    'max_completions': [],\n",
    "    'programming_language': []\n",
    "}\n",
    "\n",
    "for model in results.keys():\n",
    "    model_results = results[model]\n",
    "    model_name = model.split('_results.json')[0]\n",
    "    if model_name in model_names_dict.keys():\n",
    "        model_name = model_names_dict[model_name]\n",
    "    dataset_name = model_results['pass@10']['dataset']\n",
    "    pass_10 = model_results['pass@10']['estimate']\n",
    "    pass_100 = model_results['pass@100']['estimate']\n",
    "    num_problems = model_results['pass@10']['num_problems']\n",
    "    min_completions = model_results['pass@10']['min_completions']\n",
    "    max_completions = model_results['pass@10']['max_completions']\n",
    "    if 'humaneval_py' in dataset_name:\n",
    "        programming_language = 'python'\n",
    "    elif 'humaneval_java' in dataset_name:\n",
    "        programming_language = 'java'\n",
    "    else:\n",
    "        programming_language = 'unknown'\n",
    "    dict_df['model'].append(model_name)\n",
    "    dict_df['dataset'].append(dataset_name)\n",
    "    dict_df['pass@10'].append(pass_10)\n",
    "    dict_df['pass@100'].append(pass_100)\n",
    "    dict_df['num_problems'].append(num_problems)\n",
    "    dict_df['min_completions'].append(min_completions)\n",
    "    dict_df['max_completions'].append(max_completions)\n",
    "    dict_df['programming_language'].append(programming_language)\n",
    "\n",
    "df = pd.DataFrame(dict_df)\n",
    "\n",
    "#Drop the dataset column\n",
    "df = df.drop(columns=['dataset'])\n",
    "\n",
    "#Drop the min_completions and max_completions columns\n",
    "df = df.drop(columns=['min_completions', 'max_completions'])\n",
    "\n",
    "#Drop the num_problems column\n",
    "# df = df.drop(columns=['num_problems'])\n",
    "\n",
    "#Drop the LoRa-V3-2000 model\n",
    "df = df[df['model'] != 'LoRa-V3-2000']\n",
    "\n",
    "#Drop the LoRa-V5-1000 model\n",
    "df = df[df['model'] != 'LoRa-V5-1000']\n",
    "\n",
    "#Plot the results\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "ax = sns.barplot(x=\"model\", y=\"pass@10\", hue=\"programming_language\", data=df)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n",
    "#set the y axis to start at 0 and end at 0.5\n",
    "plt.ylim(0, 0.5)\n",
    "#writer the y values on top of the bars\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    ax.text(p.get_x()+p.get_width()/2.,\n",
    "            height + 0.01,\n",
    "            '{:1.2f}'.format(height),\n",
    "            ha=\"center\")\n",
    "#plot the legend outside the plot\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()\n",
    "\n",
    "#Plot the results\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "ax = sns.barplot(x=\"model\", y=\"pass@100\", hue=\"programming_language\", data=df)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n",
    "#set the y axis to start at 0 and end at 0.5\n",
    "plt.ylim(0, 0.5)\n",
    "#writer the y values on top of the bars\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    ax.text(p.get_x()+p.get_width()/2.,\n",
    "            height + 0.01,\n",
    "            '{:1.2f}'.format(height),\n",
    "            ha=\"center\")\n",
    "#plot the legend outside the plot\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALL FLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    " \n",
    "# get current directory\n",
    "cwd = os.getcwd()\n",
    "parent = os.path.dirname(cwd)\n",
    "parent = os.path.dirname(parent)\n",
    "\n",
    "src_folder = parent + '\\\\Generation\\\\runs\\\\' \n",
    "dst_folder = './src/'\n",
    "\n",
    "# Copy all files from src_folder to dst_folder\n",
    "for file_name in os.listdir(src_folder):\n",
    "    shutil.copy(src_folder + file_name, dst_folder)\n",
    "\n",
    "\n",
    "#Loop over all the files in the src folder and get the experiment names to run\n",
    "for file_name in os.listdir(src_folder):\n",
    "    exp_name = file_name.split('.jsonl')[0]\n",
    "    #Print the experiment name\n",
    "    print(f'Now running experiment: {exp_name}')\n",
    "\n",
    "    #Convert the jsonl file to the pre-evaluation format\n",
    "    sf = f'src/{exp_name}.jsonl'\n",
    "    td = f'tgt/{exp_name.replace(\"-\", \"\")}'\n",
    "    print(f'Converting {sf} to {td}')\n",
    "    !python convert_to_pre_eval.py --source_file $sf --target_dir $td\n",
    "    print('Done!')\n",
    "\n",
    "    #Run the evaluation\n",
    "    print(f'Running evaluation on {td}')\n",
    "    eval_cmd = f\"podman run --rm --network none -v ./{td}:/{td}:rw multipl-e-eval --dir /{td} --output-dir /{td} --recursive\"\n",
    "    # subprocess.run(eval_cmd, shell=True)\n",
    "    print(\"### EVALUATION COMMAND ###\")\n",
    "    print('#',eval_cmd)\n",
    "    print(\"### EVALUATION COMMAND ###\")\n",
    "    # print('Done!')\n",
    "\n",
    "    # Run the pass@k\n",
    "    print(f'Running pass@k on {td}')\n",
    "    target_dir = f'{td}'\n",
    "    output_file = f'res/{exp_name}_results.json'\n",
    "    !python pass_k.py $target_dir --output $output_file\n",
    "    print('Done!')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amazon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
