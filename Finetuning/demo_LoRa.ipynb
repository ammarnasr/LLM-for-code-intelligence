{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjujWVI7TMIV"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjCe83B93L4p"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRI1zO80TUkB"
      },
      "outputs": [],
      "source": [
        "!chmod +x setup.sh\n",
        "!./setup.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGN9suZUTX92"
      },
      "outputs": [],
      "source": [
        "project_name = 'LLM-for-code-intelligence-Project'\n",
        "import os\n",
        "\n",
        "path= f'/content/drive/MyDrive/{project_name}'\n",
        "\n",
        "if not os.path.exists(path):\n",
        "  os.mkdir(path)\n",
        "\n",
        "os.chdir(path)\n",
        "\n",
        "repo_name = 'LLM-for-code-intelligence'\n",
        "repo_path = f'{path}/{repo_name}'\n",
        "url = f'https://github.com/ammarnasr/{repo_name}.git'\n",
        "\n",
        "\n",
        "\n",
        "if not os.path.exists(repo_path):\n",
        "    #clone the repo\n",
        "    print('Cloning the repo...')\n",
        "    !git clone $url\n",
        "else:\n",
        "    #pull the repo\n",
        "    print('Pulling the repo...')\n",
        "    !git -C $repo_name pull\n",
        "\n",
        "os.chdir(repo_path)\n",
        "\n",
        "print(f'Current Dir: {os.getcwd()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJd7NEkQBr3-"
      },
      "source": [
        "#### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rNF9Wwg2TYuv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# os.chdir('/content/drive/MyDrive/LLM-for-code-intelligence-Project/LLM-for-code-intelligence/Finetuning')\n",
        "import torch\n",
        "from dataclasses import dataclass\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import IterableDataset\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from tqdm.notebook import tqdm\n",
        "from peft import LoraConfig, get_peft_model, PeftConfig, PeftModel\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    logging,\n",
        "    set_seed\n",
        ")\n",
        "from finetuning_datasets import ConstantLengthDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MmN5qhHhUA8I"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Sequence Length: 2048\n",
            "Effective Sequence Length: 2048\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found cached dataset parquet (C:/Users/Edin/.cache/huggingface/datasets/ammarnasr___parquet/ammarnasr--bigcode-the-stack-dedup-java-small-subset-21491941b0298a53/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b7bf73967d8d428c8a0d1f22b0957463",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Length: 806789\n",
            "New Length: 790789\n",
            "Precentage of data used: 0.9801682967913544\n",
            "Train Dataset Length: 790789\n",
            "Valid Dataset Length: 897\n"
          ]
        }
      ],
      "source": [
        "model_id = \"Salesforce/codegen-350M-mono\"\n",
        "tokenizer_id = \"Salesforce/codegen-350M-mono\"\n",
        "dataset_id = \"ammarnasr/bigcode-the-stack-dedup-java-small-subset\"\n",
        "using_checkpoint = False\n",
        "checkpoint_number = 1000\n",
        "checkpoint_batch_size = 16\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True, use_cache=False)\n",
        "model_seq_length = model.config.max_position_embeddings\n",
        "effective_seq_length = model_seq_length//1\n",
        "print(f\"Model Sequence Length: {model_seq_length}\")\n",
        "print(f\"Effective Sequence Length: {effective_seq_length}\")\n",
        "dataset = load_dataset(dataset_id)\n",
        "train_ds = dataset[\"train\"]\n",
        "valid_ds = dataset[\"valid\"]\n",
        "#use only the first 100 validation examples\n",
        "valid_ds = valid_ds.select(list(range(100)))\n",
        "\n",
        "#if using_checkpoint, skip the first checkpoint_number batches from the training dataset\n",
        "if using_checkpoint:\n",
        "    orginal_length = len(train_ds)\n",
        "    indices_to_keep = list(range(checkpoint_number*checkpoint_batch_size, orginal_length))\n",
        "    train_ds = train_ds.select(indices_to_keep)\n",
        "    print(f\"Original Length: {orginal_length}\")\n",
        "    print(f\"New Length: {len(train_ds)}\")\n",
        "    print(f\"Precentage of data used: {len(train_ds)/orginal_length}\")\n",
        "\n",
        "\n",
        "train_dataset = ConstantLengthDataset(tokenizer, train_ds, infinite=True, seq_length=effective_seq_length)\n",
        "valid_dataset = ConstantLengthDataset(tokenizer, valid_ds, infinite=False, seq_length=effective_seq_length)\n",
        "print(f\"Train Dataset Length: {len(train_ds)}\")\n",
        "print(f\"Valid Dataset Length: {len(valid_ds)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-lIezhFwLPh"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True, use_cache=False)\n",
        "lora_rank = 64\n",
        "lora_config = LoraConfig(\n",
        "    r = lora_rank,\n",
        "    lora_alpha=lora_rank*2,\n",
        "    lora_dropout= 0.05,\n",
        "    bias=\"all\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules = [\"qkv_proj\", \"out_proj\", \"lm_head\", \"fc_in\", \"fc_out\"]\n",
        ")\n",
        "model.enable_input_require_grads()\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Owl82a54vSO"
      },
      "outputs": [],
      "source": [
        "#  %reload_ext tensorboard\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir codegne-finetuned-LoRa-the-stack-java-v4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjObKxJWwLMA"
      },
      "outputs": [],
      "source": [
        "training_args_dict = {}\n",
        "#Add Default Args\n",
        "training_args_dict.update({\n",
        "        \"output_dir\": \"codegne-finetuned-LoRa-the-stack-java-v4\",\n",
        "        \"run_name\": \"run-1-LoRa-v4\",\n",
        "        \"dataloader_drop_last\": True,\n",
        "        \"max_steps\": 1000,\n",
        "        \"eval_steps\": 50,\n",
        "        \"save_steps\": 100,\n",
        "        \"evaluation_strategy\": \"steps\",\n",
        "        \"logging_steps\": 1,\n",
        "        # \"push_to_hub\": True\n",
        "})\n",
        "#Add Optimizer Args\n",
        "training_args_dict.update({\n",
        "        # \"optim\": \"adafactor\",\n",
        "        \"learning_rate\": 5e-5,\n",
        "        \"warmup_steps\": 100,\n",
        "        \"lr_scheduler_type\": \"cosine\",\n",
        "        # \"weight_decay\": 0.05,\n",
        "})\n",
        "# Add Mempry and Speed Args\n",
        "training_args_dict.update({\n",
        "        \"gradient_checkpointing\": True,\n",
        "        # \"gradient_accumulation_steps\": 2,\n",
        "        \"per_device_train_batch_size\": 32,\n",
        "        \"per_device_eval_batch_size\": 32,\n",
        "        \"fp16\": True,\n",
        "})\n",
        "training_args = TrainingArguments(**training_args_dict)\n",
        "\n",
        "print('============Default Training Args============')\n",
        "print(f'Output Dir: {training_args.output_dir}')\n",
        "print(f'Run Name: {training_args.run_name}')\n",
        "print(f'Dataloader Drop Last: {training_args.dataloader_drop_last}')\n",
        "print(f'Max Steps: {training_args.max_steps}')\n",
        "print(f'Eval Steps: {training_args.eval_steps}')\n",
        "print(f'Save Steps: {training_args.save_steps}')\n",
        "print(f'Evaluation Strategy: {training_args.evaluation_strategy}')\n",
        "print(f'Logging Steps: {training_args.logging_steps}')\n",
        "print(f'Push To Hub: {training_args.push_to_hub}')\n",
        "\n",
        "print('============Optimizer Training Args============')\n",
        "print(f'Optimizer: {training_args.optim}')\n",
        "print(f'Learning Rate: {training_args.learning_rate}')\n",
        "print(f'Warmup Steps: {training_args.warmup_steps}')\n",
        "print(f'LR Scheduler Type: {training_args.lr_scheduler_type}')\n",
        "print(f'Weight Decay: {training_args.weight_decay}')\n",
        "\n",
        "print('============Memory and Speed Training Args============')\n",
        "print(f'Gradient Checkpointing: {training_args.gradient_checkpointing}')\n",
        "print(f'Gradient Accumulation Steps: {training_args.gradient_accumulation_steps}')\n",
        "print(f'Per Device Train Batch Size: {training_args.per_device_train_batch_size}')\n",
        "print(f'Per Device Eval Batch Size: {training_args.per_device_eval_batch_size}')\n",
        "print(f'FP16: {training_args.fp16}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7auA_RBwLJX"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(model, training_args, train_dataset=train_dataset, eval_dataset=valid_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C19OvVLOwLG1"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrjT_VWmgELx"
      },
      "outputs": [],
      "source": [
        "from peft import PeftConfig, PeftModel\n",
        "\n",
        "checkpoint_number = 100\n",
        "repo_name = f'./codegne-finetuned-LoRa-the-stack-java-v3/checkpoint-{checkpoint_number}'\n",
        "config = PeftConfig.from_pretrained(repo_name)\n",
        "lora_rank = 64\n",
        "config.r = lora_rank\n",
        "config.lora_alpha=lora_rank*2\n",
        "config.lora_dropout= 0.05\n",
        "config.bias=\"none\"\n",
        "config.task_type=\"CAUSAL_LM\"\n",
        "\n",
        "\n",
        "\n",
        "config.target_modules = [\"qkv_proj\"]\n",
        "ckpt =  AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, trust_remote_code=True, use_cache=False)\n",
        "ckpt.enable_input_require_grads()\n",
        "ckpt = PeftModel.from_pretrained(ckpt, repo_name)\n",
        "for name, param in ckpt.named_parameters():\n",
        "  if 'lora' in name:\n",
        "    param.requires_grad = True\n",
        "ckpt.print_trainable_parameters()\n",
        "\n",
        "training_args.run_name = f'run-2-LoRa-v3-checkpoint-{checkpoint_number}'\n",
        "training_args.warmup_steps = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkitH4lZeOHx"
      },
      "outputs": [],
      "source": [
        "trainer_ckpt = Trainer(ckpt, training_args, train_dataset=train_dataset, eval_dataset=valid_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5MHrCRByfXs"
      },
      "outputs": [],
      "source": [
        "trainer_ckpt.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6TRUh0BeOFC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3e4eg2xeOCY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQN0UiwYeN_n"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNtHr1HExqrN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOvt0AnOmefx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gkuxp-W7Bv7x"
      },
      "source": [
        "#### Push to hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idhqalh4BpYt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/LLM-for-code-intelligence-Project/LLM-for-code-intelligence/Finetuning')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQ4fzVwTBpVJ"
      },
      "outputs": [],
      "source": [
        "from peft import PeftConfig, PeftModel\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "repo_name = './codegne-finetuned-LoRa-the-stack-java-v3/checkpoint-800'\n",
        "config = PeftConfig.from_pretrained(repo_name)\n",
        "ckpt =  AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, trust_remote_code=True, use_cache=False)\n",
        "ckpt.enable_input_require_grads()\n",
        "ckpt = PeftModel.from_pretrained(ckpt, repo_name)\n",
        "for name, param in ckpt.named_parameters():\n",
        "  if 'lora' in name:\n",
        "    param.requires_grad = True\n",
        "ckpt.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kratjcM3BpR-"
      },
      "outputs": [],
      "source": [
        "repo_name = 'codegne-finetuned-LoRa-the-stack-java-v3-checkpoint-800'\n",
        "ckpt.push_to_hub(repo_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLwezuTfDKIW"
      },
      "outputs": [],
      "source": [
        "#  %reload_ext tensorboard\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir codegne-finetuned-LoRa-the-stack-java-v2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZxyK0g9Di2H"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
