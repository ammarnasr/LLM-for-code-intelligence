{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjujWVI7TMIV"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRI1zO80TUkB"
      },
      "outputs": [],
      "source": [
        "!chmod +x setup.sh\n",
        "!./setup.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGN9suZUTX92"
      },
      "outputs": [],
      "source": [
        "project_name = 'LLM-for-code-intelligence-Project'\n",
        "import os\n",
        "\n",
        "path= f'/content/drive/MyDrive/{project_name}'\n",
        "\n",
        "if not os.path.exists(path):\n",
        "  os.mkdir(path)\n",
        "\n",
        "os.chdir(path)\n",
        "\n",
        "repo_name = 'LLM-for-code-intelligence'\n",
        "repo_path = f'{path}/{repo_name}'\n",
        "url = f'https://github.com/ammarnasr/{repo_name}.git'\n",
        "\n",
        "\n",
        "\n",
        "if not os.path.exists(repo_path):\n",
        "    #clone the repo\n",
        "    print('Cloning the repo...')\n",
        "    !git clone $url\n",
        "else:\n",
        "    #pull the repo\n",
        "    print('Pulling the repo...')\n",
        "    !git -C $repo_name pull\n",
        "\n",
        "os.chdir(repo_path)\n",
        "\n",
        "print(f'Current Dir: {os.getcwd()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNF9Wwg2TYuv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# os.chdir('/content/drive/MyDrive/LLM-for-code-intelligence-Project/LLM-for-code-intelligence/Finetuning')\n",
        "import torch\n",
        "from dataclasses import dataclass\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import IterableDataset\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    logging,\n",
        "    set_seed\n",
        ")\n",
        "from finetuning_datasets import ConstantLengthDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9oW3CZRTzMz"
      },
      "outputs": [],
      "source": [
        "model_id = \"Salesforce/codegen-350M-mono\"\n",
        "tokenizer_id = \"Salesforce/codegen-350M-mono\"\n",
        "dataset_id = \"ammarnasr/bigcode-the-stack-dedup-java-small-subset\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True, use_cache=False)\n",
        "model_seq_length = model.config.max_position_embeddings\n",
        "effective_seq_length = model_seq_length//32\n",
        "print(f\"Model Sequence Length: {model_seq_length}\")\n",
        "print(f\"Effective Sequence Length: {effective_seq_length}\")\n",
        "dataset = load_dataset(dataset_id)\n",
        "dataset = dataset['train']\n",
        "dataset = dataset.train_test_split(test_size=0.0001, shuffle=True)\n",
        "train_ds = dataset[\"train\"]\n",
        "valid_ds = dataset[\"test\"]\n",
        "train_dataset = ConstantLengthDataset(tokenizer, train_ds, infinite=True, seq_length=effective_seq_length)\n",
        "valid_dataset = ConstantLengthDataset(tokenizer, valid_ds, infinite=False, seq_length=effective_seq_length)\n",
        "print(f\"Train Dataset Length: {len(train_ds)}\")\n",
        "print(f\"Valid Dataset Length: {len(valid_ds)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Uo3yPu4sOUI"
      },
      "outputs": [],
      "source": [
        "# %load_ext tensorboard\n",
        "# %tensorboard --logdir codegne-finetuned-the-stack-java-v2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bGBPwhpU0fk"
      },
      "outputs": [],
      "source": [
        "training_args_dict = {}\n",
        "#Add Default Args\n",
        "training_args_dict.update({\n",
        "        \"output_dir\": \"codegne-finetuned-the-stack-java-v3\",\n",
        "        \"run_name\": \"run-1-full-v3\",\n",
        "        \"dataloader_drop_last\": True,\n",
        "        \"max_steps\": 500000,\n",
        "        \"eval_steps\": 50,\n",
        "        \"save_steps\": 100,\n",
        "        \"evaluation_strategy\": \"steps\",\n",
        "        \"logging_steps\": 1,\n",
        "        # \"push_to_hub\": True\n",
        "})\n",
        "#Add Optimizer Args\n",
        "training_args_dict.update({\n",
        "        # \"optim\": \"adafactor\",\n",
        "        \"learning_rate\": 5e-5,\n",
        "        \"warmup_steps\": 10,\n",
        "        \"lr_scheduler_type\": \"cosine\",\n",
        "        \"weight_decay\": 0.05,\n",
        "})\n",
        "# Add Mempry and Speed Args\n",
        "training_args_dict.update({\n",
        "        \"gradient_checkpointing\": True,\n",
        "        # \"gradient_accumulation_steps\": 1,\n",
        "        \"per_device_train_batch_size\": 1,\n",
        "        \"per_device_eval_batch_size\": 1,\n",
        "        \"fp16\": True,\n",
        "})\n",
        "training_args = TrainingArguments(**training_args_dict)\n",
        "\n",
        "print('============Default Training Args============')\n",
        "print(f'Output Dir: {training_args.output_dir}')\n",
        "print(f'Dataloader Drop Last: {training_args.dataloader_drop_last}')\n",
        "print(f'Max Steps: {training_args.max_steps}')\n",
        "print(f'Eval Steps: {training_args.eval_steps}')\n",
        "print(f'Save Steps: {training_args.save_steps}')\n",
        "print(f'Evaluation Strategy: {training_args.evaluation_strategy}')\n",
        "print(f'Logging Steps: {training_args.logging_steps}')\n",
        "print(f'Push To Hub: {training_args.push_to_hub}')\n",
        "\n",
        "print('============Optimizer Training Args============')\n",
        "print(f'Optimizer: {training_args.optim}')\n",
        "print(f'Learning Rate: {training_args.learning_rate}')\n",
        "print(f'Warmup Steps: {training_args.warmup_steps}')\n",
        "print(f'LR Scheduler Type: {training_args.lr_scheduler_type}')\n",
        "print(f'Weight Decay: {training_args.weight_decay}')\n",
        "\n",
        "print('============Memory and Speed Training Args============')\n",
        "print(f'Gradient Checkpointing: {training_args.gradient_checkpointing}')\n",
        "print(f'Gradient Accumulation Steps: {training_args.gradient_accumulation_steps}')\n",
        "print(f'Per Device Train Batch Size: {training_args.per_device_train_batch_size}')\n",
        "print(f'Per Device Eval Batch Size: {training_args.per_device_eval_batch_size}')\n",
        "print(f'FP16: {training_args.fp16}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqpOEwOKr8Fa"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(model, training_args, train_dataset=train_dataset, eval_dataset=valid_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XabuVJVcEdus"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bm8mC2wq4kxM"
      },
      "outputs": [],
      "source": [
        "trainer.train(resume_from_checkpoint=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kDthPnU4kuL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcnHM0Dm-MmE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdW-CP3zxQeh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooXYwRR9gVoc"
      },
      "source": [
        "#### Push to hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIijmbz7xQbE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/LLM-for-code-intelligence-Project/LLM-for-code-intelligence/Finetuning')\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_id = 'Salesforce/codegen-350M-mono'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "main_dir = 'codegne-finetuned-the-stack-java-v2'\n",
        "checkpoint = 'checkpoint-200'\n",
        "ckpt_path = f'{main_dir}/{checkpoint}'\n",
        "repo_path = f'{main_dir}-{checkpoint}'\n",
        "model = AutoModelForCausalLM.from_pretrained(ckpt_path, trust_remote_code=True, use_cache=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5MHrCRByfXs"
      },
      "outputs": [],
      "source": [
        "checkpoint = 'checkpoint-800'\n",
        "repo_path = f'{main_dir}-{checkpoint}'\n",
        "model.push_to_hub(repo_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OONPXiZyi19Y"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
