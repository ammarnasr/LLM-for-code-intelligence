{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# from accelerate import Accelerator\n",
    "from finetuning_datasets import create_datasets\n",
    "from peft import LoraConfig, get_peft_model#, prepare_model_for_int8_training\n",
    "from peft import PeftConfig, PeftModel\n",
    "from finetuning_utils import get_args, SavePeftModelCallback, LoadBestPeftModelCallback\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, logging, set_seed\n",
    "\n",
    "def run_training_from_checkpoint(args, train_dataset, eval_dataset):\n",
    "    checkpoint_parent_dir = 'checkpoints'\n",
    "    all_checkpoints = os.listdir(checkpoint_parent_dir)\n",
    "    all_checkpoints = [x for x in all_checkpoints if x.startswith('checkpoint-')]\n",
    "    print(all_checkpoints)\n",
    "    all_checkpoints_steps = [int(x.split('-')[1]) for x in all_checkpoints]\n",
    "    index_of_max = all_checkpoints_steps.index(max(all_checkpoints_steps))\n",
    "    checkpoint_dir = os.path.join(checkpoint_parent_dir, all_checkpoints[index_of_max])\n",
    "    print(\"Loading the model from checkpoint: \", checkpoint_dir)\n",
    "    config = PeftConfig.from_pretrained(checkpoint_dir)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        config.base_model_name_or_path,\n",
    "        use_auth_token=True,\n",
    "        # use_cache=not args.no_gradient_checkpointing,\n",
    "        # load_in_8bit=True,\n",
    "        # device_map={\"\": Accelerator().process_index},\n",
    "    )\n",
    "    # Load the LoRA model\n",
    "    model = PeftModel.from_pretrained(model, checkpoint_dir)\n",
    "    return model\n",
    "\n",
    "args = None\n",
    "train_dataset = None\n",
    "eval_dataset = None\n",
    "\n",
    "model = run_training_from_checkpoint(args, train_dataset, eval_dataset)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c597968d291641a59ea450049c4e7004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.bin:   0%|          | 0.00/5.26M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44c25ed64266498390a60b03445bbe5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/ammarnasr/codegen-350M-mono_the-stack-dedup_java_train_peft/commit/07d1bbe10967f69997b73aa79a9cc4b6ddc691fb', commit_message='Upload model', commit_description='', oid='07d1bbe10967f69997b73aa79a9cc4b6ddc691fb', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_name = 'codegen-350M-mono_the-stack-dedup_java_train_peft'\n",
    "# push_to_hub(model, repo_name, use_auth_token=True)\n",
    "model.push_to_hub(repo_name, use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find folders in current directory that start with \"checkpoint-\"\n",
    "import os\n",
    "import glob\n",
    "\n",
    "\n",
    "\n",
    "os.chdir(\"C:/Users/rober/OneDrive/Documents/GitHub/CodeBERT/Finetuning\")\n",
    "\n",
    "for file in glob.glob(\"checkpoint-*\"):\n",
    "    print(file)\n",
    "\n",
    "#delete all folders in current directory that start with \"checkpoint-\" except for ones that are devideable by 50\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "os.chdir(\"C:/Users/rober/OneDrive/Documents/GitHub/CodeBERT/Finetuning\")\n",
    "\n",
    "for file in glob.glob(\"checkpoint-*\"):\n",
    "    if file != \"checkpoint-1000\":\n",
    "        shutil.rmtree(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, logging, set_seed\n",
    "\n",
    "model_path = 'Salesforce/codegen-350M-mono'\n",
    "save_path = \"ammarnasr/codegen-350M-mono_the-stack-dedup_java_train_peft\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_auth_token=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/ammarnasr/codegen-350M-mono_the-stack-dedup_java_train_peft/commit/9cdfaf2f7d4848a7c52804401a110350c592c283', commit_message='Upload tokenizer', commit_description='', oid='9cdfaf2f7d4848a7c52804401a110350c592c283', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Push tokenizer to hub\n",
    "tokenizer.push_to_hub(save_path, use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amazon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
