{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import IterableDataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    logging,\n",
    "    set_seed,\n",
    ")\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--model_path\", type=str, default=\"bigcode/santacoder\")\n",
    "    parser.add_argument(\"--dataset_name\", type=str, default=\"bigcode/the-stack-dedup\")\n",
    "    parser.add_argument(\"--subset\", type=str, default=\"data\")\n",
    "    parser.add_argument(\"--split\", type=str, default=\"train\")\n",
    "    parser.add_argument(\"--size_valid_set\", type=int, default=4000)\n",
    "    parser.add_argument(\"--streaming\", action=\"store_true\")\n",
    "    parser.add_argument(\"--shuffle_buffer\", type=int, default=5000)\n",
    "    parser.add_argument(\"--data_column\", type=str, default=\"content\")\n",
    "\n",
    "    parser.add_argument(\"--seq_length\", type=int, default=1024)\n",
    "    parser.add_argument(\"--max_steps\", type=int, default=10000)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=2)\n",
    "    parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=8)\n",
    "    parser.add_argument(\"--eos_token_id\", type=int, default=49152)\n",
    "\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=5e-5)\n",
    "    parser.add_argument(\"--lr_scheduler_type\", type=str, default=\"cosine\")\n",
    "    parser.add_argument(\"--num_warmup_steps\", type=int, default=100)\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=0.05)\n",
    "\n",
    "    parser.add_argument(\"--local_rank\", type=int, default=0)\n",
    "    parser.add_argument(\"--no_fp16\", action=\"store_false\")\n",
    "    parser.add_argument(\"--bf16\", action=\"store_true\")\n",
    "    parser.add_argument(\"--no_gradient_checkpointing\", action=\"store_false\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=0)\n",
    "    parser.add_argument(\"--num_workers\", type=int, default=None)\n",
    "    parser.add_argument(\"--output_dir\", type=str, default=\"./checkpoints\")\n",
    "    parser.add_argument(\"--log_freq\", default=1, type=int)\n",
    "    parser.add_argument(\"--eval_freq\", default=1000, type=int)\n",
    "    parser.add_argument(\"--save_freq\", default=1000, type=int)\n",
    "\n",
    "    parser.add_argument(\"--fim_rate\", type=float, default=0)\n",
    "    parser.add_argument(\"--fim_spm_rate\", type=float, default=0)\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def chars_token_ratio(dataset, tokenizer, data_column, nb_examples=400):\n",
    "    \"\"\"\n",
    "    Estimate the average number of characters per token in the dataset.\n",
    "    \"\"\"\n",
    "    total_characters, total_tokens = 0, 0\n",
    "    for _, example in tqdm(zip(range(nb_examples), iter(dataset)), total=nb_examples):\n",
    "        total_characters += len(example[data_column])\n",
    "        total_tokens += len(tokenizer(example[data_column]).tokens())\n",
    "\n",
    "    return total_characters / total_tokens\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    model_path = \"bigcode/santacoder\"\n",
    "    dataset_name = \"bigcode/the-stack-dedup\"\n",
    "    subset = \"data\"\n",
    "    split = \"train\"\n",
    "    size_valid_set = 4000\n",
    "    streaming = False\n",
    "    shuffle_buffer = 5000\n",
    "    data_column = \"content\"\n",
    "    seq_length = 1024\n",
    "    max_steps = 10000\n",
    "    batch_size = 2\n",
    "    gradient_accumulation_steps = 8\n",
    "    eos_token_id = 49152\n",
    "    learning_rate = 5e-5\n",
    "    lr_scheduler_type = \"cosine\"\n",
    "    num_warmup_steps = 100\n",
    "    weight_decay = 0.05\n",
    "    local_rank = 0\n",
    "    no_fp16 = False\n",
    "    bf16 = False\n",
    "    no_gradient_checkpointing = False\n",
    "    seed = 0\n",
    "    num_workers = None\n",
    "    output_dir = \"./checkpoints\"\n",
    "    log_freq = 1\n",
    "    eval_freq = 1000\n",
    "    save_freq = 1000\n",
    "    fim_rate = 0\n",
    "    fim_spm_rate = 0\n",
    "    \n",
    "\n",
    "args=Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ConstantLengthDataset(IterableDataset):\n",
    "    \"\"\"\n",
    "    Iterable dataset that returns constant length chunks of tokens from stream of text files.\n",
    "        Args:\n",
    "            tokenizer (Tokenizer): The processor used for proccessing the data.\n",
    "            dataset (dataset.Dataset): Dataset with text files.\n",
    "            infinite (bool): If True the iterator is reset after dataset reaches end else stops.\n",
    "            seq_length (int): Length of token sequences to return.\n",
    "            num_of_sequences (int): Number of token sequences to keep in buffer.\n",
    "            chars_per_token (int): Number of characters per token used to estimate number of tokens in text buffer.\n",
    "            fim_rate (float): Rate (0.0 to 1.0) that sample will be permuted with FIM.\n",
    "            fim_spm_rate (float): Rate (0.0 to 1.0) of FIM permuations that will use SPM.\n",
    "            seed (int): Seed for random number generator.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        dataset,\n",
    "        infinite=False,\n",
    "        seq_length=1024,\n",
    "        num_of_sequences=1024,\n",
    "        chars_per_token=3.6,\n",
    "        content_field=\"content\",\n",
    "        fim_rate=0.5,\n",
    "        fim_spm_rate=0.5,\n",
    "        seed=0,\n",
    "    ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.concat_token_id = (\n",
    "            tokenizer.eos_token_id if tokenizer.eos_token_id else args.eos_token_id\n",
    "        )\n",
    "        self.dataset = dataset\n",
    "        self.seq_length = seq_length\n",
    "        self.infinite = infinite\n",
    "        self.current_size = 0\n",
    "        self.max_buffer_size = seq_length * chars_per_token * num_of_sequences\n",
    "        self.content_field = content_field\n",
    "        self.fim_rate = fim_rate\n",
    "        self.fim_spm_rate = fim_spm_rate\n",
    "        self.seed = seed\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        iterator = iter(self.dataset)\n",
    "        more_examples = True\n",
    "        while more_examples:\n",
    "            buffer, buffer_len = [], 0\n",
    "            while True:\n",
    "                if buffer_len >= self.max_buffer_size:\n",
    "                    break\n",
    "                try:\n",
    "                    buffer.append(next(iterator)[self.content_field])\n",
    "                    buffer_len += len(buffer[-1])\n",
    "                except StopIteration:\n",
    "                    if self.infinite:\n",
    "                        iterator = iter(self.dataset)\n",
    "                    else:\n",
    "                        more_examples = False\n",
    "                        break\n",
    "            tokenized_inputs = self.tokenizer(buffer, truncation=False)[\"input_ids\"]\n",
    "            all_token_ids = []\n",
    "\n",
    "            np_rng = np.random.RandomState(seed=self.seed)\n",
    "            for tokenized_input in tokenized_inputs:\n",
    "                all_token_ids.extend(tokenized_input + [self.concat_token_id])\n",
    "            examples = []\n",
    "            for i in range(0, len(all_token_ids), self.seq_length):\n",
    "                input_ids = all_token_ids[i : i + self.seq_length]\n",
    "                if len(input_ids) == self.seq_length:\n",
    "                    examples.append(input_ids)\n",
    "            random.shuffle(examples)\n",
    "            for example in examples:\n",
    "                self.current_size += 1\n",
    "                yield {\n",
    "                        \"input_ids\": torch.LongTensor(example),\n",
    "                        \"labels\": torch.LongTensor(example),\n",
    "                    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_datasets(tokenizer, args):\n",
    "    dataset = load_dataset(\n",
    "        args.dataset_name,\n",
    "        data_dir=args.subset,\n",
    "        split=args.split,\n",
    "        use_auth_token=True,\n",
    "        num_proc=args.num_workers if not args.streaming else None,\n",
    "        streaming=args.streaming,\n",
    "    )\n",
    "    if args.streaming:\n",
    "        print(\"Loading the dataset in streaming mode\")\n",
    "        valid_data = dataset.take(args.size_valid_set)\n",
    "        train_data = dataset.skip(args.size_valid_set)\n",
    "        train_data = train_data.shuffle(buffer_size=args.shuffle_buffer, seed=args.seed)\n",
    "    else:\n",
    "        dataset = dataset.train_test_split(test_size=0.005, seed=args.seed)\n",
    "        train_data = dataset[\"train\"]\n",
    "        valid_data = dataset[\"test\"]\n",
    "        print(\n",
    "            f\"Size of the train set: {len(train_data)}. Size of the validation set: {len(valid_data)}\"\n",
    "        )\n",
    "    chars_per_token = chars_token_ratio(train_data, tokenizer, args.data_column)\n",
    "    print(f\"The character to token ratio of the dataset is: {chars_per_token:.2f}\")\n",
    "    train_dataset = ConstantLengthDataset(\n",
    "        tokenizer,\n",
    "        train_data,\n",
    "        infinite=True,\n",
    "        seq_length=args.seq_length,\n",
    "        chars_per_token=chars_per_token,\n",
    "        content_field=args.data_column,\n",
    "        fim_rate=args.fim_rate,\n",
    "        fim_spm_rate=args.fim_spm_rate,\n",
    "        seed=args.seed,\n",
    "    )\n",
    "    valid_dataset = ConstantLengthDataset(\n",
    "        tokenizer,\n",
    "        valid_data,\n",
    "        infinite=False,\n",
    "        seq_length=args.seq_length,\n",
    "        chars_per_token=chars_per_token,\n",
    "        content_field=args.data_column,\n",
    "        fim_rate=args.fim_rate,\n",
    "        fim_spm_rate=args.fim_spm_rate,\n",
    "        seed=args.seed,\n",
    "    )\n",
    "\n",
    "    return train_dataset, valid_dataset\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(args, train_data, val_data):\n",
    "    print(\"Loading the model\")\n",
    "    # disable caching mechanism when using gradient checkpointing\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.model_path,\n",
    "        trust_remote_code=True,\n",
    "        use_cache=not args.no_gradient_checkpointing,\n",
    "    )\n",
    "    train_data.start_iteration = 0\n",
    "\n",
    "    print(f\"Starting main loop\")\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=args.output_dir,\n",
    "        dataloader_drop_last=True,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        max_steps=args.max_steps,\n",
    "        eval_steps=args.eval_freq,\n",
    "        save_steps=args.save_freq,\n",
    "        logging_steps=args.log_freq,\n",
    "        per_device_train_batch_size=args.batch_size,\n",
    "        per_device_eval_batch_size=args.batch_size,\n",
    "        learning_rate=args.learning_rate,\n",
    "        lr_scheduler_type=args.lr_scheduler_type,\n",
    "        warmup_steps=args.num_warmup_steps,\n",
    "        gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "        gradient_checkpointing=args.no_gradient_checkpointing,\n",
    "        fp16=args.no_fp16,\n",
    "        bf16=args.bf16,\n",
    "        weight_decay=args.weight_decay,\n",
    "        run_name=f\"santacoder-{args.subset}\",\n",
    "        report_to=\"wandb\",\n",
    "        push_to_hub=True,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model, args=training_args, train_dataset=train_data, eval_dataset=val_data\n",
    "    )\n",
    "\n",
    "    print(\"Training...\")\n",
    "    trainer.train()\n",
    "\n",
    "    print(\"Saving last checkpoint of the model\")\n",
    "    model.save_pretrained(os.path.join(args.output_dir, \"final_checkpoint/\"))\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_path, use_auth_token=True)\n",
    "\n",
    "    train_dataset, eval_dataset = create_datasets(tokenizer, args)\n",
    "\n",
    "    run_training(args, train_dataset, eval_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = get_args()\n",
    "    set_seed(args.seed)\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "    logging.set_verbosity_error()\n",
    "\n",
    "    main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amazon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
