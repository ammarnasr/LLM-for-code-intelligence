# LLM-for-code-intelligence
large language models for code intelligence - Msc Project



## Fine-tuning CodeGen on CodexGLUE

Colab: <a href="https://colab.research.google.com/drive/1B_cfCkliI-UuNemmgMmXqxgjnDLslAq1?usp=sharing"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=20></a>  

Huggingface: <a href="https://huggingface.co/ammarnasr/codegen-350M-multi-ft-on-code-x-glue-tc"><img src="https://img.shields.io/badge/huggingface-Model-blue"></a>

Weights & Biases: <a href="https://wandb.ai/ammarnasr/codegen_ft_on_codexglue?workspace=user-ammarnasr"><img src="https://img.shields.io/badge/wandb-Report-blue"></a>


## Evaluation on HumanEval

### CodeGen with 350M parameters - No fine-tuning

Colab: <a href="https://colab.research.google.com/drive/17cnKw8n2ELFLdu9i7zqi5mb99QdZdaax?usp=sharing"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=20></a>

<!-- TODO: add Hugging Face and WandB -->

Huggingface: TODO

Weights & Biases: TODO

### CodeGen with 350M parameters - Fine-tuned on CodexGLUE
<!-- TODO: add Colab, HuggingFace and WandB -->

Colab: TODO

Huggingface: TODO

Weights & Biases: TODO